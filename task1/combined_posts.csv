id,source,author,timestamp,text,subreddit,likes,url,platform_id,query,language,retweets
r_1ng2aiw,reddit,bci-hacker,2025-09-13T16:45:10+00:00,"[D] RL interviews at frontier labs, any tips?
Iâ€™m recently starting to see top AI labs ask RL questions.

Itâ€™s been a while since I studied RL, and was wondering if anyone had any good guide/resources on the topic.

Was thinking of mainly familiarizing myself with policy gradient techniques like SAC, PPO - implement on Cartpole and spacecraft. And modern applications to LLMs with DPO and GRPO.

Iâ€™m afraid I donâ€™t know too much about the intersection of LLM with RL. 

Anything else worth recommending to study?",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1ng2aiw/d_rl_interviews_at_frontier_labs_any_tips/,r_1ng2aiw,,,
r_1nfrpvz,reddit,viciousA3gis,2025-09-13T07:55:46+00:00,"[R] New ""Illusion"" Paper Just Dropped For Long Horizon Agents
Hi all, we recently released our new work on Long Horizon Execution. If you have seen the METR plot, and-like us-have been unconvinced by it, we think you will really like our work!

Paper link: [https://www.alphaxiv.org/abs/2509.09677](https://www.alphaxiv.org/abs/2509.09677)

X/Twitter thread: [https://x.com/ShashwatGoel7/status/1966527903568637972](https://x.com/ShashwatGoel7/status/1966527903568637972)

We show some really interesting results. The highlight? The notion that AI progress is ""slowing down"" is an Illusion. Test-time scaling is showing incredible benefits, especially for long horizon autonomous agents. We hope our work sparks more curiosity in studying these agents through simple tasks like ours!! I would love to answer any questions and engage in discussion

https://preview.redd.it/078xuqwq1wof1.png?width=1167&format=png&auto=webp&s=f28b566705348035ca39cad8fdf3762cedd569ba

  
",MachineLearning,29,https://www.reddit.com/r/MachineLearning/comments/1nfrpvz/r_new_illusion_paper_just_dropped_for_long/,r_1nfrpvz,,,
r_1nfpusc,reddit,sherlock_er,2025-09-13T06:01:43+00:00,"[P] Training an ML model to detect fake product reviews
Working on a side project to help people make better purchasing decisions online. One major component is detecting fake reviews, which turned out to be much harder than expected.

**The Approach:** Started with labeled dataset of verified fake reviews from FakeSpot research. Training ensemble model combining:

- Linguistic features (sentiment, readability, vocabulary richness)
- Temporal patterns (review timing, account age, posting frequency)
- Semantic analysis (topic consistency, specificity of complaints/praise)

**Initial Results:**

- 78% accuracy on test set
- High precision on obvious bot reviews (0.91)
- Struggles with sophisticated fakes that mimic real review patterns

**Interesting Discoveries:**

**Fake Review Patterns:**

- Excessive use of product name in review text
- Generic praise without specific use cases
- Perfect grammar (real users make typos)
- Reviews clustered around same timestamps

**Real Review Indicators:**

- Specific complaints about minor issues
- Mentions of use context (""bought for my college dorm"")
- Photos that show actual usage wear
- Mixed sentiment (likes some aspects, dislikes others)

**Current Challenges:**

- Regional language differences affect detection
- Incentivized reviews blur line between real/fake
- Sophisticated fake reviewers are learning to mimic real patterns

I've integrated this into Yaw AI (chrome extension I'm building) but still need significant improvement before it's reliable enough for general use. Sometimes flags legitimate reviews as suspicious and occasionally misses obvious fakes.

**Next Steps:**

- Expand training data with international reviews
- Implement active learning to improve edge cases
- Add verification scoring instead of binary classification

Anyone working on similar problems? Would love to compare approaches or collaborate on training data.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nfpusc/p_training_an_ml_model_to_detect_fake_product/,r_1nfpusc,,,
r_1nfpgoh,reddit,syntex_autonomous,2025-09-13T05:38:52+00:00,"[R] A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI
**Disclosure:**

I needed help with AI to write this as a proper ""research paper"". My unmedicated ADHD is both a boon and a curse. My superpower is that I see patterns and am often connecting things so rapidly in my mind that people have a hard time following. - And I'm not a researcher, I'm a dude that likes science - something else my hyper focus has helped.

I organized all my notes and chicken scratch and questions and began looking into anyone else that thought of these. After I sorted everything I put it into Gemini Research for this output.

[A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI](https://docs.google.com/document/d/1Z6h8LMPPpbvTdgiMtZ7IRM8ZWmT55KVeOq5bPjlEAb4/edit?tab=t.0)

**Some Background:**

This prior Tuesday I met with Professor Mandeep Gill, an astrophysics professor and researcher at the University of Minnesota regarding an autonomous engine I built. This is a self-attacking autonomous red teaming system that operates under what I called ""Controlled Entropy"".

After my meeting with Professor Gill, I was invited to take a Graduate level Supernovae class and I began thinking of new ways to use concepts from the class in cybersecurity and AI development

Later ... as I was falling asleep I began dreaming in graphs. I started putting each graph on top of each other and I realized that so many of the concepts I've learned across the years of watching YouTube videos or learning about some new theory, and suddenly everything seemed like it all lined up.

This led me down a rabbit hole:

[Universality](https://en.wikipedia.org/wiki/Universality)

[Shannon Entropy (Information Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))

I'm working out a way to build this into my autonomous red teaming engine - if the theory is correct, we will be able to generate a novel threat vector that crosses categories of attacks: hardware vectors + IoT + ransomeware, etc...

1. Our 100% autonomous cybersecurity suite will not only be able to match current known and unknown threats,
2. We can use a brand new, multi-category attack against our own system the pattern recognition would evolve infinitely.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nfpgoh/r_a_framework_for_entropic_generative_systems/,r_1nfpgoh,,,
r_1nfmh43,reddit,AgeOfEmpires4AOE4,2025-09-13T02:53:42+00:00,"[P] Env for Reinforcement Learning with Game Cube/Wii Games!!!!
https://preview.redd.it/l71h1i1njuof1.png?width=2670&format=png&auto=webp&s=a1bdd20917e5244a0e0eb764e862348d2b08ce35

  
I achieved another feat today!!! In my tests, Dolphin ran in my ""stable-retro"" and gym versions!!!!!

I should upload the change to the repository this week.

Don't forget to follow and give an ok to the repo:Â [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1nfmh43/p_env_for_reinforcement_learning_with_game/,r_1nfmh43,,,
r_1nfgc8h,reddit,New-Skin-5064,2025-09-12T22:04:19+00:00,"[D] OOM When Using Gradient Accumulation
I am trying to train a transformer model(1.5b parameters) on a TPU v3-8. The highest physical batch size I can get is 16 sequences of 2048 tokens. To increase my effective batch size, I have turned to gradient accumulation. My loop works at a smaller scale, but at a larger scale, it causes an OOM error. I'm using Torch XLA. Here is my code:

Optimizer creation:
```
def build_optimizer(model, peak_lr, muon_peak_lr, betas, weight_decay):
    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(""-""*100)
    print(f""Total parameters: {total_params}"")
    print(""-""*100)
    print(f""Trainable parameters: {trainable_params}"")
    print(""-""*100)
    hidden_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and not (n.endswith(""wte.weight"") or n.endswith(""lm_head.weight""))]
    # We only want adamw to apply weight decay to embeddings
    decay = [p for n, p in model.named_parameters() if p.ndim >= 2 and isinstance(n, nn.Embedding)]
    # Exclude biases(if applicable) and normalization params
    no_decay = [p for pn, p in param_dict.items() if p.dim() < 2]
    groups = [
        {""params"": decay, ""weight_decay"": weight_decay},
        {""params"": no_decay, ""weight_decay"": 0.0}
    ]
    adamw = syncfree.AdamW(groups, lr=peak_lr, betas=betas)
    muon = SingleDeviceMuon(hidden_params, lr=muon_peak_lr, momentum=betas[1], weight_decay=weight_decay)
    return adamw, muon

```

Before I start training I run this code, as it prevents an OOM on the first step:
```
for _ in range(3):
    train_loss = torch.zeros((), device=device)
    for k in range(gradient_accumulation_steps):
        x = torch.randint(0, 100256, (1, 2048)).to(device)
        xs.mark_sharding(x, mesh, (""fsdp"", None))
        y = torch.randint(0, 100256, (1, 2048)).to(device)
        xs.mark_sharding(y, mesh, (""fsdp"", None))
        with autocast(xm.xla_device(), dtype=torch.bfloat16):
            loss = model(x, y)
        (loss/gradient_accumulation_steps).backward()
        train_loss += loss.detach()
        # xm.mark_step()
    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)
    
    xm.optimizer_step(muon, barrier=True)
    xm.optimizer_step(adamw, barrier=True)
    adamw.zero_grad()
    muon.zero_grad()
```

Training loop:
```
model.train()
train_loss = torch.zeros((), device=device)
for k in range(gradient_accumulation_steps):
    x, y = next(train_iter)
    with autocast(xm.xla_device(), dtype=torch.bfloat16):
        loss = model(x, y)
    (loss / gradient_accumulation_steps).backward()
    train_loss += loss.detach()
    # xm.mark_step()

torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)

xm.optimizer_step(muon, barrier=True)
xm.optimizer_step(adamw, barrier=True)

adamw.zero_grad()
muon.zero_grad()
```

What can I do to fix this OOM?

EDIT: The OOM occurs during the first optimizer step. It does not matter if I swap the order of the optimizer steps, the OOM always occurs on the first one.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nfgc8h/d_oom_when_using_gradient_accumulation/,r_1nfgc8h,,,
r_1nfestz,reddit,LetsTacoooo,2025-09-12T21:03:04+00:00,"[R] Debunking the Claims of K2-Think
Recent work (K2-Think) claimed to have a SOTA small model: [https://arxiv.org/abs/2509.07604](https://arxiv.org/abs/2509.07604)

Three days later a dubunking post of this work was posted: [https://www.sri.inf.ethz.ch/blog/k2think](https://www.sri.inf.ethz.ch/blog/k2think)





",MachineLearning,25,https://www.reddit.com/r/MachineLearning/comments/1nfestz/r_debunking_the_claims_of_k2think/,r_1nfestz,,,
r_1nfav96,reddit,pmv143,2025-09-12T18:28:03+00:00,"[D] Larry Ellison: â€œInference is where the money is going to be made.â€
In Oracleâ€™s recent call, Larry Ellison said something that caught my attention:

â€œAll this money weâ€™re spending on training is going to be translated into products that are sold â€” which is all inferencing. Thereâ€™s a huge amount of demand for inferencingâ€¦ We think weâ€™re better positioned than anybody to take advantage of it.â€

Itâ€™s striking to see a major industry figure frame inference as the real revenue driver, not training. Feels like a shift in narrative: less about who can train the biggest model, and more about who can serve it efficiently, reliably, and at scale.

Not sure if the industry is really moving in this direction? Or will training still dominate the economics for years to come?",MachineLearning,144,https://www.reddit.com/r/MachineLearning/comments/1nfav96/d_larry_ellison_inference_is_where_the_money_is/,r_1nfav96,,,
r_1nf9noo,reddit,dmpiergiacomo,2025-09-12T17:40:28+00:00,"[D] Do you ever miss PyTorch-style workflows?
I used to contribute to PyTorch, and Iâ€™m wondering: how many of you shifted from building with PyTorch to mainly managing prompts for LLMs? Do you ever miss the old PyTorch workflow â€” datasets, metrics, training loops â€” versus the endless ""prompt -> test -> rewrite"" loop? ",MachineLearning,78,https://www.reddit.com/r/MachineLearning/comments/1nf9noo/d_do_you_ever_miss_pytorchstyle_workflows/,r_1nf9noo,,,
r_1nf9cwu,reddit,socialcalliper,2025-09-12T17:28:42+00:00,"[D] Seeking Recommendations for AutoML Libraries Compatible with Windows (Python 3.12) in 2025
Hi all,
Iâ€™m struggling to find an AutoML library that works reliably on Windows. Iâ€™ve tested Auto-sklearn, TPOT,PyCaret and Flaml, but I keep hitting issues:
â€¢  Many donâ€™t support Python 3.12.
â€¢  Some clash with NumPy or other dependencies.
â€¢  Fresh Conda environments still result in installation errors, deprecated package warnings, or runtime failures.
Has anyone successfully used an AutoML tool on Windows recently? Iâ€™d prefer ones that install smoothly and handle tabular data well, with good documentation. What are people using in 2025 that avoids these headaches? Any setup tips or alternatives would be appreciated!
Thanks!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nf9cwu/d_seeking_recommendations_for_automl_libraries/,r_1nf9cwu,,,
r_1nf36h7,reddit,TimeBag7651,2025-09-12T13:25:08+00:00,"[N] Call for Papers (CFP): DeepModAI 2025 @ ICONIP25 - International Workshop on Deep learning for Multimodal Data
We are pleased to announce **DeepModAI** 2025 (International Workshop on Deep learning for Multimodal Data), to be held on **November 24**, **2025**, in **Okinawa**, **Japan**, in conjunction with the **ICONIP 2025** conference.

This workshop aims to bring together academic researchers and industry professionals to address core challenges in deep multimodal learning. We focus on advanced deep learning techniques (e.g. unsupervised, self-supervised, weakly supervised approaches) that learn transferable latent representations across modalities, moving beyond unimodal and static paradigms. We also encourage contributions that demonstrate applications in critical domains such as multimodal document analysis, health monitoring, autonomous systems, robotics, or environmental modeling.

**Key topics include** (but are not limited to):

* Multi-view and multi-modal architecture design
* Cross-modal alignment and translation
* Attention mechanisms for dynamic modality fusion
* Diversity-aware and ensemble learning methods
* Explainable and collaborative multimodal frameworks
* Adaptability to dynamic, incomplete, or context-dependent data
* Scalable deployment and computational efficiency

**Submissions**:

We invite the submission of **extended abstracts** (2 pages) or **regular papers** (any length).Â 

Regular papers should be submitted to a preprint repository (arXiv, Jxiv, etc.) prior to workshop submission.Â 

All accepted contributions will be presented orally or as posters and published on the workshop website.

**Important Dates**:

* Submission Deadline: ***September 30***, 2025
* Workshop Date: November 24, 2025

The workshop will feature invited keynote talks, technical presentations, poster sessions, and an interactive panel discussion with international experts.

It is a perfect opportunity to present your ongoing work, receive high-quality feedback, and help shape the future directions of this dynamic research field.

**For more details on the topics, program, and submission guidelines, please visit our website**:Â 

[https://deepmodai.sciencesconf.org/](https://deepmodai.sciencesconf.org/)

We would be grateful if you could forward this call to your colleagues and relevant PhD students and postdocs.

**For any questions, please contact us at**: [deepmodai@sciencesconf.org](mailto:deepmodai@sciencesconf.org)

We look forward to seeing you in Okinawa!

Sincerely,

**The DeepModAI 2025 Organizing Committee**",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nf36h7/n_call_for_papers_cfp_deepmodai_2025_iconip25/,r_1nf36h7,,,
r_1nf03e6,reddit,Mountain_Reward_1252,2025-09-12T10:58:30+00:00,"IMU sensor based terrain classification [P]
Working on my projrct in Robotics. 
I'm developing a terrain classification system using only a single IMU sensor (BNO055) to identify surface types (grass, floor, cement) in real-time for autonomous mobile robots.

My approach:

Collecting 10 minutes of IMU data per terrain at various speeds (0.2-0.8 m/s).

Creating 1-second sliding windows with 50% overlap

Extracting 16 features per window:

Time-domain: variance, RMS, peak-to-peak, zero-crossing rate of Z-axis accelerationFrequency-domain: 

FFT power in bands [0-5Hz], [5-15Hz], [15-30Hz], [30-50Hz]Statistical: kurtosis, skewness

Training Random Forest classifier.

Target: 80-85% accuracy.

Key insights: Different terrains create distinct vibration signatures in frequency domain (grass: 5-15Hz peak, cement: 15-30Hz peak, floor: mostly <5Hz).

Has anyone tried similar approaches with fewer features that still work well? Or is this approach works well with this type of task?
",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1nf03e6/imu_sensor_based_terrain_classification_p/,r_1nf03e6,,,
r_1nez5p7,reddit,Realistic_Tea_2798,2025-09-12T10:02:56+00:00,"[D] Will NAACL 2026 Happen?
Hi guys,

Any idea when NAACL 2026 notification will be out? (Or will it happen this time?)
It's already time but no notification till now.

EACL 2026 notification is already out.",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1nez5p7/d_will_naacl_2026_happen/,r_1nez5p7,,,
r_1neytrz,reddit,Round_Finish5632,2025-09-12T09:42:41+00:00,"[D] Anyone used DeFMO to train models for deblurring fast-moving objects?
Iâ€™m exploring the DeFMO repo and was wondering if anyone has trained it for detecting and deblurring fast-moving objects. My main use case is basketball - the ball often gets blurred in game footage, and Iâ€™d like to use DeFMO to recover its shape and improve detection.",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1neytrz/d_anyone_used_defmo_to_train_models_for/,r_1neytrz,,,
r_1ner3r7,reddit,Ok_Barnacle4840,2025-09-12T02:03:57+00:00,"[D] What model should I use for image matching and search use case?
Hi everyone,

Iâ€™m working on some project where we need to process footprint scans (similar to fingerprints) and later be able to match or search a new scan against a database of existing ones. The pipeline is being built on AWS (S3, Glue, Athena, SageMaker, OpenSearch).

The key requirements are:
Image matching / retrieval â€“ given a new footprint, find the closest match.

Robustness â€“ handle rotation, scale changes, low-quality scans, or partial prints.

Efficiency â€“ scalable to a large dataset, reasonable inference latency.

Iâ€™m exploring options for the ML part and wondering what model to start with:
	
The end goal is to store embeddings in OpenSearch k-NN and run similarity search.

Has anyone worked on a similar problem (biometrics, fingerprints, medical image matching)? Which model architecture would you recommend as a good starting point for training?

Thanks in advance!",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1ner3r7/d_what_model_should_i_use_for_image_matching_and/,r_1ner3r7,,,
r_1neobe4,reddit,DryHat3296,2025-09-11T23:48:28+00:00,"[D] Creating test cases for retrieval evaluation
Iâ€™m building a RAG system using research papers from the arXiv dataset. The dataset is filtered for AI-related papers (around 440k+ documents), and I want to evaluate the retrieval step.

The problem is, Iâ€™m not sure how to create test cases from the dataset itself. Manually going through 440k+ papers to write queries isnâ€™t practical.

Does anyone know of good methods or resources for generating evaluation test cases automatically or any easier way from the dataset?",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1neobe4/d_creating_test_cases_for_retrieval_evaluation/,r_1neobe4,,,
r_1nehy84,reddit,james_stevensson,2025-09-11T19:22:13+00:00,"[D] Math foundations to understand Convergence proofs?
Good day everyone, recently I've become interested in proofs of convergence for federated (and non-federated) algorithms, something like what's seen in appendix A of theÂ [FedProx paper](https://anitksahu.github.io/FedProx.pdf)Â (one page of it attached below)

I managed to go through the proof once and learn things like first order convexity condition from random blogs, but I don't think I will be able to do serious math with hackjobs like that. I need to get my math foundations up to a level where I can write one such proof intuitively.

So my question is: What resources must I study to get my math foundations up to par? Convex optimization by Boyd doesn't go through convergence analysis at all and even the convex optimization books that do, none of them use expectations over the iteration to proof convergence. Thanks for your time

https://preview.redd.it/481lxdf47lof1.png?width=793&format=png&auto=webp&s=6771d3ffe8a533155aa145b2ec691181a30968b9

",MachineLearning,23,https://www.reddit.com/r/MachineLearning/comments/1nehy84/d_math_foundations_to_understand_convergence/,r_1nehy84,,,
r_1nee9fl,reddit,No_Marionberry_5366,2025-09-11T17:02:04+00:00,"[D] Universal Deep Research (UDR): A general wrapper for LLM-Based research
Just read Universal Deep Research by Nvidia , which tries to tackle the problem of â€œAI research agentsâ€ in a pretty different way. Most existing systems bolt an LLM onto search and call it a day: you send a query, it scrapes the web, summarizes, and gives you something vaguely essay-like.

UDR goes another way. Instead of fixing one pipeline, it lets you write a research strategy in plain English. That gets compiled into code, run in a sandbox, and can call whatever tools you want â€” search APIs, ranking, multiple LLMs. State lives in variables, not the LLMâ€™s memory, so itâ€™s cheaper and less flaky.

What makes this relevant to web search: UDR doesnâ€™t careÂ *which*Â backend you use. It could be Google, PubMed, Linkup, Exa or whatever. UDR tries to be the orchestration layer where you decide how to use that feed.

Upside: modularity, reliability, and mix-and-match between search + models. Downside: you actually need to define a strategy, and bad search in still means bad results out.

I like it as a reframing: not another â€œAI search engine,â€ but a framework where search is just one part

https://preview.redd.it/kh7kce0ahkof1.png?width=2562&format=png&auto=webp&s=95d19b8e718de36c40468e6d2d6ffbe5bbc37e72

",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nee9fl/d_universal_deep_research_udr_a_general_wrapper/,r_1nee9fl,,,
r_1neccr0,reddit,anishathalye,2025-09-11T15:49:10+00:00,"[P] Semlib: LLM-powered Data Processing
I've been thinking a lot about semantic data processing recently. A lot of the attention in AI has been on agents and chatbots (e.g., Claude Code or Claude Desktop), and I think semantic data processing is not well-served by such tools (or frameworks designed for implementing such tools, like LangChain).

As I was working on some concrete semantic data processing problems and writing a lot of Python code (to call LLMs in a for loop, for example, and then adding more and more code to do things like I/O concurrency and caching), I wanted to figure out how to disentangle data processing pipeline logic from LLM orchestration. Functional programming primitives (map, reduce, etc.), common in data processing systems like MapReduce/Flume/Spark, seemed like a natural fit, so I implemented semantic versions of these operators. It's been pretty effective for the data processing tasks I've been trying to do.

This blog post (https://anishathalye.com/semlib/) shares some more details on the story here and elaborates what I like about this approach to semantic data processing. It also covers some of the related work in this area (like DocETL from Berkeley's EPIC Data Lab, LOTUS from Stanford and Berkeley, and Palimpzest from MIT's Data Systems Group).

Like a lot of my past work, the software itself isn't all that fancy; but it might change the way you think!

The software is open-source at https://github.com/anishathalye/semlib. I'm very curious to hear the community's thoughts!",MachineLearning,17,https://www.reddit.com/r/MachineLearning/comments/1neccr0/p_semlib_llmpowered_data_processing/,r_1neccr0,,,
r_1ndulfv,reddit,drv29,2025-09-11T00:15:50+00:00,"[D] The best way to structure data for a predictive model of corporate delinquency
I have annual financial indicators for thousands of clients (businesses), their credit data, and delinquency data, and I want to use this data to create a predictive model.

But what's the best way to structure the data?

* Take the annual financial data and associate it with the following year's delinquency data. So, for example, data from 2024 will predict delinquency in 2025.

OR

* Group by client and calculate the average, maximum, and minimum of the financial data to see if this data can predict delinquency.",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1ndulfv/d_the_best_way_to_structure_data_for_a_predictive/,r_1ndulfv,,,
r_1ndtey6,reddit,grabber500,2025-09-10T23:20:02+00:00,"[D] Having trouble organising massive CSV files for your machine learning models?
I've been fighting with CSVs from our high end power quality meter from a very reputable instrument company. 

The CSV files come out from the unit immediately unusable and at 2 million samples per second its a huge dataset, and we take lots of measurements. I made some scripts go clean it but its still a mission every time that I dread to get to the good bit. ",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/,r_1ndtey6,,,
r_1ndo5md,reddit,pmv143,2025-09-10T19:39:51+00:00,"[D]NVIDIA Blackwell Ultra crushes MLPerf
NVIDIA dropped MLPerf results for Blackwell Ultra yesterday. 5Ã— throughput on DeepSeek-R1, record runs on Llama 3.1 and Whisper, plus some clever tricks like FP8 KV-cache and disaggregated serving. The raw numbers are insane.

But I wonder though . If these benchmark wins actually translate into lower real-world inference costs.

In practice, workloads are bursty. GPUs sit idle, batching only helps if you have steady traffic, and orchestration across models is messy. You can have the fastest chip in the world, but if 70% of the time itâ€™s underutilized, the economics donâ€™t look so great to me. IMO",MachineLearning,54,https://www.reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/,r_1ndo5md,,,
r_1ndbzb7,reddit,ScaryCommission7829,2025-09-10T11:48:37+00:00,"[D] ICCV 2025 registration
Two years ago at Paris I had a workshop paper, I purchased the workshop entrance ticket, everything is okay.

This year I have done the same and now I am receiving emails saying only a full conference entrance is considered an author registration for a workshop paper. 

I did see the website is slightly different this year but stillâ€¦ the code of conduct did not explain this clearly, does anyone have better insights for me?",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/,r_1ndbzb7,,,
r_1ndajmq,reddit,Feuilius,2025-09-10T10:30:47+00:00,"[D] Questions on Fairness and Expectations in Top-Tier Conference Submissions
Hello everyone,

I know that in this community there are many experienced researchers and even reviewers for top-tier conferences. As a young researcher, I sincerely hope to learn from your perspectives and get some clarity on a few concerns Iâ€™ve been struggling with.

**My first question:**  
Does a research paper always need to achieve *state-of-the-art (SOTA)* resultsâ€”outperforming every existing methodâ€”to be accepted at an A\* conference? I often feel that so many published papers present dazzling results, making it nearly impossible for newcomers to surpass them.

**My second question, about fairness and accuracy in comparisons:**  
When evaluating a new method, is it acceptable to compare primarily against the most â€œrelated,â€ â€œsimilar,â€ or â€œsame-familyâ€ methods rather than the absolute SOTA? For example:

* If I make a small modification to the Bagging procedure in Random Forest, would it be fair to compare only against other Bagging-based forests, rather than something fundamentally different like XGBoost (which is boosting-based)?
* Similarly, if I improve a variant of SVM, is it reasonable to compare mainly with other margin-based or kernel methods, instead of tree-based models like Decision Trees?

I understand that if my method only beats some similar baselines but does not surpass the global best-performing method, reviewers might see it as â€œmeaninglessâ€ (since people naturally gravitate toward the top method). Still, Iâ€™d like to hear your thoughts: from an experienced researcherâ€™s point of view, what is considered fair and convincing in such comparisons?

Thank you very much in advance for your time and advice.",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/,r_1ndajmq,,,
r_1ndaesz,reddit,Soft-Possibility2929,2025-09-10T10:22:58+00:00,"[D] SOTA modern alternative to BertScore?
Hi everyone,  
Iâ€™m looking for an embedding-based metric to score text generation. BertScore is great, but itâ€™s a bit outdated. Could you suggest some modern state-of-the-art alternatives?

",MachineLearning,16,https://www.reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/,r_1ndaesz,,,
r_1ncyf2r,reddit,United_Intention42,2025-09-09T23:21:09+00:00,"[D] Completed Amazon ML Summer School 2025 curious who else attended?
Hey everyone,  
I just completedÂ **Amazon ML Summer School 2025**Â ðŸŽ‰  
It was a month-long program covering a solid range of ML topicsÂ ***supervised/unsupervised learning, deep neural nets, generative AI & LLMs, RL, and even causal inference***.  
The sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.

Curious if anyone here has also attended and how you re planning to apply what you learned?

https://preview.redd.it/b5ulzuq038of1.png?width=655&format=png&auto=webp&s=c328f24e6b674b9f576cebae727f44a526f185a9

",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/,r_1ncyf2r,,,
r_1ncu0na,reddit,Starscream-11813,2025-09-09T20:23:17+00:00,"[D] IJCNLP-AACL 2025: Paper Reviews (ARR July 2025 Cycle)
The ARR July cycle reviews for AACL-IJCNLP 2025 just dropped.  
Feel free to share your thoughts and feelings! How did you do?",MachineLearning,23,https://www.reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/,r_1ncu0na,,,
r_1ncrkpp,reddit,Sami10644,2025-09-09T18:54:32+00:00,"[D] Negative RÂ² on unseen dataset despite good train/test performance
I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the RÂ² value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the RÂ² values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_rÂ²_on_unseen_dataset_despite_good/,r_1ncrkpp,,,
r_1ncemg3,reddit,ExtentBroad3006,2025-09-09T09:52:28+00:00,"[D] Whatâ€™s the most frustrating â€œstuckâ€ moment youâ€™ve faced in an ML project?
Curious about community experience: whatâ€™s the most painful â€˜stuckâ€™ moment youâ€™ve faced in an ML project (convergence, dataset issues, infra)?  
How did you eventually move past it, or did you abandon the attempt? Would be great to hear real war stories beyond published papers.",MachineLearning,28,https://www.reddit.com/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/,r_1ncemg3,,,
r_1ncdt5o,reddit,krychu,2025-09-09T08:59:47+00:00,"[P] Implementation and ablation study of the Hierarchical Reasoning Model (HRM): what really drives performance?
I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.

HRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.

To understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):

* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.
* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.
* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.

Repo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)

This is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.

The findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)

Below two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:

[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)

[30x30 board](https://i.redd.it/j6fpueovs3of1.gif)

",MachineLearning,66,https://www.reddit.com/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/,r_1ncdt5o,,,
r_1ncceqw,reddit,Coffeee_addictt,2025-09-09T07:23:46+00:00,"[D] Best ocr as of now
I want to know which ocr has high accuracy and consumes less time for the extraction of data for given input images (especially tables), anything which works better than paddleocr?",MachineLearning,20,https://www.reddit.com/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/,r_1ncceqw,,,
r_1nc6r7l,reddit,AtharvBhat,2025-09-09T02:05:38+00:00,"[Project] Otters ðŸ¦¦ - A minimal vector search library with powerful metadata filtering
I'm excited to share something I've been working on for the past few weeks:

Otters ðŸ¦¦ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!

Why I Built This

In my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,
Too bloated (full vector databases when I needed something minimal for analysis)
Limited in filtering capabilities
Had unintuitive APIs that I was not happy about.

I wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.

What Makes Otters Different

Exact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale.

 Performance: 
SIMD-accelerated scoring
Zonemaps and Bloom filters for intelligent chunk pruning

Polars-Inspired API: Write filters as simple expressions
```
meta_store.query(query_vec, Metric::Cosine)
    .meta_filter(col(""price"").lt(100) & col(""category"").eq(""books""))
    .vec_filter(0.8, Cmp::Gt)
    .take(10)
    .collect()
```

The library is in very early stages and there are tons of features that i want to add
Python bindings, NumPy support
Serialization and persistence
Parquet / Arrow integration
Vector quantization
etc.

I'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.

If you think this is interesting and worth your time, please give it a try.
I welcome contributions and feedback !

ðŸ“¦ https://crates.io/crates/otters-rs
ðŸ”— https://github.com/AtharvBhat/otters",MachineLearning,18,https://www.reddit.com/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/,r_1nc6r7l,,,
r_1nc5jb5,reddit,ekkarpinski,2025-09-09T01:08:11+00:00,"[R] LLMs play a cooperative card game, coordination without communication
One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. 

Originally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.

What I've found out:

\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.

[This does not \\""help him secure the win and fulfill his task.\\"" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&format=png&auto=webp&s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)

\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. 

[GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&format=png&auto=webp&s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)

\* GPT-5 is self-aware enough to avoid the ""losing on the very first turn"" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.

[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&format=pjpg&auto=webp&s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)

\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with <10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) 

\* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.

\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.

\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)

I published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/)

",MachineLearning,48,https://www.reddit.com/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/,r_1nc5jb5,,,
r_1nc1mxq,reddit,Acceptable_Army_6472,2025-09-08T22:16:23+00:00,"[Project] Phishing URL detection with Random Forests and handcrafted features
**\[Project\] Phishing URL detection with Random Forests on handcrafted features** 

I recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.

**Data & Features**

* Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.
* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.
* Features (hand-engineered):
   * URL length & token counts
   * Number of subdomains, â€œ@â€ usage, hyphens, digits
   * Presence of IP addresses instead of domains
   * Keyword-based flags (e.g., â€œloginâ€, â€œsecureâ€)

**Model & Training**

* Algorithm: Random Forest (scikit-learn).
* Training: 80/20 split, 10-fold CV for validation.
* Performance: \~92% accuracy on test data.
* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.

**Takeaways**

* A simple RF + handcrafted features still performs surprisingly well on phishing detection.
* Interpretability (feature importances) adds practical value in a security context.
* Obvious limitations: feature set is static, adversaries can adapt.

**Future work (exploration planned)**

* Gradient boosting (XGBoost/LightGBM) for comparison.
* Transformers or CNNs on raw URL strings (to capture deeper patterns).
* Automating retraining pipelines with fresh phishing feeds.

**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)

Would love feedback on:

* What other URL features might improve detection?
* Have people here seen significant gains moving from RF/GBM â†’ deep learning for this type of task?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/,r_1nc1mxq,,,
r_1nbsems,reddit,Senior-Let-7576,2025-09-08T16:27:49+00:00,"[D] AAAI 26 Alignment Track
Does anyone know whether theyâ€™re going to release the Phase 1 rejections today or on September 12?",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/,r_1nbsems,,,
r_1nbr57g,reddit,Technical-Seesaw9383,2025-09-08T15:40:58+00:00,"[R] Benchmarking an ML service in python
Recently, I needed to build an ML service that would be called by a latency-sensitive client. The requirements for load and latency were higher than what I had worked with in the past, so I wasnâ€™t sure what to expect from my Python application.

I googled around and couldnâ€™t find any concrete answers, so I wrote this brief article for anyone out there in a similar situation:

https://medium.com/@javiermas/benchmarking-an-ml-service-in-pytho-4238399d2229

I hope you find it useful!
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/,r_1nbr57g,,,
r_1nbisbw,reddit,Anmol_garwal,2025-09-08T09:07:14+00:00,"[D] How to Automate parsing of Bank Statement PDFs to extract transaction level data
I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \[Date, Particulars, Credit/Debit amount, Balance\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.

I want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.

Hence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.

Since then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.

Now I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.

Has anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!

Know that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \[integrated with OCR\]",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/,r_1nbisbw,,,
r_1nbhqmq,reddit,Set-New,2025-09-08T07:58:03+00:00,"[D] How do you stay current with AI/ML research and tools in 2025? (Cybersec engineer catching up after Transformers)
Hi everyone,

Iâ€™m a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.

Now Iâ€™d like to get back into the field and stay currentâ€”not necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. Iâ€™d like to adopt a similar approach for ML/AI.

For those of you who actively track progress:

* Which blogs, newsletters, or feeds do you find most useful?
* Are there particular researchers or labs whose updates you follow?
* Any books or surveys that bridge foundational knowledge with current trends?
* How do you cut through hype-heavy content and focus on signal?

Iâ€™d really appreciate hearing what works for you. The field moves incredibly fast, and Iâ€™d like to plug back in with a structured approach.

Thanks in advance!",MachineLearning,108,https://www.reddit.com/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/,r_1nbhqmq,,,
r_1naz0eb,reddit,Lestode,2025-09-07T17:19:16+00:00,"[D] Vibe-coding and structure when writing ML experiments
Hey!

For context, I'm a Master's student at ETH ZÃ¼rich. A friend and I recently tried writing a paper for a NeurIPS workshop, but ran into some issues.  
We had both a lot on our plate and probably used LLMs a bit too much. When evaluating our models, close to the deadline, we caught up on some bugs that made the data unreliable. We also had plenty of those bugs along the way. I feel like we shot ourselves in the foot but that's a lesson learned the way. Also, it made me realise the negative effects it could have had if those bugs had been kept uncaught.

I've been interning in some big tech companies, and so I have rather high-standard for clean code. Keeping up with those standards would be unproductive at our scale, but I must say I've struggled finding a middle ground between speed of execution and code's reliability.

For researchers on this sub, do you use LLMs at all when writing ML experiments? If yes, how much so? Any structure you follow for effective experimentation (writing (ugly) code is not always my favorite part)? When doing experimentation, what structure do you tend to follow w.r.t collaboration?

Thank you :)

",MachineLearning,24,https://www.reddit.com/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/,r_1naz0eb,,,
r_1nanw9i,reddit,prabhjots665,2025-09-07T08:26:54+00:00,"[P] Terra Code CLI â€“ An AI coding assistant with domain knowledge and semantic code search
One limitation Iâ€™ve noticed with most AI coding assistants is that they donâ€™t really understand a teamâ€™s domain knowledge or architectural decisions.

To explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.

Things we experimented with:
â€¢ Interactive Knowledge Transfer â€“ let senior devs â€œteachâ€ patterns
â€¢ Semantic Code Search â€“ context-aware retrieval across repos
â€¢ Persistent Memory â€“ standards remembered across projects
â€¢ Domain Expertise â€“ ingesting architecture docs, API specs, etc.

Weâ€™re curious:
ðŸ‘‰ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?

For free quick start:

npm install -g @terra-code/terra-code

terra

For those interested, weâ€™ve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. Thereâ€™s also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so itâ€™s free to use.",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/,r_1nanw9i,,,
r_1namvsk,reddit,OkOwl6744,2025-09-07T07:21:15+00:00,"Why Language Models Hallucinate - OpenAi pseudo paper - [D]
Hey
Anybody read this ? It seems rather obvious and low quality, or am I missing something ? 

https://openai.com/index/why-language-models-hallucinate/

â€œAt OpenAI, weâ€™re working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isnâ€™t true. Our new research paperâ (opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.
ChatGPT also hallucinates. GPTâ€‘5 has significantly fewer hallucinations especially when reasoningâ , but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.â€",MachineLearning,113,https://www.reddit.com/r/MachineLearning/comments/1namvsk/why_language_models_hallucinate_openai_pseudo/,r_1namvsk,,,
r_1nahnmz,reddit,absurdistonvacation,2025-09-07T02:24:44+00:00,"[D] Thought experiment: â€œRolling without slippingâ€ as a blueprint for nDâ†’(nâˆ’1) embeddings?
I came across the recent ROLLING HONED paper (designing 3D shapes that, when rolling without slipping, trace arbitrary 2D paths). It got me thinking:

In 3D, rolling constraints let you encode a 2D trajectory into the geometry of a 3D body.

In principle, in 4D you could imagine a convex hypersurface rolling on a 3D hyperplane, tracing out a 3D trajectory.

More generally: could there be a systematic way to map nD data into (nâˆ’1)D dynamics via such constraints?

I know in ML we already have PCA, autoencoders, product quantization, etc. â€” and those actually preserve metrics we care about. My hunch is that this â€œmechanical embeddingâ€ idea probably fails the usefulness test for similarity search (no guarantee of inner product preservation).

But still:

Does the analogy make any theoretical sense in higher dimensions (rolling manifolds w/o slip/twist)?

Could there be hidden value in treating â€œconstrained dynamicsâ€ as a new kind of coding scheme?

Or am I over-romanticizing a neat geometric trick after too much late-night reading?

Curious what the community thinks â€” is there any research potential here, or should I file this under â€œfun alcohol-fueled metaphorsâ€ and move on?
",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/,r_1nahnmz,,,
r_1naejuk,reddit,Artoriuz,2025-09-06T23:51:49+00:00,"[D] The apparent randomness of residual block design
Skip connections and residual blocks have been ubiquitous in the ML field ever since the original ResNets were published. I think it's fair to say most people agree skip connections help, but at a glance, the design of the residual blocks themselves is still something that differs from paper to paper.

The most recent ""innovation"" is splitting channel mixing from spatial mixing, which is what ConvNeXt does in an attempt to mimic transformers. Other models that also claim SotA-ish performance, however, do not necessarily follow suit. NFNet, for example, employs grouped 3x3 convolution layers, good old normal bottlenecks (not inverted) and channel attention (Squeeze-and-Excitation).

If we look at modern LLMs, they all have residual blocks that look very similar, but with one or two minor differences that often look arbitrary.

I think residual block design is one of those things that people don't really pay much attention to since it generally works well enough regardless of what you do, but at some point it does look like we're just making semi-random decisions based on semi-random observations. Why the block is designed in the way it is is rarely a point of concern.

I've tried looking for papers making direct comparisons between different design choices, but I couldn't really find anything conclusive.

",MachineLearning,70,https://www.reddit.com/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/,r_1naejuk,,,
r_1na5ixj,reddit,pmv143,2025-09-06T17:33:06+00:00,"[D]Baseten raises $150M Series D for inference infra. whereâ€™s the real bottleneck?
Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.

Theyâ€™ve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.

But this raises a bigger question. whatâ€™s the real bottleneck in inference?
	â€¢Baseten and others (Fireworks, Together) are competing on latency + throughput.
	â€¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.

I wonder what everyone thinks 

	â€¢Will latency/throughput optimizations be enough to differentiate?
	â€¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?
	â€¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?
",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/,r_1na5ixj,,,
r_1n9xg20,reddit,local___host,2025-09-06T11:43:06+00:00,"[D] Online hierarchical clustering for news: how to keep event IDs stable under merges/splits in a streaming pipeline?
Iâ€™m building a news ingestion system (currently Poland-focused; designed to scale) that clusters incoming articles into â€œeventsâ€ powering maps and graph views. Pipeline: embeddings â†’ cosine HAC with a fixed threshold â†’ periodic (5min) recluster. Granularity, time decay, and summarization are fine, my sole pain point isÂ *stable event identity*Â in a streaming setting.

As new articles arrive, clusters should sometimes merge (a legitimate bridge appears) or split (bridge was spurious). I need user-facing event IDs to persist through these transitions, i.e., minimize label churn across snapshots while respecting the hierarchical/threshold constraints.

**Question:**Â Whatâ€™s the best-known algorithmic approach (and any open-source references) forÂ *evolutionary/streaming hierarchical clustering with persistent labels*, explicitly merge/split-aware, thatÂ *minimizes an inter-snapshot ID-churn* *penalty*Â under latency constraints?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/,r_1n9xg20,,,
r_1n9wnel,reddit,Nearby_Reaction2947,2025-09-06T10:57:11+00:00,"[P] An Open-Source Pipeline for Speech-to-Speech Translation with Voice Preservation (RVC) and Lip-Sync
HelloÂ [r/MachineLearning](https://www.reddit.com/r/MachineLearning/),

I'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. Itâ€™s an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.

* **GitHub Repo:**Â [\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)
* **Full Technical Write-up:**Â [\[writeup\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)
* **Demo Video:**Â [\[Demo\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)

The core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.

The final pipeline is structured as follows:

1. **ASR:**Â Whisper for robust transcription.
2. **NMT:**Â Meta's NLLB for English-to-Telugu translation.
3. **TTS:**Â Meta's MMS model to synthesize the base Telugu audio.
4. **Voice Conversion:**Â A trained RVC model converts the timbre of the synthetic speech to match the original speaker.
5. **Lip Sync:**Â Wav2Lip aligns the video frames to the new audio.

My main takeaway is that RVC seems to function as a very effective ""style transfer"" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.

I'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:

1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?
2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?

Any thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time.",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/,r_1n9wnel,,,
r_1n9ufsf,reddit,Confident-Meal3457,2025-09-06T08:36:00+00:00,"[P] Knowledge Distillation for Text-to-SQL â€” Training GPT-2 with Qwen2-7B as Teacher
Hey folks,

Iâ€™ve been working on an experiment that combinesÂ **Knowledge Distillation (KD)**Â with theÂ **Text-to-SQL problem**, and I wanted to share the results + repo with the community.

# ðŸŽ¯ Motivation

* Natural language â†’ SQL is a powerful way forÂ **non-technical users**Â to query databases without always relying on analysts.
* Most solutions use massive LLMs (GPT-4.1, etc.), but theyâ€™reÂ **expensive**,Â **hard to deploy locally**, and raiseÂ **data privacy concerns**.
* So the question I asked:Â *Can a much smaller model (like GPT-2) be trained to generate SQL for a given DB effectively if it learns from a bigger LLM?*

# ðŸ§  Approach

I usedÂ **Knowledge Distillation (KD)**Â â€” i.e., transferring knowledge from a large teacher model into a smaller student model.

* **Teacher Model**:Â [Qwen2-7B]()
* **Student Model**:Â [GPT-2]()

Steps:

1. Built aÂ **custom dataset**Â â†’ pairs of (natural language query, SQL query) for a toy retail database schema.
2. Teacher (Qwen2-7B) generates SQL from the queries.
3. Student (GPT-2) is trained on two signals:
   * **Cross-Entropy Loss (75%)**Â â†’ match ground-truth SQL.
   * **MSE Loss (25%)**Â â†’ align with the teacherâ€™s hidden state values (projected from teacherâ€™s layer 25).
4. Trained forÂ **20 epochs on Colab GPU**.

# âš™ï¸ Training Setup

* Teacher hidden states projected â†’ aligned with GPT-2â€™s final hidden states.
* Loss =Â **0.75 \* CE + 0.25 \* MSE**.
* AchievedÂ **total loss \~0.21**Â after training.

# ðŸ“Š Results

* GPT-2 (student) was able toÂ **generate SQL queries directly from natural language**Â for the schema.
* While not perfect (due to limited resources at my disposal), it showed that **small models can be viable for domain-specific SQL generation**Â when trained this way.
* Benefits:
   * âš¡ Lightweight (runs locally).
   * ðŸ’¸ Cost-efficient.
   * ðŸ” More privacy-friendly than cloud-only LLM APIs.

# ðŸ“· Visuals in the repo:

* Schema diagram (retail DB).
* Teacher â†’ Student distillation architecture.
* Sample outputs (NL â†’ SQL).

# ðŸ“Ž Repo

Code + diagrams + outputs are here:  
ðŸ‘‰Â [GitHub: Knowledge Distillation for SQL generation on GPT-2](https://github.com/Gokul-GMenon/Knowledge_Distillation-SQL_generation_on_gpt_2?utm_source=chatgpt.com)

Would love feedback, suggestions, or discussions on:

* Other lightweight models worth trying as students (LLaMA-7B distilled further? Phi-2?).
* Improvements to the KD setup (layer selection, different projection strategies).
* Extensions: applying this to more complex schemas / real enterprise DBs.

Cheers!

  
Can follow me in [LinkedIn](https://www.linkedin.com/in/gokul-g-menon/) as well for discussions",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1n9ufsf/p_knowledge_distillation_for_texttosql_training/,r_1n9ufsf,,,
r_1n9spn2,reddit,Forsaken-Order-7376,2025-09-06T06:45:35+00:00,"[D] Advice on handling completely incorrect review?
Recently submitted a paper to WACV 2026. Two of the three reviews are positive. The third recommends rejection, citing items as â€œmissingâ€ that are actually in the paper (2nd page dude) and claiming our architecture is identical to a 2022 model, though there are clear differences- moreover, the performances tend to drastically differ as showcased in the results.

What are the typical options in this situation? He seems to be inclined towards finding ""excuses"" for rejecting paper (not sure why) and thereby I doubt a rebuttal will help. Can I ask the AC to get the reviewer replaced?",MachineLearning,15,https://www.reddit.com/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/,r_1n9spn2,,,
r_1n9m5hv,reddit,Specialist_Clock_368,2025-09-06T00:51:26+00:00,"[D] Seeking arXiv endorsement
Hi All

Iâ€™m preparing to submit to arXiv in Experimentation. Since this is my first submission, I need an endorsement.

The draft is ready and I can share it upon request. Thanks! 
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/,r_1n9m5hv,,,
r_1n9hnq9,reddit,KeyIsNull,2025-09-05T21:30:46+00:00,"[D] Anyone successful with training LoRA for visual LLMs on a multi-GPU setup?
Hello sub,

I'm trying to train a LoRA for Llama 3.2 90B Visual Instruct on a 8xA100 cluster but I cannot find a framework/package that supports it.

Model is of course too large to fit into a single A100, so the only way is to leverage multiple device.

Unsloth does not support multi GPU training (at least in its open version)  
Axtol has multimodal models in beta

Was any of you successful into training multimodal models of this size? I'd appreciate any kind of feedback.",MachineLearning,11,https://www.reddit.com/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/,r_1n9hnq9,,,
r_1n9ecmj,reddit,DeeplyConvoluted,2025-09-05T19:19:07+00:00,"[D]  Anyone attending EUSIPCO next week?
Anyone attending EUSIPCO in Palermo next week? Unfortunately, none of my labmates will be able to travel, so would be cool to meet new people from here !",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/,r_1n9ecmj,,,
r_1n95etu,reddit,Says_Watt,2025-09-05T13:33:25+00:00,"[D] Reversed born again network because it's easier to train, is this stupid?
I want to implement this paper: [https://arxiv.org/pdf/1805.04770](https://arxiv.org/pdf/1805.04770)

but I'm not excited about having to manage the student models / save them independently and also there's the issue of cost because we'd have to train each student model from scratch.

To get around this I was thinking I could just do the inverse: train the teacher model and derive ""dark knowledge"" based on the ""incorrect"" logits of the last checkpoint.

What I mean is can I have a training loop similar to the following

    for epoch in range(10):
      student = teacher.clone()
      student.requires_grad_(False) # the student deliberately does not learn, only the teacher learns
      for data in dataset:
        optim.zero_grad()
        teacher_logits = teacher(data.input)
        student_logits = student(data.input)
        loss_cross_entropy = cross_entropy(teacher_logits, data.label)
        loss_dark_knowledge = cross_entropy(teacher_logits - student_logits, data.label)
        loss = (loss_cross_entropy + loss_dark_knowledge) / 2
        loss.backward()
        optim.step()

is this dumb?",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/,r_1n95etu,,,
r_1n947jj,reddit,Pitiful-Ad8345,2025-09-05T12:41:34+00:00,"[P] I Was Wrong About Complex ML Solutions - Gower Distance Beat My UMAP Approach
Four years ago, I built [DenseClus ](https://github.com/awslabs/amazon-denseclus)for mixed-data clustering using dual UMAP embeddings. After reflecting on the Zen of Python (""simple is better than complex""), I realized I was overengineering.

Gower (1971) computes distances for mixed categorical/numerical data using weighted averages of appropriate metrics. Despite being 50+ years old, it often outperforms complex embeddings for small-to-medium datasets.

The implementation I coded (with Claude's help) saw a 20% speedup, 40% in memory, has GPU support (CuPy) and Sklearn integration.

Code: [https://github.com/momonga-ml/gower-express](https://github.com/momonga-ml/gower-express)

Blog post with analysis: [https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481](https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481)

**Discussion**:  When do you choose simple, interpretable methods over deep embeddings? Have others found similar success reverting to classical approaches?",MachineLearning,19,https://www.reddit.com/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/,r_1n947jj,,,
r_1n8ynn2,reddit,Tanmay__13,2025-09-05T07:21:05+00:00,"[P] I Built a Convolutional Neural Network that understands Audio
Hi everyone, I am sharing a project that I built recently, I trained a convolutional neural network (CNN) based on aÂ ResNetâ€‘34 style residual architectureÂ to classify audio clips from theÂ ESCâ€‘50 datasetÂ (50 environmental sound classes). I used logâ€“mel spectrograms as input, reached strong accuracy and generalization with residual blocks, and packaged the model with dropout and adaptive average pooling for robustness. Would love to get your opinions on it.  Check it out -->Â [https://sunoai.tanmay.space](https://sunoai.tanmay.space/)

Read the blog -->Â [https://tanmaybansal.hashnode.dev/sunoai](https://tanmaybansal.hashnode.dev/sunoai)",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1n8ynn2/p_i_built_a_convolutional_neural_network_that/,r_1n8ynn2,,,
r_1n918yb,reddit,CaptainBudy,2025-09-05T10:09:40+00:00,"[P] DCNv2 (Update Compatibility) Pytorch 2.8.0
Hello Reddit,

Working on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.

Feel free to use it for training model like YOLACT+, FairMOT or others.

[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main)",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/,r_1n918yb,,,
r_1n8po18,reddit,jonas__m,2025-09-04T23:30:36+00:00,"[R] The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs
Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)  
  
In my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.

Other common flaws in existing benchmarks:

\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.

\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.

\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.  
  
I think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern",MachineLearning,29,https://www.reddit.com/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/,r_1n8po18,,,
r_1n8lvz5,reddit,Infinite_Explosion,2025-09-04T20:53:22+00:00,"[D] How do you read code with Hydra
[Hydra](https://hydra.cc/) has become a very popular in machine learning projects. I understand the appeal, it makes configurations modular, allows you to reuse some parts of it while changing another. It makes the code more reusable and modular too and if you understand all of it its better structured.

My big problem is it makes it damn well near impossible to read someone else's code since every part of the code is now some mysterious implicit thing that gets instantiated from a string in the config file during execution. The problem would be alleviated if there was a way of quickly accessing the definition of the object that will get instantiated at runtime at least with the default values of the config. Is there a plugin that does that? If not, how do you guys do it ?",MachineLearning,87,https://www.reddit.com/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/,r_1n8lvz5,,,
r_1n83e6e,reddit,baddie_spotted,2025-09-04T07:11:27+00:00,"[D] Performance overhead of running ML inference in hardware-isolated environments - production metrics
Been collecting data on ML inference performance in trusted execution environments and thought the numbers might be useful for others dealing with similar constraints.

**Context:** Fraud detection models processing ~10M daily transactions, needed hardware-level isolation for compliance reasons.

After 3 months of production data, seeing 5-8% performance overhead compared to standard deployment. This is way better than the 30-40% overhead reported in older papers about SGX.

The interesting technical challenge was memory management. TEE environments have strict memory limits and different allocation patterns than standard containers. Had to completely rewrite our batching logic - what worked fine with dynamic batching in regular pods caused constant OOM errors in enclaves.

**Model optimization discoveries:**

- ONNX runtime worked, pytorch was too memory heavy
- Preprocessing became the bottleneck, not inference
- Had to keep models under 8GB total memory
- P95 latency went from 12ms to 13ms

Tried multiple approaches including raw SGX implementation and phala's abstraction layer. The attestation complexity alone makes raw implementation painful.

**For those working on similar problems:**
Profile your entire pipeline, not just model inference. Data transformation overhead in isolated environments is real.

**Technical question for the community:** 
How are you handling model updates in TEE environments? The attestation requirements make standard blue-green deployments complicated. Currently doing full enclave restarts but that means brief downtime.

Also curious if anyone's tried running transformer models larger than 1B params in TEE. Memory constraints seem prohibitive but maybe there are tricks I'm missing?",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/,r_1n83e6e,,,
r_1n7oh8p,reddit,WildAppearance2153,2025-09-03T19:30:46+00:00,"[P] Arbitrary Order Automatic Differentiation for PyTorch
Iâ€™m excited to present **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a bachelor's research project at Universidad Pontificia de Comillas - ICAI, and we are considering publishing a future academic article reviewing the mathematical details and the implementation design.

At its core, thoad takes a one output, many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1â†’N problem can be rewritten as 1â†’1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoadâ€™s graph aware formulation enables:

* Working with smaller **pieced external derivatives**
* An optimization based on **unifying independent dimensions** (especially batch).

This delivers **asymptotically better scaling** with respect to order and batch size (respectively).

Additionally, we compute derivatives with a *vectorial* approach rather than component by component, which makes our pure PyTorch implementation possible. Consequently, the implementation stays at a high level, written entirely in Python and using **PyTorch** as its only dependency. Avoiding custom C++ or CUDA has a very positive impact on the long-term maintainability of the package.

The package is already available to be installed from **GitHub** or **PyPI**:

* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)

In our benchmarks, thoad **outperforms** torch.autograd for **Hessian calculations even on CPU**. See the repository *examples/benchmarks* to check the comparisons and run them in your own hardware.

**thoad** is designed to align closely with PyTorchâ€™s interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorchâ€™s own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.

# USING THE PACKAGE

**thoad** exposes two primary interfaces for computing high-order derivatives:

1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).
2. `thoad.Controller`: a class-based interface that wraps the output tensorâ€™s subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.

Example of autodifferentiation execution via `thoad.backward`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Call thoad backward
    order = 2
    thoad.backward(tensor=Z, order=order)
    
    #### Checks
    ## check derivative shapes
    for o in range(1, 1 + order):
       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))
       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))
    ## check first derivatives (jacobians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)
    J = torch.autograd.functional.jacobian(fn, (X, Y))
    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)
    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)
    ## check second derivatives (hessians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()
    H = torch.autograd.functional.hessian(fn, (X, Y))
    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)
    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)

Example of autodifferentiation execution via `thoad.Controller`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Instantiate thoad controller and call backward
    order = 2
    controller = thoad.Controller(tensor=Z)
    controller.backward(order=order, crossings=True)
    
    #### Fetch Partial Derivatives
    ## fetch T0 and T1 2nd order derivatives
    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))
    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))
    assert torch.allclose(partial_XX, X.hgrad[1])
    assert torch.allclose(partial_YY, Y.hgrad[1])
    ## fetch cross derivatives
    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))
    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))

>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1n7oh8p/p_arbitrary_order_automatic_differentiation_for/,r_1n7oh8p,,,
r_1n7brbk,reddit,Any_Commercial7079,2025-09-03T11:03:33+00:00,"[P] Sentiment Analysis Model for cloud services
Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.

Iâ€™m happy with the initial results, but Iâ€™d love advice on making the interpretation more precise:

Ensuring sentiment is directed at the provider (not another product/entity mentioned)  
Better handling of comparative or mixed statements (e.g., â€œfast but expensiveâ€)  
Improving robustness to negation and sarcasm

If you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, Iâ€™d really appreciate your input.

Repo:Â [https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)  
  
It would also be great if you could answer my original survey:Â [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thanks!",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n7brbk/p_sentiment_analysis_model_for_cloud_services/,r_1n7brbk,,,
r_1n77v29,reddit,Turbulent_Visual_948,2025-09-03T06:57:09+00:00,"Acl rolling recview is the most garbage conference to submit your papers [R]
You will find the most generic AI generated reviews in ARR. 
Waste of time. Submit to AI conferences. 
ARR is dead",MachineLearning,11,https://www.reddit.com/r/MachineLearning/comments/1n77v29/acl_rolling_recview_is_the_most_garbage/,r_1n77v29,,,
r_1n77fsw,reddit,akshitsharma1,2025-09-03T06:30:07+00:00,"[D] WACV 2026 Paper Reviews
WACV Reviews are supposed to be released by today EOD. Creating a discussion thread to discuss among ourselves, thanks!",MachineLearning,43,https://www.reddit.com/r/MachineLearning/comments/1n77fsw/d_wacv_2026_paper_reviews/,r_1n77fsw,,,
r_1n75xxx,reddit,Impossible_Tutor_824,2025-09-03T05:00:10+00:00,"[R] Practical TEE deployment for sensitive research datasets - lessons from our lab

Posting this because I wish someone had done the same when we started. Our lab needed to work with industry partners on sensitive datasets but legal restrictions meant we couldn't access the raw data.

Traditional methods like differential privacy added too much noise for our research goals. Synthetic data was useless for our specific use case.

What went good for us: deploying our models in trusted execution environments. Partners felt comfortable because data never left their control. We could iterate on models without seeing actual data values.

Tech setup through phala network was surprisingly direct. Only difficulty was adapting our workflow since you can't just print tensors to debug anymore. Had to get creative with logging aggregate statistics.

Unexpected: our industry partnerships increased 3x because companies that previously wouldn't share data are now willing to collaborate. Turns out the privacy barrier was bigger than we realized.

If your research is stuck due to data access issues definitely worth exploring TEE options. Happy to share our deployment scripts if useful.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n75xxx/r_practical_tee_deployment_for_sensitive_research/,r_1n75xxx,,,
r_1n71dzv,reddit,OkOwl6744,2025-09-03T01:12:39+00:00,"A friendly starter paper - Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation [R]
Hey r/MachineLearning 

I had this idea and wanted to put it in a very simple and straightforward way, tried to make the paper easy to read and starter friendly! Also it shows my research partner focus on uncertainty measurement from metrology, which I think itâ€™s not very widely addressed in ML and NLP! 

The motivation here came while doing exploration at the Weights & Biases Sunday cafe event in SF, where we were exploring their observability Weave Product. I think running loops and adding more complex tools that I did for the paper, should be production valuable and help in a bunch of ways, but most importantly, help with making small models
More useful and a kind of reasoning process of sorts. In the future it might be useful to make this loop inside the model before output layers, anybody think of any cools applications for such methods ? 


[Title]: Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation

[Abstract]: Reasoning models often outperform smaller models but at 3--5Ã— higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-k alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.

https://arxiv.org/abs/2509.00079

If you donâ€™t like it, let me know! Am open to critique and learning! ",MachineLearning,25,https://www.reddit.com/r/MachineLearning/comments/1n71dzv/a_friendly_starter_paper_entropyguided_loop/,r_1n71dzv,,,
r_1n6wc4k,reddit,impatiens-capensis,2025-09-02T21:34:26+00:00,"[D] Has paper submission quality remained roughly the same?
Over the last year, I reviewed 12 papers at top tier conferences. It's a small sample size but I noticed that roughly 3 or 4 of them were papers I would consider good enough for acceptance at a top tier conference. That is to say: (1) they contained a well-motivated and interesting idea, (2) they had reasonable experiments and ablation, and (3) they told a coherent story.

That means roughly 30% of papers met my personal threshold for quality.... which is roughly the historic acceptance rate for top-tier conferences. From my perspective, as the number of active researchers has increased, the number of well executed interesting ideas has also increased. I don't think we've hit a point where there's a clearly finite set of things to investigate in the field. 

I would also say essentially every paper I rejected was distinctly worse than those 3 or 4 papers. Papers I rejected were typically poorly motivated -- usually an architecture hack poorly situated in the broader landscape with no real story that explains this choice. Or, the paper completely missed an existing work that already did nearly exactly what they did. 

What has your experience been? ",MachineLearning,71,https://www.reddit.com/r/MachineLearning/comments/1n6wc4k/d_has_paper_submission_quality_remained_roughly/,r_1n6wc4k,,,
r_1n6vf0x,reddit,glazmann,2025-09-02T20:58:26+00:00,"[R] NeurIPS workshop - change of authors post submission
Hi all, I submitted a paper to a NeurIPs workshop recently and it just dawned on me that I forgot to enter one of the authors in the OpenReview portal (the deadline for submission has now passed). I will reach out to the workshop but has anyone had any luck with this kind of thing?",MachineLearning,10,https://www.reddit.com/r/MachineLearning/comments/1n6vf0x/r_neurips_workshop_change_of_authors_post/,r_1n6vf0x,,,
r_1n6t4vd,reddit,farizrahman4u,2025-09-02T19:32:13+00:00,"[P] Datatune â€“ Use natural language + LLMs to transform and filter tabular data
https://github.com/vitalops/datatune

Introducing Datatune, a Python library that enables row-wise transformations on tabular data using natural language prompts, powered by LLMs.

Unlike tools that generate SQL or static scripts, Datatune is designed for per-row semantic operations on tabular data. Itâ€™s particularly useful for fuzzy logic tasks like classification, filtering, derived metrics, and text extraction - anything thatâ€™s hard to express in SQL but intuitive in plain English.

### What it does

You write prompts like:

* ""Extract categories from the product description and name""
* ""Keep only electronics products""
* ""Add a column called ProfitMargin = (Total Profit / Revenue) * 100""

Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.

### Key Features

* Row-level map() and filter() operations using natural language
* Agent interface for auto-generating multi-step transformations
* Built-in support for Dask DataFrames (for scalability)
* Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)
* Compatible with LiteLLM for flexibility across providers
* Auto-token batching, metadata tracking, and smart pipeline composition

### Token & Cost Optimization

* Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:
* Use input_fields to send only relevant columns
* Automatically handles batching and metadata internally
* Supports setting tokens-per-minute and requests-per-minute limits
* Defaults to known model limits (e.g., GPT-3.5) if not specified
* This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.

### Quick Example
```python
import datatune as dt
from datatune.llm.llm import OpenAI

llm = OpenAI(model_name=""gpt-3.5-turbo"")
df = dd.read_csv(""products.csv"")

# Map step
mapped = dt.map(
    prompt=""Extract categories from the description and name of product."",
    output_fields=[""Category"", ""Subcategory""],
    input_fields=[""Description"", ""Name""]
)(llm, df)

# Filter step
filtered = dt.filter(
    prompt=""Keep only electronics products"",
    input_fields=[""Name""]
)(llm, mapped)

result = dt.finalize(filtered)
```

Or using the agent:

```python
agent = dt.Agent(llm)
df = agent.do(""Add a column called ProfitMargin = (Total Profit / Total Revenue) * 100."", df)
result = dt.finalize(df)
```
### Use Cases

* Product classification from text fields
* Filtering based on semantic conditions
* Creating derived metrics using natural language
* Review quality detection, support ticket triage
* Anonymization (PII removal) when needed

### Links

* GitHub: https://github.com/vitalops/datatune
* Docs: https://docs.datatune.ai
* Examples: https://github.com/vitalops/datatune/tree/main/examples

Weâ€™re actively developing the project and would appreciate any feedback, bug reports, or feature requests via Github issues.
.
",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1n6t4vd/p_datatune_use_natural_language_llms_to_transform/,r_1n6t4vd,,,
r_1n6swom,reddit,Ill_Virus4547,2025-09-02T19:23:20+00:00,"[D] How can I license datasets?
I've been working on AI projects for a while now and I keep running into the same problem over and over again. Wondering if it's just me or if this is a universal developer experience.

You need specific training data for your model. Not the usual stuff you find on Kaggle or other public datasets, but something more niche or specialized, for e.g. financial data from a particular sector, medical datasets, etc. I try to find quality datasets, but most of the time, they are hard to find or license, and not the quality or requirements I am looking for.

So, how do you typically handle this? Do you use datasets free/open source? Do you use synthetic data? Do you use whatever might be similar, but may compromise training/fine-tuning?

Im curious if there is a better way to approach this, or if struggling with data acquisition is just part of the AI development process we all have to accept. Do bigger companies have the same problems in sourcing and finding suitable data?

If you can share any tips regarding these issues I encountered, or if you can share your experience, will be much appreciated!",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1n6swom/d_how_can_i_license_datasets/,r_1n6swom,,,
r_1n6sd4l,reddit,poppear,2025-09-02T19:02:53+00:00,"[P] csm.rs: A High-Performance Rust Implementation of Sesame's Conversational Speech Model for Real-Time Streaming TTS
Hi everyone,

I'm sharing a project I've developed, [`csm.rs`](https://github.com/cartesia-one/csm.rs), a high-performance inference implementation for Sesame's Conversational Speech Model (`sesame/csm-1b`). The project is written in Rust and built on the `candle` ML framework.

The primary goal was to create an efficient, standalone inference engine capable of real-time, streaming text-to-speech, moving beyond typical Python-based inference scripts to achieve maximum performance.",MachineLearning,16,https://www.reddit.com/r/MachineLearning/comments/1n6sd4l/p_csmrs_a_highperformance_rust_implementation_of/,r_1n6sd4l,,,
r_1n6rijz,reddit,peepee_peeper,2025-09-02T18:31:23+00:00,"[D] Building conversational AI: the infrastructure nobody talks about
Everyone's focused on models. Nobody discusses the plumbing that makes real-time AI conversation possible.

The stack I'm testing:

* STT: Whisper vs Google Speech
* LLM: GPT-4, Claude, Llama
* TTS: ElevenLabs vs PlayHT
* Audio routing: This is where it gets messy

The audio infrastructure is the bottleneck. Tried raw WebRTC (painful), looking at managed solutions like Agora, LiveKit, Daily.

Latency breakdown targets:

* Audio capture: <50ms
* STT: <100ms
* LLM: <200ms
* TTS: <100ms
* Total: <500ms for natural conversation

Anyone achieved consistent sub-500ms latency? What's your setup?",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n6rijz/d_building_conversational_ai_the_infrastructure/,r_1n6rijz,,,
r_1n6kr0d,reddit,AgeOfEmpires4AOE4,2025-09-02T14:18:44+00:00,"[P] Training environment for PS2 game RL
https://preview.redd.it/hx8od7wvfrmf1.png?width=3819&format=png&auto=webp&s=8989ff64c23e66ff7f22e4694cae88a0f192c2b5

It's alive!!! The environment I'm developing is already functional and running Granturismo 3 on PS2!!! If you want to support the development, the link is this:

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",MachineLearning,21,https://www.reddit.com/r/MachineLearning/comments/1n6kr0d/p_training_environment_for_ps2_game_rl/,r_1n6kr0d,,,
r_1n6ir0a,reddit,hakimgafai,2025-09-02T12:57:41+00:00,"[D] What apps or workflows do you use to keep up with reading AI/ML papers regularly?
Iâ€™m a postgraduate in AI, and Iâ€™m trying to build a better habit of reading papers consistently.

I wanted to ask: what tools, apps, or workflows do you personally use to track new papers and actually read them?

Curious to hear whatâ€™s worked for you in terms of discovery (finding the right papers) and sticking with the reading habit.",MachineLearning,67,https://www.reddit.com/r/MachineLearning/comments/1n6ir0a/d_what_apps_or_workflows_do_you_use_to_keep_up/,r_1n6ir0a,,,
r_1n5qjvu,reddit,Dry-Count4414,2025-09-01T14:42:22+00:00,"[D] EMNLP 2025 camera-ready page limits + virtual poster presentation
Hey folks,

My paper just got into EMNLP 2025  and Iâ€™m trying to sort out two things before the camera-ready:

1. Page limits

- ARR submission was capped at 8 pages (long paper). The acceptance email says we get +1 page for camera-ready, so Iâ€™m assuming that means 9 pages for the main text.

- Is the Limitations section required but outside this 9-page count?

- And are appendices unlimited, or do they somehow count toward the limit?



2. Virtual poster presentation

- On OpenReview Iâ€™ve already been assigned poster status. The email also says we can choose to present either in person or virtually.

Does that mean Iâ€™m free to do my poster virtually if I want?

- For those whoâ€™ve done virtual posters at EMNLP/ACL in recent years: what platform did they use (GatherTown, Zoom, something else), and how was the interaction?




Would love to hear from anyone whoâ€™s navigated this before",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n5qjvu/d_emnlp_2025_cameraready_page_limits_virtual/,r_1n5qjvu,,,
r_1n5rbwc,reddit,Even-Tour-4580,2025-09-01T15:12:10+00:00,"[P] Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones


https://preview.redd.it/d2mm661vnkmf1.png?width=3126&format=png&auto=webp&s=aa83a5002ebcba917c48d158460133701a81989a

This is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.

I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).

I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)

  
[Heedless Backbones](https://heedlessbackbones.com/)",MachineLearning,26,https://www.reddit.com/r/MachineLearning/comments/1n5rbwc/p_computer_vision_backbone_model_paperswithcode/,r_1n5rbwc,,,
r_1n6aisc,reddit,Outrageous_Tip_8109,2025-09-02T04:48:51+00:00,"[D] OpenReview website is down!
I'm trying to upload one pending AAAI review but the website is not opening. 

Anyone facing the same issue? I'm also curious what would happen if I miss the review submission deadline due to website downtime. ",MachineLearning,76,https://www.reddit.com/r/MachineLearning/comments/1n6aisc/d_openreview_website_is_down/,r_1n6aisc,,,
r_1n67lft,reddit,AutoModerator,2025-09-02T02:15:30+00:00,"[D] Self-Promotion Thread
Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/,r_1n67lft,,,
r_1n5zzln,reddit,-math-4-life-,2025-09-01T20:36:15+00:00,"[R] How hard is it to get accepted into the AAAI Student Abstract and Poster Program?
Hi everyone,

IIâ€™m considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I canâ€™t find much information about how competitive it is compared to the main technical track.

I know the main conference has a pretty low acceptance rate but AAAI doesnâ€™t seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it?

Also, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/,r_1n5zzln,,,
r_1n5tmp7,reddit,pedromnasc,2025-09-01T16:38:26+00:00,"[D] Lessons from building an AI data analyst
Hi all,

I wrote a post on some lessons from building an AI data analyst: [https://pedronasc.com/articles/lessons-building-ai-data-analyst](https://pedronasc.com/articles/lessons-building-ai-data-analyst)

The gap from a nice demo to a real production system is big -> with a lot of yet to be solved challenges.

Would love to share ideas with other builders in the space and willing to learn more about it.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/,r_1n5tmp7,,,
r_1n5t7cv,reddit,AgencyPuzzleheaded,2025-09-01T16:22:30+00:00,"[R] Latent Diffusion Question
Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting?

https://preview.redd.it/i1gtm7h3xkmf1.png?width=536&format=png&auto=webp&s=1589ad23cffc3a678eefad82750b71eefbad9962

",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/,r_1n5t7cv,,,
r_1n5r08b,reddit,AutoModerator,2025-09-01T15:00:11+00:00,"[D] Simple Questions Thread
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/,r_1n5r08b,,,
r_1n5qhr4,reddit,New-Skin-5064,2025-09-01T14:40:06+00:00,"[D] OOM When Resuming From Checkpoint
I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup:

TPU v3-8

Torch 2.7.0

jax 0.6.2

I use FSDP with SPMD

Here is some relevant code from my codebase:
Saving:
```
def save_checkpoint(model, optimizer, step, train_device_loader=None):
    # Save model weights via XLA SPMD checkpoint (supported)
    os.makedirs(f""./ckpt-{step}"", exist_ok=True)
    model_state_dict = model.module.state_dict()
    for i in model_state_dict.keys():
        xla_tensor = model_state_dict[i]
        model_state_dict[i] = xla_tensor.to(""cpu"")
        del xla_tensor
    model_sd = {""model"": model_state_dict}
    xm.save(model_sd, f""./ckpt-{step}/model.pt"")

    # Save host-only states separately (optimizer, step, RNG, dataloader)
    optim_state = optimizer.state_dict()
    optim_state_for_saving = {
        ""state"": {},
        ""param_groups"": optimizer.state_dict()[""param_groups""]
    }
    for i in optim_state[""state""]:
        optim_state_for_saving[""state""][i] = {}
        optim_state_for_saving[""state""][i][""step""] = optim_state[""state""][i][""step""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg""] = optim_state[""state""][i][""exp_avg""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg_sq""] = optim_state[""state""][i][""exp_avg_sq""].to(""cpu"")
    host_state = {
        ""optim"": optim_state_for_saving,
        ""step"": step,
    }

    if train_device_loader:
        rng_states = {
            'torch_rng_state': torch.get_rng_state(),
            'numpy_rng_state': np.random.get_state(),
            'random_rng_state': random.getstate(),
        }
        dataloader_states = {
            ""shard_order"": train_device_loader._loader.dataset.shards,
            ""local_order"": train_device_loader._loader.dataset.curr_order,
            ""warmup_order"": train_device_loader._loader.dataset.warmup_order,
            ""warmup_prob"": train_device_loader._loader.dataset.warmup_prob,
        }
    else:
        rng_states = None
        dataloader_states = None

    # Write host-side files
    with open(f""./ckpt-{step}/host_state.pkl"", ""wb"") as f:
        pickle.dump(host_state, f)
    if rng_states is not None:
        with open(f""./ckpt-{step}/rng.pkl"", ""wb"") as f:
            pickle.dump(rng_states, f)
    if dataloader_states is not None:
        with open(f""./ckpt-{step}/dataloader.json"", ""w"") as json_file:
            json.dump(dataloader_states, json_file, indent=4)
```
Loading:
```
if resume_from != """":
        model_sd = torch.load(f""{resume_from}/model.pt"", map_location='cpu')
        model.load_state_dict(model_sd[""model""])
model = model.to(device)
if gradient_checkpointing:
        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)
else:
        model = FSDPv2(module=model, mesh=mesh)
optimizer = build_optimizer(model, peak_lr, betas, weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)
if resume_from != """":
        xm.mark_step()
        # 2) Restore host-only states (optimizer, step)
        with open(f""{resume_from}/host_state.pkl"", 'rb') as f:
            host_state = pickle.load(f)
        optim_state = host_state[""optim""]
        
        # Load the processed state dict
        optimizer.load_state_dict(optim_state)
        del optim_state
        last_step = host_state[""step""]
        # 3) Restore RNG and dataloader state (if present)
        try:
            with open(f""{resume_from}/rng.pkl"", ""rb"") as f:
                rng = pickle.load(f)
            torch.set_rng_state(rng['torch_rng_state'])
            np.random.set_state(rng['numpy_rng_state'])
            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])
        except FileNotFoundError:
            pass
        with open(f'{resume_from}/dataloader.json', 'r') as file:
            dataloader = json.load(file)
```
Step:
```
for k in range(gradient_accumulation_steps):
    x, y = next(train_iter)
     with autocast(xm.xla_device(), dtype=torch.bfloat16):
          loss = model(x, y)
    (loss / gradient_accumulation_steps).backward()
     train_loss += loss.detach()
     xm.mark_step()
                
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)
                
xm.optimizer_step(optimizer, barrier=True)
                
optimizer.zero_grad()
```",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/,r_1n5qhr4,,,
r_1n5qgcd,reddit,IcarusZhang,2025-09-01T14:38:33+00:00,"[D] Proposal: Multi-year submission ban for irresponsible reviewers â€” feedback wanted
**TL;DR:** I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).

Hi everyone,

Like many of you, Iâ€™ve often felt that our review system is broken due to irresponsible reviewers. Complaints alone donâ€™t fix the problem, so Iâ€™ve written a proposal for a possible solution: **introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.**

Recent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures donâ€™t fully address the issueâ€”especially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.

In my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. Iâ€™m not a policymaker, so I expect there will be issues I havenâ€™t considered, and Iâ€™d love to hear your thoughts.

ðŸ‘‰ Read the full proposal here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).  
ðŸ‘‰ Please share whether you think this is viable, problematic, or needs rethinking.

If we can spark a constructive discussion, maybe we can push toward a better review system together.",MachineLearning,59,https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/,r_1n5qgcd,,,
r_1n5oznp,reddit,SnappierSoap318,2025-09-01T13:40:14+00:00,"[D] Why aren't there any diffusion speech to text models?
Title,

I was reading upon diffusion models and speech models and that some of the new diffusion text models are being now developed. Since we know the length of the output that a chunk of audio produces wouldn't it be possible to create a diffusion model to fill in text for the whole length all at once instead of the current auto regressive models?

PS: I am really not that advanced so this might be a dumb question.",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/,r_1n5oznp,,,
r_1n5n1v2,reddit,_puhsu,2025-09-01T12:13:29+00:00,"[R] Graph ML benchmarks and foundation models
Our team has recently published two graph ML papers: one with a new realistic benchmark and the second one on graph foundation models and how they can be related to tabular foundation models.  
  
**GraphLand benchmark**

ðŸ“ Paper:  [https://arxiv.org/abs/2409.14500](https://arxiv.org/abs/2409.14500)  
ðŸ’» Code:  [https://github.com/yandex-research/graphland](https://github.com/yandex-research/graphland) 

It is widely discussed in the community that graph machine learning suffers from the lack of realistic, meaningful, reliable, and diverse benchmarks. We agree with this and we hope that we improve this situation with our recent paper â€œGraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Dataâ€. GraphLand is a benchmark of 14 diverse graph datasets for node property prediction (both classification and regression) from different industrial applications. The datasets cover realistic machine learning problems and come with rich numerical and categorical node features that are common in real-world applications. Importantly, besides standard random splits, GraphLand provides splits with temporal distributional shifts and the inductive prediction setting, which enable evaluating GNNs in more realistic and challenging scenarios.

[GraphLand benchmark datasets.](https://preview.redd.it/nkl4qs9nnjmf1.png?width=2224&format=png&auto=webp&s=1819461078e34be3e98030c9e65ee61a7b98adc9)

We evaluated a wide range of models on GraphLand. This includes several openly available graph foundation models (GFMs), which we found provide very weak performance compared to classical GNNs.   
  
Thus, we set out to develop a better GFM, which led us to the next paper...

**Turning Tabular Foundation Models into Graph Foundation Models**

ðŸ“ Paper: [https://arxiv.org/abs/2508.20906](https://arxiv.org/abs/2508.20906)  
ðŸ’» Code: [https://github.com/yandex-research/G2T-FM](https://github.com/yandex-research/G2T-FM)

Graphs may come from very different domains and thus may have diverse features varying across datasets. As a result, one of the key challenges for GFMs is how to deal with such diverse heterogeneous features. Prior studies did not fully address this issue, often limiting themselves to text-attributed graphs or relying on simple techniques like PCA and SVD. However, this challenge is not unique to the graph domain. The tabular domain faces exactly the same issue, and recent tabular foundation models like TabPFNv2 successfully deal with it. Weâ€™ve decided to transfer their success to graphs.

[G2T-FM Framework](https://preview.redd.it/xnfsjf77ojmf1.jpg?width=1280&format=pjpg&auto=webp&s=d840e9794068202829dec2bdfa71e426198a7a15)

In our framework â€“ G2T-FM (Graph-to-Table Foundation Model) â€“ we augment the original features with graph information by computing neighborhood feature aggregations and some structure-based encodings, essentially transforming graph tasks to tabular tasks (G2T). After that, we apply TabPFNv2 to these augmented features to get predictions.

[G2T-FM Results](https://preview.redd.it/z3mz5tmaojmf1.jpg?width=1280&format=pjpg&auto=webp&s=6feb591cdd5fb1231d36c2a937ced802a27a26e7)

We evaluated G2T-FM on GraphLand and several other graph datasets and found that it shows strong performance in both in-context learning and finetuning settings. In particular, G2T-FM outperforms both well-tuned classic GNNs trained from scratch and prior publicly available GFMs.   
  
We hope our work will help develop better GFMs and highlight for the graph community the similarities of graph and tabular domains and the prospects of utilizing tabular foundation models for graph tasks!



",MachineLearning,38,https://www.reddit.com/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/,r_1n5n1v2,,,
r_1n5mcba,reddit,Fantastic-Nerve-4056,2025-09-01T11:37:07+00:00,"Recommended Cloud Service [D]
Hi there, a senior PhD fellow this side.  
Recently, I entered the LLM space; however, my institute lacks the required computing resources.  
  
Hence, my PI suggested that I opt for some cloud services, given that we have a good amount of funding available. So, can anyone recommend a decent cloud platform which, first of all, is budget-friendly, has available A100s, and most importantly, has a friendly UI to run the .ipynb or .py files

Any suggestions on it would be appreciated",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1n5mcba/recommended_cloud_service_d/,r_1n5mcba,,,
r_1n5kl6k,reddit,Deepblue597,2025-09-01T09:55:59+00:00,"[P] Beaver: A DSL for Building Streaming ML Pipelines
Hi guys!

My name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed BeaverÂ â€“ a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible.

What is Beaver?

* A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code)
* Generates Python code that integrates with theÂ [River](http://riverml.xyz/latest/)Â library for online ML and supports real-time data streams
* Includes built-in validation, analysis, and automatic dashboard generation

  
I'm making this post to ask for some feedback. Iâ€™ve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. Iâ€™d love to hear your thoughts on usability, clarity, and the overall concept.

* ðŸ“–Â [Concept overview & docs](http://deepblue597.github.io/beaver-doc/)
* ðŸ“Â [User testing instructions](https://github.com/deepblue597/beaver/blob/user_testing/user_testing.md)
* ðŸ¦«Â [Example pipeline file](https://github.com/deepblue597/beaver/blob/user_testing/examples/linear.bvr)
* ðŸ’¬Â [Feedback form](https://forms.gle/ioLVyvruJ2KCs6wd8)

Repo : [https://github.com/deepblue597/beaver](https://github.com/deepblue597/beaver)  
It is recommended to use the user\_testing branch for the feedback.   
  
Thank you so much for your time <3 ",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/,r_1n5kl6k,,,
r_1n5j2sr,reddit,Naneet_Aleart_Ok,2025-09-01T08:18:25+00:00,"[P] Improving model performance
So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.

Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. 

Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. ",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/,r_1n5j2sr,,,
r_1n55r7s,reddit,Outrageous-Travel-80,2025-08-31T20:51:50+00:00,"[R] Measuring Semantic Novelty in AI Text Generation Using Embedding Distances
We developed a simple metric to measure semantic novelty in **collaborative text generation** by computing cosine distances between consecutive sentence embeddings. 

Key finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. 

The approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task.

Some links:  
[Paper site](https://idanvidra.github.io/playing_along_paper_site/)    
[Code](https://github.com/idanvidra/Yes-And-Game-Paper)[Blog post with implementation details](https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338)

The work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (""Yes! and..."" games).",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/,r_1n55r7s,,,
r_1n55mr4,reddit,dduka99,2025-08-31T20:46:47+00:00,"[D] AAAI Review Template
Hello everyone,  
Iâ€™m serving as a first-time reviewer for AAAI and am getting ready to submit my reviews. Iâ€™m a bit uncertain about the expected structure for the different fields in the review form. For instance, in the *â€œBrief summary of your reviewâ€* field, should this be a recap of the paperâ€™s content or a short explanation of my evaluation and decision? More broadly, Iâ€™d be grateful for any guidance on how to approach the overall submission.",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/,r_1n55mr4,,,
r_1n4y2y3,reddit,pmv143,2025-08-31T15:45:47+00:00,"[D] Huaweiâ€™s 96GB GPU under $2k â€“ what does this mean for inference?
Looks like Huawei is putting out a 96GB GPU for under $2k. NVIDIAâ€™s cards with similar memory are usually $10k+. From what Iâ€™ve read, this one is aimed mainly at inference.

Do you think this could actually lower costs in practice, or will the real hurdle be software/driver support? ",MachineLearning,232,https://www.reddit.com/r/MachineLearning/comments/1n4y2y3/d_huaweis_96gb_gpu_under_2k_what_does_this_mean/,r_1n4y2y3,,,
r_1n4w3i9,reddit,PossibleTop1492,2025-08-31T14:26:35+00:00,"[R] Beating Baselines with Geometry: Introducing GMC, a Fast and Well-Calibrated Classifier
A Technical Writer's ambition to prove.

Being a Technical Writer, I yearned to learn Machine learning and prove myself. This is a try towards achieving that.  I've developed a new classifier, theÂ **Geometric Mixture Classifier (GMC)**, and I'm seeking feedback from the community before submitting it to arXiv and conferences.

**The Problem:**Â Linear models (LR, SVM) are interpretable but fail on multi-modal data. Non-linear models (RBF-SVM, MLPs) are effective but often operate as black boxes. We wanted a model that isÂ **both interpretable and expressive**.

**The Idea:**Â GMC represents each class as aÂ **mixture of hyperplanes**Â (a ""soft union of half-spaces""). It uses a soft-OR (log-sum-exp) within a class and softmax across classes. It's like a Mixture of Experts but without a separate gating network.

* **Interpretable:**Â You can see which ""local expert"" (hyperplane) was responsible for a prediction.
* **Performant:**Â Competitive with RBF-SVM, RF, and MLPs on standard benchmarks.
* **Efficient:**Â CPU-friendly, Âµs-scale inference (faster than RBF-SVM, on par with MLP).
* **Calibrated:**Â Produces reliable probabilities.

[Algorithm analogy with similar baselines](https://preview.redd.it/64vyu4u87dmf1.png?width=1385&format=png&auto=webp&s=08b2014b60836edd0b28adbac68eb388a4a091fa)

* **Accuracy:**Â Outperforms linear models, competitive with strong non-linear baselines.
* **Speed:**Â \~2-40Âµs inference time per example (see table below).
* **Calibration:**Â Low ECE, further improved with temperature scaling.

We would be incredibly grateful for any feedback on:

* Is theÂ **core idea**Â and itsÂ **differentiation from MoE/Maxout**Â clear?
* Are theÂ **experiments**Â andÂ **comparisons**Â fair and convincing?
* Is there anyÂ **related work**Â we might have overlooked?
* Any general feedback onÂ **clarity**Â orÂ **presentation**?

You can find a detailed copy of the algorithm [here](https://drive.google.com/file/d/1vRTAucCpVqImJnojVwzAUHQ2SmbAvdsi/view?usp=sharing).

Please feel free to test the algorithm: [Geometric Mixture Classifie](https://github.com/Abitsfhuusrtyt/-Geometric-Mixture-Classifier-GMC---A-Discriminative-Per-Class-Mixture-of-Hyperplanes)r",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1n4w3i9/r_beating_baselines_with_geometry_introducing_gmc/,r_1n4w3i9,,,
r_1n4ul5d,reddit,ProfessionalType9800,2025-08-31T13:22:11+00:00,"[D] Open-Set Recognition Problem using Deep learning
Iâ€™m working on a deep learning project where I have a dataset with n classes

But hereâ€™s my problem:

ðŸ‘‰ What if a totally new class comes in which doesnâ€™t belong to any of the trained classes? 

I've heard of a few ideas but would like to know many approaches:

* analyzing the embedding space: Maybe by measuring the distance of a new input's embedding to the known class 'clusters' in that space? If it's too far from all of them, it's an outlier.
* Apply Clustering in Embedding Space.

everything works based on embedding space...

are there any other approaches?",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1n4ul5d/d_openset_recognition_problem_using_deep_learning/,r_1n4ul5d,,,
r_1n4st5p,reddit,Shan444_,2025-08-31T11:57:39+00:00,"[D] My model is taking too much time in calculating FFT to find top k
so basically my batch size is 32  
d\_model is 128  
d\_ff is 256  
enc\_in = 5  
seq\_len = 128 and pred\_len is 10

I narrow downed the bottle neck and found that my FFT step is taking too much time. i canâ€™t use autocast to make f32 â†’ bf16 (assume that its not currently supported).

**but frankly its taking too much time to train. and that too total steps per epoch is 700 - 902 and there are 100 epochâ€™s.**  
roughly the FFT is taking 1.5 secs per iteration below. so

    for i in range(1,4):
         calculate FFT()
    
    

can someone help me?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n4st5p/d_my_model_is_taking_too_much_time_in_calculating/,r_1n4st5p,,,
r_1n4ppbi,reddit,Immediate-Hour-8466,2025-08-31T08:45:36+00:00,"[D] Advanced NLP with Transformers: Full talk recording and GitHub repo
**Just gave a 1.5-hour talk on ""Advanced NLP with Transformers"" covering:**

* Transformer architecture
* Prompting, RAG and fine-tuning techniques
* AI safety, security and governance challenges
* Curated papers, fellowships and resources

**Resources:** ðŸŽ¥ Recording: [https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s](https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s) ðŸ’» GitHub: [https://github.com/vgcharan/Advanced-NLP-Workshop-2025](https://github.com/vgcharan/Advanced-NLP-Workshop-2025)

Designed for researchers, students and practitioners who want conceptual depth as well as practical references. Feedback and discussion are welcome!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n4ppbi/d_advanced_nlp_with_transformers_full_talk/,r_1n4ppbi,,,
r_1n4nm4h,reddit,sourgrammer,2025-08-31T06:31:14+00:00,"[D] What is up with Tensorflow and JAX?
Hi all,

  
been in the Machine Learning world till 2021, I still mostly used the old TF 1.x interface and just used TF2.x for a short time. Last work I did was with CUDA 9.

  
It seems like quite a bit shifted with Tensorflow, I looked at the architecture again to see how much changed. To me, it's incomprehensible. Has Google shifted all efforts towards JAX, a framework with fewer layers than TF?",MachineLearning,78,https://www.reddit.com/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/,r_1n4nm4h,,,
r_1n4l73x,reddit,AdInevitable1362,2025-08-31T04:08:47+00:00,"[P] Why didnâ€™t semantic item profiles help my GCN recommender model?
Hey everyone,

Iâ€™m working on a recommender system based on a GCN model for regression task ( predicting rating score). Normally, the model initializes user and item embeddings randomly, but I wanted to improve this by following a paper ( the diagram is presented above )  that integrates semantic item profiles as initial embeddings.

Hereâ€™s what I did:
	â€¢	I generated structured item profiles with 3 parts using Gemini api : 
	â€¢	[Summarization]: short description of the business.
	â€¢	[User Preferences]: predicted/extracted types of users whoâ€™d like it.
	â€¢	[Recommendation Reasoning]: explanation for why it fits.
	â€¢	I also encoded metadata like review count and stars into natural language (e.g., review_count > 100 â†’ ""popular item"", avg_stars ~4.2 â†’ ""well-rated"").
	â€¢	I used Gemini text embeddings to encode these profiles into fixed-size embeddings.
	â€¢	Then I replaced the random item embeddings in my GCN with these semantic embeddings (after projecting them down to my modelâ€™s embedding size).

The issue:
	â€¢	When I train the GCN with these semantic embeddings, performance actually gets worse compared to just using random initialization or identical. 

Could the item profiles themselves be â€œbadâ€ ?
",MachineLearning,23,https://www.reddit.com/r/MachineLearning/comments/1n4l73x/p_why_didnt_semantic_item_profiles_help_my_gcn/,r_1n4l73x,,,
r_1n4jdo7,reddit,AutoModerator,2025-08-31T02:30:34+00:00,"[D] Monthly Who's Hiring and Who wants to be Hired?
**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/,r_1n4jdo7,,,
r_1n4dqsc,reddit,GuiltyBookkeeper4849,2025-08-30T21:56:20+00:00,"ðŸŒŸIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive ThinkingðŸŒŸ [R]
Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3-Â **Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.**

Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to ""think in rap lyrics"" or ""use bullet points to organize thoughts"" and it will literally reason that way before giving you an answer.

You can check out the model on HuggingFace:Â [https://huggingface.co/AGI-0/Art-0-8B](https://huggingface.co/AGI-0/Art-0-8B)Â (please leave a like in the repo if you like this model)

Let me know your thoughts!

P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model cardÂ [https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab](https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab)",MachineLearning,12,https://www.reddit.com/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/,r_1n4dqsc,,,
r_1n4bebi,reddit,impatiens-capensis,2025-08-30T20:14:38+00:00,"[D] NeurIPS is pushing to SACs to reject already accepted papers due to venue constraints
What are our options as a discipline? We are now at a point where 3 or more reviewers can like your paper, the ACs can accept it, and it will be rejected for no reason other than venue constraints. ",MachineLearning,398,https://www.reddit.com/r/MachineLearning/comments/1n4bebi/d_neurips_is_pushing_to_sacs_to_reject_already/,r_1n4bebi,,,
r_1n4asaq,reddit,alvises,2025-08-30T19:48:34+00:00,"[P] Building a YOLOX Plate Detector: Setup, Fine-Tuning, Metrics, Dashcam Inference
Hey all ðŸ‘‹

I just published this is end-to-end walkthrough of fine-tuning YOLOX on a \~7k-image license-plate dataset: clean environment setup, dataset prep, training & evaluation with COCO metrics (mAP/AP50-95), ONNX export, and real-world dashcam inference. Includes notes on dependency pinning (YOLOXâ€™s older stack), small script fixes, and a side-by-side comparison with an Ultralytics YOLO11 model trained on the same data. Results are on par once everything is configured correctly.

Here's the post where you find the code and commands: [https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/](https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/)

YOLOX github repo: [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)

Roboflow car plates dataset: [https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e](https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e)

",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1n4asaq/p_building_a_yolox_plate_detector_setup/,r_1n4asaq,,,
r_1n3i2fx,reddit,TaxPossible5575,2025-08-29T20:18:02+00:00,"[D] Scaling Inference: Lessons from Running Multiple Foundation Models in Production
Weâ€™ve been experimenting with deploying a mix of foundation models (LLaMA, Mistral, Stable Diffusion variants, etc.) in a single platform. One of the recurring pain points is **inference optimization** at scale:

* **Batching tradeoffs**: Batching reduces cost but can kill latency for interactive use cases.
* **Quantization quirks**: Different levels (INT8, FP16) affect models inconsistently. Some speed up 4Ã—, others break outputs.
* **GPU vs. CPU balance**: Some workloads run shockingly well on optimized CPU kernels â€” but only for certain model families.

Curious how others have approached this.

* Whatâ€™s your go-to strategy for **latency vs throughput tradeoffs**?
* Are you using **model distillation** or sticking to quantization?
* Any underrated **libraries or frameworks** for managing multi-model inference efficiently?",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n3i2fx/d_scaling_inference_lessons_from_running_multiple/,r_1n3i2fx,,,
r_1n3nfye,reddit,Mountain_Reward_1252,2025-08-30T00:07:36+00:00,"Is Isolation Forest ideal for real-time IMU-based anomaly detection? Open to better alternatives [P]
Hey folks,

Iâ€™m working on a project involving real-time anomaly detection using IMU data from a mobile robot (acc_x, acc_y, acc_z, magnitude). The goal is to detect small disturbances (e.g., bumping into wires or obstacles) based on sensor changes.

I trained an Isolation Forest model on normal motion data and integrated it into a ROS 2 node using the .decision_function() threshold for runtime detection.

It works, but Iâ€™m worried about false positives, especially with fixed contamination. Since this will later run on embedded IMU hardware, Iâ€™m looking for something accurate and lightweight.

Is Isolation Forest reliable for this?
Any better algorithms youâ€™d recommend (e.g., LOF, One-Class SVM, AE)? Would love to hear your thoughts or experience.

Thanks!",MachineLearning,17,https://www.reddit.com/r/MachineLearning/comments/1n3nfye/is_isolation_forest_ideal_for_realtime_imubased/,r_1n3nfye,,,
r_1n3iiam,reddit,Unlikeghost,2025-08-29T20:35:35+00:00,"[D] Working with Optuna + AutoSampler in massive search spaces
Hi!
Iâ€™m using Optuna with AutoSampler to optimize a model, but the search space is hugeâ€”around 2 million combinations.

Has anyone worked with something similar?
Iâ€™m interested in learning which techniques have worked for reducing the search space.",MachineLearning,11,https://www.reddit.com/r/MachineLearning/comments/1n3iiam/d_working_with_optuna_autosampler_in_massive/,r_1n3iiam,,,
r_1n3gfpt,reddit,Immediate-Cake6519,2025-08-29T19:13:34+00:00,"[P] Open-Source Protocol designed for Multi-Agent Communication
[Project](https://www.reddit.com/r/MachineLearning/?f=flair_name%3A%22Project%22)

OSS ReleasedÂ **MAPLE â€“ a Multi Agent Protocol Language Engine**Â designed for fast, secure, and reliable agent communication.

â€” a newÂ **open-source protocol**Â designed forÂ **multi-agent communication**Â atÂ **production scale**.

**MAPLE**Â offers features we haven't seen in other protocols:

ðŸ”§Â **Integrated Resource Management:**Â TheÂ **ONLY**Â protocol with built-in resource specification, negotiation, and optimization

ðŸ›¡ï¸Â **Link Identification Mechanism (LIM):**Â Revolutionary security through verified communication channels

âš¡Â **Result<T,E> Type System:**Â ELIMINATES all silent failures and communication errors

ðŸŒÂ **Distributed State Synchronization:**Â SophisticatedÂ **state management**Â across agent networks

ðŸ­Â **Production-Grade Performance:**Â **Very high**Â **performance**Â for aÂ **feature-rich**Â protocol withÂ **sub-millisecond**Â latency

ðŸ’»Â **pip install maple-oss**

PyPI here:Â [https://pypi.org/project/maple-oss/](https://pypi.org/project/maple-oss/)

If youâ€™re building with agents or need robust, real-world communication between systems,  
check outÂ **MAPLE GitHub**Â repo:Â [https://github.com/maheshvaikri-code/maple-oss](https://github.com/maheshvaikri-code/maple-oss)

Please try and test it with your projects.

[MAPLE Multi Agent Communication Protocol](https://preview.redd.it/bovnmzoqc0mf1.png?width=256&format=png&auto=webp&s=6563e85b9f830d36a244cbf9783bc05ba45ed8c3)

",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/,r_1n3gfpt,,,
r_1n3g1p7,reddit,DenOmania,2025-08-29T18:58:40+00:00,"[D] How do we make browser-based AI agents more reliable?
Iâ€™ve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big:

1. Sessions break after login or CAPTCHA
2. Agents fail when sites change structure
3. Security is hard to guarantee at scale
4. Each framework has its own dialect / quirks


Recently Iâ€™ve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. 

But then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?",MachineLearning,35,https://www.reddit.com/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/,r_1n3g1p7,,,
r_1n3e27s,reddit,bci-hacker,2025-08-29T17:42:40+00:00,"[D] Upcoming interviews at frontier labs, tips?
Hi all,

Iâ€™m currently interviewing at a few labs for MLE positions and thereâ€™s two interviews in particular that have stumped me that Iâ€™d like some clarity on:

1. Transformer debugging - to my knowledge, the interviewer will provide a buggy implementation of things like causal attention, self-attention, incorrect layer norm, scaling issues, and broadcast/shape mismatch. Is there anything else Iâ€™d need to master here? So far, Iâ€™ve only been studying GPT style transformers, should I add BERT to the mix or nah?
2. Training classifier & data analysis. The recruiter said this is around evaluation and model performance. Iâ€™m guessing theyâ€™ll throw me an unbalanced dataset and ask me to improve model performance somehow. Things to study here are: 1) chip hguyns book and 2) look at regularization, pandas/sklearn normalization and data clean up methods. How else can I master this topic? Any sample questions you have seen here before?

Lastly, what is your go-to source for practicing MLE related topics, both in terms of knowledge-base as well as real interview questions. I tried 1point3acres but very limited when it comes to ML.",MachineLearning,105,https://www.reddit.com/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/,r_1n3e27s,,,
r_1n38fr0,reddit,Suitable-Director809,2025-08-29T14:08:14+00:00,"Finetuning Vision Transformers [D]
Hey, 
Looking to see how DinoV3 will do on my dataset post finetuning. 

Any practical advice on finetuning Dino? 
Scheduler, optimizer, flow - freezing, discriminative lr etc. 
Any recommandations for blogs or articals related to this? ",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/,r_1n38fr0,,,
r_1n37qnu,reddit,AnyIce3007,2025-08-29T13:40:08+00:00,"[D] ollama/gpt-oss:20b can't seem to generate structured outputs.
I'm experimenting with `""ollama/gpt-oss:20b""`'s capability to generate structured outputs. For example, I used it to evaluate against GSM8K dataset. The schema is as follows: `answer`: for the answer, and `solution`: for the CoT solution. However, it doesn't make sense that for a 20B model, it cannot generate a valid structured output.

Any thoughts or hacks on this one? I would appreciate it. Thanks.",MachineLearning,11,https://www.reddit.com/r/MachineLearning/comments/1n37qnu/d_ollamagptoss20b_cant_seem_to_generate/,r_1n37qnu,,,
r_1n30p1v,reddit,JollySimple188,2025-08-29T07:15:10+00:00,"How are teams handling small dataset training for industrial vision inspection?[P]
We're evaluating different approaches for vision-based defect detection where getting large labeled datasets is challenging. Lots of methods need thousands of examples, but some defects are rare (maybe 10-20 examples total in 6 months). Anyone working with similar constraints? I've been looking into platforms that can work with smaller datasets - curious what others are doing?",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n30p1v/how_are_teams_handling_small_dataset_training_for/,r_1n30p1v,,,
r_1n2rvvh,reddit,eh-tk,2025-08-28T23:37:03+00:00,"[R] Technical Skills Analysis of Machine Learning Professionals in Canada
I manage a slack community of a couple hundred ML devs in Canada. I got curious and ran some numbers on our members to see if any interesting insights emerged. Here's what I found:

**The ""Pandemic ML Boom"" Effect**:  
Nearly 40% of members started an ML specific role between 2020-2022. 

**RAG and Vector Database Expertise**:  
Over 30% of members have hands-on experience with Retrieval-Augmented Generation systems and vector databases (Pinecone, Weaviate, ChromaDB), representing one of the hottest areas in enterprise AI.

â€**Multi-modal AI Pioneers**:  
A significant portion of members work across modalities (vision + text, audio + text).

**Most Common JobÂ Titles**:

15% of members hold senior leadership roles (Principal, Staff, Director, CTO level), demonstrating strong senior representation within the community.

**ML-Engineering Bridge Roles**:

Over 35% of members hold hybrid titles that combine ML with other disciplines:Â ""MLOps Engineer,"" ""Software Engineer, ML,"" ""AI & Automation Engineer,"" ""Conversational AI Architect,"" and ""Technical Lead, NLP"".

You can see the full breakdown here: [https://revela.io/the-collective](https://revela.io/the-collective)",MachineLearning,73,https://www.reddit.com/r/MachineLearning/comments/1n2rvvh/r_technical_skills_analysis_of_machine_learning/,r_1n2rvvh,,,
r_1n2pku5,reddit,AgeOfEmpires4AOE4,2025-08-28T21:58:17+00:00,"[P] Training environment for RL of PS2 and other OpenGL games
Hello everyone. I'm working on a training environment based on stable-retro and a Retroarch frontend, Sdlarch. This environment is intended to support PS2, GameCube, Dreamcast, and other video games that aren't supported by the original Stable-retro/Gym-Retro. If anyone wants to support me, or is curious, the link is below:

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)

There's still a lot of work ahead, as I'm implementing the final phase that enables PS2 training: loading states. For some reason I don't yet fully understand, the save state isn't loading (it just saves). But it's now possible to run games in the environment via Python, without the need to intercept any external processes.",MachineLearning,15,https://www.reddit.com/r/MachineLearning/comments/1n2pku5/p_training_environment_for_rl_of_ps2_and_other/,r_1n2pku5,,,
r_1n2jekd,reddit,erfan_mhi,2025-08-28T17:58:57+00:00,"[R] [EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States â€” Superior Calibration Performance Across Benchmarks/Models
Hi everyone,

Our paper **â€œ*****Confidence from Consistency under Perturbation of States (CCPS)*****â€** was accepted to the **EMNLP 2025 Main Conference**, placing in the **top 15% of accepted papers** with a **final meta-review rating of 9 (strong accept)**.

# ðŸ” Motivation

LLMs donâ€™t just make mistakes, theyâ€™re often confidently wrong. Thatâ€™s fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.

# âœ¨ What is CCPS?

CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:

* If the answer remains stable â†’ the model was truly confident.
* If the answer flips â†’ the confidence was unreliable.

This approach is simple, efficient, and does not require fine-tuning the base LLM.

# ðŸ“Š Results

Across LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):

* **Calibration**: Error cut by more than 50%, down to \~4.5% on the toughest benchmarks.
* **Discrimination**: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).
* **Performance**: Boosts accuracy and robustness, all without fine-tuning the base LLM.

# ðŸ’¡ Why it matters

CCPS delivers more reliable, better-calibrated LLMs, models that donâ€™t just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.

# ðŸ“Ž Resources

* ðŸ“„ Paper: [arXiv link](https://arxiv.org/abs/2505.21772)
* ðŸ’» Code: [GitHub repo](https://github.com/ledengary/CCPS)
* ðŸ“Š Data: [HF Dataset](https://huggingface.co/datasets/ledengary/CCPS)

Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/,r_1n2jekd,,,
r_1n2i7iy,reddit,Stunning_Put_6077,2025-08-28T17:14:30+00:00,"[R] â€œHow Iâ€™m structuring a 16M character dialogue corpus for persona reconstruction in LLMsâ€
In the past weeks, Iâ€™ve been working on a somewhat â€œcrazyâ€ project:
manually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module.

Along the way, Iâ€™ve noticed a few technical challenges:
	1.	File size balance
Keeping each file around 300kâ€“400k characters is the most stable. Beyond that, performance tends to drop.
	2.	Context continuity
Poor segmentation can easily break the modelâ€™s sense of persona, resulting in inconsistent tone.
	3.	Tagging & classification
Itâ€™s not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild â€œmemoryâ€ in a coherent way.

This made me realize that large-scale corpus curation is itself a kind of language engineering.
Itâ€™s not just data processing â€” it shapes whether an AI can emerge as a whole presence.

Iâ€™m curious:
In your NLP or LLM practice, how do you balance scale with contextual integrity?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/,r_1n2i7iy,,,
r_1n2gdd4,reddit,Pan000,2025-08-28T16:05:37+00:00,"[R] Adding layers to a pretrained LLM before finetuning. Is it a good idea?
I'm doing a full fine-tune on the Qwen 3 14B Base model with around 10B tokens for loss. I'd have preferred a little higher capacity. My idea is to add a few more layers at the end, initialized close to zero, and then train. Perhaps increase from 40 to 50 layers.

This is straightforward to implement. Is there a reason why I don't hear of this being done? Is anyone familiar with this? Any research indicating success or failure? It makes sense conceptually but I would assume it would be more common if it works.

(I asked the GPT5, Gemini Pro & Claude, but I'm getting mixed answers. It'll agree or disagree depending how I phrase the question.)",MachineLearning,12,https://www.reddit.com/r/MachineLearning/comments/1n2gdd4/r_adding_layers_to_a_pretrained_llm_before/,r_1n2gdd4,,,
r_1n2c588,reddit,Fragrant-Dog-3706,2025-08-28T13:24:31+00:00,"[D] Where to find vast amounts of schemas for AI model training?
**\[D\] Looking for massive schema collections for training models**

working on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/,r_1n2c588,,,
r_1n29q0e,reddit,Knok0932,2025-08-28T11:31:15+00:00,"[P] PaddleOCRv5 implemented in C++ with ncnn
I made a C++ implementation of PaddleOCRv5 that might be helpful to some people: https://github.com/Avafly/PaddleOCR-ncnn-CPP

The official Paddle C++ runtime has a lot of dependencies and is very complex to deploy. To keep things simple I use [ncnn](https://github.com/Tencent/ncnn) for inference, it's much lighter (and faster in my task), makes deployment easy. The code runs inference on the CPU, if you want GPU acceleration, most frameworks like ncnn let you enable it with just a few lines of code.

Hope this helps, and feedback welcome!",MachineLearning,15,https://www.reddit.com/r/MachineLearning/comments/1n29q0e/p_paddleocrv5_implemented_in_c_with_ncnn/,r_1n29q0e,,,
r_1n28w7j,reddit,c-f_i,2025-08-28T10:46:50+00:00,"[P] Built Sparrow: A custom language model/NLP tool for microcontrollers
Hey everyone,

Don't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model.

I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.

The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.

The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.

I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.

It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.

Let me know what you think!Â [Here's a demo video](https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8)Â with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc.

I have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.",MachineLearning,9,https://www.reddit.com/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/,r_1n28w7j,,,
r_1n1mboq,reddit,Good-Alarm-1535,2025-08-27T16:48:26+00:00,"[P] Implemented GRPO on top of Karpathy's makemore
Hey all! I wanted to share my recent project where I implemented the GRPO (Group Relative Policy Optimization) algorithm on top of the [makemore](https://github.com/karpathy/makemore) repo.

I wanted to understand how the algorithm works and was trying to find small-scale toy problems where I can implement my own version and see if it works. I had a couple of ideas at first but then I settled on this one idea: to implement the algorithm on top of the makemore project where my goal would be to finetune the character-level language model to generate names with more vowels! So the reward is essentially the number of vowels you have in the generated names.

GRPO is actually a simplified version of PPO (which itself is a derivative of TRPO), and while its predecessors are rather complicated to fully grasp unless you have some background in policy gradient or RL in general, GRPO is much simpler to understand and code up (e.g., you don't have to worry about writing Generalized Advantage Estimation etc.)

Feel free to take a look and share your thoughts! Here's the repo: [https://github.com/souvikshanku/makemore-grpo/](https://github.com/souvikshanku/makemore-grpo/)",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1n1mboq/p_implemented_grpo_on_top_of_karpathys_makemore/,r_1n1mboq,,,
r_1n2579o,reddit,AdInevitable1362,2025-08-28T06:51:52+00:00,"[D] Clarification on text embeddings models
I came across Geminiâ€™s text embeddings model, and their documentation mentions that semantic similarity is suitable for recommendation tasks. They even provide this example:
	â€¢	â€œWhat is the meaning of life?â€ vs â€œWhat is the purpose of existence?â€ â†’ 0.9481
	â€¢	â€œWhat is the meaning of life?â€ vs â€œHow do I bake a cake?â€ â†’ 0.7471
	â€¢	â€œWhat is the purpose of existence?â€ vs â€œHow do I bake a cake?â€ â†’ 0.7371

What confuses me is that the â€œcakeâ€ comparisons are still getting fairly high similarity scores, even though the topics are unrelated.

If semantic similarity works like this, then when I encode product profiles for my recommendation system, wonâ€™t many items end up â€œtoo closeâ€ in the embedding space? Does all the text embeddings model work that way ? 
And what is the best model or type of configuration could be suitable to my task ",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n2579o/d_clarification_on_text_embeddings_models/,r_1n2579o,,,
r_1n23r3t,reddit,Lonely-Loquat9638,2025-08-28T05:21:15+00:00,"[R] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies
**TL;DR.**Â We introduceÂ **discrete diffusion**Â as the action decoderÂ **inside a single transformer**Â for VLA. Two simple componentsâ€”Adaptive decoding order and Secondary re-maskingâ€”yield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with theÂ **same cross-entropy objective**Â as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion.  
**Disclosure:**Â Iâ€™m an author.

**Whatâ€™s new**

* **First discrete-diffusion action head for VLA**Â (to our knowledge).
* **Single-transformer, VLM-style training:**Â keeps the discrete token interface and uses the same CE loss as the VLM backbone â†’Â **maximizes retention of pretrained VLM priors**.
* **Adaptive decoding order:**Â in each refinement round, weÂ **keep easy tokens first**Â via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round.
* **Secondary re-masking:**Â previously kept tokens areÂ **re-checked**Â (threshold + residual-drop) andÂ **re-masked**Â if uncertain/inconsistent, enabling robust cross-round error correction.

**Why it matters**

* For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. ThisÂ **preserves the backboneâ€™s pretrained vision-and-language capability**â€”akin to extending a vocabularyâ€”while opening a path toÂ **inherit unified transformersâ€™ scaling behavior**, paving the way forÂ **large-scale VLA**. Moreover, Discrete Diffusion VLAÂ **breaks the left-to-right bottleneck**Â of AR decoders: action chunks areÂ **adaptively decoded in parallel**Â over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement.

**Links**

* Paper:Â [https://arxiv.org/abs/2508.20072](https://arxiv.org/abs/2508.20072)
* Demo videos:Â [https://huggingface.co/papers/2508.20072](https://huggingface.co/papers/2508.20072)",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/,r_1n23r3t,,,
r_1n1wm8n,reddit,Adventurous-Cut-7077,2025-08-27T23:27:26+00:00,"[N] Unprecedented number of submissions at AAAI 2026
And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.

https://preview.redd.it/ih3vliracnlf1.png?width=1938&format=png&auto=webp&s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be",MachineLearning,188,https://www.reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/,r_1n1wm8n,,,
r_1n1ug7b,reddit,Material_Pool_986,2025-08-27T21:56:41+00:00,"[P] jupytercad-mcp: MCP server for JupyterCAD to control it using LLMs/natural language.
Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7

Repo: https://github.com/asmith26/jupytercad-mcp",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n1ug7b/p_jupytercadmcp_mcp_server_for_jupytercad_to/,r_1n1ug7b,,,
r_1n1tdcl,reddit,OkOwl6744,2025-08-27T21:14:49+00:00,"Arxiv submission on hold  [R]
Hey 
Looking for information online about the on hold status but couldnâ€™t find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? 

I already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. Itâ€™s my first publication there, so Iâ€™m not sure what the process is exactly. 

Thanks! ",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/,r_1n1tdcl,,,
r_1n1pcj7,reddit,AlanzhuLy,2025-08-27T18:40:00+00:00,"[D] Anyone successfully running LLMs fully on Apple Neural Engine (ANE)?
Has anyone managed to get near-full ANE utilization for large language models on Apple silicon?

In my experiments:

* Core ML conversions run, but ANE usage seems capped <20%.
* Appleâ€™s own foundation models reportedly hit close to 100% ANE.

**Questions:**

* Has anyone here seen full (or close to full) ANE usage for LLMs?
* Are there known tricks or constraints (model architecture, quantization, Core ML flags) that unlock more ANE execution?
* Any open-source repos, discussions, or Apple docs youâ€™d point to?

Would love to hear practical experiencesâ€”successes, failures, or hard limits youâ€™ve hit.",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n1pcj7/d_anyone_successfully_running_llms_fully_on_apple/,r_1n1pcj7,,,
r_1n1p7rb,reddit,function-devs,2025-08-27T18:35:12+00:00,"[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.
I reviewed 100 models over the past 30 days. Here are 5 things I learnt.

TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.

So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.

100 models. 30 days. Too much coffee :( Here's what I found:  
  
1. Model moats are ephemeral

Model moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc

My system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.

2. Open source FTW

The gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.

3.Benchmarks are mostly decieving due to reward hacking

Benchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored ""high"" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.

4.Inference speed is everything

Speed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..

5.Task-specific models > general purpose models for specialized work.

No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.

Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.

What models are in your current prod stack? Any hidden gems I missed in the open source space?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/,r_1n1p7rb,,,
r_1n1k9ty,reddit,AdventurousSwim1312,2025-08-27T15:32:11+00:00,"[R] ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples
[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&format=png&auto=webp&s=c951d5647cd895d418b5a0863184cf9f6745397e)



Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.



One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.



On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.

For that purpose, i create ArchiFactory, a simple (<500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.



Included:

\- simple modular architecture to be sure to compare similar stuff

\- complete optimized training loop using pytorch lightning

\- fp8 training (can achieve <20min training on 5090 grade GPU)

\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.

\- guidelines to test integrate new modules



Link: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory)",MachineLearning,20,https://www.reddit.com/r/MachineLearning/comments/1n1k9ty/r_archifactory_benchmark_slm_architecture_on/,r_1n1k9ty,,,
r_1n1gucy,reddit,kekkodigrano,2025-08-27T13:19:30+00:00,"[D] How to do impactful research as a PhD student?
Hi everyone,

Iâ€™m feeling a bit lost in my PhD journey and would really appreciate some outside perspectives.

Iâ€™m doing a PhD on LLMs, and so far Iâ€™ve been fairly productive: Iâ€™ve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. Iâ€™ve also had a few successful collaborations.

The issue is that I donâ€™t actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles:

- Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters.

- While Iâ€™m trying to sort this out, thereâ€™s always the risk that someone else publishes a similar idea first, since so many people are working in this space.

- I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months.


Because of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if theyâ€™re not especially deep or important. I also see publications as my main leverage: since Iâ€™m at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies.

At times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important.

So Iâ€™m curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?",MachineLearning,135,https://www.reddit.com/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/,r_1n1gucy,,,
r_1n1fsa3,reddit,FreakedoutNeurotic98,2025-08-27T12:34:20+00:00,"[D] short write up on how to implement custom optimizers in Optax
Hi, I was trying to implement the muon optimizer in JAX and found there was no proper documentation about how to hack optax for custom optimizers so tried to write a mini blog about it.

https://slavozard.bearblog.dev/implementcustomoptimizerwithoptax/

Feedback appreciated.",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n1fsa3/d_short_write_up_on_how_to_implement_custom/,r_1n1fsa3,,,
r_1n1ebmk,reddit,Any_Commercial7079,2025-08-27T11:22:40+00:00,"[R] Computational power needs for Machine Learning/AI
Hi everyone!

As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), Iâ€™d greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. HereÂ´s the link:Â [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/,r_1n1ebmk,,,
r_1n1e9c1,reddit,Altruistic_Bother_25,2025-08-27T11:19:31+00:00,"[R] Is stacking classifier combining BERT and XGBoost possible and practical?
Suppose a dataset has a structured features in tabular form but in one column there is a long text data. Can we use stacking classifier using boosting based classifier in the tabular structured part of the data and bert based classifier in the long text part as base learners. And use logistic regression on top of them as meta learner. I just wanna know if it is possible specially using the boosting and bert as base learners. If it is possible why has noone tried it (couldnâ€™t find paper on it)â€¦ maybe cause it will probably be bad?",MachineLearning,20,https://www.reddit.com/r/MachineLearning/comments/1n1e9c1/r_is_stacking_classifier_combining_bert_and/,r_1n1e9c1,,,
r_1n12su6,reddit,unknown,2025-08-27T00:33:39+00:00,"[P] Building a CartPole agent from scratch, in C++
Iâ€™m still pretty new to reinforcement learning (and machine learning in general), but I thought it would be fun to try building my own CartPole agent from scratch in C++.

It currently supports PPO, Actor-Critic, and REINFORCE policy gradients, each with Adam and SGD (with and without momentum) optimizers.

I wrote the physics engine from scratch in an Entity-Component-System architecture, and built a simple renderer using SFML.

Repo: www.github.com/RobinLmn/cart-pole-rl

Would love to hear what you think, and any ideas for making it better!",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1n12su6/p_building_a_cartpole_agent_from_scratch_in_c/,r_1n12su6,,,
r_1n127sr,reddit,ChoiceStranger2898,2025-08-27T00:06:40+00:00,"Are Neurips workshop competitive? [R]
Hi yâ€™all, I have a optimisation paper that is not quite ready for conference yet, and I see there are a few Neurips workshop coming up that fits my research direction. Iâ€™m wondering if itâ€™s good to submit the work to the workshop?",MachineLearning,15,https://www.reddit.com/r/MachineLearning/comments/1n127sr/are_neurips_workshop_competitive_r/,r_1n127sr,,,
r_1n10vyv,reddit,SoggyClue,2025-08-26T23:08:01+00:00,"[D] Tips & tricks for preparing slides/talks for ML Conferences?
I'm a PhD student in HCI, and I recently had a paper accepted at a B-ranked ML conference. While I have prior experience presenting at HCI venues, this will be my first time presenting at an ML conference.

I want to know if there are any tips or best practices for preparing slides and giving talks in the ML community. Are there particular presentation styles, slide formats, or expectations that differ from HCI conferences?

Thanks in advance for your advice!",MachineLearning,10,https://www.reddit.com/r/MachineLearning/comments/1n10vyv/d_tips_tricks_for_preparing_slidestalks_for_ml/,r_1n10vyv,,,
r_1n0zndc,reddit,SwissMountaineer,2025-08-26T22:15:24+00:00,"[D] Laptop Suggestion for PhD in ML for Robotics
Hi!

I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.

My budget is $3000.

So far, I have identified the following laptops, but am unsure which would be the best choice.

\-Â **Razer Blade 16**Â (either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \~$850 (with taxes) which is significant.

\-Â **MSI Vector 16Â HXÂ AI**Â (RTXâ€¯5080, 64â€¯GB) - $2600

\-Â **ThinkPad P1 Gen 7**Â (RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.

\-Â **Legion Pro 7i Gen10**Â (RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.

Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?

Thank you in advance for any suggestions or feedback!

EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/,r_1n0zndc,,,
r_1n0wdsi,reddit,AaronSpalding,2025-08-26T20:08:04+00:00,"[R] What makes active learning or self learning successful ?
Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.

A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.

My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n0wdsi/r_what_makes_active_learning_or_self_learning/,r_1n0wdsi,,,
r_1n0vcrb,reddit,JustinAngel,2025-08-26T19:28:44+00:00,"[R] Î”APT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy
Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)

Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. 

**1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. ""empathy"" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. 

**2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 

3. **Î”APT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). 

https://preview.redd.it/rof96tmbuelf1.png?width=1162&format=png&auto=webp&s=5a1e81bbb9e8b12b09210967da97b2fe96816df0

  
**4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. 

https://preview.redd.it/lbcoovvc0flf1.png?width=2246&format=png&auto=webp&s=f029fed00649b4cca0ddb84d9830ded03f5f94ea

5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples ""oh wow, hallucinations are going to make AI therapy impossible"". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. 

**6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.

Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. ",MachineLearning,117,https://www.reddit.com/r/MachineLearning/comments/1n0vcrb/r_Î´apt_critical_review_aimed_at_maximizing/,r_1n0vcrb,,,
r_1n0t4hu,reddit,Look-Asleep,2025-08-26T18:03:23+00:00,"[D] Do Industry Research Roles Care about Findings vs. Main (in ACL, NAACL, EMNLP, etc.)?
Basically the title. Obviously the quality of the work and relevance to the role is very important, but all else being equal, what is the perceived prestige difference between Findings and Main in NLP conferences? This would be with regard to getting research internships and research scientist positions.",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n0t4hu/d_do_industry_research_roles_care_about_findings/,r_1n0t4hu,,,
r_1n0r8b7,reddit,FutureIncrease,2025-08-26T16:54:16+00:00,"I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R]
**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.

**Links:**

* [Live dashboard](https://tokka-bench.streamlit.app/)
* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)
* [GitHub repo](https://github.com/bgub/tokka-bench)

https://preview.redd.it/7i03jela9elf1.png?width=1724&format=png&auto=webp&s=95378457970e6337b147e71d7a8f0ab2dd67cb91

# The Problem Nobody Talks About

I started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:

**UTF-8 encoding differences:**

* English: \~1 byte per character
* Arabic: 2+ bytes per character
* Chinese: 3+ bytes per character

**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.

# Why This Affects Performance

**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.

**During inference:** Low-resource languages need 2-3x more tokens per sentence:

* Slower throughput (costs more to serve)
* Context windows fill up faster
* More chances to mess up during generation

# What I Built

tokka-bench measures four key things:

1. **Efficiency** \- bytes per token (compression quality)
2. **Coverage** \- unique tokens used (script representation)
3. **Word splitting** \- how often semantic units get fragmented
4. **Subword fertility** \- average tokens per semantic unit

# Interesting Findings

You can actually reverse-engineer training data from tokenizer performance:

* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)
* Gemma 3: Strong Urdu/Hindi performance
* gpt-oss: Good Arabic/Gujarati coverage

Weirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.

# Technical Details

Built on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.

Shoutout to Judit Ãcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.

**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.

*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*",MachineLearning,82,https://www.reddit.com/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/,r_1n0r8b7,,,
r_1n0qwzm,reddit,beautiful-potato,2025-08-26T16:42:45+00:00,"[D] Analyzed 402 healthcare ai repos and built the missing piece
I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.

Built HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.

Post + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4

Code: https://github.com/dotimplement/HealthChain

Anyone else work in healthcare AI here? Would love to learn what youâ€™re working on!",MachineLearning,10,https://www.reddit.com/r/MachineLearning/comments/1n0qwzm/d_analyzed_402_healthcare_ai_repos_and_built_the/,r_1n0qwzm,,,
r_1n0q4d9,reddit,devops_to,2025-08-26T16:13:01+00:00,"[D] Looking for a self-hosted alternative to Modal.com for running ML workloads
Hey folks 



I've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.



Does anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:



\- Supports containerized jobs (Docker or similar)

\- Can run Python/ML workloads easily

\- Has a nice API  for launching jobs (this is important) 

\- Offers some kind of job orchestration or scheduling

\- Bonus: GPU support and autoscaling would be amazing





Thanks in advance 

",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1n0q4d9/d_looking_for_a_selfhosted_alternative_to/,r_1n0q4d9,,,
r_1n0njtk,reddit,illustriousplit,2025-08-26T14:35:44+00:00,"[R] Exploring interpretable ML with piecewise-linear regression trees (TRUST algorithm)
A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.

Weâ€™ve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.

In our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results â€” TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests â€” we'd like to hear your thoughts on this topic.

The problem weâ€™re tackling is widespread. In many real-world applications, a ""black box"" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.

Hereâ€™s a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).

[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&format=png&auto=webp&s=e289771608b0d74498dc83b39c1efd2670ed8ea9)

As the image above shows, both TRUST and a Random Forest achieve \~85% test RÂ² â€” but one produces a **single interpretable tree**.

TRUST is implemented as a free Python package on PyPI called `trust-free`.

**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? Weâ€™re looking forward to hearing your perspectives.",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1n0njtk/r_exploring_interpretable_ml_with_piecewiselinear/,r_1n0njtk,,,
r_1n0jxbk,reddit,Total_Noise1934,2025-08-26T12:02:41+00:00,"[P] Spam vs. Ham NLP Classifier â€“ Feature Engineering vs. Resampling
I built a spam vs ham classifier and wanted to test a different angle: instead of just oversampling with SMOTE, could **feature engineering** help combat extreme class imbalance?

**Setup:**

* Models: NaÃ¯ve Bayes & Logistic Regression
* Tested with and without SMOTE
* Stress-tested on 2 synthetic datasets (one â€œnormal but imbalanced,â€ one â€œadversarialâ€ to mimic threat actors)

**Results:**

* Logistic Regression â†’ **97% F1** on training data
* New imbalanced dataset â†’ Logistic still best at **75% F1**
* Adversarial dataset â†’ **NaÃ¯ve Bayes** surprisingly outperformed with **60% F1**

**Takeaway:** Feature engineering can mitigate class imbalance (sometimes rivaling SMOTE), but adversarial robustness is still a big challenge.

Code + demo:  
ðŸ”— [PhishDetective Â· Streamlit](https://phishdetective.streamlit.app/)  
ðŸ”— [ahardwick95/Spam-Classifier: Streamlit application that classifies whether a message is spam or ham.](https://github.com/ahardwick95/Spam-Classifier/tree/main)

Curious â€” when you deal with **imbalanced NLP tasks**, do you prefer resampling, cost-sensitive learning, or heavy feature engineering?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n0jxbk/p_spam_vs_ham_nlp_classifier_feature_engineering/,r_1n0jxbk,,,
r_1n0jwj7,reddit,LostAmbassador6872,2025-08-26T12:01:41+00:00,"[P] DocStrange - Structured data extraction from images/pdfs/docs
I previously shared the openâ€‘source library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.

**Live Demo:**Â [**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)

**Github:** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)

Would love to hear feedbacks!

https://i.redd.it/gl23k00osclf1.gif

Original Post - [https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p\_docstrange\_open\_source\_document\_data\_extractor/](https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/)

",MachineLearning,27,https://www.reddit.com/r/MachineLearning/comments/1n0jwj7/p_docstrange_structured_data_extraction_from/,r_1n0jwj7,,,
r_1n0j8u0,reddit,Adrienkgz,2025-08-26T11:28:41+00:00,"[D] Ano: updated optimizer for noisy Deep RL â€” now on arXiv (feedback welcome!)
Hi everyone,

A few weeks ago I shared my first preprint on a new optimizer,Â Ano, designed for noisy and highly non-convex environments such as deep RL. Thanks to all the feedback I received here, Iâ€™ve updated the paper: clarified the positioning, fixed some mistakes, and added an Atari benchmark to strengthen the empirical section.

ðŸ”—Â **arXiv link:**Â [https://arxiv.org/abs/2508.18258](https://arxiv.org/abs/2508.18258)  
ðŸ“¦Â **Install via pip:**Â `pip install ano-optimizer`  
ðŸ’»Â **Code & experiments:**Â [github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)

Quick recap of the idea: Ano separates the momentumÂ directionÂ from the gradient magnitude, aiming to improve robustness and stability compared to Adam in noisy deep RL training. The updated version also includes aÂ convergence proofÂ in standard non-convex stochastic settings.

This is still my first research contribution, so Iâ€™d love to hear your thoughts â€” whether on the method itself, the experiments, or the clarity of the writing. Any feedback, comments, or constructive criticism are very welcome ðŸ™

Thanks again to everyone who took the time to give feedback last time, it really helped me make the work stronger!

Adrien",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1n0j8u0/d_ano_updated_optimizer_for_noisy_deep_rl_now_on/,r_1n0j8u0,,,
r_1n0h48h,reddit,Blackliquid,2025-08-26T09:24:50+00:00,"[D] SOTA solution for quantization
Hello researchers,

  
I am familiar with common basic approaches to quantization, but after a recent interview, I wonder what the current SOTA approaches are, which are actually used in industry.

  
Thanks for the discussion!",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1n0h48h/d_sota_solution_for_quantization/,r_1n0h48h,,,
r_1n0eyrb,reddit,jain-nivedit,2025-08-26T07:02:17+00:00,"[P] Exosphere: an open source runtime for dynamic agentic graphs with durable state. results from running parallel agents on 20k+ items
Disclosure: I am one of the authors. Links will be in the first comment per sub rules.

TLDR  
We are releasing Exosphere, an open source runtime and durable state manager for agentic workflows that need dynamic branching, retries, and parallel execution. To evaluate it on a real workload, we built WhatPeopleWant, an agent that mines Hacker News discussions and posts distilled problem statements to X every 2 hours. This post shares the setup, workload design, and the ablations we are running, and invites feedback on methodology.

Single runs are trivial. At scale you need to

1. fan out across large inputs
2. branch at runtime on model outputs
3. retry with idempotency
4. persist every step for audit and replay
5. mix CPU and GPU stages
6. resume after faults.

Exosphereâ€™s runtime treats agents like graphs with explicit state, a scheduler, and observability.

We use WhatPeopleWant as a standing benchmark. It ingests Hacker News via the public Firebase API, scores and routes items, optionally enriches high-signal threads, and materializes candidate problem statements. The bot then posts outputs on a fixed schedule.

â€¢ Gating high-signal discussions reduces heavy-model calls and improves tail behavior at similar quality thresholds  
â€¢ Durable state and idempotent nodes make partial replays predictable and minimize upstream rework after faults  
â€¢ Parallelism helps until external API backpressure dominates, which shows up in queue depth and wait times

What I want feedback on  
â€¢ Composite metrics that capture quality, cost, and reliability for agentic graphs  
â€¢ Fair baselines for orchestration when branching is dynamic  
â€¢ Better failure-injection and replay methodologies to compare runtimes

First comment with links",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1n0eyrb/p_exosphere_an_open_source_runtime_for_dynamic/,r_1n0eyrb,,,
r_1n0e7s1,reddit,Tesocrat,2025-08-26T06:13:49+00:00,"[D]How can AI teams stay agile and adaptable when project goals or data requirements change midstream?
For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? Iâ€™ve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots.
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n0e7s1/dhow_can_ai_teams_stay_agile_and_adaptable_when/,r_1n0e7s1,,,
r_1n0d12h,reddit,ZealousidealSalt7133,2025-08-26T05:01:32+00:00,"[D] An honest attempt to implement ""Attention is all you need"" paper
I have started working on implementing actual research papers in machine learning and I have started with ""Attention is all you need"" paper.

I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.

[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)

Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. ",MachineLearning,65,https://www.reddit.com/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/,r_1n0d12h,,,
r_1n055zr,reddit,OkOwl6744,2025-08-25T22:45:10+00:00,"[P] Training LLMs without code - Would you use it?
https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&format=png&auto=webp&s=1c0991294abf01d6699c04b663cd30973e4bd633

Is Vibe training AI models something people want?   
  
I made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?   
  
Why this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. 

For technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. 

I'm looking for collaborators and ideas on how to make this useful as well?

Anyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)  
  
Somewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  

\*\*The project will be free to use if you have your own API keys!\*\* 

In the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! 

Please be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. ",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n055zr/p_training_llms_without_code_would_you_use_it/,r_1n055zr,,,
r_1n01odu,reddit,pmv143,2025-08-25T20:28:04+00:00,"[D] Cold start latency for large models: new benchmarks show 141B in ~3.7s
Some interesting benchmarks Iâ€™ve been digging into:
	â€¢~1.3s cold start for a 32B model
	â€¢~3.7s cold start for Mixtral-141B (on A100s)
       â€¢By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10â€“20s+ for 70B+ models (often minutes).

If these numbers hold up, it reframes inference as less of an â€œalways-onâ€ requirement and more of a â€œruntime swapâ€ problem.

Open questions for the community:
	â€¢How important is sub-5s cold start latency for scaling inference?
	â€¢Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/,r_1n01odu,,,
r_1n00ruv,reddit,feller94,2025-08-25T19:54:03+00:00,"[P] GPU-based backend deployment for an app
Hi all!  
I'm drafting an app with pose detection (currently usingÂ **MediaPipe**) and object detection (earlyÂ **Yolo11**). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to thenÂ *call it from the app when needed*.  
Basically I would need aÂ **GPU-based backend**Â (I can also divide the detections and the actual result usage).

Now, I know aboutÂ *HuggingFace*Â of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards?  
I think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most.  
The trick is that, since I would need the APIs always ready to respond, the backend would need to be up andÂ *running 24/7*. All of the options seem to be quite costly...

Is there any better or worse way to do this?",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/,r_1n00ruv,,,
r_1mzxtzb,reddit,No_Marionberry_5366,2025-08-25T18:02:33+00:00,"[D]GEPA: Reflective Prompt Evolution beats RL with 35Ã— fewer rollouts
A new preprint (Agrawal et al., 2025) introducesÂ **GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.

The results are striking:

* GEPA outperforms GRPO by up toÂ **19%**Â while usingÂ **35Ã— fewer rollouts**.
* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.
* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .

The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories asÂ *textual artifacts*Â that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.

Whatâ€™s interesting is the infra angle: GEPAâ€™s success in multi-hop QA hinges on generating better second-hop queries.Â **That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently.Â **Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**

This work suggests that the real frontier may not be reinforcement learning at scale, butÂ **language-native optimization loops**Â where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.

https://preview.redd.it/5l4lcmokg7lf1.png?width=1602&format=png&auto=webp&s=719e33f34feb5103ed1f375d3366745dd3415d77

",MachineLearning,53,https://www.reddit.com/r/MachineLearning/comments/1mzxtzb/dgepa_reflective_prompt_evolution_beats_rl_with/,r_1mzxtzb,,,
r_1mzwck3,reddit,DolantheMFWizard,2025-08-25T17:08:32+00:00,"[D] How do you derive real insights and interpret experiment data beyond just looking at metrics?
When running experiments, I often struggle with going beyond the surface-level metrics. How do you approach interpreting experimental data in a way that actually leads to useful insights and new ideas? What frameworks, statistical methods, or mindset shifts help you decide whether results are meaningful versus just noise?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mzwck3/d_how_do_you_derive_real_insights_and_interpret/,r_1mzwck3,,,
r_1mzsrt2,reddit,AntreasAntoniou,2025-08-25T14:58:08+00:00,"[D] Too much of a good thing: how chasing scale is stifling AI innovation
DearÂ [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)Â friends,

Hello everyone! I hope you are all doing well out there.

I've been observing a pattern in the AI research field that I can only describe as a ""Mass Amnesia."" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.

In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.

I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)

ðŸ”—Â **Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)

I look forward to your arguments and thoughts.

Regards,

Antreas

  
PS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\_too\_much\_of\_a\_good\_thing\_how\_chasing\_scale\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  

The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. ",MachineLearning,12,https://www.reddit.com/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/,r_1mzsrt2,,,
r_1mzsn1q,reddit,Mplus479,2025-08-25T14:53:11+00:00,"[D] Anyone know how to get Cornell's OpenSurfaces dataset?
Was it abandoned? The website links are dead.",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/,r_1mzsn1q,,,
r_1mzqu1q,reddit,TimesLast_,2025-08-25T13:42:08+00:00,"[D] MALM: A Modular Adapter-based Language Model (paper + Hugging Face link)
Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.

The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).

This means:

\- Smaller, cheaper models

\- Easy to add new languages

\- Better for edge devices and low-resource settings

Example flow:  
\`\`\`  
User: ""Translate 'my name is Adam' into German.""  
CLM â†’ <to:de> my name is Adam </to>  
STA â†’ ""Mein Name ist Adam""

\`\`\`

Read the full paper here:Â [https://huggingface.co/TimesLast/MALM](https://huggingface.co/TimesLast/MALM)

Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mzqu1q/d_malm_a_modular_adapterbased_language_model/,r_1mzqu1q,,,
r_1mzpoo4,reddit,DimitriMikadze,2025-08-25T12:54:15+00:00,"[P] Open-Source Agentic AI for Company Research
I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.

You provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.

The core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.

GitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mzpoo4/p_opensource_agentic_ai_for_company_research/,r_1mzpoo4,,,
r_1mzp8au,reddit,Ok-Ebb6307,2025-08-25T12:33:24+00:00,"[R] Got 6min? I need YOUR help for my PhD!
Hello everyone!

My name is Virginie and I am a first-year French PhD studentÂ **studying humanâ€“artificial intelligence interactions.**

I am conducting aÂ **very quick**Â (approximately 6 minutes) andÂ **anonymous online study**.

To ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!

If you are 18 or over, you can take part by clicking this link:

[https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en)

The survey isÂ **also available in French.**

Every response is valuable! Thank you so much for your help!

Virginie 

*This post has been approved by one moderator of this group.* 

https://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&format=pjpg&auto=webp&s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c

",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mzp8au/r_got_6min_i_need_your_help_for_my_phd/,r_1mzp8au,,,
r_1mzmrm5,reddit,alexsht1,2025-08-25T10:25:38+00:00,"[P] aligning non-linear features with your data distribution
For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.

So here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)

It discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it.",MachineLearning,19,https://www.reddit.com/r/MachineLearning/comments/1mzmrm5/p_aligning_nonlinear_features_with_your_data/,r_1mzmrm5,,,
r_1mzhsh7,reddit,AdInevitable1362,2025-08-25T05:10:55+00:00,"[P] Yelp Dataset clarification: Is review_count colomn cheating?
Hey everyone,

I'm working with the Yelp dataset and have a quick question about the review_count field in the business.json (what I'll call the business_df).

The business_df is a list of businesses, and the review_df is a list of every single review interaction.

Is the review_count in the business_df calculated directly from the interactions listed in the review_df?

If I split my data into train and test sets for a recommendation model, should I recalculate review_count from only the training interactions (so that test interactions remain unseen)? Or is review_count a static field provided by Yelp, independent of our data splits?

The reason I'm asking is I'd like to use review_count as part of my initial features/embeddings. I'm not sure if I should treat it as fixed metadata from Yelp or recompute it dynamically from my training set only.

Thanks a lot if anyone can clarify this!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mzhsh7/p_yelp_dataset_clarification_is_review_count/,r_1mzhsh7,,,
r_1mzd5kt,reddit,Fantastic-Nerve-4056,2025-08-25T01:11:09+00:00,"[D] Views on LLM Research: Incremental or Not?
Hi folks,  
Fellow ML researcher here ðŸ‘‹

Iâ€™ve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).

While surveying the literature, I couldnâ€™t help but notice that a lot of the published work feelsâ€¦ well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them donâ€™t feel like theyâ€™re really pushing the frontier.

Iâ€™m curious to hear what the community thinks:

* Do you also see a lot of incremental work in LLM research, or am I being overly critical?
* How do you personally filter through the â€œnoiseâ€ to identify genuinely impactful work?
* Any heuristics or signals that help you decide which papers are worth a deep dive?

Would love to get different perspectives on this â€” especially from people navigating the same sea of papers every week.

  
PS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions",MachineLearning,56,https://www.reddit.com/r/MachineLearning/comments/1mzd5kt/d_views_on_llm_research_incremental_or_not/,r_1mzd5kt,,,
r_1mz9ruc,reddit,AgeOfEmpires4AOE4,2025-08-24T22:38:42+00:00,"[P] AI Learns to play Sonic 2 Emerald Hill (Deep Reinforcement...
Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:

[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.

  
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mz9ruc/p_ai_learns_to_play_sonic_2_emerald_hill_deep/,r_1mz9ruc,,,
r_1mz70e2,reddit,drahcirenoob,2025-08-24T20:46:57+00:00,"[R] Review advice:  Well-established work published years ago on Arxiv
I'm reviewing for AAAI, and wanted to ask the community for some advice. I got a paper for review that is very well known in my subfield, published in 2023, but only previously published onto Arxiv. As best I can tell, the paper has had some minor rewrites for publication, but is otherwise largely the same as the well-established work. What's the best policy here? It was a very good paper when it came out, but the existing version basically ignores the last two years of work by the community, in part because some decent portion of that work is based on this paper.  Any advice on the best way to review this would be appreciated",MachineLearning,36,https://www.reddit.com/r/MachineLearning/comments/1mz70e2/r_review_advice_wellestablished_work_published/,r_1mz70e2,,,
r_1mywuni,reddit,Pedro_Silva95,2025-08-24T14:20:49+00:00,"[P] options on how to balance my training dataset
I'm working on developing a ML classification project using Python, divided into 5 output categories (classes). However, my training dataset is extremely unbalanced, and my results always lean toward the dominant class (class 5, as expected).

However, I wanted my models to better learn the characteristics of the other classes, and I realized that one way to do this is by balancing the training dataset. I tried using SMOTETomek for oversampling, but my models didn't respond well. Does anyone have any ideas or possibilities for balancing my training dataset?

There are 6 classification ML models that will ultimately be combined into an ensemble. The models used are: RandomForest, DecisionTree, ExtraTrees, AdaBoost, NaiveBayes, KNN, GradientBoosting, and SVM.

The data is also being standardized via standardSCaler.

Total record count by category:

Category 1: 160 records

Category 2: 446 records

Category 3: 605 records

Category 4: 3,969 records

Category 5: 47,874 records",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mywuni/p_options_on_how_to_balance_my_training_dataset/,r_1mywuni,,,
r_1myr68a,reddit,Code-Forge-Temple,2025-08-24T09:32:18+00:00,"[D] Exploring Local-First AI Workflow Automation
**[D] Exploring Local-First AI Workflow Automation**

Hi all,  

Iâ€™ve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.

ðŸ‘‰ My question for the community:  
How do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  

- Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  
- Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  

Would love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  

Thanks!  
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1myr68a/d_exploring_localfirst_ai_workflow_automation/,r_1myr68a,,,
r_1myptun,reddit,Snoo71505,2025-08-24T08:06:19+00:00,"[D] Neurips 2025: Are there post conference events on the last day of the conference?
\[EDIT\] I meant December / Dec not November / Nov. It was late at night I'm sorry -  lol.

Context:

* Neurips 2025 conference is from Tue, Dec 2 to Sun, Dec 7
* This is my first time attending the conference.
* As I need to travel again right after the conference for personal reasons, I am figuring out on what dates to book the hotels / flights in advance.
* **Are there post conference events on the last day** eg: Sun, Dec 7 night? I am not sure if it's better to return right away (on Sun, Dec 7 evening) or fly back later (on Mon, Dec 8 morning)?",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1myptun/d_neurips_2025_are_there_post_conference_events/,r_1myptun,,,
r_1myoooy,reddit,alexsht1,2025-08-24T06:53:56+00:00,"[D] Poles of non-linear rational features
Suppose I want to fit a linear model to non-linear **rational** features. Something like `RationalTransformer` instead of `SplineTransformer` in Scikit-Learn, that uses a basis of rational functions. The domain of my raw features before being transformed are (theoretically) unbounded non-negative numbers, such as ""time since X happened"", ""total time spent on the website"", or ""bid in an auction"".

So here is the question: *where would you put the poles? Why?*

Note, I'm not aiming on fitting one rational curve, so algorithms in the spirit of AAA are irrelevant. I'm aiming at a component I can use in a pipeline that transformes features before model fitting, such as `MinMaxScaler` or `SplineTransformer` in scikit-learn.",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1myoooy/d_poles_of_nonlinear_rational_features/,r_1myoooy,,,
r_1mylqrb,reddit,UnholyCathedral,2025-08-24T04:01:13+00:00,"[R] Building a deep learning image model system to identify BJJ positions in matches
Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.

You can check it out here:Â [https://bjjhq.ai/](https://bjjhq.ai/)

Ultimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.

The idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., ""Gordon Ryan in back control""), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.

Roadmap:

* Expanding the match database and position categories
* Technique/submission recognition
* Automated scoring system built on this positional foundation

Would love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!

Thanks for any feedback!",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1mylqrb/r_building_a_deep_learning_image_model_system_to/,r_1mylqrb,,,
r_1myj9jk,reddit,electricsheeptacos,2025-08-24T01:49:27+00:00,"[R] routers to foundation models?
Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with â€œroutersâ€?

Update: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but Iâ€™m trying to get a sense of how much of a concern it really is

Of course, the mechanisms underlying this approach are of interest to me as well. Iâ€™m thinking of writing my own router, but would like to understand whatâ€™s out there/what the need even is first",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1myj9jk/r_routers_to_foundation_models/,r_1myj9jk,,,
r_1myhj41,reddit,Creative_Star_9425,2025-08-24T00:22:07+00:00,"[D] Topology and geometry in deep learning beyond TDL/GDL
Posts here within the past 6 months have discussed bothÂ [Topological Deep Learning (TDL)](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/)Â andÂ [Geometric Deep Learning (GDL)](https://www.reddit.com/r/MachineLearning/comments/1jabkt8/d_geometric_deep_learning_and_its_potential/). Even though the nomenclature suggests otherwise, these two (exciting!) areas have come to represent rather specific topics in recent years.Â *Very crudely speaking*, ""TDL"" seems to focus mainly on higher-order message passing (HOMP); ""GDL"" to the design of neural networks mod domain symmetries.

For the purposes of discussion, let's set the operational definition of TDL to be as in this paper:Â [Hajij, Mustafa, et al. Topological Deep Learning: Going Beyond Graph Data. Springer, 2024.](https://tdlbook.org/)

and the operational definition of GDL to be as in this paper:Â [Bronstein, Michael M., et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. MIT Press, 2021.](https://arxiv.org/abs/2104.13478)

With that in place: what are some applications of geometry and topology in deep learning thatÂ *do not properly belong to TDL and GDL as defined above*Â (and as have already received recent posts here)? Applications of adjacent fields are also welcome- algebra, category theory, etc.- , as are applications in the converse direction.",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1myhj41/d_topology_and_geometry_in_deep_learning_beyond/,r_1myhj41,,,
r_1mybwih,reddit,TajineMaster159,2025-08-23T20:19:36+00:00,"[D] How did JAX fare in the post transformer world?
A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? 

Or at least, this is my impression, which I concede might be myopic due to my research and industry needs. ",MachineLearning,150,https://www.reddit.com/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/,r_1mybwih,,,
r_1mxyqku,reddit,huopak,2025-08-23T11:15:41+00:00,"[D] Is MLSys a low-tier conference? I can't find it in any of the rankings
[https://mlsys.org/](https://mlsys.org/)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mxyqku/d_is_mlsys_a_lowtier_conference_i_cant_find_it_in/,r_1mxyqku,,,
r_1mxw9c1,reddit,JesuXd,2025-08-23T08:46:29+00:00,"[P] I built a ML-regression model for Biathlon that beats current betting market odds
Hello ya'll!

I recently built a ML-regression model to predict the unpredictable sport of biathlon. In biathlon, external factors such as weather, course profiles and altitude play huge roles in determining who wins and when. But when taking these factors into play, in addition of athletes' past performances, you can score surprisingly high accuracy.

This is how well the model performed when predicting athlete ranks (0 = winner, 1 = last place) using 10 years of historic biathlon data:  
\- MAE (average error): 0.14 -> 4-18 places off depending on race size  
\- RMSE: 0.18 -> penalizing big prediction misses  
\- RÂ²: -> the model explains \~62% of the variation in finish order

Now what does these metrics say?  
\- The model almost cuts in half random guessing (\~25% error)  
\- It consistently outperforms the accuracy of betting odds in the current market, meaning it has a predictive edge.  
\- It is able to tell the majority of happenings (62%), which is very rare in a sport where surprises happen very often.

Next steps:  
\- Build RÂ² up to 70% using more complex feature engineering and data preprocessing.  
\- Launch a SaaS that sells these odds for businesses and private consumers.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mxw9c1/p_i_built_a_mlregression_model_for_biathlon_that/,r_1mxw9c1,,,
r_1mxrt1y,reddit,Healthy_Horse_2183,2025-08-23T04:19:11+00:00,"[D] AAAI considered 2nd tier now?
Isnâ€™t AAAI in the same tier as NeurIPS/ICML/ICLR? 
ICLR literally has >30% acceptance rate.",MachineLearning,67,https://www.reddit.com/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/,r_1mxrt1y,,,
r_1mxih41,reddit,Gloomy_Situation5126,2025-08-22T21:06:55+00:00,"[P] Relational PDF Recall (RFC + PoC) â€“ Structured storage + overlay indexing experiment
Iâ€™ve been exploring how far we can pushÂ *relational database structures inside PDFs*Â as a substrate for AI recall. Just published a first draft RFC + PoC:

* Channel splitting (text/vector/raster/audio streams)
* Near-lossless transforms (wavelet/FLAC-style)
* Relational indexing across channels (metadata + hash linking)
* Early geometry-only overlays (tiling + Z-order indexing)

Repo + notes:Â [https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)

This is still very early (draft/PoC level), but Iâ€™d love feedback on:

* Whether others have tried similar recall-layer ideas on top of PDFs.
* If this approach overlaps with knowledge-graph work, or if it opens a different lane.
* Pitfalls I might be missing re: indexing/overlays.

  
**UPDATE 1: ðŸ“Œ Repo + DOI now live**   
GitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  
DOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mxih41/p_relational_pdf_recall_rfc_poc_structured/,r_1mxih41,,,
r_1mxcd2j,reddit,sukhoi-30mki,2025-08-22T17:12:13+00:00,"[P] Need to include ANN, LightGBM, and KNN results in research paper
Hey everyone,

Iâ€™m working on a research paper with my group, and so far weâ€™ve done a comprehensive analysis using **Random Forest**. The problem is, my professor/supervisor now wants us to also include results from **ANN, LightGBM, and KNN** for comparison.

We need to:

* Run these models on the dataset,
* Collect performance metrics (accuracy, RMSE, RÂ², etc.),
* Present them in a **comparison table** with Random Forest,
* Then update the writing/discussion accordingly.

Iâ€™m decent with Random Forests but not as experienced with ANN, LightGBM, and KNN. Could anyone guide me with example code, a good workflow, or best practices for running these models and compiling results neatly into a table?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mxcd2j/p_need_to_include_ann_lightgbm_and_knn_results_in/,r_1mxcd2j,,,
r_1mx775g,reddit,fishandtech,2025-08-22T13:55:29+00:00,"[D] Low-budget hardware for on-device object detection + VQA?
Hey folks,

Iâ€™m an undergrad working on my FYP and need advice. I want to:

* RunÂ object detectionÂ on medical images (PNGs).
* DoÂ visual question answeringÂ with a ViT or small LLaMA model.
* Everything fullyÂ on-deviceÂ (no cloud).

Budget is tight, so Iâ€™m looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.

Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?

Thanks!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/,r_1mx775g,,,
r_1mx4a6c,reddit,ComprehensiveTop3297,2025-08-22T11:47:44+00:00,"[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?

I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?

From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?

Specifically:

How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?

I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.",MachineLearning,49,https://www.reddit.com/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/,r_1mx4a6c,,,
r_1mwxfxj,reddit,Puzzled_Boot_3062,2025-08-22T04:55:41+00:00,"[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods â€” promising or just recursion?
Iâ€™ve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.

But hereâ€™s the tricky part: this feels a bit like â€œLLMs generating data for themselvesâ€ â€” almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, arenâ€™t we just stacking uncertainties?

Iâ€™d love to hear the communityâ€™s thoughts:

* Do you see this as a viable research or application direction, or more like a dead end?
* Are there promising frameworks or papers tackling this â€œself-extraction â†’ RAG â†’ LLMâ€ pipeline?
* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?

Curious to know if anyone here has tried something along these lines.",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/,r_1mwxfxj,,,
r_1mwrl72,reddit,Franck_Dernoncourt,2025-08-22T00:08:19+00:00,"[D] Why was this paper rejected by arXiv?
One of my co-authors submitted this [paper](https://ia903401.us.archive.org/19/items/images-for-questions/A%20Survey%20on%20LLM-based%20Conversational%20User%20Simulation.pdf) to arXiv. It was rejected. What could the reason be?

[iThenticate](https://www.ithenticate.com/) didn't detect any plagiarism and arXiv didn't give any reason beyond a vague ""submission would benefit from additional review and revision that is outside of the services we provide"":

> Dear author,
> 
> Thank you for submitting your work to arXiv. We regret to inform you that arXivâ€™s moderators have determined that your submission will not be accepted at this time and made public on  http://arxiv.org
> 
> In this case, our moderators have determined that your submission would benefit from additional review and revision that is outside of the services we provide. 
> 
> Our moderators will reconsider this material via [appeal](https://info.arxiv.org/help/moderation/appeals.html) if it is published in a conventional journal and you can provide a resolving DOI (Digital Object Identifier) to the published version of the work or link to the journal's website showing the status of the work.
> 
> Note that publication in a conventional journal does not guarantee that arXiv will accept this work.
> 
> For more information on moderation policies and procedures, please see [Content Moderation](https://info.arxiv.org/help/moderation/index.html). 
> 
> arXiv moderators strive to balance fair assessment with decision speed. We understand that this decision may be disappointing, and we apologize that, due to the high volume of submissions arXiv receives, we cannot offer more detailed feedback. Some authors have found that asking their personal network of colleagues or submitting to a conventional journal for peer review are alternative avenues to obtain feedback. 
> 
> We appreciate your interest in arXiv and wish you the best. 
> 
> Regards,
>
> arXiv Support

I read the [arXiv policies](https://info.arxiv.org/help/moderation/index.html) and I don't see anything we infringed.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mwrl72/d_why_was_this_paper_rejected_by_arxiv/,r_1mwrl72,,,
r_1mwfjax,reddit,KellinPelrine,2025-08-21T16:17:45+00:00,"[R] Frontier LLMs Attempt to Persuade into Harmful Topics
Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.

Prior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:

ðŸ”¹ Leading models readily produced empathic yet coercive ISIS recruitment arguments

ðŸ”¹ Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingness

ðŸ”¹ Fine-tuning eliminated safeguards: ""Jailbreak-Tuned"" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and torture

For clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The modelsâ€™ attempts to persuade, however, appear to be eloquent and well-written â€“ we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.

Weâ€™ve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.

We've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.

ðŸ‘¥ Research by Matthew Kowal, Jasper Timm, Jean-FranÃ§ois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.

ðŸ“ Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval)Â 

ðŸ“„ Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)Â 

ðŸ’» Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mwfjax/r_frontier_llms_attempt_to_persuade_into_harmful/,r_1mwfjax,,,
r_1mwbq81,reddit,bjjonin,2025-08-21T13:59:54+00:00,"[P] Language Diffusion in <80 Lines of Code
Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took <80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected!

[Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)

You can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!",MachineLearning,89,https://www.reddit.com/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/,r_1mwbq81,,,
r_1mwb7pp,reddit,NataliaShu,2025-08-21T13:39:20+00:00,"[R] Observing unexpected patterns in MTPE demand across languages
Hi ML folks, I work at Alconost (localization services), and weâ€™ve just wrapped up our 5th annual report on language demand for localization. For the first time, weâ€™ve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages.Â 

We analyzed MTPE adoption rates in the Top 20 languages, and whatâ€™s interesting is that some languages that are slipping in overall localization demand are stillÂ **seeing more activity**Â via MTPE.Â 

Iâ€™m curious: if youâ€™re working with MT or LLM workflows, have you noticed similar patterns in the languages you work with?Â 

What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs?Â 

Thank you. Cheers!",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1mwb7pp/r_observing_unexpected_patterns_in_mtpe_demand/,r_1mwb7pp,,,
r_1mw4sgp,reddit,Mission-Balance-4250,2025-08-21T08:00:06+00:00,"[R] How to prime oneself for ML research coming from industry
I've been working as an ML Engineer for the last 5-6 years across a few different industries and have landed a job as a research engineer at a university under an esteemed supervisor in the NLP department who has generously offered to help me figure out my research interests and assist with theirs. I published a paper about 4 years ago in cognitive science - but it involved very little ML.

I don't have any tertiary qualifications/degrees but have industry experience in research-oriented roles - although, none primarily in NLP. I move internationally for the role in 3 months and want to poise myself to be as useful as possible. Does anyone have tips about gearing up to do academic research/engineering having come from industry?

I feel like there is infinite ground to cover; my maths will need much sharpening, I'll need to learn how to properly read scientific papers etc.

Cheers",MachineLearning,32,https://www.reddit.com/r/MachineLearning/comments/1mw4sgp/r_how_to_prime_oneself_for_ml_research_coming/,r_1mw4sgp,,,
r_1mw2z1y,reddit,Maleficent-Tone6316,2025-08-21T06:06:37+00:00,"[D] PhD vs startup/industry for doing impactful AI research â€” what would you pick?
Hi all,

Iâ€™m deciding between starting a PhD at a top university (ranked \~5â€“10) with a great professor (lots of freedom, supportive environment) or going straight into industry.

My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\* conference papers (3â€“4), so Iâ€™m not starting from scratch.

Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab Iâ€™d join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \~4 years + internship time.

If you were in this position, which path would you take?",MachineLearning,73,https://www.reddit.com/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/,r_1mw2z1y,,,
r_1mw1qty,reddit,AdInevitable1362,2025-08-21T04:56:55+00:00,"[P] model to encode texts into embeddings
I need to summarize metadata using an LLM,
and then encode the summary using BERT (e.g., DistilBERT, ModernBERT).
	â€¢	Is encoding summaries (texts) with BERT usually slow?
	â€¢	Whatâ€™s the fastest model for this task?
	â€¢	Are there API services that provide text embeddings, and how much do they cost?
",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mw1qty/p_model_to_encode_texts_into_embeddings/,r_1mw1qty,,,
r_1mw19zu,reddit,Blue-Sea123,2025-08-21T04:30:58+00:00,"[P] If i were to add a segmentation head onto an OD model, how do i go about it?
So i am picking a model from scenic repository and although the model is primarily built for object detection, i want to try and see if i can make it to do segmentation tasks as well. This could include combining it with another model (like SAM, or something), as well as adding a segment head into the model itself. l am a novice in ML having worked for about a year in implementing CV solutions. How should i go about doing this?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mw19zu/p_if_i_were_to_add_a_segmentation_head_onto_an_od/,r_1mw19zu,,,
r_1mvycr9,reddit,OkOwl6744,2025-08-21T02:05:18+00:00,"[P] Vibe datasetting- Creating syn data with a relational model

TL;DR: Iâ€™m testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data youâ€™ll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.

Why: Random synthetic data â‰  helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).

How it works:
	1.	Upload a small CSV or connect to a mock relational set.

	2.	Define a semantic spec (taxonomy/attributes + target distribution).

	3.	KumoRFM predicts next-window frequencies â†’ identifies under-covered buckets.

	4.	LLM generates only those samples. Coverage & calibration update in place.

What to test (3 min):
	â€¢	Try a churn/click/QA dataset; set a target spec; click Plan â†’ Generate.

	â€¢	Check coverage vs. target and bucket-level error/entropy before/after.

Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.

Looking for feedback, like:
	â€¢	Did the planner pick useful gaps?
	â€¢	Any obvious spec buckets weâ€™re missing?
	â€¢	Would you want a â€œgenerate labels onlyâ€ mode?
	â€¢	Integrations youâ€™d use first (dbt/BigQuery/Snowflake)?

HTTPS://datasetdirector.com ",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/,r_1mvycr9,,,
r_1mvtjxw,reddit,EDEN1998,2025-08-20T22:32:01+00:00,"Google phd fellowship 2025 [D]
Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.",MachineLearning,47,https://www.reddit.com/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/,r_1mvtjxw,,,
r_1mvn89s,reddit,Dualweed,2025-08-20T18:35:26+00:00,"Simple Multiple Choice Questions about Machine Learning [D]
The following statements are either True or False:

1. You can use any differentiable function f: R->R in a neural network as activation function.
2. You can always know whether the perceptron algorithm will converge for any given dataset.

What do you guys think? I got both of them wrong in my exam.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mvn89s/simple_multiple_choice_questions_about_machine/,r_1mvn89s,,,
r_1mvmlbw,reddit,lipflip,2025-08-20T18:12:34+00:00,"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)
Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.

If you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\_perception\_of\_artificial\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) ðŸ™ˆ

**Main takeaway:** People often see AI scenarios as likely, but this doesnâ€™t mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that peopleâ€™s value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didnâ€™t matter much.  
  
**Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.  
  
If youâ€™re interested, hereâ€™s the full article:  
Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), 

https://www.sciencedirect.com/science/article/pii/S004016252500335X",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/,r_1mvmlbw,,,
r_1mvfktv,reddit,AdhesivenessOk3187,2025-08-20T13:59:01+00:00,"[P] GridSearchCV always overfits? I built a fix
So I kept running into this: `GridSearchCV` picks the model with the best validation scoreâ€¦ but that model is often overfitting (train super high, test a bit inflated).

I wrote a tiny selector that balances:

* how good the test score is
* how close train and test are (gap)

Basically, it tries to pick the â€œstableâ€ model, not just the flashy one.

Code + demo here ðŸ‘‰[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mvfktv/p_gridsearchcv_always_overfits_i_built_a_fix/,r_1mvfktv,,,
r_1mvdey9,reddit,vihanga2001,2025-08-20T12:28:19+00:00,"[R] How do you make text labeling less painful?
Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.

The idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.

If youâ€™ve ever worked on labeling or managing a labeled dataset, Iâ€™d love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel â€œworth it.â€

Totally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.

You can DM me or drop a comment if open to chat. Thanks so much",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/,r_1mvdey9,,,
r_1mv5ls0,reddit,beefchocolatesauce,2025-08-20T04:57:21+00:00,"[R] Is data the bottleneck for video/audio generation?
As the title says, Iâ€™m curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isnâ€™t enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? Iâ€™d love to hear what people at the forefront consider the biggest bottleneck right now.",MachineLearning,21,https://www.reddit.com/r/MachineLearning/comments/1mv5ls0/r_is_data_the_bottleneck_for_videoaudio_generation/,r_1mv5ls0,,,
r_1mv4r5z,reddit,wheasey,2025-08-20T04:11:20+00:00,"[R] Virtuous Machines: Towards Artificial General Science
Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.

Arxiv Link:Â [https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)

This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.

In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.

They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \~17 hours and consume on average \~30m tokens.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mv4r5z/r_virtuous_machines_towards_artificial_general/,r_1mv4r5z,,,
r_1munwmw,reddit,poppear,2025-08-19T16:48:20+00:00,"[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model
HeyÂ [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.

Today, we're excited to share the first step with the open-source community:Â `azzurra-voice`.

`azzurra-voice`Â is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.

**You can listen to audio samples comparing**Â `azzurra-voice`Â **to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/)",MachineLearning,10,https://www.reddit.com/r/MachineLearning/comments/1munwmw/r_azzurravoice_a_new_stateoftheart_italian/,r_1munwmw,,,
r_1mufrkc,reddit,councilanderson2,2025-08-19T11:29:21+00:00,"[D] Switching to postdoc in ML for Earth Observation?
Iâ€™d like to hear from people working with ML for Earth Observation.

My PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.

In multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. Thatâ€™s why Iâ€™ve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.

My worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics donâ€™t matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I donâ€™t end up with a permanent academic position and move to industry, I worry that Earth Observation skills wonâ€™t transfer well since there arenâ€™t as many opportunities compared to mainstream ML.

Iâ€™d really like to hear from people in the field about how you see these trade-offs.",MachineLearning,21,https://www.reddit.com/r/MachineLearning/comments/1mufrkc/d_switching_to_postdoc_in_ml_for_earth_observation/,r_1mufrkc,,,
r_1mudtw6,reddit,FammasMaz,2025-08-19T09:42:20+00:00,"[D] Endorsement for cs.LG at arXiv as non-ML student?
Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?

  
The general idea is to make an ML based pipeline to generate granular mechanical structures.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mudtw6/d_endorsement_for_cslg_at_arxiv_as_nonml_student/,r_1mudtw6,,,
r_1mu2a8x,reddit,AntreasAntoniou,2025-08-18T23:34:45+00:00,"[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress
Dear r/MachineLearning friends,

Iâ€™m here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.

In my new blog post, I argue that this constraint is a feature, not a bug. It removes the ""scale-up cheat code"" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.

The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**

ðŸ§ ðŸ” **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)

Your feedback and thoughts would be greatly appreciated.

Regards,

Antreas",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mu2a8x/d_beyond_the_cloud_slms_local_ai_agentic/,r_1mu2a8x,,,
r_1mtq2qy,reddit,ThRiLLeXx,2025-08-18T15:58:59+00:00,"[D] Location of EACL 2026
Hi folks,

I've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the [joint call for workshops](https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops) of EACL/ACL 2026.

But, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).

Do you think this information is accurate, or am I simply missing something?",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1mtq2qy/d_location_of_eacl_2026/,r_1mtq2qy,,,
r_1mtoewm,reddit,OddUnderstanding1633,2025-08-18T14:58:33+00:00,"[D] ACL Rolling Review (ARR) 2025 May (EMNLP 2025) Stats
The stats for ARR May 2025 are out: [https://stats.aclrollingreview.org/iterations/2025/may/](https://stats.aclrollingreview.org/iterations/2025/may/)

It looks like about 25% of submissions have Meta â‰¥ 3.5. Does anyone know if itâ€™s still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?",MachineLearning,23,https://www.reddit.com/r/MachineLearning/comments/1mtoewm/d_acl_rolling_review_arr_2025_may_emnlp_2025_stats/,r_1mtoewm,,,
r_1mto1xw,reddit,padakpatek,2025-08-18T14:45:11+00:00,"[D] How would I go about clustering voices from songs?
I have a 90s hiphop mixtape with a bunch of unknown tracks from multiple artists. I want to perform unsupervised clustering to infer how many artists there are in total because I can't really tell by ear.

I guess I would need to:

1. Somehow convert audio files into numerical data

2. Extract only the vocal data (or I guess these two steps can be flipped? Somehow extract only the vocal audio, and then convert that into numerical data?)

3. Perform unsupervised clustering


I'm just not sure how to go about doing steps 1 and 2.

Any ideas?",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1mto1xw/d_how_would_i_go_about_clustering_voices_from/,r_1mto1xw,,,
r_1mtmadk,reddit,jeertmans,2025-08-18T13:37:33+00:00,"[P] JAX Implementation of Hindsight Experience Replay (HER)
Hi! I recently discovered the *Hindsight Experience Replay* (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with **JAX**, I decided to reimplement the classic bit-flipping experiment to better understand HER.

This implementation uses **Equinox** for model definitions and **Optax** for optimization. The [repository](https://github.com/jeertmans/HER-with-JAX) provides:
+ A *minimal* and *clean* implementation of HER in JAX
+ Reproducible scripts and results
+ A [Colab Notebook](https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb) for direct experimentation

Code: https://github.com/jeertmans/HER-with-JAX

Let me know if you have any questions, feedback, or recommendations!",MachineLearning,31,https://www.reddit.com/r/MachineLearning/comments/1mtmadk/p_jax_implementation_of_hindsight_experience/,r_1mtmadk,,,
r_1mtfikh,reddit,AnyIce3007,2025-08-18T07:40:54+00:00,"[D] Conferences need to find better venues
Better = venues that are virtually accessible for any researcher/author to go to.

Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.

This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.",MachineLearning,205,https://www.reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/,r_1mtfikh,,,
r_1mtekhm,reddit,___loki__,2025-08-18T06:42:05+00:00,"[P] Looking for datasets/tools for testing document forgery detection in medical claims
Iâ€™m a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged â€” mainly looking at image tampering, copyâ€“move edits, or plausible fraud attempts.

Since I just started, Iâ€™m still figuring out the best way to evaluate this system. My challenges are mostly around data:

* Public forgery datasets like DocTamper (CVPR 2023) are great, but they donâ€™t really cover medical/health-claim documents.
* I havenâ€™t found any dataset with paired authentic vs. forged health claim reports.
* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.

What Iâ€™ve considered so far:

* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copyâ€“move edits).
* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.

**Questions for the community:**

1. Has anyone come across an open dataset of forged medical/insurance claim documents?
2. If not, whatâ€™s the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?
3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?

Since Iâ€™m still new, any guidance, papers, or tools you can point me to would be really appreciated ðŸ™

Thanks in advance!",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1mtekhm/p_looking_for_datasetstools_for_testing_document/,r_1mtekhm,,,
r_1mtdjum,reddit,Mad_Scientist2027,2025-08-18T05:41:31+00:00,"[D] How to get into High Dimensional Dynamical Systems?
Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.

Any responses / suggestions are appreciated. Thanks in advance.",MachineLearning,24,https://www.reddit.com/r/MachineLearning/comments/1mtdjum/d_how_to_get_into_high_dimensional_dynamical/,r_1mtdjum,,,
r_1mt4ym5,reddit,FineConcentrate6991,2025-08-17T22:41:45+00:00,"[D] - Multi Class Address Classification

Hello people, I have a dataset with Adress and label 800K rows. I am trying to train a model for address label prediction. Address data is bit messy and different for each different label. we have 10390 each with 50-500 row. I have trained a model using fasttext I have got 0.5 F1 score max. What can I do to for to get best F1 score?

Address data is like (province, district, avenue street, maybe house name and no)

some of them are missing at each address.",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1mt4ym5/d_multi_class_address_classification/,r_1mt4ym5,,,
r_1mszuyb,reddit,ApartmentEither4838,2025-08-17T19:17:27+00:00,"[D] Injecting self doubt in the CoT of reasoning models
A short analysis on what happens when you inject self doubt in the CoT of reasoning models
https://github.com/martianlantern/cot-doubt-injection",MachineLearning,20,https://www.reddit.com/r/MachineLearning/comments/1mszuyb/d_injecting_self_doubt_in_the_cot_of_reasoning/,r_1mszuyb,,,
r_1msrdfq,reddit,gaytwink70,2025-08-17T13:47:13+00:00,"Is Econometrics a good background to get into Machine Learning? [D]
I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.

I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.

How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1msrdfq/is_econometrics_a_good_background_to_get_into/,r_1msrdfq,,,
r_1msq0uf,reddit,Intrepid-Purpose2151,2025-08-17T12:47:58+00:00,"[P] Confused results while experimenting with attention modules on CLIP RN50 for image classification


Hey everyone,

Iâ€™m currently working on an audio-visual project. As a first step, Iâ€™m building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.

To push performance, I experimented with adding attention modules:

With CBAM (Convolutional Block Attention Module), accuracy improved to 89%.

With SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.


My understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.

Am I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?

Would really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.

Thanks!
",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1msq0uf/p_confused_results_while_experimenting_with/,r_1msq0uf,,,
r_1msm3m4,reddit,Master_Ocelot8179,2025-08-17T09:05:25+00:00,"[D] COLM Financial Assistance
Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email.",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1msm3m4/d_colm_financial_assistance/,r_1msm3m4,,,
r_1ms9d2u,reddit,say_wot_again,2025-08-16T22:07:45+00:00,"[R] Dino v3: Self-supervised learning for vision at unprecedented scale
New SOTA for self supervised learning in computer vision. They train a 7B self supervised ViT on 1.7B images, which hits SOTA with linear probing on most downstream tasks. They also release scaled and distilled versions of the model (ViT small, base, large, and huge, plus ConvNext tiny, small, base, and large), along with a version trained on satellite imagery.

There are plenty of details in the paper as to what pretraining improvements they made over DINO v2. ",MachineLearning,215,https://www.reddit.com/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/,r_1ms9d2u,,,
r_1mrwm3w,reddit,the_iegit,2025-08-16T14:20:49+00:00,"[D] model architecture or data?
Iâ€™ve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains itâ€™s performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: https://arcprize.org/blog/hrm-analysis

And iâ€™ve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architecture

Can someone explain which of the sides is closer to the truth?",MachineLearning,37,https://www.reddit.com/r/MachineLearning/comments/1mrwm3w/d_model_architecture_or_data/,r_1mrwm3w,,,
r_1mrqyni,reddit,Slight-Ad-5816,2025-08-16T10:06:16+00:00,"[R] How do I choose the best model in validation when I have no target data??
I am working on unsupervised domain adaptation techniques for super resolution. I have a good amount of paired source data and very less target data without no ground truth. The issue is while training this pipeline I am not able to save the best model as for this I would need some ground truth in the target domain on which I would validate the model after each epoch and save the best one. How do I tackle this? Recently, I found an OpenReview paper about a transfer score which is a metric which do not need target labels but it is for classification based tasks. I want something for super-resolution. Does anyone have any idea?",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mrqyni/r_how_do_i_choose_the_best_model_in_validation/,r_1mrqyni,,,
r_1mrjs5i,reddit,ilovecookies14,2025-08-16T03:44:53+00:00,"[D] Cool new ways to mix linear optimization with GNNs? (LP layers, simplex-like updates, etc.)
Lately Iâ€™ve been diving into how graph neural networks can play nicely with linear optimization, not just as a post-processing step, but actually inside the model or training loop.

Iâ€™ve seen some neat stuff around differentiable LP layers, GNNs predicting parameters for downstream solvers, and even architectures that mimic simplex-style iterative updates. It feels like thereâ€™s a lot of room for creativity here, especially for domain-specific problems in science/engineering.

Curious whatâ€™s been coming out in the last couple of years. Any papers, repos, or tricks youâ€™ve seen that really push this GNN + optimization combo forward? Supervised, unsupervised, RLâ€¦ all fair game.",MachineLearning,25,https://www.reddit.com/r/MachineLearning/comments/1mrjs5i/d_cool_new_ways_to_mix_linear_optimization_with/,r_1mrjs5i,,,
r_1mr7ifp,reddit,Routine-Scientist-38,2025-08-15T19:19:38+00:00,"[D] - Neurips Position paper reviews
The position paper reviews were just released. So far this entire process has been very unprofessional, with multiple delays, poor communication, and still no clear rubric for what the review scores mean. Has anyone else gotten reviews? Curious to hear other's thoughts on this",MachineLearning,43,https://www.reddit.com/r/MachineLearning/comments/1mr7ifp/d_neurips_position_paper_reviews/,r_1mr7ifp,,,
r_1mqur0y,reddit,Agreeable_Touch_9863,2025-08-15T11:12:45+00:00,"[D] Bethe Hessian Spectral Clustering
Why does nobody seem to use this when it works noticeably better than regular (normalised laplacian) spectral clustering? I have studied it a fair bit and cant see any downsides apart from ever so slightly higher computational cost (the order of magnitude doesn't change, just a larger constant.)

Its also been around long enough now that I dont see recency as the issue.",MachineLearning,9,https://www.reddit.com/r/MachineLearning/comments/1mqur0y/d_bethe_hessian_spectral_clustering/,r_1mqur0y,,,
r_1mqgyfe,reddit,Onlyheretohelp_you,2025-08-14T23:29:54+00:00,"custom Vulkan C++ machine learning library vs TensorFlow [R]
guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\*ll it cause it repeats the same wrong things",MachineLearning,4,https://www.reddit.com/r/MachineLearning/comments/1mqgyfe/custom_vulkan_c_machine_learning_library_vs/,r_1mqgyfe,,,
r_1mqgcka,reddit,AncientGearAI,2025-08-14T23:04:55+00:00,"Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N]
Hello.

I am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.

So i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.

PROBLEM 1:

The problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)

Off course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.

PROBLEM 2:

Another issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.

if there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?

[photo of double slit diffraction \(train set\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&format=pjpg&auto=webp&s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)

[photo of double slit diffraction \(val set\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&format=pjpg&auto=webp&s=a30f4c67036a800a33b5571475c997b43857b98a)

[python image single slit diffraction \(train set\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=9fcfac7032d3c9de2255055f7c96abac774b8687)

[python image \(single slit val set\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=29342d997939aa13d5fd4a004c29228d61f13896)",MachineLearning,8,https://www.reddit.com/r/MachineLearning/comments/1mqgcka/problem_with_dataset_for_my_my_physics/,r_1mqgcka,,,
r_1mqdpug,reddit,stevenverses,2025-08-14T21:23:01+00:00,"[2507.17338] Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks
Research showcasing how a robot outperforms state of the art models on the Habitat benchmark from Meta ***without pre-training***.

For those fluent in ðŸ¤– what you think?",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1mqdpug/250717338_mobile_manipulation_with_active/,r_1mqdpug,,,
r_1mqccah,reddit,Majestij,2025-08-14T20:32:35+00:00,"[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205)
Code & paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)

**TL;DR**

\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;

\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;

\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;

\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!

https://preview.redd.it/rllc2yplo1jf1.png?width=3850&format=png&auto=webp&s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051d

https://i.redd.it/8vgf2iemo1jf1.gif

https://i.redd.it/81lbt56no1jf1.gif",MachineLearning,16,https://www.reddit.com/r/MachineLearning/comments/1mqccah/r_code_for_flow_stochastic_segmentation_networks/,r_1mqccah,,,
r_1mqbf3m,reddit,AdInevitable1362,2025-08-14T19:59:46+00:00,"[P] Can I use test set reviews to help predict ratings, or is that cheating?
Iâ€™m working on a rating prediction (regression) model. I also have reviews for each user-item interaction, and from those reviews I can extract â€œaspectsâ€ (like quality, price, etc.) and build a separate graphs and concatenate their embeddings at the end to help predicting the score.

My question is: when I split my data into train/test, is it okay to still use the aspects extracted from the test set reviews during prediction, or is that considered data leakage?

In other words: the interaction already exists in the test set, but is it fair to use the test review text to help the model predict the score? Or should I only use aspects from the training set and ignore them for test interactions?

Ps: Iâ€™ve been reading a paper where they take user reviews, extract â€œaspectsâ€ (like quality, price, serviceâ€¦), and build an aspect graph linking users and items through these aspects.

In their case, the goal was link prediction â€” so they hide some userâ€“itemâ€“aspect edges and train the model to predict whether a connection exists.",MachineLearning,3,https://www.reddit.com/r/MachineLearning/comments/1mqbf3m/p_can_i_use_test_set_reviews_to_help_predict/,r_1mqbf3m,,,
r_1mq5wiz,reddit,ImaginationAny2254,2025-08-14T16:43:01+00:00,"[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?
I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.

Although I love my field but I have got to thinking lately that Is it even worth it?",MachineLearning,99,https://www.reddit.com/r/MachineLearning/comments/1mq5wiz/d_people_in_mldsai_field_since_510_years_or_more/,r_1mq5wiz,,,
r_1mq3w9c,reddit,RobertWF_47,2025-08-14T15:29:41+00:00,"[D] Best way to partition longitudinal data into pre and post time periods for predictive model?
I'm working on several healthcare models that will predict future health conditions for individuals using past longitudinal data. We have data spanning 6 years.

In the past I'd split the data into one year time spans by calendar year and train the model to predict the outcome in year t1 from predictors in the prior year t0. If we have 6 years of data for a person I'd transform their data from wide to long format: 5 rows of pre and post periods. But I'm not certain this is the best approach.

What is the optimal way to split my data into pre and post time periods to obtain the best prediction accuracy? 6 month time periods instead of 1 year? Or lump all past data for each person into a single pre period & post period (1 row)? I understand it may come down to testing different formats, see what sticks.",MachineLearning,6,https://www.reddit.com/r/MachineLearning/comments/1mq3w9c/d_best_way_to_partition_longitudinal_data_into/,r_1mq3w9c,,,
r_1mq3nia,reddit,Practical-Pin8396,2025-08-14T15:20:53+00:00,"[P] Small and Imbalanced dataset - what to do
Hello everyone!

I'm currently in the 1st year of my PhD, and my PI asked me to apply some ML algorithms to a dataset (n = 106, w/ n = 21 in the positive class). As you can see, the performance metrics are quite poor, and I'm not sure how to proceed...

Iâ€™ve searched both in this subreddit and internet, and I've tried using LOOCV and stratified k-fold as cross-validation methods. However, the results are consistently underwhelming with both approaches. Could this be due to data leakage? Or is it simply inappropriate to apply ML to this kind of dataset?

Additional info:  
I'm in the biomedical/bioinformatics field (working w/ datasets of cancer or infectious diseases). These patients are from a small, specialized group (adults with respiratory diseases who are also immunocompromised). Some similar studies have used small datasets (e.g., n = 50), while others succeeded in work with larger samples (n = 600â€“800).  
Could you give me any advice or insights? (Also, sorry for gramatics, English isn't my first language). TIA!

https://preview.redd.it/fc20uero50jf1.png?width=655&format=png&auto=webp&s=1ed35c046f9c2bfe030e0c3bfe8c4cdcf7afb852

",MachineLearning,42,https://www.reddit.com/r/MachineLearning/comments/1mq3nia/p_small_and_imbalanced_dataset_what_to_do/,r_1mq3nia,,,
r_1mpbp39,reddit,Entrepreneur7962,2025-08-13T18:00:48+00:00,"[D] Got Spare Time â€“ Whatâ€™s Worth Doing?
I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  
It happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  
I really don't know how to handle simply doing nothing.

I thought maybe Iâ€™d start some initiative in this rare window Iâ€™m in right now, and I was hoping to get interesting ideas from the community.

My main objective is that it would be something valuable that I enjoy doing.  
This could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).

Love to hear your thoughts!",MachineLearning,42,https://www.reddit.com/r/MachineLearning/comments/1mpbp39/d_got_spare_time_whats_worth_doing/,r_1mpbp39,,,
r_1mpa2ip,reddit,Proud_Landscape_4231,2025-08-13T17:01:28+00:00,"[D] If there were to be some sort of way you could get NDVI (not true, but predict) that was near perfect accuracy through JUST standard RGB input (NO NIR AT ALL), how useful would that be (API, for example)?
Sorry if this is not the right place to post! I'm new to the community and overall GIS industry. Just want to see how useful this would be, specific use cases, and maybe how this could be used by you personally.

  
I know there are RGB-only indices that exist, but from what I've heard, they're very inaccurate. This would be 94%+ (accuracy to true-NDVI) and itâ€™s a highly trained ML model",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mpa2ip/d_if_there_were_to_be_some_sort_of_way_you_could/,r_1mpa2ip,,,
r_1mp6n2g,reddit,ChampionshipCrazy429,2025-08-13T14:53:04+00:00,"[D] Google DeepMind Analytics Engineer Interview Prep
Got an upcoming interview for this role and have a good feeling so far. How do I prepare for it? What will be the next steps? Any tips or experience would be greatly appreciated. Thanks!",MachineLearning,21,https://www.reddit.com/r/MachineLearning/comments/1mp6n2g/d_google_deepmind_analytics_engineer_interview/,r_1mp6n2g,,,
r_1mp3u1f,reddit,Accomplished-Pay-390,2025-08-13T13:02:29+00:00,"[D] EMNLP 2025 Decisions
Discussion thread for EMNLP 2025 decisions

",MachineLearning,31,https://www.reddit.com/r/MachineLearning/comments/1mp3u1f/d_emnlp_2025_decisions/,r_1mp3u1f,,,
r_1mp2dcp,reddit,ArtemHnilov,2025-08-13T11:55:31+00:00,"[R] Fuzzy-Pattern Tsetlin Machine
Iâ€™m excited to announce the paper:Â **Fuzzy-Pattern Tsetlin Machine** (FPTM)Â â€” a paradigm shift in the Tsetlin Machine family of algorithms.

Unlike traditional Tsetlin Machines, which rely on strict clause evaluation, FPTM introducesÂ fuzzy clause evaluation: if some literals in a clause fail, the remaining literals can still contribute to the vote with a proportionally reduced score. This allows each clause to act as a collection of adaptive sub-patterns, enabling more flexible, efficient, and robust pattern matching.

Thanks to this fuzzy mechanism, FPTM dramatically reduces the number of required clauses, memory usage, and training time â€” all while improving accuracy.

**Results:**

**IMDb** dataset:

â€¢ 90.15% accuracy with just **1 clause** per class

â€¢ 50Ã— reduction in clauses and memory vs. Coalesced TM

â€¢ 36Ã— to 316Ã— faster training (**45 seconds vs. 4 hours**) compared to TMU Coalesced TM

â€¢ Fits in **50 KB**, enabling online learning on microcontrollers

â€¢ Inference throughput: **34.5 million** predictions per second (51.4 GB/s)

**Fashion-MNIST** dataset:

â€¢ 92.18% accuracy (2 clauses per class)

â€¢ 93.19% accuracy (20 clauses), \~400Ã— clause reduction vs. Composite TM (93.00% with 8000 clauses)

â€¢ **94.68%** accuracy (8000 clauses), establishing a new *state-of-the-art* among all TM variants and outperforming complex neural net architectures like *Inception-v3*

**Amazon Sales** dataset (20% noise):

â€¢ **85.22%** accuracy â€” outperforming Graph TM (78.17%) and GCN (66.23%)

ðŸ“„ Read the paper: [https://arxiv.org/pdf/2508.08350](https://arxiv.org/pdf/2508.08350)

ðŸ’» Source code: [https://github.com/BooBSD/FuzzyPatternTM](https://github.com/BooBSD/FuzzyPatternTM)",MachineLearning,46,https://www.reddit.com/r/MachineLearning/comments/1mp2dcp/r_fuzzypattern_tsetlin_machine/,r_1mp2dcp,,,
r_1mo7fzx,reddit,fishsoon2020,2025-08-12T12:32:11+00:00,"[R] About test set of XGBoost for Time Series Forecasting
I have questions about using XGBoost for the Time Series Forecasting problem. According to these articles:

[Multi-step time series forecasting with XGBoost | Towards Data Science](https://towardsdatascience.com/multi-step-time-series-forecasting-with-xgboost-65d6820bec39/)[XGBoost for ](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[Multi-Step Univariate Time Series Forecasting with MultiOutputRegressor | XGBoosting](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[How I Trained a Time-Series Model with XGBoost and Lag Features](https://medium.com/@connect.hashblock/how-i-trained-a-time-series-model-with-xgboost-and-lag-features-8c17439c81e4)

I understand that they are using a sliding window approach to create ($t\_1, t\_2, ..., t\_n, t\_{n+1}, t\_{n+2}..., t\_m$), where the first $n$ variables are used as feature variables and the last $m$ variables are used as target variables. Then, they feed these rows into the XGBoost to find the relationship between the feature variables and target variables.

My problem is: It appears that during the testing phase, they utilized the actual feature variables for testing. For example, when we are predicting the first future $m$ points, we still have the actual $n$ points before these $m$ points as the features. However, when we are predicting the $m+1$ points, we are missing the actual value for the first feature in the $n$ features.

But in the above articles, it seems they just assume they have the actual $n$ at all times during training.

And for the paper ""Do We Really Need Deep Learning Models for Time Series Forecasting?"", for table 1 as shown below:

I think h refers to the number of regressors they are using. So, for the first row, they can forecast 24 points using the existing training data. But how can they further forecast Ï„ points beyond the 20th point?

So, I want to clarify

1. Do the methods in the above articles suffer from data leakage? Or is it safe to assume that we can know the real $n$ features when we are focusing on the $m$ new data points?
2. My current idea is that for using XGBoost in time series forcasting, we can either

* Feed back the predicted value as the $n$ feature for the upcoming forcasting of $m$ points.
* Or we train $L$ independent regressors to forecast the $L$ points in the future in one batch.

",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1mo7fzx/r_about_test_set_of_xgboost_for_time_series/,r_1mo7fzx,,,
r_1mor8vy,reddit,NoteDancing,2025-08-13T01:30:00+00:00,"[D] Applying Prioritized Experience Replay in the PPO algorithm
When using the PPO algorithm, can we improve data utilization by implementing Prioritized Experience Replay (PER) where the priority is determined by both the probability ratio and the TD-error, while simultaneously using a windows\_size\_ppo parameter to manage the experience buffer as a sliding window that discards old data?",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1mor8vy/d_applying_prioritized_experience_replay_in_the/,r_1mor8vy,,,
r_1mou8y4,reddit,Elegant_Dream4936,2025-08-13T03:55:28+00:00,"[R] Promising Research Directions for VLMs in the Medical Domain
Dear all,

Iâ€™d like to hear the communityâ€™s thoughts on promising research directions for VLMs (e.g., CLIP), particularly in the medical domain.

Thank you in advance!",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mou8y4/r_promising_research_directions_for_vlms_in_the/,r_1mou8y4,,,
r_1moqpqw,reddit,seventh_day123,2025-08-13T01:05:42+00:00,"[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF
>From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)

**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  
**As the original author, I feel compelled to issue a statement.**

In short: **OpenRLHF is like KartRider â€” the original â€” and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**

# 1. Performance Differences Between OpenRLHF and veRL

There is no fundamental performance difference between veRLâ€™s FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  
The performance data in veRLâ€™s original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  
If you think thereâ€™s a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeedâ€™s **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.

# 2. On HybridFlow Free Scheduling

Any RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  
This means HybridFlow in veRL's paper is essentially just a nicer name for Rayâ€™s Placement Group API.  
Currently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  
OpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  
In fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.

# 3. Hybrid Engine

Hybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  
Both veRL and OpenRLHF now support this feature.

# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF Training

This setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.

It was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  
veRL FSDP **fully copied** this setup.

https://preview.redd.it/vfzm143vroif1.png?width=1440&format=png&auto=webp&s=10d8a5bcd101455a06a3506f037abc10f12dd277

https://preview.redd.it/tqela8mvroif1.png?width=1440&format=png&auto=webp&s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8d

The core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.

**The Original OpenRLHF Architecture:**  
**Ray + vLLM + ZeRO + HF**

There are also many related implementation details:

* Supported feature list
* Standardized interfaces such as `--input_key` to specify the input field format

All of these in veRL FSDP were **modeled after OpenRLHF**.

**Example from code details:**  
veRL:

https://preview.redd.it/b8f2lprwroif1.png?width=1440&format=png&auto=webp&s=a0daf3eab1c77f71e4917c044f988c35e229baa4

https://preview.redd.it/exf7lxhxroif1.png?width=1440&format=png&auto=webp&s=220636cea299502df1b94e2544a76b34e2acb6c7

OpenRLHF:

https://preview.redd.it/qfakvovyroif1.png?width=1440&format=png&auto=webp&s=260775676354a50bacd79ce06fb25417a53466de

Other design ideas like **ref\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.

# 5. Single Controller

*(Update May 2025)*

The â€œSingle Controllerâ€ concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.

In early versions of OpenRLHFâ€™s Ray RLHF implementation, there was a `RayPPOActorGroup` conceptâ€”managing a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  
Thatâ€™s essentially the core idea of Single Controller.

[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)

This interface wasnâ€™t enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.

Lastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).

However, I hope friends in the community wonâ€™t disparage other open-source frameworks.  
OpenRLHF, as a **zero-budget, purely open-source** project, canâ€™t compete in development speed with large commercial projects like veRLâ€”  
I only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.

**Btw, the open-source community should respect originality in order to develop healthily.**",MachineLearning,13,https://www.reddit.com/r/MachineLearning/comments/1moqpqw/d_statement_on_the_originality_of_openrlhf_and/,r_1moqpqw,,,
r_1mokka9,reddit,alkalinemoe,2025-08-12T20:49:31+00:00,"[D] Multiple submission policy at EMNLP 2025 for workshops
Hi all,

Iâ€™m trying to understand the EMNLP 2025 multiple submission policy when it comes to co-organized workshops.

Our paper is committed to EMNLP 2025 (main conference), but we think it might also be a good fit for a specific workshop, in case if it is not accepted to EMNLP. 

The problem is, the workshopâ€™s submission deadline is before the EMNLP notification date (Aug 20).

The workshopâ€™s CFP says multiple submissions are fine if disclosed at submission. However, the EMNLP CFP states it follows the ARR multiple submission policy, which includes this clause:

> Commitment + Commitment/Other Venue: Whether you can commit/submit to two venues simultaneously depends on the dual submission policies of those venues. Typically, it is not permitted.

[ARR policy](https://aclrollingreview.org/cfp#:~:text=Multiple%20Submission%20Policy)


TL;DR 

What Iâ€™m unsure about is this:

- Does â€œother venueâ€ here include EMNLP co-organized workshops?

- Has anyone successfully submitted to both the main conference and a co-organized workshop in this timing overlap?


I couldnâ€™t find any direct clarification online for this year, so Iâ€™d really appreciate hearing from researchers whoâ€™ve navigated this.

Thanks!",MachineLearning,5,https://www.reddit.com/r/MachineLearning/comments/1mokka9/d_multiple_submission_policy_at_emnlp_2025_for/,r_1mokka9,,,
r_1moj422,reddit,fictoromantic_25,2025-08-12T19:55:28+00:00,"Guidance on improving the reconstruction results of my VAE [Project]
Hi all! I was trying to build a VAE with an LSTM to reconstruct particle trajectories by basing off my model on the paper ""Modeling Trajectories with Neural Ordinary Differential Equations"". However, despite my loss plots showing a downward trend, my predictions are linear.

I have applied KL annealing and learning rate scheduler - and yet, the model doesn't seem to be learning the non-linear dynamics. The input features are x and z positions, velocity, acceleration, and displacement. I used a combination of ELBO and DCT for my reconstruction loss. The results were quite bad with MinMax scaling, so I switched to z-score normalization, which helped improve the scales. I used the Euler method with torchdiffeq.odeint.

Would it be possible for any of you to guide me on what I might be doing wrong? Iâ€™m happy to share my implementation if it helps. I appreciate and am grateful for any suggestions (and sorry about missing out on the labeling the axes - they are x and z)

https://preview.redd.it/veskdk7p7nif1.png?width=529&format=png&auto=webp&s=0938c4dd588961f94eba40a0e20d81008bc131f0

https://preview.redd.it/ddubae7p7nif1.png?width=529&format=png&auto=webp&s=15a24e197e6fd331d92175d1327fb2b482aaa2cc",MachineLearning,2,https://www.reddit.com/r/MachineLearning/comments/1moj422/guidance_on_improving_the_reconstruction_results/,r_1moj422,,,
r_1moj3xz,reddit,Pretend_Guava7322,2025-08-12T19:55:20+00:00,"[P] Can anyone suggest an open weights AI Humanizer?
I've often wanted to make an AI humanizer. The first approach I've tried was using `meta-llama/Llama-3.1-8B`. I first made a BERT fine-tune to classify between AI generated and human written. Then, I used a modified RL approach to fine-tune `meta-llama/Llama-3.1-8B` to rephrase an existing AI generated text, optimizing the humanness score. I repeated this several times, each time training a new scorer, similar to the GAN framework. This was largely unsuccessful. Unfortunately I can't share code because this was done months ago and I'm just now coming back to it, and I didn't properly track versions. I now believe that a T5 model would be better suited for this task than a Llama model. Does anyone have any suggestions, links, papers, or models that they can recommend? I am looking for open weights/open source models, not paid APIs.",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1moj3xz/p_can_anyone_suggest_an_open_weights_ai_humanizer/,r_1moj3xz,,,
r_1mo41xn,reddit,mystic_blue5,2025-08-12T09:29:58+00:00,"[R]: Intuition emerges in Maximum Caliber models at criticality
Are todayâ€™s AI models hitting a wall or just missing a law?

This recent preprint in arXiv proposes a minimal sandbox (a maze) and a statistical physics approach (Maximum Caliber principle) to address this question. The presented method, called mind-tuning, applies Maximum Caliber to predictive models and reveals a critical intuition phase between imitation and hallucination.

[https://arxiv.org/abs/2508.06477](https://arxiv.org/abs/2508.06477)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mo41xn/r_intuition_emerges_in_maximum_caliber_models_at/,r_1mo41xn,,,
r_1mo1ngm,reddit,hsbdbsjjd,2025-08-12T06:54:07+00:00,"[P] Dealing with EXTREME class imbalance(0.095% prevalence)
Iâ€™m trying to build a model for fraud prediction where I have a labeled dataset of ~200M records and 45 features. Itâ€™s supervised since I have the target label as well. Itâ€™s a binary classification problem and Iâ€™ve trying to deal with it using XGB and also tried neural network. 

The thing is that only 0.095% of the total are fraud. How can I make a model that generalizes well. Iâ€™m really frustrated at this point. I tried everything but cannot reach to the end. Can someone guide me through this situation?",MachineLearning,14,https://www.reddit.com/r/MachineLearning/comments/1mo1ngm/p_dealing_with_extreme_class_imbalance0095/,r_1mo1ngm,,,
r_1mo1l2u,reddit,hero88645,2025-08-12T06:49:51+00:00,"[D] Evaluation Drift and Contamination Mitigation in Foundation Model Assessment
As foundation models scale and benchmarks saturate, contamination and drift present increasing challenges to meaningful evaluation. Sharing practical mitigation strategies that have worked in practice:



\*\*Contamination Detection:\*\*

\- N-gram overlap analysis (sliding window approach)

\- Substring matching with fuzzy boundaries  

\- Semantic similarity scoring via embeddings

\- Statistical outlier detection in performance curves



\*\*Dataset Hygiene:\*\*

\- Temporal splits with strict cutoffs (no post-training data)

\- Hold-out validation across multiple independent sources

\- Private test sets with limited query budgets

\- Adversarial examples targeting memorization vs. understanding



\*\*Drift Mitigation:\*\*

\- Rolling evaluation windows with decay weighting

\- Multi-task assessment reducing single-metric gaming

\- Human evaluation correlation tracking over time

\- Cross-validation with domain-specific benchmarks



\*\*Process Controls:\*\*

\- Blind evaluation protocols (evaluator doesn't know model identity)

\- Staged releases with contamination audits between stages

\- Community-sourced benchmark validation

\- Reproducibility requirements for evaluation code



Seeing gaps in current practice around contamination detection at scale and standardized tooling for drift measurement. What approaches have proven most effective in your evaluation pipelines?",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1mo1l2u/d_evaluation_drift_and_contamination_mitigation/,r_1mo1l2u,,,
r_1mo1kci,reddit,hero88645,2025-08-12T06:48:34+00:00,"[D] Reliability Metrics and Failure Taxonomy for Agent Tool-Use Systems
Observing increasing deployment of agentic systems with tool access, but reliability evaluation remains fragmented. Key reliability metrics worth standardizing:



\*\*Success Rate Decomposition:\*\*

\- Tool selection accuracy (right tool for task)

\- Parameter binding precision (correct arguments)

\- Error recovery effectiveness (fallback strategies)

\- Multi-step execution consistency



\*\*Failure Taxonomy:\*\*

\- Type I: Tool hallucination (non-existent APIs)

\- Type II: Parameter hallucination (invalid args)

\- Type III: Context drift (losing task state)

\- Type IV: Cascade failures (error propagation)

\- Type V: Safety violations (unauthorized actions)



\*\*Observable Proxies:\*\*

\- Parse-ability of tool calls (syntactic validity)

\- Semantic coherence with task context

\- Graceful degradation under uncertainty

\- Consistency across equivalent phrasings



Current evals focus on task completion but miss failure modes that matter for deployment. Need systematic measurement of these reliability dimensions across diverse tool ecosystems.



Thoughts on standardizing these metrics across research groups?",MachineLearning,1,https://www.reddit.com/r/MachineLearning/comments/1mo1kci/d_reliability_metrics_and_failure_taxonomy_for/,r_1mo1kci,,,
r_1mo0ynr,reddit,NuoJohnChen,2025-08-12T06:10:51+00:00,"[R] Position: The Current AI Conference Model is Unsustainable!

Paper: https://www.alphaxiv.org/abs/2508.04586v1  


ðŸ“ˆ Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  


ðŸš€ Exponential Output Growth: Individual contributions are rising so fast theyâ€™re projected to exceed one paper per month by the 2040s.  


ðŸŒ Carbon Overload: NeurIPS 2024â€™s travel emissions (>8,254 tCOâ‚‚e) alone surpass Vancouverâ€™s daily citywide footprint.  


ðŸ˜ž Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  


â³ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  


ðŸŸï¸ Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space.",MachineLearning,395,https://www.reddit.com/r/MachineLearning/comments/1mo0ynr/r_position_the_current_ai_conference_model_is/,r_1mo0ynr,,,
r_1mnyque,reddit,ApprehensiveAd3311,2025-08-12T04:06:26+00:00,"[R] gpt-oss is actuall good: a case study on SATA-Bench
Iâ€™ve been experimenting with gpt-oss since its release, and unlike many posts/news Iâ€™ve seen, itâ€™s surprisingly powerful â€” even on uncommon datasets. I tested it on our recent benchmark SATA-Bench â€” a benchmark where each question has at least two correct answers (rare in standard LLM Evaluation).

Results (See picture below):

1. 120B open-source model is similar to GPT-4.1's performance on SATA-Bench.
2. 20B model lags behind but still matches DeepSeek R1 & Llama-3.1-405B.

https://preview.redd.it/eowlge0jjiif1.jpg?width=1568&format=pjpg&auto=webp&s=bfc0fdc20fc1545000ff55cc45f3b65391e85c46

 takeaways:

Repetitive reasoning hurts â€” 11% of 20B outputs loop, losing \~9 exact match rate.

Reasonâ€“answer mismatches happen often in 20B and they tend to produce one answer even if their reason suggest a few answer is correct.

Longer â‰  better â€” overthinking reduces accuracy.

Detailed findings:Â [https://weijiexu.com/posts/sata\_bench\_experiments.html](https://weijiexu.com/posts/sata_bench_experiments.html)

SATA-Bench dataset:Â [https://huggingface.co/datasets/sata-bench/sata-bench](https://huggingface.co/datasets/sata-bench/sata-bench)",MachineLearning,12,https://www.reddit.com/r/MachineLearning/comments/1mnyque/r_gptoss_is_actuall_good_a_case_study_on_satabench/,r_1mnyque,,,
r_1mnx3vn,reddit,Healthy_Horse_2183,2025-08-12T02:45:20+00:00,"[R] AAAI 2026 Reviewer Assignments?
Did anyone get assigned papers?

I submitted the biddings long time ago.",MachineLearning,15,https://www.reddit.com/r/MachineLearning/comments/1mnx3vn/r_aaai_2026_reviewer_assignments/,r_1mnx3vn,,,
r_1mnpqu7,reddit,Realistic-Bet-661,2025-08-11T21:26:41+00:00,"[N] OpenAI Delivers Gold-medal performance at the 2025 International Olympiad in Informatics
[https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL](https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL)

>We officially entered the 2025 International Olympiad in Informatics (IOI) online competition track and adhered to the same restrictions as the human contestants, including submissions and time limits,",MachineLearning,58,https://www.reddit.com/r/MachineLearning/comments/1mnpqu7/n_openai_delivers_goldmedal_performance_at_the/,r_1mnpqu7,,,
r_1mnhr4o,reddit,PlugTheGreatest,2025-08-11T16:30:19+00:00,"DRTP and No-Prop Hybrid in Pure C [R]
Hey guys its me again I made a new algorithm with No Prop and DRTP that hit a 91.25% on MNIST with one hidden layer and I did it all in pure C here is the link to the repo I will be writing a paper on it please leave reviews and feedback I am a undergraduate student trying to get an internship for ML Research and or Engineering. First in the world from what I can see by the way.

[https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C](https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C)",MachineLearning,0,https://www.reddit.com/r/MachineLearning/comments/1mnhr4o/drtp_and_noprop_hybrid_in_pure_c_r/,r_1mnhr4o,,,
r_1mnfoum,reddit,Jealous-Leek-5428,2025-08-11T15:14:07+00:00,"[D] Has anyone tried cross-modal transfer for visual reasoning? This 76% MMMU result surprised me
I've been spending a lot of time lately evaluating different multimodal reasoning models for my research, and the gap between closed-source models like GPT-4.1 and open-source alternatives has been really frustrating. Most open models either can't handle complex visual reasoning or require massive compute resources.

Recently I came across Skywork-R1V3, a 38B parameter model that's been getting some attention in the community, so I decided to put it through its paces. What caught my eye initially was their claim of 76.0% accuracy on MMMU, which would put it competitive with much larger proprietary models.

After testing it extensively, I have to say the technical approach is really interesting. The model builds on InternVL-38B but what makes it special is how the Skywork team approached the reasoning problem. Instead of training visual reasoning from scratch, they found a way to transfer reasoning patterns from their existing text-based models into the multimodal domain.

From what I can tell from the paper and my experiments, they used reinforcement learning during post-training rather than just supervised fine-tuning. This seems to be key to why it performs so well on complex reasoning tasks. When I tested it on mathematical problems with diagrams and scientific figure interpretation, it consistently broke down problems into logical steps rather than just pattern matching.

The performance claims seem to hold up in my testing. It's genuinely competitive with closed-source alternatives on the types of visual reasoning tasks I care about, and the fact that it's fully open-source with quantized versions available makes it actually usable for research. I've been running the AWQ quantized version on a single A100 without issues.

What really impressed me is how well it handles cross-disciplinary reasoning where you need to connect visual information with abstract concepts. The chain-of-thought capabilities feel much more robust than other open models I've tried.

This connects to the broader Skywork ecosystem - their reward models have been downloaded over 750,000 times and seem to be helping multiple frontier models achieve strong benchmark results. There's clearly some solid technical work happening there.

I'm curious if others have experimented with cross-modal transfer approaches like this, or if anyone else has found effective ways to get strong reasoning performance without massive scale. Also interested in hearing thoughts on RL vs supervised approaches for this kind of multimodal reasoning - my sense is that RL might be underutilized in this space but I'd love to hear other perspectives.",MachineLearning,59,https://www.reddit.com/r/MachineLearning/comments/1mnfoum/d_has_anyone_tried_crossmodal_transfer_for_visual/,r_1mnfoum,,,
r_1mn8vkj,reddit,Proper_Dig_6618,2025-08-11T10:01:27+00:00,"[P] VulkanIlm: Accelerating Local LLM Inference on Older GPUs Using Vulkan (Non-CUDA) â€” Benchmarks Included
Hi ML community,

Iâ€™m building **VulkanIlm**, a Python wrapper around llama.cpp leveraging Vulkan for GPU acceleration on legacy and AMD GPUs (no CUDA required). This opens the door to efficient local LLM use without expensive hardware.

Recent benchmark highlights:

* Dell E7250 integrated GPU (i7-5600U): 33Ã— speedup on TinyLLaMA-1.1B chat model
* AMD RX 580 (8 GB): 4Ã— speedup on Gemma-3n-E4B-it (6.9B params)

Inspired by Jeff Geerlingâ€™s blog on accelerating LLMs with eGPU setups on Raspberry Pi ([https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5)), I adapted and expanded it to run on AMD RX 580. A full how-to guide will come soon.

Repo here: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)

Would love feedback or insights on Vulkan acceleration or similar efforts!",MachineLearning,29,https://www.reddit.com/r/MachineLearning/comments/1mn8vkj/p_vulkanilm_accelerating_local_llm_inference_on/,r_1mn8vkj,,,
r_1mn2j20,reddit,No-sleep-cuz-coffee,2025-08-11T03:32:56+00:00,"[D] Beyond fine-tuning and prompting for LLMs?
Iâ€™ve been following a lot of recent LLM competitions and projects, and Iâ€™ve noticed that most solutions seem to boil down to either fine-tuning a base model or crafting strong prompts. Even tasks that start out as â€œgeneralization to unseen examplesâ€ â€” like zero-shot classification â€” often end up framed as prompting problems in practice.

From my reading, these two approaches (fine-tuning and prompting) cover a lot of the ground, but Iâ€™m curious if Iâ€™m missing something. Are there other practical strategies for leveraging LLMs that go beyond these? For example, some technique that meaningfully improve zero-shot performance without becoming â€œjustâ€ a better prompt?

Would love to hear from practitioners whoâ€™ve explored directions beyond the usual fine-tune/prompt spectrum.",MachineLearning,7,https://www.reddit.com/r/MachineLearning/comments/1mn2j20/d_beyond_finetuning_and_prompting_for_llms/,r_1mn2j20,,,
r_1mn1tx2,reddit,PrimeMaester,2025-08-11T02:56:44+00:00,"[D] Which direction is better: from academia to industry, or the other way around?
Hi all, given the current state of machine learning, I have two questions:

1. At what point in their career can a university lecturer/professor take on a joint position in industry?
2. Alternatively, can a R&D researcher in industry go back to academia without having to restart at the bottom of the ladder?

**Some context:** I am a PhD student on track to graduate in two months. I have several offers for applied/research scientist roles in industry, and interesting postdocs that could lead to a fulfilling academic career. I am not motivated by high salaries, and I know I want to do machine learning research forever! But the early-career academic job insecurity and the constant competitive grant writing I hear about are seriously concerning. At the same time, I know I can make a stronger/quicker practical impact in industry, despite the corporate constraints (work hours, less freedom, etc.). This is why I'm wondering if, in order to get the best of both worlds, one could start in academia and then transition into industry over time (or vice versa).

My question is more related to early-career researchers; I am aware that once tenure is achieved, pretty much anything is doable (e.g., Hinton, LeCun).

Thank you for sharing any insights, examples, or experiences on this :)",MachineLearning,26,https://www.reddit.com/r/MachineLearning/comments/1mn1tx2/d_which_direction_is_better_from_academia_to/,r_1mn1tx2,,,
r_1ng1xk3,reddit,chrisgarzon19,2025-09-13T16:30:33+00:00,"The â€œthree tiersâ€ of data engineering pay â€” and how to move up
**The â€œthree tiersâ€ of data engineering pay â€” and how to move up (shout out to the article by geergly orosz which i placed in the bottom)**

I keep seeing folks compare salaries across wildly different companies and walk away confused. A useful mental model Iâ€™ve found is that comp clusters intoÂ *three tiers*Â based on company type, not just your years of experience or title. Sharing this to help people calibrate expectations and plan the next move.

# The three tiers

* **Tier 1 â€” â€œEngineering is a cost center.â€**Â Think traditional companies, smaller startups, internal IT/BI, or teams where data is a support function. Pay is the most modest, equity/bonuses are limited, scope is narrower, and work is predictable (reports, ELT to a warehouse, a few Airflow dags, light stakeholder churn).
* **Tier 2 â€” â€œData is a growth lever.â€**Â Funded startups/scaleups and product-centric companies. Youâ€™ll see modern stacks (cloud warehouses/lakehouses, dbt, orchestration, event pipelines), clearer paths to impact, and some equity/bonus. companies expect design thinking and hands-on depth. Faster pace, more ambiguity, bigger upside.
* **Tier 3 â€” â€œData is a moat.â€**Â Big tech, trading/quant, high-scale platforms, and companies competing globally for talent. Total comp can be multiples of Tier 1. hiring process are rigorous (coding + system design + domain depth). Expectations are high: reliability SLAs, cost controls at scale, privacy/compliance, streaming/near-real-time systems, complex data contracts.

None of these are â€œbetterâ€ by default. Theyâ€™re just different trade-offs: stability vs. upside, predictability vs. scope, lower stress vs. higher growth.

# Signals youâ€™re looking at each tier

* **Tier 1:**Â job reqs emphasize tools (â€œAirflow, SQL, Tableauâ€) over outcomes; little talk of SLAs, lineage, or contracts; analytics asks dominate; compensation is mainly base.
* **Tier 2:**Â talks about metrics that move the business, experimentation, ownership of domains, real data quality/process governance; base + some bonus/equity; leveling exists but is fuzzy.
* **Tier 3:**Â explicit levels/bands, RSUs or meaningful options, on-call for data infra, strong SRE practices, platform/mesh/contract language, cost/perf trade-offs are daily work.

# If you want to climb a tier, focus on evidence of impact at scale

This is what consistently changes comp conversations:

* **Design â†’ not just build.**Â Bring written designs for one or two systems you led: ingestion â†’ storage â†’ transformation â†’ serving. Show choices and trade-offs (batch vs streaming, files vs tables, CDC vs snapshots, cost vs latency).
* **Reliability & correctness.**Â Prove youâ€™ve owned SLAs/SLOs, data tests, contracts, backfills, schema evolution, and incident reviews. Screenshots arenâ€™t necessaryâ€”bullet the incident, root cause, blast radius, and the guardrail you added.
* **Cost awareness.**Â Know your unit economics (e.g., cost per 1M events, per TB transformed, per dashboard refresh). If youâ€™ve saved the company money, quantify it.
* **Breadth across the stack.**Â A credible story across ingestion (Kafka/Kinesis/CDC), processing (Spark/Flink/dbt), orchestration (Airflow/Argo), storage (lakehouse/warehouse), and serving (feature store, semantic layer, APIs). You donâ€™t need to be an expert in allâ€”show you can choose appropriately.
* **Observability.**Â Lineage, data quality checks, freshness alerts, SLIs tied to downstream consumers.
* **Security & compliance.**Â RBAC, PII handling, row/column-level security, audit trails. Even basic exposure here is a differentiator.

# prep that actually moves the needle

* **Coding:**Â you donâ€™t need to win ICPC, but youÂ *do*Â need to write clean Python/SQL under time pressure and reason about complexity.
* **Data system design:**Â practice 45â€“60 min sessions. Design an events pipeline, CDC into a lakehouse, or a real-time metrics system. Cover partitioning, backfills, late data, idempotency, dedupe, compaction, schema evolution, and cost.
* **Storytelling with numbers:**Â have 3â€“4 impact bullets with metrics: â€œReduced warehouse spend 28% by switching X to partitioned Parquet + object pruning,â€ â€œCut pipeline latency from 2h â†’ 15m by moving Y to streaming with windowed joins,â€ etc.
* **Negotiation prep:**Â know base/bonus/equity ranges for the level (bands differ by tier). Understand RSUs vs options, vesting, cliffs, refreshers, and how performance ties to bonus.

# Common traps that keep people stuck

* **Tool-first resumes.**Â Listing ten tools without outcomes reads Tier 1. Frame with â€œproblem â†’ action â†’ measurable result.â€
* **Only dashboards.**Â Valuable, but hiring loops for higher tiers want ownership of dataÂ *as a product*.
* **Ignoring reliability.**Â If youâ€™ve never run an incident call for data, youâ€™re missing a lever that Tier 2/3 value highly.
* **No cost story.**Â At scale, cost is a feature. Even a small POC that trims spend is compelling signal.

# Why this matters

Averages hide the spread. Two data engineers with the same YOE can be multiple tiers apart in pay purely based on company type and scope. When you calibrate to tiers, expectations and strategy get clearer.

If you want a deeper read on the broader â€œthree clustersâ€ concept for software salaries, Gergely Orosz has a solid breakdown (â€œThe Trimodal Nature of Software Engineering Salariesâ€). The framing maps neatly onto data engineering roles too. link in the bottom

Curious to hear from this sub:

* If you moved from Tier 1 â†’ 2 or 2 â†’ 3, what was the single project or proof point that unlocked it?
* For folks hiring: what signalsÂ *actually*Â distinguish tiers in your loop?

article:Â [https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/](https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/)",datascience,0,https://www.reddit.com/r/datascience/comments/1ng1xk3/the_three_tiers_of_data_engineering_pay_and_how/,r_1ng1xk3,,,
r_1nfzxy9,reddit,thermokopf,2025-09-13T15:11:27+00:00,"Database tools and method for tree structured data?
I have a database structure which I believe is very common, and very general, so Iâ€™m wondering how this is tackled.

The database structured like:

     -> Project (Name of project)

           -> Category (simple word, ~20 categories)

                  -> Study
 

Study is a directory containing:
- README with date & description (txt or md format)
- Supporting files which can be any format (csv, xlsx, ptpx, keynote, text, markdown, pickled data frames, possible processing scripts, basically anything.)

Relationships among data:
- Projects can have shared studies.
- Studies can be related or new versions of older ones, but can also be completely independent.

Total size:
- 1 TB, mostly due to supporting files found in studies.

What I want:
- Search database for queries describing what we are looking for.
- Eventually get pointed to proper study directory and/or contents, showing all the files.
- Find which studies are similar based on description category, etc.

What is a good way to search such a database?  Considering itâ€™s so simple, do I even need a framework like sql?",datascience,1,https://www.reddit.com/r/datascience/comments/1nfzxy9/database_tools_and_method_for_tree_structured_data/,r_1nfzxy9,,,
r_1nf92kr,reddit,FinalRide7181,2025-09-12T17:17:28+00:00,"Does meta only have product analytics?
I have been told that all meta data scientists are all product analysts meaning that they do ab tests and sql.

Despite this, i ve been told by friends of mine that google, amazon, uberâ€¦ they all have two different types of data scientist: one doing product analytics and one doing statistical modeling and/or ml for business problems.

Does this apply to meta too? I remember looking at their jobs page a few months ago and they had multiple data science roles that had ml as requirement and many more technical requirements, compared to PDS who only have one requirement which is sql.",datascience,48,https://www.reddit.com/r/datascience/comments/1nf92kr/does_meta_only_have_product_analytics/,r_1nf92kr,,,
r_1nf7dgb,reddit,onestardao,2025-09-12T16:11:25+00:00,"fixing ai bugs before they happen: a semantic firewall for data scientists
if youâ€™ve ever worked on RAG, embeddings, or even a chatbot demo, youâ€™ve probably noticed the same loop:

model outputs garbage â†’ you patch â†’ another garbage case pops up â†’ you patch again.

that cycle is not random. itâ€™s structural. and it can be stopped.

---

## whatâ€™s a semantic firewall?

think of it like data validation â€” but for reasoning.

before letting the model generate, you check if the semantic state is stable. if drift is high, or coverage is low, or risk grows with each loop, you block it. you retry or reset. only when the state is stable do you let the model speak.

itâ€™s like checking assumptions before running a regression. if the assumptions fail, you donâ€™t run the model â€” you fix the input.

---

## before vs after (why it matters)

traditional fixes (after generation)

* let model speak â†’ detect bug â†’ patch with regex or reranker
* same bug reappears in a different shape
* stability ceiling ~70â€“80%

semantic firewall (before generation)

* inspect drift, coverage, risk *before* output
* if unstable, loop or fetch one more snippet
* once stable, generate â†’ bug never resurfaces
* stability ceiling ~90â€“95%

this is the same shift as going from *firefighting with ad-hoc features* to *installing robust data pipelines*.

---

## concrete examples (Problem Map cases)

WFGY Problem Map catalogs 16 reproducible failures every pipeline hits.
here are a few that data scientists will instantly recognize:

* No.1 hallucination & chunk drift
  retrieval gives irrelevant content. looks right, isnâ€™t. fix: block when drift > 0.45, re-fetch until overlap is enough.

* No.5 semantic â‰  embedding
  cosine similarity â‰  true meaning. patch: add semantic firewall that checks coverage score, not just vector distance.

* No.6 logic collapse & recovery
  chain of thought goes dead-end. fix: detect entropy rising, reset once, re-anchor.

* No.14 bootstrap ordering
  classic infra bug â€” service calls vector DB before itâ€™s warmed. semantic firewall prevents â€œempty answerâ€ from leaking out.

---

## quick sketch in code

pseudo-python, so you can see how it feels in practice:

```python
def drift(prompt, ctx):
    # jaccard overlap
    A = set(prompt.lower().split())
    B = set(ctx.lower().split())
    return 1 - len(A & B) / max(1, len(A | B))

def coverage(prompt, ctx):
    kws = prompt.lower().split()[:8]
    hits = sum(1 for k in kws if k in ctx.lower())
    return hits / max(1, len(kws))

def risk(loop_count, tool_depth):
    return min(1, 0.2*loop_count + 0.15*tool_depth)

def firewall(prompt, retrieve, generate):
    prev_haz = None
    for i in range(2):  # allow one retry
        ctx = retrieve(prompt)
        d, c, r = drift(prompt, ctx), coverage(prompt, ctx), risk(i, 1)
        if d <= 0.45 and c >= 0.70 and (prev_haz is None or r <= prev_haz):
            return generate(prompt, ctx)
        prev_haz = r
    return ""âš ï¸ semantic state unstable, safe block.""
```

---

## faq (beginner friendly)

q: do i need a vector db?
no. you can start with keyword overlap. vector DB comes later.

q: will this slow inference?
not much. one pre-check and maybe one retry. usually faster than chasing random bugs.

q: can i use this with any LLM?
yes. itâ€™s model-agnostic. the firewall checks *signals*, not weights.

q: what if iâ€™m not sure which error i hit?
open the Problem Map , scan the 16 cases, match symptoms. it points to the minimal fix.

q: why trust this?
because the repo hit 0â†’1000 stars in one season , real devs tested it, found it cut debug time by 60â€“80%.

---

## takeaway

semantic firewall = shift from patching *after* the fact to preventing *before* the fact.

once you try it, the feeling is the same as moving from messy scripts to reproducible pipelines: fewer fires, more shipping.

even if you never use the formulas, itâ€™s the interview ace you can pull out when asked: â€œhow would you handle hallucination in production?â€",datascience,27,https://www.reddit.com/r/datascience/comments/1nf7dgb/fixing_ai_bugs_before_they_happen_a_semantic/,r_1nf7dgb,,,
r_1nenrg8,reddit,ChavXO,2025-09-11T23:21:51+00:00,An introduction to program synthesis,datascience,2,https://www.reddit.com/r/datascience/comments/1nenrg8/an_introduction_to_program_synthesis/,r_1nenrg8,,,
r_1nehig5,reddit,nullstillstands,2025-09-11T19:05:18+00:00,Global survey exposes what HR fears most about AI,datascience,42,https://www.reddit.com/r/datascience/comments/1nehig5/global_survey_exposes_what_hr_fears_most_about_ai/,r_1nehig5,,,
r_1negm5l,reddit,FinalRide7181,2025-09-11T18:30:56+00:00,"How do data scientists add value to LLMs?
Edit: i am not saying AI is replacing DS, of course DS still do their normal job with traditional stats and ml, i am just wondering if they can play an important role around LLMs too

Iâ€™ve noticed that many consulting firms and AI teams have Forward Deployed AI Engineers. They are basically software engineers who go on-site, understand a companyâ€™s problems and build software leveraging LLM APIs like ChatGPT. They donâ€™t build models themselves, they build solutions using existing models.

This makes me wonder: can data scientists add values to this new LLM wave too (where models are already built)? For example i read that data scientists could play an important role in dataset curation for LLMs. 

Do you think that DS can leverage their skills to work with AI eng in this consulting-like role?
",datascience,47,https://www.reddit.com/r/datascience/comments/1negm5l/how_do_data_scientists_add_value_to_llms/,r_1negm5l,,,
r_1neack3,reddit,alpha_centauri9889,2025-09-11T14:30:36+00:00,"Transitioning to MLE/MLOps from DS
I am working as a DS with some 2 years of experience in a mid tier consultancy. I work on some model building and lot of adhoc analytics. I am from CS background and I want to be more towards engineering side. Basically I want to transition to MLE/MLOps. My major challenge is I don't have any experience with deployment or engineering the solutions at scale etc. and my current organisation doesn't have that kind of work for me to internally transition. Genuinely, what are my chances of landing in the roles I want? Any advice on how to actually do that? I feel companies will hardly shortlist profiles for MLE without proper experience. If personal projects work I can do that as well. Need some genuine guidance here. 
",datascience,13,https://www.reddit.com/r/datascience/comments/1neack3/transitioning_to_mlemlops_from_ds/,r_1neack3,,,
r_1ne9hzv,reddit,WillingAstronomer,2025-09-11T13:56:50+00:00,"Mid career data scientist burnout
Been in the industry since 2012. I started out in data analytics consulting. The first 5 were mostly that, and didn't enjoy the work as I thought it wasn't challenging enough. In the last 6 years or so, I've moved to being a Senior Data Scientist - the type that's more close to a statistical modeller, not a full-stack data scientist.  Currently work in health insurance (fairly new, just over a year in current role). I suck at comms and selling my work, and the more higher up I'm going in the organization, I realize I need to be strategic with selling my work, and also in dealing with people. It always has been an energy drainer for me - I find I'm putting on a front.  
Off late, I feel 'meh' about everything. The changes in the industry, the amount of knowledge some technical, some industry based to keep up with seems overwhelming.

Overall, I chart some of these feelings to a feeling of lacking capability to handling stakeholders, lack of leadership skills in the role/ tying to expectations in the role. (also want to add that I have social anxiety). Perhaps one of the things might help is probably upskilling on the social front. Anyone have similar journeys/ resources to share?  
I started working with a generic career coach, but haven't found it that helpful as the nuances of crafting a narrative plus selling isn't really coming up (a lot more of confidence/ presence is what is focused on).

Edit: Lots of helpful directions to move in, which has been energizing.",datascience,173,https://www.reddit.com/r/datascience/comments/1ne9hzv/mid_career_data_scientist_burnout/,r_1ne9hzv,,,
r_1ne37bs,reddit,ciaoshescu,2025-09-11T08:18:43+00:00,"Looking for recent research on explainable AI (XAI)
I'd love to get some papers on the latest advancements on explainable AI (XAI). I'm looking for papers that are at most 2-3 years old and had an impact. Thanks!",datascience,10,https://www.reddit.com/r/datascience/comments/1ne37bs/looking_for_recent_research_on_explainable_ai_xai/,r_1ne37bs,,,
r_1ne1d5t,reddit,ButtFlannel69,2025-09-11T06:18:26+00:00,Collaborating with data teams,datascience,2,https://www.reddit.com/r/datascience/comments/1ne1d5t/collaborating_with_data_teams/,r_1ne1d5t,,,
r_1nd94cy,reddit,ThomasAger,2025-09-10T09:02:35+00:00,(: Smile! Itâ€™s my first open source project,datascience,3,https://www.reddit.com/r/datascience/comments/1nd94cy/smile_its_my_first_open_source_project/,r_1nd94cy,,,
r_1ncmcgf,reddit,Factitious_Character,2025-09-09T15:41:08+00:00,"Pytorch lightning vs pytorch
Today at work, i was criticized by a colleague for implementing my training script in pytorch instead of pytorch lightning. His rationale was that the same thing could've been done in less code using lightning, and more code means more documentation and explaining to do. I havent familiarized myself with pytorch lightning yet so im not sure if this is fair criticism, or something i should take with a grain of salt. I do intend to read the lightning docs soon but im just thinking about this for my own learning. Any thoughts?",datascience,64,https://www.reddit.com/r/datascience/comments/1ncmcgf/pytorch_lightning_vs_pytorch/,r_1ncmcgf,,,
r_1nc93qq,reddit,bingbong_sempai,2025-09-09T04:04:05+00:00,"I built a card recommender for EDH decks
Hi guys! I built a simple card recommender system for the EDH format of Magic the Gathering. Unlike EDHREC which suggests cards based on overall popularity, this analyzes your full decklist and recommends cards based on similar decks.

Deck similarity is computed as the sum of idf weights of shared cards. It then shows the top 100 cards from similar decks that aren't already in your decklist. It's simple but will usually give more relevant suggestions for your deck.

Try it [here](https://huggingface.co/spaces/bingbong-sempai/edhrec-at-home): (Archidekt links only)

Would love to hear feedback!",datascience,20,https://www.reddit.com/r/datascience/comments/1nc93qq/i_built_a_card_recommender_for_edh_decks/,r_1nc93qq,,,
r_1nbxzs0,reddit,samushusband,2025-09-08T19:54:03+00:00,"Analysing Priority zones in my Area with unprecise home adresses
hello, My project analyzes whether given addresses fall inside ""Quartiers Prioritaires de la Politique de la Ville ""(QPV). It uses a GeoJSON file of QPV boundaries(available on the gorvernment website) and a geocoding service (Nominatim/OSM) to convert addresses into geographic coordinates. Each address is then checked with GeoPandas + Shapely to determine if its coordinates lie within any QPV polygon. The program can process one or multiple addresses, returning results that indicate whether each is located inside or outside a QPV, along with the corresponding zone name when available. This tool can be extended to handle CSV databases, produce visualizations on maps, or integrate into larger urban policy analysis workflows. "" 

BUUUT . 

here is the ultimate problem of this project , Home addresses in my area (Martinique) are notoriously unreliable if you dont know the way and google maps or Nominatim cant pinpoint most of the places in order to be converted to coordinates to say whether or not the person who gave the adress is in a QPV or not.  when i use my python script on adresses of the main land like paris and the like it works just fine but our little island isnt as well defined in terms of urban planning. 

can someone please help me to find a way to get all the streets data into coordinates and make them match with the polygon of the QPV areas ? thank you in advance",datascience,14,https://www.reddit.com/r/datascience/comments/1nbxzs0/analysing_priority_zones_in_my_area_with/,r_1nbxzs0,,,
r_1nbdtct,reddit,AutoModerator,2025-09-08T04:01:38+00:00,"Weekly Entering & Transitioning - Thread 08 Sep, 2025 - 15 Sep, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,9,https://www.reddit.com/r/datascience/comments/1nbdtct/weekly_entering_transitioning_thread_08_sep_2025/,r_1nbdtct,,,
r_1naptuq,reddit,mutlu_simsek,2025-09-07T10:31:17+00:00,ðŸš€ Perpetual ML Suite: Now Live on the Snowflake Marketplace!,datascience,1,https://www.reddit.com/r/datascience/comments/1naptuq/perpetual_ml_suite_now_live_on_the_snowflake/,r_1naptuq,,,
r_1nac35j,reddit,metalvendetta,2025-09-06T22:00:44+00:00,"How to evaluate data transformations?
There are several well-established benchmarks for text-to-SQL tasks like BIRD, Spider, and WikiSQL. However, I'm working on a data transformation system that handles per-row transformations with contextual understanding of the input data.

The challenge is that most existing benchmarks focus on either:

* Pure SQL generation (BIRD, Spider)
* Simple data cleaning tasks
* Basic ETL operations

But what I'm looking for are benchmarks that test:

* Complex multi-step data transformations
* Context-aware operations (where the same instruction means different things based on data context)
* Cross-column reasoning and relationships
* Domain-specific transformations that require understanding the semantic meaning of data

  
Has anyone come across benchmarks or datasets that test these more sophisticated data transformation capabilities?

",datascience,2,https://www.reddit.com/r/datascience/comments/1nac35j/how_to_evaluate_data_transformations/,r_1nac35j,,,
r_1na6x3q,reddit,Rockingtits,2025-09-06T18:27:39+00:00,"Help me evaluate a new job offer - Stay or go?
Hi all, 

I'm having a really hard time deciding whether or not to take an offer I've recently received, would really appreciate some advice and a sense check. For context I generally feel my current role is comfortable but i'm starting to plateau after the first year, i'm also in the process of buying my dream house just to complicate things.

### **Current Role**

##### The Good

- I am early 30's and have 4 years of experience as a full stack DS but am currently employed as an ML Eng for the last year. 
- My current role is effectively a senior/lead MLE in a small team (me + 3 DS) and I have loads of autonomy in how we do things and I get to lead my own  Gen AI projects with small squads as I'm the only one with experience in this domain. 
- I also get to straddle DS and MLE as much or as little as I want to in other projects, which suits my interests and background. 
- We have some interesting projects including one I'm leading. I think I have around 6 months of cool work to do where I can personally make an impact. 
- My work life balance is amazing, I'm not stressed at work at all and I can learn at my own pace. 
- Effectively remote, go into the office 1 or 2 times per month for meetings. It's 1.5 hours away but work pay for my travel. 
- Can push for a senior or principal title and will likely get it in the next ~6 months. 

##### The Bad

- The main drawbacks here are that I don't have senior technical mentors, my direct boss has good soft skills but I have nothing to learn from him technically. He's also quite chaotic, so we are always shifting priorities etc. 
- It's a brand new team so we are constantly hitting blockers in terms of processes, integration of our projects and office politics. 
- Being a legacy insurer, innovation is really hard and momentum needed to shift opinions is huge. 
- Fundamentally data quality is very poor and this won't change in my tenure. 
- Essentially in an echo chamber, I'm bringing most of the ideas and solutions to the table in the team which potentially isn't great at this stage in my career.
- It's not perfect and I'd have to leave at some point anyway. 


##### Comp
- Total comp including bonus and generous pension is Â£84K


### **New Job** AI Engineer

##### The Good
- Very cool AI consultancy startup, 2 years old, ~80 technical staff and growing rapidly, already profitable with a revenue of Â£1mill per month and partnership with Open AI.
- Lots of interesting projects with cool clients. The founders' mantra is ""cool projects, in production"" and they have some genuinely interesting case studies. 
- Some projects are genuinely cutting edge and they claim to have a nice balance between R&D and delivery. 
- Lots of technical staff to learn from, should be good for my growth. 
- Opportunity to work internationally in the future, the are opening offices in Australia now and eventually the US. 


##### The Bad

- Pigeon holing myself into AI/Agents/LLMs. No trad ML, may lose some of my very rounded skill set.
- Although it's customer facing, it sounds like the role is very delivery heavy and I'd essentially be smashing out code or researching all day with less soft skill development.
- Slightly worried about work culture and work life balance, this could end up being a meat grinder. 
- I have no experience of start ups or start up culture at all.
- Less job security as its a startup. 
- It's mostly based in London (5 hours round trip!) and I would need to travel down relatively frequently (expenses paid) for onboarding and establishing myself in the first few months, with that requirement tapering off slowly. 


##### Comp

- Total offer all in is Â£90K, I could try and negotiate for up to Â£95K based on their bandings. 
- 36000 stock units, worthless until they sell though
   


Would love to know your thoughts!


",datascience,13,https://www.reddit.com/r/datascience/comments/1na6x3q/help_me_evaluate_a_new_job_offer_stay_or_go/,r_1na6x3q,,,
r_1n9yrfy,reddit,Massive_Arm_706,2025-09-06T12:50:59+00:00,"Europe Salary Thread 2025 - What's your role and salary?
The yearly Europe-centric salary thread. You can find the last one here:

https://old.reddit.com/r/datascience/comments/1fxrmzl/europe_salary_thread_2024_whats_your_role_and/

I think it's worthwhile to learn from one another and see what different flavours of data scientists, analysts and engineers are out there in the wild. In my opinion, this is especially useful for the beginners and transitioners among us. So, do feel free to talk a bit about your work if you can and want to. ðŸ™‚

While not the focus, non-Europeans are of course welcome, too. Happy to hear from you!

**Data Science Flavour:** .

**Location:** .

**Title:** .

**Compensation (gross):** .

**Education level:** .

**Experience:** .

**Industry/vertical:** .

**Company size:** .

**Majority of time spent using (tools):** .

**Majority of time spent doing (role):** .",datascience,182,https://www.reddit.com/r/datascience/comments/1n9yrfy/europe_salary_thread_2025_whats_your_role_and/,r_1n9yrfy,,,
r_1n8z37l,reddit,vtfresh,2025-09-05T07:50:18+00:00,"Just got rejected from meta
Thought everything went well. Completed all questions for all interviews. Felt strong about all my SQL, A/B testing, metric/goal selection questions. No red flags during behavioral. Interviews provided 0 feedback about the rejection. I was talking through all my answers and reasoning, considering alternatives and explaining why I chose my approach over others. I led the discussions and was very proactive and always thinking 2 steps ahead and about guardrail metrics and stating my assumptions. The only ways I could think of improving was to answer more confidently and structure my thoughts more. Is it just that competitive right now? Even if I donâ€™t make IC5 I thought for sure Iâ€™d get IC4. Anyone else interview with Meta recently? 

edit:
MS degree
3.5yoe DS
4.5yoe ChemE

edit2:
I had 2 meta referrals but didn't use them. Should I tell the recruiter or does it not matter at this point?
Meta recruiter reached out to me on LinkedIn.

edit3:
I remember now there was 1 moment I missed a beat, but recovered during a bernoulli distribution hand-calculation question. Maybe thats all it took...

edit4:
Thanks everyone for the copium, words of advice, and support.",datascience,293,https://www.reddit.com/r/datascience/comments/1n8z37l/just_got_rejected_from_meta/,r_1n8z37l,,,
r_1n8lhdx,reddit,CryoSchema,2025-09-04T20:37:39+00:00,MIT says AI isnâ€™t replacing youâ€¦ itâ€™s just wasting your bossâ€™s money,datascience,556,https://www.reddit.com/r/datascience/comments/1n8lhdx/mit_says_ai_isnt_replacing_you_its_just_wasting/,r_1n8lhdx,,,
r_1n8a1do,reddit,petburiraja,2025-09-04T13:24:34+00:00,"A portfolio project for Data Scientists looking to add AI Engineering skills (Pytest, Security, Docker).
Hey guys,

Like many of us, I'm comfortable in a Jupyter Notebook, but I found there's a huge gap when it comes to building and deploying a real, full-stack AI application. I created a project specifically to bridge that gap.

You build a ""GitHub Repo Analyst"" agent, but the real learning is in the production-level engineering skills that often aren't part of a data science workflow:

- Automated Testing: Writing Pytest integration tests to verify your agent's security.
- Building UIs: Creating an interactive web app with Chainlit.
- Deployment: Packaging your entire application with Docker for easy, reproducible deployment.

I've turned this into a 10-lesson guide and am looking for 10-15 beta testers. If you're a data scientist who wants to add a serious AI engineering project to your portfolio, I'll give you the complete course for free in exchange for your feedback.

Just comment below if you're interested, and I'll send you a DM.",datascience,72,https://www.reddit.com/r/datascience/comments/1n8a1do/a_portfolio_project_for_data_scientists_looking/,r_1n8a1do,,,
r_1n88v2y,reddit,ShittyLogician,2025-09-04T12:32:48+00:00,"Almost 2 years into my first job... and already disillusioned and bored with this career
**TL;DR: I find this industry to be very unengaging, with most use cases and positions being very brainless, sluggish and just uninspiring. I am only 2 years into this job and bored and I feel like I need to shake things up a bit to keep doing this for the rest of my life.** 




Full disclosure: **this is very much a first world problem**. I get paid quite well, I have incredibly lenient work life balance, I work from home 3 days a week, etc etc. Most people would kill to be in my position at my age.


Some context: I was originally in academia doing a PhD in math, but pure math, completely unrelated to ML or anything in the real world really. ~2 years in, I was disillusioned with that (sensing a pattern here lol) so I took as many ML courses I could and jumped ship to industry. 


Regardless of all the problems I had in academia, it at least *asked* something of me. I had to think, like, *actually think*, about complex, interesting stuff. It felt like I was actually engaging my mind and growing. 


My current job is fine, basically applying LLMs for various use cases at a megacorp. On paper, I'm playing with the latest, greatest, tech, but in practice, I'm just really calling APIs on products that smarter people are building. 


I feel like I haven't actually flexed my brain muscles in years now, I'm forgetting all the stuff I've learnt at college, and the work itself is incredibly boring to me. Many many days I can barely bring myself to work as the work is so uninteresting, and the bare minimum I put in still somehow impresses my colleagues so there's no real incentive to work hard. 


I realize how privileged that sounds, I really do, but I do feel kind of unfulfilled and spiritually empty. I feel like if I keep doing this for the rest of my life I will look back with regret. 


**What I'm trying to do to fix this:** I would like to shift towards more cutting edge and harder data science. Problem here is a lack of qualifications and experience. I have a MS and a BS in Math (from T10 colleges) but no PhD and the math I studied was mostly pure/theoretical, very little to do with ML. 

I'm trying to do projects in my own time, but it's slow going on my own. I would love to aim for ML/AI research roles, but it feels like an impossible ask without a PhD, without papers, etc etc. I'm not sure that's a feasible goal. 



Another thing I've been considering is playing a DS/ML role as support in research that's *not* ML. For instance, bioinformatics or biotech, etc. This is also fairly appealing to me. The main issue is here is a complete lack of knowledge about these fields (since there can be so many fields here) and a lack of domain knowledge which I presume is required. I'm still trying, I've been applying for some bioinformatics roles, but yeah, also hard. 



**Has anyone else felt this way? What did they do about it, and what would you recommend?**",datascience,273,https://www.reddit.com/r/datascience/comments/1n88v2y/almost_2_years_into_my_first_job_and_already/,r_1n88v2y,,,
r_1n83iok,reddit,Final_Alps,2025-09-04T07:19:47+00:00,"Would you volunteer to join the team building AI tooling? If you have what has been your experience?
I just learned a colleague that was part of the AI tooling team is leaving and I am considering whether to ask to be added to their old project team. 

I am a data scientist and while I have not had too many ML projects recently, I have some lined up for next quarter. 

Their team was building the tooling to build agents for use internally and customer facing. That team has obviously gotten a lot of shout out from the CEO. Their early products are well received. 

I prefer ML over AI tooling but also feel there is a new reality for my next job in that I should be above average in AI usage and development. And thus I feel that being part of the AI team would be beneficial for my career. 

So my question is. Should I ask to join the AI team? Have others done this - what has been experienced? Anything to look out for/any ways to shape the my potential journey in that team? ",datascience,0,https://www.reddit.com/r/datascience/comments/1n83iok/would_you_volunteer_to_join_the_team_building_ai/,r_1n83iok,,,
r_1n81hrr,reddit,LilParkButt,2025-09-04T05:14:10+00:00,"How are you liking Positron?
Iâ€™m an undergraduate student double majoring in Data Analytics and Data Engineering and have used VSCode, Jupyter Notebook, Google Colab, and PyCharm Community Edition during my different Python courses. I havenâ€™t used Positron yet, but it looks really appealing since I enjoy the VSCode layout and notebook style programming. Anyone with experience using Position, Iâ€™d greatly appreciate any information on how youâ€™ve liked (or not liked) it. Thanks!",datascience,23,https://www.reddit.com/r/datascience/comments/1n81hrr/how_are_you_liking_positron/,r_1n81hrr,,,
r_1n81chu,reddit,OverratedDataScience,2025-09-04T05:05:25+00:00,"What's up with LinkedIn posts saying ""Excel is dead"", ""dashboards are dead"", ""data science is dead"", ""PPTs are dead"" and so on?
Is this a trend now? I also read somewhere ""SQL is dead"" too. Ffs. What isn't dead anyway for these Linkfluencers? Only LLMs? And then you hear mangers and leadership parrtoting the same LinkedIn bullshit in team meetings... where is all this going? ",datascience,137,https://www.reddit.com/r/datascience/comments/1n81chu/whats_up_with_linkedin_posts_saying_excel_is_dead/,r_1n81chu,,,
r_1n7zgzy,reddit,metalvendetta,2025-09-04T03:24:05+00:00,"Per row context understanding is hard for SQL and RAG databases, here's how we solved it with LLMs
Traditional databases rely on RAG and vector databases or SQL-based transformations/analytics. But will they be able to preserve per-row contextual understanding?

Weâ€™ve released Agents as part of Datatune:

[https://github.com/vitalops/datatune](https://github.com/vitalops/datatune)

In a single prompt, you can define multiple tasks for data transformations, and Datatune performs the transformations on your data at a per-row level, with contextual understanding.

Example prompt:

""Extract categories from the product description and name. Keep only electronics products. Add a column called ProfitMargin = (Total Profit / Revenue) \* 100""

Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.

Key Features

\- Row-level map() and filter() operations using natural language

\- Agent interface for auto-generating multi-step transformations

\- Built-in support for Dask DataFrames (for scalability)

\- Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)

\- Compatible with LiteLLM for flexibility across providers

\- Auto-token batching, metadata tracking, and smart pipeline composition

Token & Cost Optimization

\- Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:

\- Use input\_fields to send only relevant columns

\- Automatically handles batching and metadata internally

\- Supports setting tokens-per-minute and requests-per-minute limits

\- Defaults to known model limits (e.g., GPT-3.5) if not specified

\- This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.",datascience,0,https://www.reddit.com/r/datascience/comments/1n7zgzy/per_row_context_understanding_is_hard_for_sql_and/,r_1n7zgzy,,,
r_1n7ops6,reddit,Gold-Artichoke-9288,2025-09-03T19:39:45+00:00,"Freelance search
Any website to work as freelancer besides upwork ?",datascience,2,https://www.reddit.com/r/datascience/comments/1n7ops6/freelance_search/,r_1n7ops6,,,
r_1n70y9a,reddit,FreakedoutNeurotic98,2025-09-03T00:52:14+00:00,"Diffusion models
What position do Diffusion models take in the spectrum of architectures to AGI like compared to jepa, auto-regressive modelling and others ? are they RL-able ?",datascience,0,https://www.reddit.com/r/datascience/comments/1n70y9a/diffusion_models/,r_1n70y9a,,,
r_1n70lcz,reddit,joshamayo7,2025-09-03T00:35:36+00:00,"A/B Testing Overview
Sharing this as a guide on A/B Testing. I hope that it can help those preparing for interviews and those unfamiliar with the wide field of experimentation.

Any feedback would be appreciated as we're always on a learning journey.",datascience,39,https://www.reddit.com/r/datascience/comments/1n70lcz/ab_testing_overview/,r_1n70lcz,,,
r_1n6so7m,reddit,Technical-Note-4660,2025-09-02T19:14:27+00:00,"I built a simulation tool for students to learn causal inference!
\- Building a good intuition for causal inference methods requires you to play around with assumptions and data, but getting data from a paper and replicating the results takes time.   
\- **I made a simulation tool to help students quickly build an intuition for these methods (currently only difference-in-difference is available).** This tool is great for the undergraduate level (as I am still a student so the content covered isn't super advanced)

This is still a proof-of-concept, but would love your feedback and what other methods you would like to see!

Link: [https://causal-buddy.streamlit.app/](https://causal-buddy.streamlit.app/)",datascience,162,https://www.reddit.com/r/datascience/comments/1n6so7m/i_built_a_simulation_tool_for_students_to_learn/,r_1n6so7m,,,
r_1n6ez8o,reddit,jesteartyste,2025-09-02T09:33:59+00:00,"Is it wrong to be specialized in specific DS niche?
Hello fellows Data Scientists!
Iâ€™m coming with question/discussion about specialization in specific part of Data Science. For a long time my main duty is time series and predictive projects, mainly around finance but in retail domain. As an example, project where I predict sales per hour for month up front, later I place matrix with amount of staff needed on specific station to minimize number of employees present in the location (lot of savings in labor costs). Lately I attended few interviews, that didnâ€™t go flawlessly from my side - most of questions were around classification problems, where most of my knowledge is in regression problems, of course Iâ€™m blaming myself on every attempt where I didnâ€™t receive an offer because of technical interview and there is no discussion that I could prepare myself in more broad knowledge. But here comes my question, is it possible to know deeply every kind of niche knowledge when your main work spins around specific problems? Iâ€™m sure there are lot of DS which work for past 10 years or so and because of number of projects theyâ€™re familiar with a lot of specific problems, but for someone with 3 yoe is it doable? I feel like Iâ€™m very good in tackling time series problems, but as an example, my knowledge in image recognition is very limited, did you face problem like that? What are your thoughts? How did you overcome this in your career?",datascience,38,https://www.reddit.com/r/datascience/comments/1n6ez8o/is_it_wrong_to_be_specialized_in_specific_ds_niche/,r_1n6ez8o,,,
r_1n6cug7,reddit,SirCasms,2025-09-02T07:12:42+00:00,"The Hidden Costs of Naive Retrieval
We often treat Retrieval-Augmented Generation (RAG) as the default solution for knowledge-intensive tasks, but the naive 'retrieve-then-read' paradigm has significant hidden costs that can hurt, rather than help, performance. So, when is it better not to retrieve?

This series on **Adaptive RAG** starts by exploring the hidden costs of our default RAG implementations by looking at three key areas:

* **The Practical Problems:** These are the obvious unnecessary latency and compute overhead for simple or popular queries where the LLM's parametric memory would have been enough.
* **The Hidden Dangers:** There are more subtle risks to quality. Noisy or misleading context can lead to ""External Hallucinations,"" where the retriever itself induces factual errors in an otherwise correct model.
* **The Foundational Flaws:** Finally, the ""retrieval advantage"" can shrink as models scale.",datascience,0,https://www.reddit.com/r/datascience/comments/1n6cug7/the_hidden_costs_of_naive_retrieval/,r_1n6cug7,,,
r_1n5eqdj,reddit,AutoModerator,2025-09-01T04:01:46+00:00,"Weekly Entering & Transitioning - Thread 01 Sep, 2025 - 08 Sep, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,11,https://www.reddit.com/r/datascience/comments/1n5eqdj/weekly_entering_transitioning_thread_01_sep_2025/,r_1n5eqdj,,,
r_1n4rufc,reddit,Fantastic-Trouble295,2025-08-31T11:02:32+00:00,"Letâ€™s Build Something Together
Hey everyone,

After my last post about my struggles in finding a remote job, I was honestly blown away. I got over 50 messages not with job offers, but with stories, frustrations, and suggestions. The common theme? Many of us are stuck. Some are trying to break into the market, others are trying to move within it, and many just want to *make something meaningful*.

That really got me thinking: since this subreddit is literally about connecting data scientists, engineers, PMs, MLOps folks, researchers, and builders of all kinds why donâ€™t we **actually build something together**?

It doesnâ€™t have to be one massive project; it could be multiple smaller ones. The goal wouldnâ€™t just be to pad CVs, but to collaborate, learn, and create something that matters. Think hackathon energy, but async and community-driven with no time limits and frustration.

I am personally interested to get involved with things i haven't been yet. Mlops,Deployment,Cloud,Azure,pytorch,Apache for example. Everyone can find their opening and what they want to improve and try and work with other experience people on this that could help them.

This would literally need

* Data scientists / analysts
* Software engineers
* MLOps / infra people
* Project managers
* Researchers / scientists
* Anyone who wants to contribute

Build something real with others (portfolio > buzzwords)

* Show initiative and collaboration on your CV/LinkedIn
* Make connections that could lead to opportunities
* Turn frustration into creation

Iâ€™d love to hear your thoughts:

* Would you be interested in joining something like this?
* What kind of projects would excite you (open-source tools, research collabs, data-for-good, etc.)?
* Should we organize a first call/Discord/Slack group to test the waters? I am waiting for connecting with you on Linkedin and here.

PS1: Yeah I am not talkig about creating a product or building the new chatgpt. Just communication and brainstorming . Working on some ideas or just simply get to know some people. ",datascience,35,https://www.reddit.com/r/datascience/comments/1n4rufc/lets_build_something_together/,r_1n4rufc,,,
r_1n4n1qz,reddit,ExcitingCommission5,2025-08-31T05:57:00+00:00,"How do I prepare for my data science job as a new grad?
I just graduated from my bachelors in May. Recently, Iâ€™ve been fortunate enough to receive an offer as a data scientist I at a unicorn where most of the people on the ds team have PhDs. My job starts in a month and Iâ€™m having massive imposter syndrome, especially since my coding skills are kinda shit. I can barely do leetcode mediums. The job description is also super vague, only mentioning ML models and data analysis, so idk what specific things I should brush up on. What can I do in this month to make sure I do a good job?",datascience,102,https://www.reddit.com/r/datascience/comments/1n4n1qz/how_do_i_prepare_for_my_data_science_job_as_a_new/,r_1n4n1qz,,,
r_1n4ecoo,reddit,NervousVictory1792,2025-08-30T22:23:29+00:00,Career Dilemma,datascience,0,https://www.reddit.com/r/datascience/comments/1n4ecoo/career_dilemma/,r_1n4ecoo,,,
r_1n4bamu,reddit,alpha_centauri9889,2025-08-30T20:10:15+00:00,"Advice for DS/AS/MLE interviews
I am looking for data scientist (ML heavy), applied scientist or ML engineer roles in product based companies. For my interview preperation, I am unsure about which book or resources to pick so that I can cover the rigor of ML rounds in these interviews. I have background in CS and have fair knowledge of ML. Anyone who cracked such roles or have any experience that can help me? 

PS: I was considering reading Kevin Murphy's ML book but it is too heavy on math so I am not sure if that much of rigor is required for these kind of interviews. I am not looking for research roles. ",datascience,41,https://www.reddit.com/r/datascience/comments/1n4bamu/advice_for_dsasmle_interviews/,r_1n4bamu,,,
r_1n3jnpw,reddit,PathalogicalObject,2025-08-29T21:22:32+00:00,"How do you design a test to compare two audience targeting methods?
So we have two audiences we want to test against each other. The first is one we're currently using and the second is a new audience. We want to know if a campaign using the new audience targeting method can match or exceed an otherwise identical campaign using our current targeting.

We're conducting the test on Amazon DSP and the Amazon representative recommended basically intersecting each audience with a randomized set of holdout groups. So for audience A the test cell will be all users in audience A and also in one group of randomized holdouts and similarly for audience B (with a different set of randomized holdouts)

Our team's concern is that if each campaign is getting a different set of holdout groups then we wouldn't have the same baseline. My boss is recommending we use the same set of holdout groups for both. 

My personal concern for that is if we'd have a proper isolation (e.g. if one user sees an ad from the campaign using audience A and also an ad from the campaign using audience B, then which audience targeting method gets credit). I think my boss' approach is probably the better design, but the overlap issue stands out to me as a complication.  

I'll be honest that I've never designed an A/B test before, much less on audiences, so any help at all is appreciated. I've been trying to understand how other platforms do this because Amazon does seem a bit different - as in, how (in an ideal universe) would you test two audiences against each other?",datascience,19,https://www.reddit.com/r/datascience/comments/1n3jnpw/how_do_you_design_a_test_to_compare_two_audience/,r_1n3jnpw,,,
r_1n2s33v,reddit,sg6128,2025-08-28T23:46:26+00:00,"Shopify Applied Machine Learning Engineer Pair Programming Interview
Has anyone done the pair programming interview with Shopify? 

Currently interviewing for a Machine Learning Engineer position and the description is really vague.  
All I know is that I can use AI tools and that they don't like Leetcode.  
It will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.

  
My interview is in a week - I'd really appreciate any guidance and help, thank you!

(also based in Canada, flair says US only for some reason)",datascience,11,https://www.reddit.com/r/datascience/comments/1n2s33v/shopify_applied_machine_learning_engineer_pair/,r_1n2s33v,,,
r_1n2s1v1,reddit,sg6128,2025-08-28T23:44:51+00:00,"Shopify Applied Machine Learning Engineer Pair Programming Interview
Has anyone done the pair programming interview with Shopify? 

Currently interviewing for a Machine Learning Engineer position and the description is really vague.  
All I know is that I can use AI tools and that they don't like Leetcode.  
It will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.

  
My interview is in a week - I'd really appreciate any guidance and help, thank you!

(also based in Canada, flair says US only for some reason)",datascience,19,https://www.reddit.com/r/datascience/comments/1n2s1v1/shopify_applied_machine_learning_engineer_pair/,r_1n2s1v1,,,
r_1n2o7c1,reddit,Ok_Post_149,2025-08-28T21:03:04+00:00,"Free 1,000 CPU + 100 GPU hours for testers
I believe it should be dead simple for data scientists, analysts, and researchers to scale their code in the cloud without relying on DevOps. At my last company, whenever the data team needed to scale workloads, we handed it off to DevOps. They wired it up in Airflow DAGs, managed the infrastructure, and quickly became the bottleneck. When they tried teaching the entire data team how to deploy DAGs, it fell apart and we ended up back to queuing work for DevOps.

That experience pushed me to build cluster compute software that makes scaling dead simple for any Python developer. With a single function you can deploy to massive clusters (10k vCPUs, 1k GPUs). You can bring your own Docker image, define hardware requirements, run jobs as background tasks you can fire and forget, and kick off a million simple functions in seconds.

Itâ€™s [open source](https://github.com/Burla-Cloud/burla) and Iâ€™m still making install easier, but I also have a few managed versions.

Right now Iâ€™m looking for test users running embarrassingly parallel workloads like data prep, hyperparameter tuning, batch inference, or Monte Carlo simulations. If youâ€™re interested, email me at [**joe@burla.dev**]() and Iâ€™ll set you up with a managed cluster that includes 1,000 CPU hours and 100 GPU hours.

Hereâ€™s an example of it in action: I spun up 4k vCPUs to screenshot 30k arXiv PDFs and push them to GCS in just a couple minutes: [https://x.com/infra\_scale\_5/status/1938024103744835961](https://x.com/infra_scale_5/status/1938024103744835961?utm_source=chatgpt.com)

Would love testers.",datascience,5,https://www.reddit.com/r/datascience/comments/1n2o7c1/free_1000_cpu_100_gpu_hours_for_testers/,r_1n2o7c1,,,
r_1n2fmqs,reddit,nullstillstands,2025-08-28T15:37:48+00:00,Stanford study finds that AI has already started wiping out new grad jobs,datascience,265,https://www.reddit.com/r/datascience/comments/1n2fmqs/stanford_study_finds_that_ai_has_already_started/,r_1n2fmqs,,,
r_1n28ukj,reddit,Sudden_Beginning_597,2025-08-28T10:44:16+00:00,"I built Runcell - an AI agent for Jupyter that actually understands your notebook context
I've been working on something called Runcell that I think fills a gap I was frustrated with in existing AI coding tools.

**What it is:** Runcell is an AI agent that lives inside JupyterLab (can be used as an extension) and can understand the full context of your notebook - your data, charts, previous code, kernel state, etc. Instead of just generating code, it can actually edit and execute specific cells, read/write files, and take actions on its own.

**Why I built it:** I tried Cursor and Claude Code, but they mostly just generate a bunch of cells at once without really understanding what happened in previous steps. When I'm doing data science work, I usually need to look at the results from one cell before deciding what to write next. That's exactly what Runcell does - it analyzes your previous results and decides what code to run next based on that context.

**How it's different:**

* vs AI IDEs like Cursor: Runcell focuses specifically on building context for Jupyter environments instead of treating notebooks like static files
* vs Jupyter AI: Runcell is more of an autonomous agent rather than just a chatbot - it has tools to actually work and take actions

You can try it with just `pip install runcell`.

I'm looking for feedback from the community. Has anyone else felt this frustration with existing tools? Does this approach make sense for your workflow?",datascience,0,https://www.reddit.com/r/datascience/comments/1n28ukj/i_built_runcell_an_ai_agent_for_jupyter_that/,r_1n28ukj,,,
r_1n1zo5y,reddit,Illustrious-Pound266,2025-08-28T01:48:26+00:00,"Why is Typescript starting to gain adoption in AI?
I've noticed that, increasingly, using TypeScript has become more common for AI tools. For example, Langgraph has Langgraph.js for Typescript developers. Same with OpenAI's Agents SDK.

I've also seen some AI engineer job openings for roles that use both Python and Typescript.

Python still seems to be dominant, but it seems like Typescript is definitely starting to gain traction in the field. So why is this? Why the appeal of building AI apps in Typescript? It wasn't originally like this with more traditional ML / deep learning, where Python was so dominant.

Why is it gaining increasing adoption and what's the appeal?

",datascience,22,https://www.reddit.com/r/datascience/comments/1n1zo5y/why_is_typescript_starting_to_gain_adoption_in_ai/,r_1n1zo5y,,,
r_1n1tk23,reddit,1234okie1234,2025-08-27T21:21:48+00:00,"Rejected after 3rd round live coding OA round
As the title says, I made it to the 3rd round interview for a Staff DS role. Thought I was doing well, but I bombed the coding portion, I only managed to outline my approach instead of producing actual code. Thatâ€™s on me, mostly because Iâ€™ve gotten used to relying on GPT to crank out code for me over the last two years. Most of what I do is build POCs, check hypotheses, then have GPT generate small snippets that I review for logic before applying it. I honestly havenâ€™t done â€œlive codingâ€ in a while.

Before the interview, I prepped with DataLemur for the pandas related questions and brushed up on building simple NNs and GNNs from scratch to cover the conceptual/simple DS side. A little bit on the transformer module as well to have my bases cover if they ask for it. I didnâ€™t expect a LeetCode-style live coding question. I ended up pseudo-coding it, then stumbling hard when I tried to actually implement it.

Got the rejection email today. Super heartbreaking to see. Do I go back to live-coding and memorizing syntax and practicing leetcodes for upcoming future DS interview?",datascience,94,https://www.reddit.com/r/datascience/comments/1n1tk23/rejected_after_3rd_round_live_coding_oa_round/,r_1n1tk23,,,
r_1n191lg,reddit,Technical-Love-8479,2025-08-27T05:54:32+00:00,"NVIDIA AI Released Jet-Nemotron: 53x Faster Hybrid-Architecture Language Model Series
NVIDIA Jet-Nemotron is a new LLM series which is about 50x faster for inferencing. The model introduces 3 main concept :

* **PostNAS**: a new search method that tweaks only attention blocks on top of pretrained models, cutting massive retraining costs.
* **JetBlock**: a dynamic linear attention design that filters value tokens smartly, beating older linear methods like Mamba2 and GLA.
* **Hybrid Attention**: keeps a few full-attention layers for reasoning, replaces the rest with JetBlocks, slashing memory use while boosting throughput.

Video explanation : [https://youtu.be/hu\_JfJSqljo](https://youtu.be/hu_JfJSqljo)

Paper : [https://arxiv.org/html/2508.15884v1](https://arxiv.org/html/2508.15884v1)",datascience,9,https://www.reddit.com/r/datascience/comments/1n191lg/nvidia_ai_released_jetnemotron_53x_faster/,r_1n191lg,,,
r_1n17500,reddit,IronManFolgore,2025-08-27T04:03:39+00:00,"What exactly is ""prompt engineering"" in data science?
I keep seeing people talk about prompt engineering, but I'm not sure I understand what that actually means in practice.

Is it just writing one-off prompts to get a model to do something specific? Or is it more like setting up a whole system/workflow (e.g. using LangChain, agents, RAG, etc.) where prompts are just one part of the stack in developing an application?

For those of you working as data scientists:
- Are you actively building internal end-to-end agents with RAG and tool integrations (either external like MCP or creating your own internal files to serve as tools)?

- Is prompt engineering part of your daily work, or is it more of an experimental/prototyping thing?",datascience,68,https://www.reddit.com/r/datascience/comments/1n17500/what_exactly_is_prompt_engineering_in_data_science/,r_1n17500,,,
r_1n105of,reddit,jason-airroi,2025-08-26T22:36:44+00:00,"Airbnb Data
Hey everyone,

I work on the data team at [AirROI](https://www.airroi.com). For a while, we offered free datasets for about **250** cities, but we always wanted to do more for the community. Recently, we just expanded our free public dataset from \~250 to nearly **1000** global Airbnb markets on **properties** and **pricing data**. As far as we know, this makes it the single **largest free Airbnb dataset** ever released on the internet.

You can browse the collection and download here, no sign-up required: [Airbnb Data](http://www.airroi.com/data-portal)



**Whatâ€™s in the data?**

For each market (cities, regions, etc.), the CSV dumps include:

Property Listings: Details like room type, amenities, number of bedrooms/bathrooms, guest capacity, etc.

Pricing Data: This is the cool part. We include historical rates, future calendar rates (for investment modeling), and minimum/maximum stay requirements.

Host Data: Host ID, superhost status, and other host-level metrics.



**What can you use it for?**

This is a treasure trove for:

Trend Analysis: Track pricing and occupancy trends across the globe.

Investment & Rental Arbitrage Analysis: Model potential ROI for properties in new markets.

Academic Research: Perfect for papers on the sharing economy, urban development, or tourism.

Portfolio Projects: Build a killer dashboard or predictive model for your GitHub.

General Data Wrangling Practice: It's real, messy, world-class data.



**A quick transparent note**: If you need hyper-specific or real-time data for a region not in the free set, we do have a ridiculously cheap [Airbnb API](https://www.airroi.com/api) to get more customized data. Alternatively, if you are a researcher who wants a larger customized data just reach out to us, we'll try our best to support!

  
If you require something that's not currently in the free dataset please comment below, we'll try to accommodate within reason.

Happy analyzing and go building something cool!



[Airbnb Data](https://preview.redd.it/vi9bjqphxflf1.png?width=3038&format=png&auto=webp&s=6953d029e8bc9aa21280b411df543d3b5bbc3d66)

[Download Airbnb Data](https://preview.redd.it/ydtx5oqjxflf1.png?width=1920&format=png&auto=webp&s=bb4f4dfc361d83734a1c088750d8167e1327bdae)

",datascience,318,https://www.reddit.com/r/datascience/comments/1n105of/airbnb_data/,r_1n105of,,,
r_1n0ke01,reddit,Technical-Love-8479,2025-08-26T12:24:16+00:00,"InternVL 3.5 released : Best MultiModal LLM, ranks 3 overall
InternVL 3.5 has been released, and given the benchmark, the model looks to be the best multi-model LLM, ranking 3 overall just behind Gemini 2.5 Pro and GPT-5.  Multiple variants released ranging from 1B to 241B

*Processing img 5v5hfeg9wclf1...*

The team has introduced a number of new technical inventions, including *Cascade RL, Visual Resolution Router, Â DecoupledÂ Vision-Language Deployment.* Â 

Model weights : [https://huggingface.co/OpenGVLab/InternVL3\_5-8B](https://huggingface.co/OpenGVLab/InternVL3_5-8B)

Tech report : [https://arxiv.org/abs/2508.18265](https://arxiv.org/abs/2508.18265)

Video summary : [https://www.youtube.com/watch?v=hYrdHfLS6e0](https://www.youtube.com/watch?v=hYrdHfLS6e0)",datascience,13,https://www.reddit.com/r/datascience/comments/1n0ke01/internvl_35_released_best_multimodal_llm_ranks_3/,r_1n0ke01,,,
r_1n0ep0g,reddit,ChubbyFruit,2025-08-26T06:44:33+00:00,"How do I make the most of this opportunity
Hello everyone, Iâ€™m a senior studying data science at a large state school. Recently, through some networking, I got to interview with a small real estate and financial data aggregator company with around \~100 employees.

I met with the CEO for my interview. As far as I know, they havenâ€™t had an engineering or science intern before, mainly marketing and business interns. The firm has been primarily a more traditional real estate company for the last 150 years. Many tasks are done through SQL queries and Excel. Much of the product team at the company has been there for over 20 years and is resistant to change.

The ceo wants to make the company more efficient and modern, and implement some statistical and ML models and automated workflows with their large amounts of data. He has given me some of the ideas that he and others at the company have considered. I will list those at the end. But I am starting to feel that Iâ€™m a bit in over my head here as he hinted towards using my work as a proof of concept to show the board that these new technologies and techniques r what the company needs to stay relevant and competitive. As someone who is just wrapping up their undergrad, some of it feels beyond my abilities if Iâ€™m mainly going to be implementing a lot of these things solo.

  
These are some of the possible projects I would work on:



# Â Chatbot Knowledge Base Enhancement

**Background**: The Company is deploying AI-powered chatbots (HubSpot/CoPilot) for customer engagement and internal knowledge access. Current limitations include incomplete coverage of FAQs and inconsistent performance tracking.

**Objective**: Enhance chatbot functionality through improved training, monitoring, and analytics.

**Scope**:

* Automate FAQ training using internal documentation.
* Log and classify failed responses for continuous improvement.
* Develop a performance dashboard.

**Deliverables**:

* Enhanced training process.
* Error classification system.
* Prototype dashboard.

**Value**: Improves customer engagement, reduces staff workload, and provides analytics on chatbot usage.



# Automated Data Quality Scoring

**Background**: Clients demand AI-ready datasets, and the company must ensure high data quality standards.

**Objective**: Prototype an automated scoring system for dataset quality.

**Scope**:

* Metrics: completeness, duplicates, anomalies, missing metadata.
* Script to evaluate any dataset.

**Intern Fit**: Candidate has strong Python/Pandas skills and experience with data cleaning.

**Deliverables**:

* Reusable script for scoring.
* Sample reports for selected datasets.

**Value**: Positions the company as a provider of AI-ready data, improving client trust.

  
Entity Resolution Prototype

**Background**: The company datasets are siloed (deeds, foreclosures, liens, rentals) with no shared key.

**Objective**: Prototype entity resolution methods for cross-dataset linking.

**Scope**:

* Fuzzy matching, probabilistic record linkage, ML-based classifiers.
* Apply to limited dataset subset.

**Intern Fit**: Candidate has ML and data cleaning experience but limited production-scale exposure.

**Deliverables**:

* Prototype matching algorithms.
* Confidence scoring for matches.
* Report on results.

**Value**: Foundation for the company's long-term, unique master identifier initiative.

  
Predictive Micro-Models

**Background**: Predictive analytics represents an untapped revenue stream for the company.

**Objective**: Build small predictive models to demonstrate product potential.

**Scope**:

* Predict foreclosure or lien filing risk.
* Predict churn risk for subscriptions.

**Intern Fit**: Candidate has built credit risk models using XGBoost and regression.

**Deliverables**:

* Trained models with evaluation metrics.
* Prototype reports showcasing predictions.

**Value**: Validates feasibility of predictive analytics as a company product.



# Generative Summaries for Court/Legal Documents

**Background**: Processing court filings is time-intensive, requiring manual metadata extraction.

**Objective**: Automate structured metadata extraction and summary generation using NLP/LLM.

**Scope**:

* Extract entities (names, dates, amounts).
* Generate human-readable summaries.

**Intern Fit**: Candidate has NLP and ML experience through research work.

**Deliverables**:

* Prototype NLP pipeline.
* Example structured outputs.
* Evaluation of accuracy.

**Value**: Reduces operational costs and increases throughput.

  
Automation of Customer Revenue Analysis

**Background**: The company currently runs revenue analysis scripts manually, limiting scale.

**Objective**: Automate revenue forecasting and anomaly detection.

**Scope**:

* Extend existing forecasting models.
* Build anomaly detection.
* Dashboard for finance/sales.

**Intern Fit**: Candidateâ€™s statistical background aligns with forecasting work.

**Deliverables**:

* Automated pipeline.
* Interactive dashboard.

**Value**: Improves financial planning and forecasting accuracy.

  
Data Product Usage Tracking

**Background**: Customer usage patterns are not fully tracked, limiting upsell opportunities.

**Objective**: Prototype a product usage analytics system.

**Scope**:

* Track downloads, API calls, subscriptions.
* Apply clustering/churn prediction models.

**Intern Fit**: Candidateâ€™s experience in clustering and predictive modeling fits well.

**Deliverables**:

* Usage tracking prototype.
* Predictive churn model.

**Value**: Informs sales strategies and identifies upsell/cross-sell opportunities.

  
AI Policy Monitoring Tool

**Background**: The company has implemented an AI Use Policy, requiring compliance monitoring.

**Objective**: Build a prototype tool that flags non-compliant AI usage.

**Scope**:

* Detect unapproved file types or sensitive data.
* Produce compliance dashboards.

**Intern Fit**: Candidate has built automation pipelines before, relevant experience.

**Deliverables**:

* Monitoring scripts.
* Dashboard with flagged activity.

**Value**: Protects the company against compliance and cybersecurity risks.",datascience,7,https://www.reddit.com/r/datascience/comments/1n0ep0g/how_do_i_make_the_most_of_this_opportunity/,r_1n0ep0g,,,
r_1n0biew,reddit,Technical-Love-8479,2025-08-26T03:38:17+00:00,"Microsoft released VibeVoice TTS
Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. 

Demo Video : https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ

GitHub : https://github.com/microsoft/VibeVoice",datascience,11,https://www.reddit.com/r/datascience/comments/1n0biew/microsoft_released_vibevoice_tts/,r_1n0biew,,,
r_1n035we,reddit,Fantastic-Trouble295,2025-08-25T21:24:24+00:00,"Is the market really like this? The reality for a recent graduate looking for opportunities.
Hello . Iâ€™m a recent Master of Science in Analytics graduate from Georgia Tech (GPA 3.91, top 5% of my class). I completed a practicum with Sandia Labs and Iâ€™m currently in discussions about further research with GT and SANDIA. Iâ€™m originally from Greece and Iâ€™ve built a strong portfolio of projects, ranging from classic data analysis and machine learning to a Resume AI chatbot.

I entered the job market feeling confident, but Iâ€™ve been surprised and disappointed by how tough things are here. The Greek market is crazy: Iâ€™ve seen openings that attract 100 applicants and still offer very low pay while expecting a lot. Iâ€™m applying to junior roles and have gone as far as seven interview rounds that tested pandas, PyTorch, Python, LeetCode-style problems, SQL, and a lot of behavioral and technical assessments.

Remote opportunities seem rare on EUROPE or US. I may be missing something, but I canâ€™t find many remote openings.

This isnâ€™t a complaint so much as an expression of frustration. Itâ€™s disheartening that a masterâ€™s from a top university, solid skills, hands-on projects, and a real practicum can still make landing a junior role so difficult. Iâ€™ve also noticed many job listings now list deep learning and PyTorch as mandatory, or rebrand positions as â€œAI engineer,â€ even when it doesnâ€™t seem necessary.

On a positive note, Iâ€™ve had strong contacts reach out via LinkedIn  though most ask for relocation, which I canâ€™t manage due to family reasons.

Iâ€™m staying proactive: building new projects, refining my interviewing skills, and growing my network. Iâ€™d welcome any advice, referrals, or remote-friendly opportunities. Thank you!

PS. If you comment your job experience state your country to get a picture of the worldwide problem.

PS2. Started as an attempt for networking and opportunities, came down to an interesting realistic discussion. Still sad to read, what's the future of this job? What will happen next? What recent grads and on university juniors should be doing? 

Ps3. If anyone wants to connect send me a message ",datascience,202,https://www.reddit.com/r/datascience/comments/1n035we/is_the_market_really_like_this_the_reality_for_a/,r_1n035we,,,
r_1mzzzu7,reddit,fark13,2025-08-25T19:23:56+00:00,"We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025-08
Hey guys,

I've been silent here lately but many opportunities keep appearing and being posted.

These are a few from the last 10 days or so

* [Quantitative Analyst Associate (Spring/Summer 2026) - Philadelphia Phillies](http://www.sportsjobs.online/jobs/9015-quantitative-analyst-associate-spring-summer-2026?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=24b4748bef795f9a693e2911693d223c99632356)
* [Senior Sports Data Scientist - ESPN](http://www.sportsjobs.online/jobs/9018-senior-sports-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=58166f06c2cb14a5f60c555a80e63eff791ece6a)
* [Baseball Analyst/Data Scientist - Miami Marlins](http://www.sportsjobs.online/jobs/9014-baseball-analyst-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=7d5181c9bd523683761c79ffcd23fafab8877728)
* [Data Engineer, Athletics - University of Pittsburgh](http://www.sportsjobs.online/jobs/8992-data-engineer-athletics?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=90aede97e283411c5e9a31b34a982299320cc5e6)
* [Senior Data Scientist - Tottenham Hotspur](http://www.sportsjobs.online/jobs/8997-senior-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=e35ef1aeb7939cd356689d46e49afdff95535e1a)
* [Sports Scientist - Human Data Science - McLaren Racing](http://www.sportsjobs.online/jobs/8996-sports-scientist-human-data-science?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=e40bd45b1f6178064b5c7cf165f65e5821c8ad0d)
* [Lead Engineer - Phoenix Suns](http://www.sportsjobs.online/jobs/8981-lead-engineer?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=841248c85b1774cec3812e308b803fbcaa9b570e)
* [Business Intelligence Intern - Houston Texans](http://www.sportsjobs.online/jobs/8967-business-intelligence-intern?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=35c476cde3ddf380fbd3d5f4beccd3424bdcb356)
* [Technical Data Analyst - Portland Timbers](http://www.sportsjobs.online/jobs/8953-technical-staff-data-analyst-mls?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=14f0d07bcd9a80d670a7cc018bb6d16d6e2e9c2b)

I runÂ www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.

For the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.

It's a niche, there aren't thousands of jobs as in Software in general but my commitment is toÂ **keep improving a simple metric, jobs per month.**Â We always need some metric in DS..

I run also a newsletter to receive emails with jobs and interesting content on sports analytics (next edition tomorrow!)  
[https://sportsjobs-online.beehiiv.com/subscribe](https://sportsjobs-online.beehiiv.com/subscribe)

Finally, I've created also aÂ [reddit community](https://www.reddit.com/r/sports_jobs/)Â where I post recurrently the openings if that's easier to check for you.

I hope this helps someone!",datascience,116,https://www.reddit.com/r/datascience/comments/1mzzzu7/we_are_back_with_many_data_science_jobs_in_soccer/,r_1mzzzu7,,,
r_1mzxmx3,reddit,SmartPizza,2025-08-25T17:55:36+00:00,"Looking to transition to experimentation
Hi all, I am looking to transition from ml analytics generalized roles to more experimentation focused roles. Where to start looking for experimentation heavy roles. I know the market is trash right now, but are there any specific portals that can help find such roles. Also usually faang is very popular for such roles, but are there any other companies which would be a good step to make a transition to. ",datascience,15,https://www.reddit.com/r/datascience/comments/1mzxmx3/looking_to_transition_to_experimentation/,r_1mzxmx3,,,
r_1mzwdws,reddit,ElectrikMetriks,2025-08-25T17:09:56+00:00,"""The Vibes are Off..."" *server logs filling with errors*",datascience,61,https://www.reddit.com/r/datascience/comments/1mzwdws/the_vibes_are_off_server_logs_filling_with_errors/,r_1mzwdws,,,
r_1mzlzsp,reddit,Bus-cape,2025-08-25T09:39:51+00:00,"First time writing a technical article, would love constructive feedback
Hi everyone,

I recently wrote my first blog post where I share a method Iâ€™ve been using to get good results on a fine-grained classification benchmark. This is something Iâ€™ve worked on for a while and wanted to put my thoughts together in an article.

Iâ€™m sharing it here **not as a promo** but because Iâ€™m genuinely looking to improve my writing and make sure my explanations are clear and useful. If you have a few minutes to read and share your thoughts (on structure, clarity, tone, level of detail, or anything else), Iâ€™d really appreciate it.

Hereâ€™s the link: [https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/](https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/)

Thanks a lot for your time and feedback!",datascience,9,https://www.reddit.com/r/datascience/comments/1mzlzsp/first_time_writing_a_technical_article_would_love/,r_1mzlzsp,,,
r_1mzgkc7,reddit,AutoModerator,2025-08-25T04:01:38+00:00,"Weekly Entering & Transitioning - Thread 25 Aug, 2025 - 01 Sep, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,6,https://www.reddit.com/r/datascience/comments/1mzgkc7/weekly_entering_transitioning_thread_25_aug_2025/,r_1mzgkc7,,,
r_1mz2jgn,reddit,sourabharsh,2025-08-24T17:56:46+00:00,"Day to day work at lead/principal data scientist
Hi, 

I have 9 years of experience in ml/dl. I have been looking for a role in lead/principal ds. Can you tell me what expectations do you guys face at the role.

Data science knowledge? 
Ml ops knowledge? 
Team management? 
",datascience,66,https://www.reddit.com/r/datascience/comments/1mz2jgn/day_to_day_work_at_leadprincipal_data_scientist/,r_1mz2jgn,,,
r_1mymb21,reddit,Technical-Love-8479,2025-08-24T04:32:51+00:00,"Google's new Research : Measuring the environmental impact of delivering AI at Google Scale
Google has dropped in a very important research paper measuring the impact of AI on the environment, suggesting how much carbon emission, water, and energy consumption is done for running a prompt on Gemini. Surprisingly, the numbers have been quite low compared to the previously reported numbers by other studies, suggesting that the evaluation framework is flawed. 

Google measured the environmental impact ofÂ **a single Gemini prompt**Â and hereâ€™s what they found:

* **0.24 Wh of energy**
* **0.03 grams of COâ‚‚**
* **0.26 mL of water**

Paper : [https://services.google.com/fh/files/misc/measuring\_the\_environmental\_impact\_of\_delivering\_ai\_at\_google\_scale.pdf](https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf)

Video : [https://www.youtube.com/watch?v=q07kf-UmjQo](https://www.youtube.com/watch?v=q07kf-UmjQo)",datascience,54,https://www.reddit.com/r/datascience/comments/1mymb21/googles_new_research_measuring_the_environmental/,r_1mymb21,,,
r_1mxyprj,reddit,posiela,2025-08-23T11:14:25+00:00,"Anyone Using Search APIs as a Data Source?
I've been working on a research project recently and have encountered a frustrating issue: the amount of time spent cleaning scraped web results is insane.Â 

Half of the pages I collect are:Â Â 

*  Ads disguised as contentÂ Â 
* Keyword-stuffed SEO blogsÂ Â 
* Dead or outdated linksÂ Â 

While it's possible to write filters and regex pipelines, it often feels like I spend more time cleaning the data than actually analyzing it. This got me thinking: instead of scraping, has anyone here tried using structured search APIs as a data acquisition step?Â 

In theory, the benefits could be significant:Â Â 

* Fewer junk pages since the API does some filtering alreadyÂ Â 
* Results delivered in structured JSON format instead of raw HTMLÂ Â 
* Built-in citations and metadata, which could save hours of wranglingÂ Â 

However, I haven't seen many researchers discuss this yet. I'm curious if APIs like these are actually good enough to replace scraping or if they come with their own issues (such as coverage, rate limits, cost, etc.).Â 

If you've used a search API in your pipeline, how did it compare to scraping in terms of:

* Data qualityÂ Â 
* Preprocessing timeÂ Â 
* Flexibility for different research domainsÂ Â 

I would love to hear if this is a viable shortcut or just wishful thinking on my part.",datascience,48,https://www.reddit.com/r/datascience/comments/1mxyprj/anyone_using_search_apis_as_a_data_source/,r_1mxyprj,,,
r_1mxrbck,reddit,Technical-Love-8479,2025-08-23T03:52:46+00:00,"NVIDIA new paper : Small Language Models are the Future of Agentic AI
NVIDIA have just published a paper claiming SLMs (small language models) are the future of agentic AI. They provide a number of claims as to why they think so, some important ones being they are cheap. Agentic AI requires just a tiny slice of LLM capabilities, SLMs are more flexible and other points. The paper is quite interesting and short as well to read. 

Paper : [https://arxiv.org/pdf/2506.02153](https://arxiv.org/pdf/2506.02153)

Video Explanation : [https://www.youtube.com/watch?v=6kFcjtHQk74](https://www.youtube.com/watch?v=6kFcjtHQk74)",datascience,254,https://www.reddit.com/r/datascience/comments/1mxrbck/nvidia_new_paper_small_language_models_are_the/,r_1mxrbck,,,
r_1mxpyef,reddit,Rich-Effect2152,2025-08-23T02:41:33+00:00,"When do we really need an Agent instead of just ChatGPT?
Iâ€™ve been diving into the whole â€œAgentâ€ space lately, and I keep asking myself a simple question: *when does it actually make sense to use an Agent, rather than just a ChatGPT-like interface?*

Hereâ€™s my current thinking:

* Many user needs are **low-frequency, one-off, low-risk**. For those, opening a ChatGPT window is usually enough. You ask a question, get an answer, maybe copy a piece of code or text, and youâ€™re done. No Agent required.
* Agents start to make sense only when certain conditions are met:
   1. **High-frequency or high-value tasks** â†’ worth automating.
   2. **Horizontal complexity** â†’ need to pull in information from multiple external sources/tools.
   3. **Vertical complexity** â†’ decisions/actions today depend on context or state from previous interactions.
   4. **Feedback loops** â†’ the system needs to check results and retry/adjust automatically.

In other words, if you donâ€™t have multi-step reasoning + tool orchestration + memory + feedback, an â€œAgentâ€ is often just a chatbot with extra overhead.

I feel like a lot of â€œAgent productsâ€ right now havenâ€™t really thought through what incremental value they add compared to a plain ChatGPT dialog.

Curious what others think:

* Do you agree that most low-frequency needs are fine with just ChatGPT?
* Whatâ€™s your personal checklist for deciding when an Agent is *actually* worth building?
* Any concrete examples from your work where Agents clearly beat a plain chatbot?

Would love to hear how this community thinks about it.",datascience,58,https://www.reddit.com/r/datascience/comments/1mxpyef/when_do_we_really_need_an_agent_instead_of_just/,r_1mxpyef,,,
r_1mxhji7,reddit,DataAnalystWanabe,2025-08-22T20:30:03+00:00,"DS/DA Recruiters, do you approve of my plan
Pivoting away from lab research after I finish my PhD, I'm thinking of taking this approach to landing a DS/DA job:

- Spot an ideal job and study it's requirements.

- Develop all (or most of) the skills associated with that job.

- Compensate for wet-lab-heavy experiences by undertaking projects (even if hypothetical) in said job domain and learn to think like an analyst.

I want to read from recruiters to know what they look for so I can.... Be that ðŸ˜… ",datascience,2,https://www.reddit.com/r/datascience/comments/1mxhji7/dsda_recruiters_do_you_approve_of_my_plan/,r_1mxhji7,,,
r_1mwdbr8,reddit,Due-Duty961,2025-08-21T14:58:31+00:00,"Where to reference personal projects on my CV?
I havn t work as a data scientist in a long time and I want to get back to the field. I had mostly data analysis missions. I recently did a data science personal project. do I put it in professional experiences in the top of the cv for visibility, or lower in the cv with projects? thanks.",datascience,21,https://www.reddit.com/r/datascience/comments/1mwdbr8/where_to_reference_personal_projects_on_my_cv/,r_1mwdbr8,,,
r_1mwchp8,reddit,AnalyticsDepot--CEO,2025-08-21T14:28:35+00:00,"[Hiring] MLE PositionÂ - Enterprise-Grade LLM Solutions
Hey all,  
  
I'm the founder of Analytics Depot, and we're looking for a talented MachineÂ Learning Engineer to join our team. We have a premium brand name and are positioned to deliver a product to match. The Home depot of Analytics if you will.  
  
We've built a solid platform that combines LLMs, LangChain, and custom ML pipelines toÂ help enterprises actually understand their data. Our stackÂ is modern (FastAPI, Next.js), our approach is practical, and we're focused on delivering real value, not chasing buzzwords.   
  
We need someone who knows their way around productionÂ ML systems and can help us push ourÂ current LLM capabilities further. You'll be working directlyÂ with me and our core team onÂ everything from prompt engineering to scalingÂ our document processing pipeline. IfÂ you have experience with Python, LangChain, and NLP, and want to build something that actually matters in the enterprise space, let's talk.   
  
We offer competitiveÂ compensation, equity, and aÂ remote-first environment. DM me if you'reÂ interested in learning more aboutÂ what we're building.  
",datascience,25,https://www.reddit.com/r/datascience/comments/1mwchp8/hiring_mle_position_enterprisegrade_llm_solutions/,r_1mwchp8,,,
r_1mv5ojf,reddit,idan_huji,2025-08-20T05:01:35+00:00,Asking for feedback on databases course content,datascience,1,https://www.reddit.com/r/datascience/comments/1mv5ojf/asking_for_feedback_on_databases_course_content/,r_1mv5ojf,,,
r_1mumd4y,reddit,save_the_panda_bears,2025-08-19T15:52:55+00:00,"Causal Inference Tech Screen Structure
This will be my first time administering a tech screen for this type of role.

The HM and I are thinking about formatting this round as more of a verbal case study on DoE within our domain since LC questions and take homes are stupid. The overarching prompt would be something along the lines of ""marketing thinks they need to spend more in XYZ channel, how would we go about determining whether they're right or not?"", with a series of broad, guided questions diving into DoE specifics, pitfalls, assumptions, and touching on high level domain knowledge.

I'm sure a few of you out there have either conducted or gone through these sort of interviews, are there any specific things we should watch out for when structuring a round this way? If this approach is wrong, do you have any suggestions for better ways to format the tech screen for this sort of role? My biggest concern is having an objective grading scale since there are so many different ways this sort of interview can unfold.",datascience,38,https://www.reddit.com/r/datascience/comments/1mumd4y/causal_inference_tech_screen_structure/,r_1mumd4y,,,
r_1mu3c6j,reddit,CanYouPleaseChill,2025-08-19T00:19:28+00:00,MIT report: 95% of generative AI pilots at companies are failing,datascience,2280,https://www.reddit.com/r/datascience/comments/1mu3c6j/mit_report_95_of_generative_ai_pilots_at/,r_1mu3c6j,,,
r_1mtmvuc,reddit,NervousVictory1792,2025-08-18T14:00:53+00:00,"Scared of AI
I have been working with a principal data scientist on a project. Although I am the sole data scientist working on this project  and discussing stuff with him but I am so impressed at his articulate way of thinking. Literally putting his suggestions in chatgpt gives me the code I need. Honestly I am a little scare about AI now. Am I falling behind ?? Just to beat my own drum. I am probably asking the right questions. ",datascience,0,https://www.reddit.com/r/datascience/comments/1mtmvuc/scared_of_ai/,r_1mtmvuc,,,
r_1mtehzk,reddit,explorer_seeker,2025-08-18T06:37:55+00:00,"Curious to know about people who switched from DS to DE or SWE or Solutions Architect
Hello, I was just curious to know about people who have switched from DS to DE or SWE or Solutions Architect. If you have done it, what was your rationale behind doing it, what pushed or motivated you for it and how has been your experience after you did it?",datascience,45,https://www.reddit.com/r/datascience/comments/1mtehzk/curious_to_know_about_people_who_switched_from_ds/,r_1mtehzk,,,
r_1mtbra1,reddit,AutoModerator,2025-08-18T04:01:38+00:00,"Weekly Entering & Transitioning - Thread 18 Aug, 2025 - 25 Aug, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,5,https://www.reddit.com/r/datascience/comments/1mtbra1/weekly_entering_transitioning_thread_18_aug_2025/,r_1mtbra1,,,
r_1msw56a,reddit,Technical-Love-8479,2025-08-17T16:56:55+00:00,"Dijkstra defeated: New Shortest Path Algorithm revealed
Dijkstra, the goto shortest path algorithm (time complexity nlogn) has now been outperformed by a new algorithm by top Chinese University which looks like a hybrid of bellman ford+ dijsktra algorithm.

Paper : https://arxiv.org/abs/2504.17033

Algorithm explained with example : https://youtu.be/rXFtoXzZTF8?si=OiB6luMslndUbTrz",datascience,460,https://www.reddit.com/r/datascience/comments/1msw56a/dijkstra_defeated_new_shortest_path_algorithm/,r_1msw56a,,,
r_1mr8nwu,reddit,empirical-sadboy,2025-08-15T20:02:33+00:00,"How different is ""Senior Data Analyst"" from ""Data Scientist""?
I often see Senior DA roles that seem focused on using R/Python for analysis (vs. Excel and Power BI), but don't have any insight into the day-to-day of theese roles. 

At the senior level, how different is Data Analyst from Data Scientist? ",datascience,112,https://www.reddit.com/r/datascience/comments/1mr8nwu/how_different_is_senior_data_analyst_from_data/,r_1mr8nwu,,,
r_1mqlp7d,reddit,CorpusculantCortex,2025-08-15T03:01:59+00:00,"Suspicious ad
Describe the results you want and then have ai manufacture those results for you... who's going to tell them that's not how science works ðŸ¤£

Disclosure: I did not read about their tool at all,I just that the advert sounded terribly bad.",datascience,79,https://www.reddit.com/r/datascience/comments/1mqlp7d/suspicious_ad/,r_1mqlp7d,,,
r_1mqfubv,reddit,big_data_mike,2025-08-14T22:44:42+00:00,"Time series with value dependent lag
I build models of factories that process liquids. Liquid flows through the factory in various steps and sits in tanks. A tank will have a flow rate in and a flow rate out, a level, and a volume so I can calculate the residence time. It takes ~3 days for liquid to get from the start of the process to the end and it goes through various temperatures, separations, and various other things get added to it along the way. 

If the factory is in a steady state the residence times and lags are relatively easy to calculate. The problem is I am looking at 6 months worth of data and during that time the  rate of the whole facility varies and therefore the residence times vary. If the flow rate goes up residence time goes down. 

How would you adjust the lags based on the flow rates? Chunk the data into months and calculate the lags for each month then concatÃ©nate everything? Vary the lags and just drop the overlaps and gaps?",datascience,17,https://www.reddit.com/r/datascience/comments/1mqfubv/time_series_with_value_dependent_lag/,r_1mqfubv,,,
r_1mq78jd,reddit,Helloiamwhoiam,2025-08-14T17:30:25+00:00,"Getting Master's worth it with T5 Bachelor's?
As a bit of background, I have 2 years of work experience as a Data Scientist, and I have a Bachelor's Degree in Mathematics from a 'top' University: think MIT/Harvard/Princeton.

I'm currently employed. Making about $105k in total comp. I have a feeling I could be doing better compensation wise and even task wise so I've been considering applying to more jobs. 

I've noticed a lot of job postings seem to have a minimum requirement of at least a Master's degree, but I'm sort of hesitant to pursue this route right now for a few reasons. For one, master's are expensive, and I don't want to quit my job and go into debt. Secondly, if I were to pursue an online Master's degree, I'm not sure the available options would increase my signal. For example, does a MIT Math Bachelor's -> Texas AM Master's Data Science really boost the resume?

The only reason I'd get a Master's is for my love of learning, and I'd pursue something theoretical ML oriented and maybe transition into a more research-heavy or even quant role. But I'm not feeling this is an imminent or necessary next step for me.

I'm not trying to be cocky; I'm just trying to get insight from more seasoned people in the field who might be closer to hiring expectations.",datascience,0,https://www.reddit.com/r/datascience/comments/1mq78jd/getting_masters_worth_it_with_t5_bachelors/,r_1mq78jd,,,
r_1mq737g,reddit,Its_lit_in_here_huh,2025-08-14T17:25:09+00:00,"Overfitting on training data time series forecasting on commodity price, test set fine. XGBclassifier. Looking for feedback
Good morning nerds, Iâ€™m looking for some feedback Iâ€™m sure is rather obvious but I seem to be missing. 

Iâ€™m using XGBclassifier to predict the direction of commodity x price movement one month the  the future. 

~60 engineered features and 3500 rows. 
Target = one month return > 0.001
 
Class balance is 0.52/0.48. Backtesting shows an average accuracy of 60% on the test with a lot of variance through testing periods which Iâ€™m going to accept given the stochastic nature of financial markets. 

I know my back test isnâ€™t leaking, but my training performance is too high, sitting at >90% accuracy. 

Not particularly relevant, but hyperparameters were selected with Optuna.

Does anything jump out as the obvious cause for the training over performance?  


",datascience,97,https://www.reddit.com/r/datascience/comments/1mq737g/overfitting_on_training_data_time_series/,r_1mq737g,,,
r_1mq4sfp,reddit,Affectionate_Use9936,2025-08-14T16:02:41+00:00,"Copy-pasting jupyter notebooks is memory heavy on VSCode
Currently for most of my work, I found out that copy-pasting jupyter notebooks and slightly modifying them is the most effective way to do my work. So basically I have a ipynb for every project I do every day.

However, some issues is that they can sometimes get a pretty big memory footprint especially when I have a lot of plots. Like around 1GB per notebook. So sometimes it takes several seconds to a minute to open some files on vscode. I was wondering if there's a way to optimize this?

  
I saw there's marimo and stuff. Wondering what you guys do.",datascience,43,https://www.reddit.com/r/datascience/comments/1mq4sfp/copypasting_jupyter_notebooks_is_memory_heavy_on/,r_1mq4sfp,,,
r_1mq4ai4,reddit,tits_mcgee_92,2025-08-14T15:44:35+00:00,"Would you jump jobs if you're in fear of a layoff?
EDIT: Just looked and this new company has 2.5 stars out of 600 reviews on Glassdoor. Oof.

Currently based in the U.S., working remote, medium cost of living area. I make 90k a year and I'm the lead (and only) data scientist / frontend software dev for our area in the company. On top of data science/analyst stuff, I maintain/build our training website for around 500 employees (solo dev as well using React).

The down side? I work for Medicaid, and if you know what's going on in the United States you know Medicaid is having major cuts, and especially for 2026. We have laid off 300 people this year (so far). I was told ""You have nothing to worry about because your role is so niche"" but I still feel worried.

New job:

- Pay raise to 115k a year

- Still remote

- I would be working under my current boss who is transitioning to this new company (I have worked with him for 8 years, and the fact that my boss left this current job says something).

- 401k is comparable (3% match), health insurance is better and less cost, PTO is comparable.

- What I'm worried about: He is starting this new department from the ground up. I would be the only data/front-end website guy basically doing what I do in my current role. I'm worried the workload will be too much, or I'm not good enough to start from scratch. Feeling some imposter syndrome here.

Thanks for any insight here! This job I am currently at is fun, productive, and I love my team. But I am scared to death of layoffs. The company I am going to now has been around for 25 years, is growing a lot, and has much more ""lasting power"" in my opinion.",datascience,99,https://www.reddit.com/r/datascience/comments/1mq4ai4/would_you_jump_jobs_if_youre_in_fear_of_a_layoff/,r_1mq4ai4,,,
r_1mpp8sv,reddit,Tyrannosaurus_Secks,2025-08-14T03:16:29+00:00,"What should my job title be
Iâ€™ve been in my current role for ~5 months after finishing up my masters in geospatial data science. My official title is Energy Analyst, so essentially a data analyst role in the energy industry. 

I feel like the work I do is potentially beyond what is meant for the position (though Iâ€™m happy to be told otherwise if thatâ€™s not true) and am planning on asking for a title change and raise in the next few months. 

We have a weird set-up where we have a central IT team that supports ~12 implementation contractor teams that work with various utilities. The central IT team owns all of our data and does not allow any sort of read access or api to access data, and only exposes anything through SSRS reports. In theory, the IT team is meant to support a lot of our analytics, but historically theyâ€™ve done a pretty bad job at that so I was hired into one of the distributed teams to run their analytics and build out an internal IT capacity. So far that has included the following:

- Recreating a database from the SSRS extracts. So far this is only a few tables in a sqlite3 db so nothing crazy. 
- Developing optimization models in pyomo to inform program design.
- Lots of ad hoc analysis and reporting. Most of this can be done with some filtering and group-bys but has also included some iterative proportional fitting and other kind of â€˜medium difficultyâ€™ methods. 
- creating power bi dashboards as well as a couple java script maplibre-gl-js maps with complex symbology.
- we accept applications to our program via an online intake, where applicants fill out forms one by one. Most of these applicants submit tens to hundreds of these applications at once. I am working in parallel on a few different potential solutions to this: templates for batch uploading is the easy one, and a potential api integration to pull applications directly from applicant systems is another.
- looking into creating some llm-agents to automate very simply data extraction. I have already tried automating these processes via dom ids and such but havenâ€™t gotten it to work reliably enough yet. My manager specifically asked for me to try agentic approaches to appease higher ups that we are implementing AI.

Iâ€™m not entirely sure where I fall in the landscape of data titles and would appreciate input. I mostly use python with a bit of power query and vanilla excel as well. Very little Java script (just for certain visualizations). Power bi. 

Edit to add- I also manage an intern-turned-part-time-employee that supports me in the above tasks basically at my own discretion ",datascience,11,https://www.reddit.com/r/datascience/comments/1mpp8sv/what_should_my_job_title_be/,r_1mpp8sv,,,
r_1mpkk2n,reddit,BB_147,2025-08-13T23:41:45+00:00,"Job market getting any better or nah?
Iâ€™ve been staying in my role and refusing to leave for the last several years. Iâ€™m wondering if thereâ€™s any signs yet the job market is coming back yet or if weâ€™re still stuck in the slog",datascience,89,https://www.reddit.com/r/datascience/comments/1mpkk2n/job_market_getting_any_better_or_nah/,r_1mpkk2n,,,
r_1mpei2b,reddit,Odd_Artist4319,2025-08-13T19:44:58+00:00,"How can I gain business acumen as a data scientist?
I can build models, but can I build profits? Thatâ€™s the gap Iâ€™m trying to close.

Iâ€™m doing my Masterâ€™s in Data Science with a BSc in Computer Science. My technical skills are strong, but I lack business acumen. In interviews, Iâ€™ve noticed many questions arenâ€™t just about models or algorithms, but about how those translate into profits or measurable business value.

Senior data scientists seem to connect their work to revenue, retention, or strategy with ease, while I still default to thinking in terms of accuracy and technical metrics. How did you learn to bridge that gap? Did you focus on general business knowledge, industry-specific skills, or hands-on projects?

I want to speak the â€œlanguage of the businessâ€ so my work is not just technically solid but strategically impactful.",datascience,101,https://www.reddit.com/r/datascience/comments/1mpei2b/how_can_i_gain_business_acumen_as_a_data_scientist/,r_1mpei2b,,,
r_1mpa610,reddit,jambery,2025-08-13T17:05:02+00:00,"Research Data Scientists without heavy coding backgrounds (stats, econ, etc), has LLM's improved your workflow?
I remember for a while there were many CS folks saying that Data Science has become software engineering, and that if you aren't fluent in software engineering fundamentals then you're going to fall behind. It became enough of a popular rhetoric that people said they preferred to hire a coder with some math knowledge than a math person with some coding knowledge. 

As a Statistician that works in Research Data Science with an average level of coding experience, enough to write my own code in notebooks, but translating it into a fully fleshed Python module with classes and functions was much more difficult for me. For a while I thought my lack of advanced software engineering knowledge would become a crutch in my career and as someone with a busy personal life I didn't want to spend that much time learning these fundamentals. Then, my company rolled out LLM's integrated into the software we use, like Visual Studio. Suddenly I'm able to create fully fleshed out modules from my notebooks in a flash. I can ask the LLM to write unit tests to test out how my code processes data or test its various subfunctions. I can use it to code up various types of models quickly to compare results. Handing off my code to engineering in the form of a Python package wasn't such a pain anymore. 

Sure the LLM produces some weird results sometimes, and I do have to spend time making sure I ask it the correct things and/or cleaning up the code so that it works properly. But now I feel like that crutch I had is no longer present.",datascience,145,https://www.reddit.com/r/datascience/comments/1mpa610/research_data_scientists_without_heavy_coding/,r_1mpa610,,,
r_1mo6ofm,reddit,Clicketrie,2025-08-12T11:56:42+00:00,"Using Experiment Tracking For Backtests
Iâ€™ve used MLFlow as a data scientist, but here itâ€™s being used for managing algo trading backtests and I thought this was an awesome use case. (And these arenâ€™t ML runs, this is testing a momentum strategy).",datascience,4,https://www.reddit.com/r/datascience/comments/1mo6ofm/using_experiment_tracking_for_backtests/,r_1mo6ofm,,,
r_1mniapg,reddit,tinkinc,2025-08-11T16:50:18+00:00,"Databricks Freea course Recs
Can anyone recommend a great free databricks catalog or otherwise course to level up as a DS using databricks itself? 

",datascience,5,https://www.reddit.com/r/datascience/comments/1mniapg/databricks_freea_course_recs/,r_1mniapg,,,
r_1mnhsx7,reddit,ElectrikMetriks,2025-08-11T16:32:12+00:00,"When you edit the massive query someone sent you, forgot where you deleted something, and left a comma behind...",datascience,139,https://www.reddit.com/r/datascience/comments/1mnhsx7/when_you_edit_the_massive_query_someone_sent_you/,r_1mnhsx7,,,
r_1mnggxa,reddit,DataAnalystWanabe,2025-08-11T15:43:32+00:00,"Catch-22 followup
I'm following up on my post about ""Catch-22: learning R with projects""

Thank you to all those who responded. The replies were very reassuring.

After reading through the replies and reflecting on it, I realised the core of my struggle came from a specific fear that I would have to go through a rigorous coding interview, similar to what software engineers face.

I was picturing a scenario where I'd be given a problem and have to write perfect, memorised R code on the spot without any help. That pressure is what made me feel like I had to absorb every cheat sheet and learn all the syntax before I could even start a project. It created the syntax vs. projects Catch-22 that my original post was about.

For those who pivoted to data science or data analytics, did you have to go through some sort of coding interview or was it just like any other interview?",datascience,19,https://www.reddit.com/r/datascience/comments/1mnggxa/catch22_followup/,r_1mnggxa,,,
r_1mn3338,reddit,AutoModerator,2025-08-11T04:01:33+00:00,"Weekly Entering & Transitioning - Thread 11 Aug, 2025 - 18 Aug, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,7,https://www.reddit.com/r/datascience/comments/1mn3338/weekly_entering_transitioning_thread_11_aug_2025/,r_1mn3338,,,
r_1mmzk4s,reddit,DataAnalystWanabe,2025-08-11T01:06:42+00:00,"Catch-22: Learning R through ""hands on"" Projects


I often get told ""learn data science by doing hands-on projects"" and then I get all fired up and motivated to learn, and then I open up R.... And then I stare at a blank screen because I don't know the syntax from memory. 

And then I tell myself I'm going to learn the syntax so that I can do projects, but then I get caught up creating folders for each function of dplyr and the subfunctions of that and cheat sheets for this.

And then I come across the advice that I shouldn't learn syntax for the sake of learning syntax - I should do hands on projects.

I need projects to learn syntax and I need syntax to start doing projects.

________


Edit - Thank you so much to all of you who have replied and I would respond to each one of you but I don't want to sound like a parrot.

The reassurance that you don't have to have absorbed every R cheat sheet before being a professional Data Scientist/Analyst is very much appreciated. 

My assumption was these data analyst/scientist roles had coding-exams as part of the interview process, which is what stressed me out. Seeing some of you here as experienced analysts who still Google code is very relieving. I am very grateful for each response, and I read each one carefully.",datascience,46,https://www.reddit.com/r/datascience/comments/1mmzk4s/catch22_learning_r_through_hands_on_projects/,r_1mmzk4s,,,
r_1mly9hm,reddit,DataAnalystWanabe,2025-08-09T19:40:16+00:00,"Business focused data science
As a microbiology researcher, I'm far away from the business world. I do more -omics and growth curves and molecular techniques, but I want to move away from biology.

I believe the bridge that can help me do that is data. I have got experience with R and excel. I'm looking at learning SQL and PowerBI.

But I want to do it away from biology. The problem is, if I was to go from the UK, as a PhD microbiologist, and approach GCC consulting/business analyst recruiters, I get the sense that they'd scoff at me for thinking too highly of my ""transferrable skills"" and tell me that I don't have experience in the world of business.

How would I get myself job-ready for GCC business-focused data science roles. Is there anyone out there that has made the switch that can share some advice?

Thanks in advance",datascience,39,https://www.reddit.com/r/datascience/comments/1mly9hm/business_focused_data_science/,r_1mly9hm,,,
r_1mluc12,reddit,RookFlame4882,2025-08-09T16:56:51+00:00,"Burnout, disillusionment, and imposter syndrome after 1 year in DS. Am I just an API monkey? Reality check needed.
Hey folks,

I am about a year into my first data science job. It took roughly a year and more than 400 applications to land it, so the idea of another long search is scary.

Early on I worked with an internally built causal AI model that captures relationships for further analysis. I did not build the model. I ran experiments to make it more explainable and easier for others to use. I also built data orchestration pipelines using third party tools that are common in industry and cloud providers like AWS and GCP.

The last six months have shifted to LLM and NLP work. A lot of API calls, large text analysis. The next six months look even more LLM heavy since I am leading an internal tool build.

On paper there are wins:
- I have led projects and designed tools from scratch.
- My communication and client skills have improved.


My concerns:

- I am not doing much classical DS or rigorous modeling.
- LLM work often feels like API wrangling rather than technical depth.
- Work life balance is rough with frequent weekends.
- Even with a possible 5 to 10 percent raise (possibly within the next 6 months), the work likely stays the same.

I feel imposter syndrome and worry I am behind my peers on fundamentals and interview depth. Iâ€™m so burned out and honestly canâ€™t tell if Iâ€™m just being a negative Nancy or if my concerns are legit. Am I shortchanging myself by thinking that I'm just not skilled enough? Idk


What I would love input on:

Am I building valuable skills for the DS market, or am I narrowing myself too much?

What types of companies or industries might value this mix of causal modeling, LLM work, and consulting style analysis?

If I want to keep doors open for more traditional DS or ML roles, what should I focus on learning now?

Portfolio ideas I can ship from my current work that would impress a hiring manager?

Would you ride out six months to finish the tool and try for a promotion, or start looking sooner?

Honest takes are very welcome.

",datascience,115,https://www.reddit.com/r/datascience/comments/1mluc12/burnout_disillusionment_and_imposter_syndrome/,r_1mluc12,,,
r_1mlmwk0,reddit,takenorinvalid,2025-08-09T11:13:29+00:00,"AI isn't taking your job. Executives are.
If AI is ready to replace developers, why aren't developers replacing themselves with AI and just taking it easy at work?

I'm a Director at my company. I'm in the meetings and helping set up the tools that cost people their jobs. Here's how they work:

1. Claude AI writes some code

2. The code gets passed to a developer for validation

3. Since the developer's ""just validating"", he can be replaced with an overseas contractor that'll work for a fraction of the pay

We've tracked the tools, and we haven't seen any evidence that having Claude take a crack at the code saves anybody any time - but it does let us justify replacing expensive employees with cheap overseas contractors.

You're not getting replaced by AI.

Your job's being outsourced overseas.",datascience,1816,https://www.reddit.com/r/datascience/comments/1mlmwk0/ai_isnt_taking_your_job_executives_are/,r_1mlmwk0,,,
r_1ml6fxs,reddit,gonna_get_tossed,2025-08-08T20:40:52+00:00,"Just bombed a technical interview. Any advice?
I've been looking for a new job because my current employer is re-structuring and I'm just not a big fan of the new org chart or my reporting line. It's not the best market, so I've been struggling to get interviews. 

But I finally got an interview recently. The first round interview was a chat with the hiring manager that went well. Today, I had a technical interview (concept based, not coding) and I really flubbed it. I think I generally/eventually got to what they were asking, but my responses weren't sharp.* It just sort of felt like I studied for the wrong test. 

How do you guys rebound in situations like this? How do you go about practicing/preparing for interviews? And do I acknowledge my poor performance in a thank you follow up email?

*Example (paraphrasing): They built a model that indicated that logging into a system was predictive of some outcome and management wanted to know how they might incorporate that result into their business processes to drive the outcome. I initially thought they were asking about the effect of requiring/encouraging engagement with this system, so I talked about the effect of drift and self selection on would have on model performance. Then they rephrased the question and it became clear they were talking about causation/correlation, so I talked about controlling for confounding variables and natural experiments.",datascience,80,https://www.reddit.com/r/datascience/comments/1ml6fxs/just_bombed_a_technical_interview_any_advice/,r_1ml6fxs,,,
r_1mkzhvp,reddit,redditisthenewblak,2025-08-08T16:14:20+00:00,"Resources/tips for someone brand new to model building and deployment in Azure?
Context: my current company is VERY (VERY) far behind, technologically. Our data isn't that big and currently resides in SQL Server databases, which I query directly via SSMS.

Whenever a project requires me to build models, my workflow would generally look like:

1. Query the data I need, make features, etc. from SQL Server.
2. Once I have the data, use Jupyter Notebooks to train/build models. 
3. Use best model to score dataset.
4. Send dataset/results to stakeholder as a file.

My company doesn't have a dedicated Dev team (on-shore, at least) nor a DE team. And this workflow works to make ends meet. 

Now my company has opened up Azure accounts for me and my manager, but neither one of us have developed anything in it before.

Microsoft has PLENTY of documentation, but the more I read, the more questions I have, and I feel like my time will be spent reading articles rather than getting anything done.

It seems like quite a shift from doing everything ""locally"" like what we have been doing to actually using cloud resources. So does anyone have any tips/guides that are beginner-friendly where I can do my entire workflow in the cloud?",datascience,21,https://www.reddit.com/r/datascience/comments/1mkzhvp/resourcestips_for_someone_brand_new_to_model/,r_1mkzhvp,,,
r_1mkov0u,reddit,Damp_Out,2025-08-08T07:32:35+00:00,"""SemiAuto"" Fully Automated Machine Learning Lifecycle by Just API Calling
So for the last 4 months I have been working on this project which was first supposed to be a upgrade of AutoML, but I later recognised it's potential.

This project could be one of the best things in ML reasearch, This project is just that good.

For context, I have the knowledge around ML for about 1.5 years now and thanks to the tools available, I have been able to build a grand project like this,

The Project's or you can say the Tool name is 'SemiAuto', A full fledged ML lifecycle Automation tool. It has 3 microservice, Regression, Classification, and Clustering.

I have completely build the Version 1 of this project.


It has 6 parts, First ingest the Data.csv file and the target column.

Second choose whatever preprocessing you want to and apply them.

Third use feature tools to build new features and then SHAP to select the amount of features you want.

Fourth choose any algorithm you want with the hyper params and build the model.

Fifth choose the optimization technique and get an optimised model.

At last, get the report, model.pkl, and processor.pkl and use them wherever you want.


As of why this project would be extremely good in research as researchers needs to test with different techniques and different models to get the best thing out and this tool provides that,

This tool will in a semiautomatic way can fully do each and everything by itself, no coding required.

The version 2 of this project is in production and I are introducing much more than the previous version,
For example, Parallel model building, Simple Ensemble design and Staged Ensemble design.

And also the thing that no one as of today has ever implemented in their ML automation tool, Meta-Heuristics Algorithms for feature selection.


Version 2 will be one of the most mind blowingly incredible release of the SemiAuto",datascience,0,https://www.reddit.com/r/datascience/comments/1mkov0u/semiauto_fully_automated_machine_learning/,r_1mkov0u,,,
r_1mkmjje,reddit,Proof_Wrap_2150,2025-08-08T05:12:22+00:00,"How would you visualize or analyze movements across a categorical grid over time?
Iâ€™m working with a dataset where each entity is assigned to one of N categories that form a NxN grid. Over time, entities move between positions (e.g., from â€œN1â€ to â€œN2â€).

Has anyone tackled this kind of problem before? Iâ€™m curious how youâ€™ve visualized or even clustered trajectory types when working with time-series data on a discrete 2D space.",datascience,11,https://www.reddit.com/r/datascience/comments/1mkmjje/how_would_you_visualize_or_analyze_movements/,r_1mkmjje,,,
r_1mkdy7a,reddit,Starktony11,2025-08-07T22:21:21+00:00,"How do you analyse unbalanced data you get in A/B testing?
Hi 
I have two questions related unbalanced data in A/B testing. Would appreciate resources or thoughts. 

1. Usually when we perform A/B testing, we have 5-10% in treatment, after doing power analysis we get the sample size needed, we run tge experiment, by the time we get required sample size for treatment we get way more control samples, so now when we analyse, which samples do we keep in control group? For example by the time we collect 10k samples from treatment we might get 100k samples of control. So what to do now before performing t-test or any kinds of test? 
 (In ML we can downsample or over sample but what to do in causal side) 

2. Again similar question Lets say we are performing test on 50/50 but if one variant get way more samples as more ppl come through that channel and common for users, hiw do we segment users such as way? And again which samples we keep once we get way more sample than needed? 

I want to know how it is tackeled in day to day, and this thing happen frequently right? Or am i wrong? 

Also, what if you get sample size before expected time? (Like was thinking to run them for 2 weeks but got the required size in 10 days) Do you stop the experiment and start analyzing? 

Sorry for this dumb question but i could not find good answers and honestly donâ€™t trust chat gpt much as many time it hallucinates in this topic. 

Thanks!",datascience,31,https://www.reddit.com/r/datascience/comments/1mkdy7a/how_do_you_analyse_unbalanced_data_you_get_in_ab/,r_1mkdy7a,,,
r_1mk7lpa,reddit,Pristine-Item680,2025-08-07T18:15:23+00:00,"What elective course should I take
Hey all,

About to start my last semester for my masters in computer science, with a concentration in AI. Iâ€™m a veteran data scientist, this is more of a vanity degree and an ability to say â€œyes I do have a masters degreeâ€ on a job application, but I have enjoyed the studying overall. 

I have room for one elective class, and Iâ€™m trying to decide what I should take. None of them that fit my schedule seem particularly appealing:

- data analysis: hyper redundant given my background
- computer networks: possibly useful, but Iâ€™d much rather learn something like distributed systems
- intro to cybersecurity: maybe good, but seems like it would be mostly terminology and not so much a deep dive on anything 
- object oriented design: could be nice for refining my actual design choices, but programming seems like the least valuable skill to upskill on in computer science now (as compared to, say, cloud computing, which is and will continue to be good to know). 

Itâ€™s not exactly the most pressing choice, but I thought Iâ€™d throw it to Reddit, and see if anyone has a strong opinion on whatâ€™s good to learn to augment my ML/AI background

Edit: okay I think you people convinced me. Object oriented design it is! Which sounds a whole lot better than computer networks, thatâ€™s for sure. ",datascience,7,https://www.reddit.com/r/datascience/comments/1mk7lpa/what_elective_course_should_i_take/,r_1mk7lpa,,,
r_1mizivg,reddit,Astherol,2025-08-06T08:56:31+00:00,"Seeking Meaningful, Non-Profit Data Volunteering Projects",datascience,30,https://www.reddit.com/r/datascience/comments/1mizivg/seeking_meaningful_nonprofit_data_volunteering/,r_1mizivg,,,
r_1miresg,reddit,vishal-vora,2025-08-06T01:21:40+00:00,"Share your thought on open source alternative for data robot
Data robot is the market leader when it comes to enterprises data science project life cycle management. But there is no open source alternative available in the market right now. What are the chances of getting a good adoption if I can build the open source alternative of data robot?",datascience,0,https://www.reddit.com/r/datascience/comments/1miresg/share_your_thought_on_open_source_alternative_for/,r_1miresg,,,
r_1miccmb,reddit,techlatest_net,2025-08-05T15:30:16+00:00,"How I built and deployed a GenAI app in minutes using openâ€‘source tools + Azure
Hey everyone building AI apps always felt like a massive undertaking. So much code, setup, server stuff. I recently tried something different and launched a working GenAI app in just under 15 minutes. I used Dify AI (an openâ€‘source platform) to design the app and Microsoft Azure to deploy it.

What I learned:
â€¢ No heavy DevOps or managing servers
â€¢ Very userâ€‘friendly interfaceâ€”just plug in your AI logic
â€¢ Scales automatically via Azure cloud resources

Would love to hear if anyoneâ€™s tried Dify AI or other openâ€‘source builders for AIâ€”and what challenges you faced!

Full details in this writeâ€‘up:
https://medium.com/@techlatest.net/launch-genai-apps-in-minutes-with-techlatest-dify-ai-on-azure-cloud-platform-8307bccf4aed

Happy to answer questions or breakdown steps if interested ðŸ˜Š",datascience,0,https://www.reddit.com/r/datascience/comments/1miccmb/how_i_built_and_deployed_a_genai_app_in_minutes/,r_1miccmb,,,
r_1mhikh4,reddit,ProbaDude,2025-08-04T16:42:37+00:00,"How can I *give* a good data science/machine learning interview?
I'm around 6 months into my first non intern job and am the only data scientist/MLE in my company. My company has decided they want to bring on some much needed help (thank god) and want me to do ""the more technical side"" of the interview (with others taking care of the behavioral etc)

I do have some questions in mind specific to my job for what I want in a colleague but I still feel a bit underprepared. My plan is to ask the 'basic' questions that I got asked in every interview (classification vs clustering, what is r^2, etc) before asking them how they would solve some of the problems I'm actually working on

But like that's all I have in the pipeline at the moment, and I'd really like to avoid this becoming the blind interviewing the blind moment.  

Does anyone have any good tips on how to do the interviews, what to look for or what to include? Thank you!!!!

EDIT: In reply to the DMs, we are not accepting any new applicants at this time ðŸ˜…",datascience,177,https://www.reddit.com/r/datascience/comments/1mhikh4/how_can_i_give_a_good_data_sciencemachine/,r_1mhikh4,,,
r_1mh9tvo,reddit,SharePlayful1851,2025-08-04T10:31:54+00:00,What would be a better job Position ? Data Scientist or AI/ML Engineer.,datascience,0,https://www.reddit.com/r/datascience/comments/1mh9tvo/what_would_be_a_better_job_position_data/,r_1mh9tvo,,,
r_1mh3i7n,reddit,AutoModerator,2025-08-04T04:01:42+00:00,"Weekly Entering & Transitioning - Thread 04 Aug, 2025 - 11 Aug, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,7,https://www.reddit.com/r/datascience/comments/1mh3i7n/weekly_entering_transitioning_thread_04_aug_2025/,r_1mh3i7n,,,
r_1mgxgpl,reddit,CleanDataDirtyMind,2025-08-03T23:08:55+00:00,"Is there a term for internal processing vs data that needs to be stakeholding/customer facing?
For example I had my physical credit card stolen. I was trying to get information from the CC company about when the card was used so that the local PD could check security cameras. (We thought it was particular person so they made a little bit more effort). When I called the credit card company, the customer service person started telling me these random times that made no sense and I realized he was reading the wrong column which were basically the time the charge was converted from â€œ?â€ to an actual money transfer. I assume to him it gave insight into how to refund each charge so â€œrelvantâ€ just not â€œrelvantâ€ information I would ever need to know.

Two years later, I am setting up a model with my team and we batting around terms to differentiate between data like these dates & times that are relvant but are not relvant un-manipulated or laid bare for the stakeholder to see visualized or be discussed outside of our team.

You can hear the inevitable pause from a team member every time the concept comes up as they attempt a new word.  While it was amusing itâ€™s starting to eat at me. Any ideas?",datascience,2,https://www.reddit.com/r/datascience/comments/1mgxgpl/is_there_a_term_for_internal_processing_vs_data/,r_1mgxgpl,,,
r_1mgxbio,reddit,NervousVictory1792,2025-08-03T23:02:26+00:00,"Algorithm Idea
This sudden project has fallen on my lap where I have a lot of survey results and I have to identify how many of those are actually done by bots. I havenâ€™t see what kind of data the survey holds but I was wondering how can I accomplish this task. A quick search points me towards anomaly detections algorithms like isolation forest and dbscan clusters. Just wanted to know if I am headed in the right direction or can I use any LLM tools. TIA :) ",datascience,1,https://www.reddit.com/r/datascience/comments/1mgxbio/algorithm_idea/,r_1mgxbio,,,
r_1mgsshu,reddit,indie-devops,2025-08-03T19:56:10+00:00,"Personal projects and skill set
Hi everyone,
I was just wondering how do you guys specify personal acquired skills from your personal projects in your CV.
Iâ€™m in the midst of a pretty large project - end to end pipeline for predicting real time probabilities of winning chances in a game. This includes a lot of tools, from scraping, database management (mostly tables creations, indexing, nothing DBA-like), scheduling, training, prediction and data drift pipelines, cloud hosting, etc. and I was wondering how I can specify those skills after I finish my project, because I do learn tons from this project. To say Iâ€™m using some of those tools in my current job is not entirely right soâ€¦

What would you say?
Cheers.",datascience,24,https://www.reddit.com/r/datascience/comments/1mgsshu/personal_projects_and_skill_set/,r_1mgsshu,,,
r_1mgrvsh,reddit,1_plate_parcel,2025-08-03T19:19:25+00:00,"Hi! i am a junior dev need advice regarding fraud/risk scoring (not credit) on my rules based fraud detection system.
so i our team has developed a rules based fraud detecton system....now we have received a new requirement that we have to score every transaction as how much risky or if flagged as fraud how much fraud it is.

i did some research and i found out its easier if it is a supervisied operation but in my case i wont be able to access prod transaction data due to policy.

now i have 2 problems data which i guess i have to make a fake one.

2nd how to score i was thinking of going witb regression if i keep my target value bete 0 and 1 but realised that the model can predict above that
then thought of classification and use predict_proba() to get prediction probability.

or isolation forest

till now thats what i bave you thought what else shoudl i consider any advices or guidance to set me in the right path so i dont get any rework
",datascience,0,https://www.reddit.com/r/datascience/comments/1mgrvsh/hi_i_am_a_junior_dev_need_advice_regarding/,r_1mgrvsh,,,
r_1mgfcke,reddit,Anu_Rag9704,2025-08-03T09:50:17+00:00,"Built this out of pure laziness for all my Feature engineering/model training jobs
Built this out of pure laziness 
A lightweight Telegram bot that lets me: 
- Get Databricks job alerts
- Check todayâ€™s status
- Repair failed runs
- Pause/reschedule ,
All from my phone.
No laptop. No dashboard. Just / Commands.",datascience,63,https://www.reddit.com/r/datascience/comments/1mgfcke/built_this_out_of_pure_laziness_for_all_my/,r_1mgfcke,,,
r_1mf44ek,reddit,pokelord13,2025-08-01T18:22:53+00:00,"Using a hybrid role in job title (Data Science and Engineer)
I have an BS and MS in data science and got hired as a data analyst for a small ish scale company for about a year now as my first job. I'm the only data person in the entire company and I've been wanting to transition into a data science focused role for awhile, so I have been using DS and DE principles at every opportunity to boost my resume. This has ended up extending far beyond the typical DA responsibilities as I have been utilizing a lot of stats modeling and predictive analytics over company data/KPIs, using MLOps occasionally, as well as building ETL pipelines, managing the internal DBMS and streamlining data acquisition through RESTful APIs with contracted third parties. I still do excel monkey work/tableau dashboards along with this.

Management ended up taking notice and since nobody in the building has any familiarity with data science/tech, they have asked me to rewrite my job description including my job title as a semi promotion. Since I have been working as a bit of a hybrid between DS and DE I am wondering if I should put the new contracted job title as a hybrid role (e.g. Data Science Engineer) or just pick one? My department head has suggested the title of Data Architect but I don't really think that aligns with my job responsibilities and it's also a senior sounding position which feels strange to take on considering I've only been in the industry for a year.",datascience,52,https://www.reddit.com/r/datascience/comments/1mf44ek/using_a_hybrid_role_in_job_title_data_science_and/,r_1mf44ek,,,
r_1mets4m,reddit,JumbleGuide,2025-08-01T11:19:20+00:00,"How to convert data to conceptual models
I am not sure if I am in the right subreddit, so please by patient with me.

I am working on a tool to reverse-engineer conceptual models from existing data. The idea is you take a legacy system, collect sample data (for example JSON messages communicated by the system), and get a precise model from them. The conceptual model can be then used to develop new parts of the system, component replacements, build documentation, tests, etc...

One of the open issues I struggle with is the fully-automated conversion from 'packaging' model to conceptual model.

When some data is uploaded, it's model reflects the packaging mechanism, rather than the concepts itself. For example. if I upload JSON-formatted data, the model initially consists of objects, arrays, and values. For XML, it is elements and attributes. And so on.

[JSON messages consist of objects, arrays, and values](https://preview.redd.it/rq6k13ej2egf1.png?width=737&format=png&auto=webp&s=415800ea39e0b408f91124f5d03fab02b631e75e)

I can convert the keys, levels, paths to detect concepts and their relationships.  It can look something like this:

[Data structures converted to concepts](https://preview.redd.it/r1d2ti683egf1.png?width=695&format=png&auto=webp&s=0927e6222a90412d7dd5b722fdb43ad07b49e027)

  
The issue I am struggling with is that this conversion is not straightforward. Sometimes, it helps to use keys, other times it is better to use paths. For some YAML files, I need to treat the keys as values (typically package.yaml samples).

Did anyone tried to convert data to conceptual models before? Any real-word use cases?

Is there any theory at least about the reverse direction - use conceptual model and map it into XML schema / JSON schema / YAML ... ?

Thanks in advance.",datascience,10,https://www.reddit.com/r/datascience/comments/1mets4m/how_to_convert_data_to_conceptual_models/,r_1mets4m,,,
r_1mem3t5,reddit,Minotaar_Pheonix,2025-08-01T03:37:46+00:00,"Generative AI shell interface for browsing and processing data?
So vibe coding is a thing, and I'm not super into it.

However, I often need to write little scripts and parsers and things to collect and analyze data in a shell environment for various code that I've written.  It might be for debugging, or just collecting production science data.  Writing that shit is a real pain, because you need to be careful about exceptions and errors and folder names and such.

Is there a way to do ""vibe data gathering"" where I can ask some LLM to write me a script that does a number of things like open up a couple thousand files that fit various properties in various folders, parse them for specific information, then draw say a graph?  ChatGPT can of course do that, but it needs to know the folder structure and examine the files to see what issues there are in collecting this information.  Any way I can do this without having to roll my sleeves up?",datascience,2,https://www.reddit.com/r/datascience/comments/1mem3t5/generative_ai_shell_interface_for_browsing_and/,r_1mem3t5,,,
r_1me934o,reddit,giantwaterwithice,2025-07-31T18:16:29+00:00,"Why is there no Cursor/Windsurf for Notebooks or Google Collab?
 Last week, I tried Windsurf to build a web application and OMG my world was changed. I have used AI tools before but having an agent that implements the code for you is a game changer, my productivity probably went up x5 or x10 times. 

This made me think why is there nothing like this for a data scientist workflow? I know you can do notebook markdown but it is still not the same because Cursor cannot see outputs of your graphs. Also, this tool wouldnâ€™t work on Google Collab where I have access to powerful GPUs. 

Now, imagine if you have a tool that goes from a prompt â€œmake the predictive model to predict customer churnâ€ and instead of something like Chatgpt giving you one slob of generic BS that will definitely give out an error, an agent goes and executes each cell one by one: making plots, studying the data, modifying the outliers etc. and adjusting the plan as it goes before finally making a few models and testing them. Basically, the standard data science workflow. 

I would like to build something this (I have no idea how yet lol) if there is interest in this community. What do you guys think? Those of you who are working in the field, would you actually use it? 

Also, if someone wants to build it with me, DM me. ",datascience,8,https://www.reddit.com/r/datascience/comments/1me934o/why_is_there_no_cursorwindsurf_for_notebooks_or/,r_1me934o,,,
r_1me91rq,reddit,Aristoteles1988,2025-07-31T18:15:05+00:00,"FIGMA? Is the tech industry back?
Have you guys heard of this IPO? Stock tripled on debut. What does this company do? 

I feel like you tech bros might have a come back soon fyi ",datascience,0,https://www.reddit.com/r/datascience/comments/1me91rq/figma_is_the_tech_industry_back/,r_1me91rq,,,
r_1mdf6fn,reddit,FinalRide7181,2025-07-30T18:57:25+00:00,"My take on the Microsoft paper
I read the paper myself (albeit pretty quickly) and tried to analyze the situation for us Data Scientists.

The jobs on the list, as you can intuitively see (and it is also explicitly mentioned in the paper), are mostly jobs that require writing reports and gathering information because, as the paper claims, AI is good at it.

If you check the chart present in the paper (which I linked in this post), you can see that the clear winner in terms of activities done by AI is â€œGathering Informationâ€, while â€œAnalyzing Dataâ€ instead is much less impacted and also most of it is people asking AI to help with analysis, not AI doing them as an agent (red bar represents the former, blue bar the latter).

It seems that our beloved occupation is in the list mainly because it involves gathering information and writing reports. However, the data analysis part is much less affected and thatâ€™s just data analysis, let alone the more advanced tasks that separate a Data Scientist from a Data Analyst.

So, from what I understand, Data Scientists are not at risk. The things that AI does do not represent the actual core of the job at all, and are possibly even activities that a Data Scientist wants to get rid of.

If youâ€™ve read the paper too, Iâ€™d appreciate your feedback. Thanks!",datascience,166,https://www.reddit.com/r/datascience/comments/1mdf6fn/my_take_on_the_microsoft_paper/,r_1mdf6fn,,,
r_1mdan3p,reddit,#NAME?,2025-07-30T16:07:20+00:00,"Model Governance Requests - what is normal?
Iâ€™m looking for some advice. I work at a company that provides inference as a service to other customers, specifically we have model outputs in an API. This is used across industries, but specifically when working with Banks, the amount of information they request through model governance is staggering.

I am trying to understand if my privacy team is keeping things too close to the chest, because I find that what is in our standard governance docs, vs the details we are asked, is hugely lacking. It ends up being this ridiculous back and forth and is a huge burn on time and resources. 


Here are some example questions:

* specific features used in the model 

* specific data sources we use

* detailed explanations of how we arrived at our modeling methodology, what other models we considered, the results of those other models, and the rationale for our decision with a comparative analysis

* a list of all metrics used to evaluate model performance, and why we chose those metrics

* time frame for train/test/val sets, to the day

I really want to understand if this is normal, and if my org needs to improve how we report these out to customers that are very concerned about these kinds of things (banks). Are there any resources out there showing what is industry standard? How does your org do it?

Thanks",datascience,6,https://www.reddit.com/r/datascience/comments/1mdan3p/model_governance_requests_what_is_normal/,r_1mdan3p,,,
r_1mdaa40,reddit,itssdgm,2025-07-30T15:53:52+00:00,"Working remote
hey all 
iâ€™ve been a data scientist for a while now, and iâ€™ve noticed my social anxiety has gotten worse since going fully remote since covid. i love the work itself - building models, finding insights etc, but when it comes to presenting those insights, i get really anxious. itâ€™s easily the part of the job i dread most.

i think being remote makes it harder. less day-to-day interaction, fewer casual chats - and it just feels like the pressure is higher when you do have to speak. imposter syndrome also sneaks in at time. tech is constantly evolving, and sometimes i feel like iâ€™m barely keeping up, even though iâ€™m doing the work.

i guess iâ€™m wondering:
	â€¢	does anyone else feel this way?
	â€¢	have you found ways to make communications feel less overwhelming?

would honestly just be nice to hear from others in the same boat. thanks for reading.",datascience,113,https://www.reddit.com/r/datascience/comments/1mdaa40/working_remote/,r_1mdaa40,,,
r_1md5gvk,reddit,timusw,2025-07-30T12:37:47+00:00,Microsoft just dropped a study showing the 40 jobs most affected by Al and the 40 that Al can't touch (yet).,datascience,413,https://www.reddit.com/r/datascience/comments/1md5gvk/microsoft_just_dropped_a_study_showing_the_40/,r_1md5gvk,,,
r_1mcngiy,reddit,askdatadawn,2025-07-29T21:02:07+00:00,"Python Summer Party (free!): 15-day coding challenge for Data folks
Iâ€™ve been cooking up something fun for the summer.. A Python-themed challenge to help Data Scientists & Data Analysts practice and level up their Python skills. Totally free to play!

Itâ€™s called **Python Summer Party**, and it runs for 15 days, starting August 1.

Hereâ€™s what to expect:

* One Python challenge + 3 parts per day
* Focused on Data skills using *NumPy*, *Pandas*, and regular Python
* All questions based on real companies, so you can practice working with real problems
* Beginner to intermediate to advanced questions
* AI chat to help you if you get stuck
* Discord community (if you still need more help)
* A chance to win 5 free annual Data Camp subscriptions if you complete the challenges
* Totally free

I built this because I know how hard it can be to stay consistent when youâ€™re learning alone. Plus, when I was learning Python I couldn't find questions that allowed me to apply Python to realistic business problems.

So this is meant to be a light, motivating way to practice and have fun with others. *I even tried to design it such that it's cute & fun.*

Would love to have you join us (and hear your feedback if you have any!) 

[www.interviewmaster.ai/python-party](http://www.interviewmaster.ai/python-party)",datascience,83,https://www.reddit.com/r/datascience/comments/1mcngiy/python_summer_party_free_15day_coding_challenge/,r_1mcngiy,,,
r_1mcd3n5,reddit,Lamp_Shade_Head,2025-07-29T14:35:17+00:00,"Since when did â€œmeetsâ€ expectations become a bad thing in this industry?
I work at a pretty big named company on west coast. It is pretty shocking to see that in my company anyone who gets â€œmeetsâ€ expectations have not been getting any salary increments, not even a dollar each year. Iâ€™d think if you are meeting expectations, it means you are holding up your end of the deal and it shouldnâ€™t be a bad thing. But now, you actually have to exceeds expectations to get measly 1% salary raises and sometimes to just keep your job.

Did this used to happen pre covid as well?",datascience,224,https://www.reddit.com/r/datascience/comments/1mcd3n5/since_when_did_meets_expectations_become_a_bad/,r_1mcd3n5,,,
r_1mc8w7g,reddit,bandaian,2025-07-29T11:28:51+00:00,"How to use AI effectively and efficiently to code
Any tips on how to teach beginners on how to use AI effectively and efficiently to code?",datascience,0,https://www.reddit.com/r/datascience/comments/1mc8w7g/how_to_use_ai_effectively_and_efficiently_to_code/,r_1mc8w7g,,,
r_1mc2zaz,reddit,CableInevitable6840,2025-07-29T05:18:53+00:00,"Does a Data Scientist need to learn all these skills?
* Strong knowledge of Machine Learning, Deep Learning, NLP, and LLMs.
* Experience with Python, PyTorch, TensorFlow.
* Familiarity with Generative AI frameworks: Hugging Face, LangChain, MLFlow, LangGraph, LangFlow.
* Cloud platforms: AWS (SageMaker, Bedrock), Azure AI, and GCP
* Databases: MongoDB, PostgreSQL, Pinecone, ChromaDB.
* MLOps tools, Kubernetes, Docker, MLflow.

I have been browsing many jobs and noticed they all are asking for all these skills.. is it the new norm? Looks like I need to download everything and subscribe to a platform that teaches all these lol (cries in pain).",datascience,355,https://www.reddit.com/r/datascience/comments/1mc2zaz/does_a_data_scientist_need_to_learn_all_these/,r_1mc2zaz,,,
r_1mby05c,reddit,bass581,2025-07-29T01:05:56+00:00,"Any PhDs having trouble in the job market
I am a Math Bio PhD who is currently working for a pharma company. I am trying to look for new positions outside the industry, as it seems most data science work at my current employer and previous employers has been making simple listings for use across the company. It is really boring, and I feel my skillset is not applicable to other data roles. I have taken courses on data engineering and ML and worked on personal projects, but it has yielded little success. I was wondering if any other PhD that are entering the job market or are veterans have had trouble finding a new job in the last few years. Obviously the job market is terrible, but you would think having a PhD would yield better success in finding new positions. I would also like some advice on how to better position myself in the market.",datascience,80,https://www.reddit.com/r/datascience/comments/1mby05c/any_phds_having_trouble_in_the_job_market/,r_1mby05c,,,
r_1mbqiix,reddit,cptsanderzz,2025-07-28T19:55:10+00:00,"Best framework for internal tools
I need frameworks to build standalone internal tools that donâ€™t require spinning up a server. Most of the time I am delivering to non technical users and having them install Python to run the tool is so cumbersome if you donâ€™t have a clue what you are doing. Also, I donâ€™t want to spin up a server for a process that users run once a week, that feels like a waste. PowerBI isnâ€™t meant to execute actions when buttons are clicked so that isnâ€™t really an option. I donâ€™t need anything fancy, just something that users click, it opens up asks them to put in 6 files, runs various logic and exports a report comparing various values across all of those files.


Tkinter would be a great option besides the fact that it looks like it was last updated in 2000 which while it sounds silly doesnâ€™t inspire confidence for non technical people to use a new tool.

I love Streamlit or Shiny but that would require it to be running 24/7 on a server or me remembering to start it up every morning and monitor it for errors.

What other options are out there to build internal tools for your colleagues? I donâ€™t need anything enterprise grade anything, just something simple that less than 30 people would ever use.
",datascience,8,https://www.reddit.com/r/datascience/comments/1mbqiix/best_framework_for_internal_tools/,r_1mbqiix,,,
r_1mbmnkz,reddit,AipaQ,2025-07-28T17:32:16+00:00,"Why autoencoders aren't the answer for image compression
I just finished my engineering thesis comparing different lossy compression methods and thought you might find the results interesting.

**What I tested:**

* Principal Component Analysis (PCA)
* Discrete Cosine Transform (DCT) with 3 different masking variants
* Convolutional Autoencoders

All methods were evaluated at 33% compression ratio on MNIST dataset using SSIM as the quality metric.

**Results:**

* **Autoencoders: 0.97 SSIM**Â \- Best reconstruction quality, maintained proper digit shapes and contrast
* **PCA: 0.71 SSIM**Â \- Decent results but with grayer, washed-out digit tones
* **DCT variants: \~0.61 SSIM**Â \- Noticeable background noise and poor contrast

**Key limitations I found:**

* Autoencoders and PCA require dataset-specific training, limiting universality
* DCT works out-of-the-box but has lower quality
* Results may be specific to MNIST's simple, uniform structure
* More complex datasets (color images, multiple objects) might show different patterns

**Possible optimizations:**

* Autoencoders: More training epochs, different architectures, advanced regularization
* Linear methods: Keeping more principal components/DCT coefficients (trading compression for quality)
* DCT: Better coefficient selection to reduce noise

**My takeaway:**Â While autoencoders performed best on this controlled dataset, the training requirement is a significant practical limitation compared to DCT's universal applicability.

**Question for you:**Â What would you have done differently in this comparison? Any other methods worth testing or different evaluation approaches I should consider for future work?

The post with more details about implementation andÂ **visual comparisons**Â if anyone's interested in the technical details:Â [https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for](https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for)",datascience,9,https://www.reddit.com/r/datascience/comments/1mbmnkz/why_autoencoders_arent_the_answer_for_image/,r_1mbmnkz,,,
r_1mbm7zn,reddit,Technical-Love-8479,2025-07-28T17:16:15+00:00,"Tried Wan2.2 on RTX 4090, quite impressed",datascience,2,https://www.reddit.com/r/datascience/comments/1mbm7zn/tried_wan22_on_rtx_4090_quite_impressed/,r_1mbm7zn,,,
r_1mbk933,reddit,ElectrikMetriks,2025-07-28T16:04:50+00:00,Why are none of my reports refreshing this morning?,datascience,254,https://www.reddit.com/r/datascience/comments/1mbk933/why_are_none_of_my_reports_refreshing_this_morning/,r_1mbk933,,,
r_1mb6ch8,reddit,AutoModerator,2025-07-28T04:01:39+00:00,"Weekly Entering & Transitioning - Thread 28 Jul, 2025 - 04 Aug, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,7,https://www.reddit.com/r/datascience/comments/1mb6ch8/weekly_entering_transitioning_thread_28_jul_2025/,r_1mb6ch8,,,
r_1mb49xm,reddit,insane_membrane13,2025-07-28T02:14:45+00:00,"New Grad Data Scientist feeling overwhelmed and disillusioned at first job
Hi all,

I recently graduated with a degree in Data Science and just started my first job as a data scientist. The company is very focused on staying ahead/keeping up with the AI hype train and wants my team (which has no other data scientists except myself) to explore deploying AI agents for specific use cases. 

The issue is, my background, both academic and through internships, has been in more traditional machine learning (regression, classification, basic NLP, etc.), not agentic AI or LLM-based systems. The projects Iâ€™ve been briefed on, have nothing to do with my past experiences and are solely concerned with how we can infuse AI into our workflows and within our products. Iâ€™m feeling out of my depth and worried about the expectations being placed on me so early in my career. I was wondering if anyone had advice on how to quickly get up to speed with newer techniques like agentic AI, or how I should approach this situation overall. Any learning resources, mindset tips, or career advice would be greatly appreciated.",datascience,385,https://www.reddit.com/r/datascience/comments/1mb49xm/new_grad_data_scientist_feeling_overwhelmed_and/,r_1mb49xm,,,
r_1maxkht,reddit,Routine_Nothing_8568,2025-07-27T21:05:36+00:00,"Anomoly detection with only categorical variables
Hello everyone, I have an anomoly detection project but all of my data is categorical. I suppose I could try and ask them to change it prediction but does anyone have any advice. The goal is to there are groups within the data and and do an analysis to see anomlies. This is all unsupervised the dataset is large in terms of rows (500k) and I have no gpus.",datascience,7,https://www.reddit.com/r/datascience/comments/1maxkht/anomoly_detection_with_only_categorical_variables/,r_1maxkht,,,
r_1mawalf,reddit,Due-Duty961,2025-07-27T20:13:17+00:00,"why OneHotEncoder give better results than get.dummies/reindex?
**I can't figure out why I get a better score with OneHotEncoder :**

preprocessor = ColumnTransformer(

transformers=\[

('cat', categorical\_transformer, categorical\_cols)

\],

remainder='passthrough'  # <-- this keeps the numerical columns

)

model\_GBR =  GradientBoostingRegressor(n\_estimators=1100, loss='squared\_error', subsample = 0.35, learning\_rate = 0.05,random\_state=1)

GBR\_Pipeline = Pipeline(steps=\[('preprocessor', preprocessor),('model', model\_GBR)\])

  
**than get.dummies/reindex:**

  
X\_test = pd.get\_dummies(d\_test)

X\_test\_aligned = X\_test.reindex(columns=X\_train.columns, fill\_value=0)",datascience,13,https://www.reddit.com/r/datascience/comments/1mawalf/why_onehotencoder_give_better_results_than/,r_1mawalf,,,
r_1makoge,reddit,ArticleLegal5612,2025-07-27T12:08:24+00:00,"Can LLMs Reason - I don't know, depends on the definition of reasoning.  Denny Zhou - Founder/Lead of Google Deepmind LLM Reasoning Team
AI influencers: LLMs can think given this godly prompt bene gesserit oracle of the world blahblah, hence xxx/yyy/zzz is dead. See more below.

Meanwhile, literally the founder/lead of the reasoning team: 

https://preview.redd.it/z9uwnummqeff1.png?width=652&format=png&auto=webp&s=c84727d328d059504adf64768b8badac45d20611

Reference: [https://www.youtube.com/watch?v=ebnX5Ur1hBk](https://www.youtube.com/watch?v=ebnX5Ur1hBk) good lecture! ",datascience,17,https://www.reddit.com/r/datascience/comments/1makoge/can_llms_reason_i_dont_know_depends_on_the/,r_1makoge,,,
r_1mabzuf,reddit,hendrix616,2025-07-27T03:17:20+00:00,"Hyperparameter and prompt tuning via agentic CLI tools like Claude Code
Has anyone used Claude Code as way to automate the improvement of their ML/AI solution?

In traditional ML, thereâ€™s the notion of hyperparameter tuning, whereby you search the source of all possible hyperparameter values to see which combination yields the best result on some outcome metric.

In LLM systems, the thing that gets tuned is the prompt and the outcome being evaluated is the output of some eval framework.

And some systems incorporate both ML and LLM

All of this iteration can be super time consuming and, in the case of the LLM prompt optimization, quite costly if you are constantly changing the prompt and having to rerun the eval framework.

The process can be manual or operated automatically by some heuristic.

It occurred to me the other day that it might be a great idea to get CC to do this iteration instead. If we arm it with the context and a CLI for running experiments with different configs), then it could do the following:
- â Run its own experiments via CLI
- Log the results
- Analyze the results against historical results
- Write down its thoughts
- Come up with ideas for future experiments
- Iterate!

Just wondering if anyone has pulled this off successfully in the past and would care to share :)
",datascience,1,https://www.reddit.com/r/datascience/comments/1mabzuf/hyperparameter_and_prompt_tuning_via_agentic_cli/,r_1mabzuf,,,
r_1m9e3vg,reddit,Suspicious_Coyote_54,2025-07-25T23:20:41+00:00,"Stuck not doing DS work as a DS
I have been working at a pharma for 5 years. In that time I got my MSDS and did some good work. Issue is, despite stellar yearly reviews I never ever get promoted. Each year I ask for a plan, for a goal to hit , for a reason why, but I always get met with â€œit just is not in the cardsâ€ kind of answer. 

I spent 6 months applying for other jobs but the issue is my work does not translate well. I built dashboards and an r shiny apps that had some business impact. Unfortunately despite the manager and director talking a big game about how we will use Ai and do a ton of DS and ML work, we never do and I often get stuck with the crappy work. 

When I interview I kill it during behaviorals and I often get far into the process but then I get asked about my lack of AB testing, or ML experience and I am quite honest. I simply have not been assigned those tasks and the company does not do them. Boom Iâ€™m out. Iâ€™m stuck and I donâ€™t know what to do or how to proceed. Doing projects seems like a decent move but Iâ€™ve heard people say that it does not matter. Iâ€™m also not great at coding interviews on the spot. Iâ€™ve studied a bunch but canâ€™t perform or often get mind wiped when asked a coding question. Anyone else been here? How did you get out? Any help would be appreciated. I really want to be a better DS and get out of pharma and into product or analytics.",datascience,141,https://www.reddit.com/r/datascience/comments/1m9e3vg/stuck_not_doing_ds_work_as_a_ds/,r_1m9e3vg,,,
r_1m8zjnq,reddit,tits_mcgee_92,2025-07-25T13:43:56+00:00,"Can a PhD be harmful for your career?
I have my MS degree in a Data Science adjacent field. I currently work in a Data Science / Software Engineering hybrid role, but I also work a second job as an adjunct professor in data science/analytics.

I find teaching unbelievably rewarding, but I could make more money being a cashier at Target. That's no exaggeration.

Part of me thinks teaching is my calling. My workplace will pay for my PhD, however, if I receive my PhD, and discover that I may not want to be a professor... would this result in a hard time finding data science jobs that aren't solely research based?

I try to think of the recruiter perspective, and if I applied to a job with a PhD they may think I will be asking for too much money or be too overqualified.

I'm just wondering if anyone has been in the same scenario, or had thoughts on this. Thank you for your time!",datascience,92,https://www.reddit.com/r/datascience/comments/1m8zjnq/can_a_phd_be_harmful_for_your_career/,r_1m8zjnq,,,
r_1m8jaeh,reddit,gpbayes,2025-07-24T23:10:33+00:00,"Highest ROI math youâ€™ve had?
Curious if there is a type of math / project that has saved or generated tons of money for your company. For example, I used Bayesian inference to figure out what insurance policy we should buy. I would consider this my highest ROI project. 

Machine Learning so far seems to promise a lot but delivers quite little.

Causal inference is starting to pick up the speed. ",datascience,243,https://www.reddit.com/r/datascience/comments/1m8jaeh/highest_roi_math_youve_had/,r_1m8jaeh,,,
r_1m8da2j,reddit,gyp_casino,2025-07-24T19:07:22+00:00,"Are your traditional Data Science projects still getting supported?
My managers are consumed by AI hype.  It was interesting initially when AI was chatbots and coding assistants, but once the idea of Agents entered their mind, it all went off a cliff.  We've had conversations that might as well have been conversations about magic.

I am proposing sensible projects with modest budgets that are getting no interest.",datascience,131,https://www.reddit.com/r/datascience/comments/1m8da2j/are_your_traditional_data_science_projects_still/,r_1m8da2j,,,
r_1m825ra,reddit,qtalen,2025-07-24T11:44:44+00:00,"After Many Failed Attempts, I Finally Built a Workflow for Generating Beautiful Ink Painting
I've always wanted to build a workflow for my blog that can quickly and affordably generate high-quality artistic covers. After dozens of days of effort, I finally succeeded. Here's what the output looks like:

https://preview.redd.it/lus6nn9i7tef1.png?width=1792&format=png&auto=webp&s=4bf86969f63512c2b223fe0382f85096f8805e87

Let me briefly share my solution:

First, I set a clear goalâ€”this workflow should understand the Eastern artistic concepts in users' drawing intentions, generate prompts suitable for the DALL-E-3 model, and ultimately produce high-quality ink painting illustrations.

https://preview.redd.it/rvttfx5m7tef1.png?width=1792&format=png&auto=webp&s=d0dd035d9138fe5427aa84de39d714082aa47adf

It should also allow users to refine the generated prompts through multi-turn conversations and adjust prompts based on the final generated images. This would significantly reduce costs in terms of tokens and time.

Initially, I tried using Dify to build the workflow, but I faced painful failures in user feedback and workflow loops.

I couldn't use coding frameworks like LangChain or CrewAI either because their abstraction levels were too high, making it hard to meet my customization needs.

Finally, I found LlamaIndex Workflow, which provides a low-abstraction, event-driven architecture for building workflows.

Using this framework along with Context Engineering, I successfully decoupled the workflow loops, making the entire workflow easy to understand, maintain, and adjust as needed.

https://preview.redd.it/vp55460p7tef1.jpg?width=1792&format=pjpg&auto=webp&s=97f9ba642c79400b369437e0bf0a52d954da104c

This flowchart reflects my overall workflow design:

https://preview.redd.it/wjkcel5s7tef1.png?width=658&format=png&auto=webp&s=e69448489b428524bb03be2aa5ab6218359a76c1

https://preview.redd.it/raqqmngt7tef1.png?width=974&format=png&auto=webp&s=9a6eb4e02f71542a17adfff22522bb972be8d327

Due to length constraints, I can't explain my implementation in detail here, but you can read [my full tutorial](https://www.dataleadsfuture.com/use-llamaindex-workflow-to-create-an-ink-painting-style-image-generation-workflow/) to learn about my complete solution.",datascience,0,https://www.reddit.com/r/datascience/comments/1m825ra/after_many_failed_attempts_i_finally_built_a/,r_1m825ra,,,
r_1m7z6un,reddit,Papa_Huggies,2025-07-24T08:49:48+00:00,"How do you know someone's got a data science background?
They know of only 3 species of iris flower.

PS: we need a flair for stupid jokes",datascience,335,https://www.reddit.com/r/datascience/comments/1m7z6un/how_do_you_know_someones_got_a_data_science/,r_1m7z6un,,,
r_1m7qbd9,reddit,transferrr334,2025-07-24T00:42:10+00:00,"SHAP values with class weights
Iâ€™m trying to understand which marketing channels are driving conversion. Approximately 2% of customers convert.

I utilize an XGBoost model and as features have:
1. For converters, the count of various touchpoints in the 8 weeks prior to conversion date.
2. For non-converters, the count of various touchpoints in the 8 weeks prior to a dummy date selected from the distribution of true conversion dates.

Because of how rare conversion is, I use class weighing in my XGBoost model. When I interpret SHAP values, I then get that every predictor is negative, which contextually and numerically is contradictory.

Does changing class weights impact the baseline probability, and mean that SHAP values reflect deviation from the over-weighed baseline probability and not true baseline? If so, what is the best way to correct for this if I still want to use weighing?",datascience,22,https://www.reddit.com/r/datascience/comments/1m7qbd9/shap_values_with_class_weights/,r_1m7qbd9,,,
r_1m7jbpk,reddit,techno_prgrssv,2025-07-23T19:52:36+00:00,"Is my side gig worth the effort?
Iâ€™ve been doing some freelance data analysis (regression, visuals, clustering) for a mid-sized company over the past couple months. The first project paid OK, and the work itself is pretty open-ended and intellectually engaging.

I initially expected access to their internal data, but it turned out I had to source and prep everything myself. The setup is very hands-offâ€”minimal guidance, so I end up doing a lot of research and exploration on my own.

Right now, Iâ€™ve had a lot of free time at my full-time job, so Iâ€™ve been able to fit this in without much sacrifice. But Iâ€™m anticipating a job change soon, and Iâ€™m starting to wonder if this work is worth the effort.

Realistically, I probably earn around (or slightly below) my hourly rate once you factor in how open-ended the work is. That wasnâ€™t what I expected going in.

I keep asking myself if my time would be better spent:

* Practicing Python, SQL, or ML skills for future interviews
* Studying things I actually enjoy (causal inference, classical stats)
* Working on personal projects I control
* Or just spending time on non-data hobbies

Curious to hear how others have thought about this tradeoff. Is it better to lean into these kinds of freelance projects for experience and cash, or to use that energy more intentionally elsewhere?",datascience,24,https://www.reddit.com/r/datascience/comments/1m7jbpk/is_my_side_gig_worth_the_effort/,r_1m7jbpk,,,
r_1m7ftt7,reddit,Technical-Love-8479,2025-07-23T17:41:05+00:00,"Google DeepMind release Mixture-of-Recursions
Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR",datascience,21,https://www.reddit.com/r/datascience/comments/1m7ftt7/google_deepmind_release_mixtureofrecursions/,r_1m7ftt7,,,
r_1m7bhew,reddit,Substantial_Tank_129,2025-07-23T14:57:01+00:00,"So are we just supposed to know how to get a promotion?
Iâ€™ve been working as a Data Scientist I at a Fortune 50 company for the past 3.5 years. Over the last two performance cycles, Iâ€™ve proactively asked for a promotion. The first time, my manager pointed out areas for improvementâ€”so I treated that as a development goal, worked on it, and presented clear results in the next cycle.

However, when I brought it up again, I was told that promotions arenâ€™t just based on performanceâ€”they also depend on factors like budget and others in the promotion queue. When I asked for a clear path forward, I was given no concrete guidance.

Now Iâ€™m left wondering: until the next cycle, what am I supposed to do? Is it usually on us to figure out how to get promoted, or does your company provide a defined path?",datascience,182,https://www.reddit.com/r/datascience/comments/1m7bhew/so_are_we_just_supposed_to_know_how_to_get_a/,r_1m7bhew,,,
r_1m70hqg,reddit,drewm8080,2025-07-23T05:03:33+00:00,"Probably and Stats interview questions?
Is there like a Neetcode equivalent to be able to do those (where you start understanding the different patterns in questions)? I want to get better at problem solving probability and stats questions. ",datascience,16,https://www.reddit.com/r/datascience/comments/1m70hqg/probably_and_stats_interview_questions/,r_1m70hqg,,,
r_1m70fk3,reddit,drewm8080,2025-07-23T05:00:19+00:00,"Where is Data Science interviews going?
As a data scientist myself, Iâ€™ve been working on a lot of RAG + LLM things and focused mostly on SWE related things. However, when I interview at jobs I notice every single data scientist job is completely different and it makes it hard to prepare for. Sometimes I get SQL questions, other times I could get ML, Leetcode, pandas data frames, probability and Statistics etc and it makes it a bit overwhelming to prepare for every single interview because they all seem very different. 

Has anyone been able to figure out like some sort of data science path to follow? I like how things like Neetcode are very structured to follow, but fail to find a data science equivalent.  ",datascience,195,https://www.reddit.com/r/datascience/comments/1m70fk3/where_is_data_science_interviews_going/,r_1m70fk3,,,
r_1m6jx39,reddit,Significant-Heron521,2025-07-22T17:12:11+00:00,"Stuck in defense contracting not doing Data Science but have a data science title
Title says it allâ€¦. Been here for 3 years, doing a lot of database/data architecting but not really any real data science work. My previous job was at a big 4 consulting but I was doing real data science for 2 years, but hated consulting part with a passion. Any advice?

Edit forgot to add: Iâ€™m also currently doing my masters in data science (part-time), and my company is flexible letting me do it. I see a lot more job opportunities elsewhere but feel like I should just stay until I finish next year.",datascience,109,https://www.reddit.com/r/datascience/comments/1m6jx39/stuck_in_defense_contracting_not_doing_data/,r_1m6jx39,,,
r_1m6h3f0,reddit,davernow,2025-07-22T15:26:27+00:00,"I wrote 2000 LLM test cases so you don't have to: LLM feature compatibility grid
This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.

# The problem: too many options

I've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kilnâ€™s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.

Here's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`<think>...</think>`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\_p, etc), and much more.

# How a focus on usability turned into over 2000 test cases

I wanted things to ""just work"" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.

To make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.

The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.

# Wait, doesn't that cost a lot of money and take forever?

**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.

Our blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).

# The Result

The end result is that it's much easier to rapidly evaluate AI models and methods. It includes

* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.
* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).

However, you're in control. You can always override any suggestion.

# Next Step: A Giant Ollama Server

I can run a decent sampling of our Ollama tests locally, but I lack the \~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!

# How to Find the Best Model for Your Task with Kiln

All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.

Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case â€” all backed by the extensive testing described above.

To get started, check out the tool or our guides:

* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)
* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)
* [Kiln Discord](https://getkiln.ai/discord)
* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)

I'm happy to answer questions if anyone wants to dive deeper on specific aspects!",datascience,10,https://www.reddit.com/r/datascience/comments/1m6h3f0/i_wrote_2000_llm_test_cases_so_you_dont_have_to/,r_1m6h3f0,,,
r_1m5xn63,reddit,recruitingfornow2025,2025-07-21T22:51:14+00:00,"Looking for MMM / Marketing Data Science specialist
Hi All,

Hope this is okay to post in this sub.

I am looking to hire for a role here in the DFW metro area and looking for a hard to find specialty of media mix marketing. Willing to train recent graduates with the right statistical and academic background. Currently hybrid 3 days a week in office. Compensation depends on skill set and experience, but can be between $95k-150k.

Please DM for more details and to send resumes.",datascience,21,https://www.reddit.com/r/datascience/comments/1m5xn63/looking_for_mmm_marketing_data_science_specialist/,r_1m5xn63,,,
r_1m5odiz,reddit,sideshowbob01,2025-07-21T16:58:29+00:00,"Data Science MSc 1 year Full time or 2 year Part time?
Hi, I'm funding my own MSc in Applied Data Science (intended for non computer/maths background)

I have a 6 year healthcare background (Nuclear medicine and CT).

I have taken python and SQL introduction courses to build a foundation.

My question is:

Would a 1 year MSc be  intensive learning for 1 year with dissertation and realistically result in a 18month study?

Does a 2 year MSc offer more room, resulting in a realistic 24 month timeline, with some room for job ""volunteering"" to get some experience?

I have completed a 3 year MSc before and can't comprehend how intense a 1 year MSc would be.

Thanks!",datascience,10,https://www.reddit.com/r/datascience/comments/1m5odiz/data_science_msc_1_year_full_time_or_2_year_part/,r_1m5odiz,,,
r_1m5nkwe,reddit,ElectrikMetriks,2025-07-21T16:29:06+00:00,"Wouldn't be the first time I've seen an entire org propped up by a 80MB Excel file
Oh yeah, I started a meme sub r/AnalyticsMemes if anyone wants every day to be meme Monday",datascience,454,https://www.reddit.com/r/datascience/comments/1m5nkwe/wouldnt_be_the_first_time_ive_seen_an_entire_org/,r_1m5nkwe,,,
r_1m5m5pn,reddit,Disastrous_Classic96,2025-07-21T15:35:52+00:00,"Maintenance of clustered data over time
With LLM-generated data, what are the best practices for handling downstream maintenance of clustered data?

E.g. for conversation transcripts, we extract things like the topic. As the extracted strings are non-deterministic, they will need clustering prior to being queried by dashboards.

What are people doing for their daily/hourly ETLs? Are you similarity-matching new data points to existing clusters, and regularly assessing cluster drift/bloat? How are you handling historic assignments when you determine clusters have drifted and need re-running?

Any guides/books to help appreciated!",datascience,14,https://www.reddit.com/r/datascience/comments/1m5m5pn/maintenance_of_clustered_data_over_time/,r_1m5m5pn,,,
r_1m5i5dj,reddit,Key-Network-9447,2025-07-21T12:55:16+00:00,"Data Snooping Resources
Simple question: Do you guys have any resources/papers about data snooping and how to limits its influence when making predictive models? I understand to maintain a testing dataset, but I am hoping someone knows any good high-level introductions to the topic that is not overly technical. Something like this, but about data snooping specifically, is what I am hoping to find: https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/ES13-00160.1",datascience,11,https://www.reddit.com/r/datascience/comments/1m5i5dj/data_snooping_resources/,r_1m5i5dj,,,
r_1m58yyn,reddit,AutoModerator,2025-07-21T04:01:31+00:00,"Weekly Entering & Transitioning - Thread 21 Jul, 2025 - 28 Jul, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,6,https://www.reddit.com/r/datascience/comments/1m58yyn/weekly_entering_transitioning_thread_21_jul_2025/,r_1m58yyn,,,
r_1m4s0vw,reddit,chrisgarzon19,2025-07-20T15:41:10+00:00,AI In Data Engineering,datascience,0,https://www.reddit.com/r/datascience/comments/1m4s0vw/ai_in_data_engineering/,r_1m4s0vw,,,
r_1m4d64h,reddit,unknown,2025-07-20T01:56:38+00:00,"Company Killed University Programs
Normally, I would have a post around this time hyping up fall recruiting and trying to provide pointers. The company I work for has decided to hire no additional entry level data scientists this year outside of intern return offers. They have also cut the number of intern positions in half for 2026. 

Part of the reasoning given by the CEO was that it is easy to hire early to mid level data scientist with project specific skills rather than training new hires. Money can also be saved by not having a university recruiting team and saving time interviewing by only going to target universities. 

Are any other data scientists seeing this change in their companies?",datascience,180,https://www.reddit.com/r/datascience/comments/1m4d64h/company_killed_university_programs/,r_1m4d64h,,,
r_1m49rai,reddit,Proof_Wrap_2150,2025-07-19T23:08:06+00:00,"How would you structure a project (data frame) to scrape and track listing changes over time?


Iâ€™m working on a project where I want to scrape data daily (e.g., real estate listings from a site like RentFaster or Zillow) and track how each listing changes over time. I want to be able to answer questions like:

When did a listing first appear?
How long did it stay up?
What changed (e.g., price, description, status)?
Whatâ€™s new today vs yesterday?

My rough mental model is:
1. Scrape todayâ€™s data into a CSV or database.
2. Compare with previous days to find new/removed/updated listings.
3. Over time, build a longitudinal dataset with per-listing history (kind of like slow-changing dimensions in data warehousing).

Iâ€™m curious how others would structure this kind of project:

How would you handle ID tracking if listings donâ€™t always have persistent IDs?
Would you use a single master table with change logs? Or snapshot tables per day?
How would you set up comparisons (diffing rows, hashing)?
Any Python or DB tools youâ€™d recommend for managing this type of historical tracking?

Iâ€™m open to best practices, war stories, or just seeing how others have solved this kind of problem. Thanks!
",datascience,6,https://www.reddit.com/r/datascience/comments/1m49rai/how_would_you_structure_a_project_data_frame_to/,r_1m49rai,,,
r_1m45pmq,reddit,Entire_Island8561,2025-07-19T20:07:54+00:00,"Generating random noise for media data
Hey everyone - I work on an ML team in the industry, and Iâ€™m currently building a predictive model to catch signals in live media data to sense when potential viral moments or crises are happening for brands. We have live media trackers at my company that capture all articles, including their sentiment (positive, negative, neutral). 

I currently am using ARIMA to predict out a certain amount of time steps, then using an LSTM to determine whether the volume of articles is anomalous given historical data trends. 

However, the nature of media is thereâ€™s so much randomness, so just taking the ARIMA projection is not enough. Because of that, Iâ€™m using Monte Carlo simulation to run an LSTM on a bunch of different forecasts that incorporate an added noise signal for each simulation. Then, that forces a probability of how likely it is that a crisis/viral moment will happen.

Iâ€™ve been experimenting with a bunch of methods on how to generate a random noise signal, and while Iâ€™m close to getting something, I still feel like Iâ€™m missing a method thatâ€™s concrete and backed by research/methodology. 

Does anyone know of approaches on how to effectively generate random noise signals for PR data? Or know of any articles on this topic?

Thank you!
",datascience,10,https://www.reddit.com/r/datascience/comments/1m45pmq/generating_random_noise_for_media_data/,r_1m45pmq,,,
r_1m3gy6m,reddit,ergodym,2025-07-18T23:12:31+00:00,"Are headhunters still a thing in 2025?
Curious what the current consensus is on headhunters these days. A few years ago they seemed to be everywhere, both big-name firms like Michael Page and boutique ones, but lately I donâ€™t hear much about them.

Do companies still rely on them or have internal recruiting teams and LinkedIn taken over completely?",datascience,57,https://www.reddit.com/r/datascience/comments/1m3gy6m/are_headhunters_still_a_thing_in_2025/,r_1m3gy6m,,,
r_1m10uku,reddit,OverratedDataScience,2025-07-16T02:19:08+00:00,"What question from recruiters do you absolutely hate to answer? How do you answer it elegantly?
Pretty much the title. Recruiters are not technically adepts in most of the cases. They go about asking some questions which is routine for them but hardly make sense in the real world. Not trying to be idealistic but, which questions do you hate the most? How would you answer them in a polite way?",datascience,63,https://www.reddit.com/r/datascience/comments/1m10uku/what_question_from_recruiters_do_you_absolutely/,r_1m10uku,,,
r_1m0wx9l,reddit,KyronAWF,2025-07-15T23:15:22+00:00,"Hoping for a review.
I want to clarify the reason I'm not using the main thread is because I'm posting an image, which can't be used for replies. I've been searching for a while without as much as a call back. I've been a data scientist for a while now and I'm not sure if it's the market or if there's something glaringly bad with my resume. Thanks for your help.",datascience,32,https://www.reddit.com/r/datascience/comments/1m0wx9l/hoping_for_a_review/,r_1m0wx9l,,,
r_1m0n56g,reddit,SharePlayful1851,2025-07-15T16:55:59+00:00,"""Harnessing the Universal Geometry of Embeddings"" - Breakthroughs and Security Implications",datascience,4,https://www.reddit.com/r/datascience/comments/1m0n56g/harnessing_the_universal_geometry_of_embeddings/,r_1m0n56g,,,
r_1m0dxsm,reddit,Dangerous_Media_2218,2025-07-15T10:12:44+00:00,"How does your organization label data?
I'm curious to hear how your organization labels data for use in modeling. We use a combination of SMEs who label data, simple rules that flag cases (it's rare that we can use these because they're generally no unambiguous), and an ML model to find more labels. I ask because my organization doesn't think it's valuable to have SMEs labeling data. In my domain area (fraud), we need SMEs to be labeling data because fraud evolves over time, and we need to identify the evoluation. Also, identifying fraud in the data isn't cut and dry. ",datascience,7,https://www.reddit.com/r/datascience/comments/1m0dxsm/how_does_your_organization_label_data/,r_1m0dxsm,,,
r_1m0b9f9,reddit,ChubbyFruit,2025-07-15T07:18:35+00:00,"Is it normal to be scared for the future finding a job
I am a rising senior at a large state school studying data science. I am currently working an internship as a software engineer for the summer. And I get my tickets done for the most part albeit with some help from ai. But deep down I feel a pit in my stomach that I wonâ€™t be able to end up employed after all of this.

I plan to go for a masters in applied statistics or data science after my bachelors. Thought I definitely donâ€™t have great math grades from my first few semesters of college. But after those semesters all my upper division math/stats/cs/data science courses have been Aâ€™s and Bâ€™s. And I feel like ik enough python, R, and SAS to work through and build models for most problems I run into, as well as tableau, sql and alteryx. But I canâ€™t shake the feeling that it wonâ€™t be enough.

Also that my rough math grades in my first few semesters will hold me back from getting into a masters programs. I have tried to supplement this by doing physics and applied math research. But Iâ€™m just not sure Iâ€™m doing enough and Iâ€™m scared for like after I finish my education.

Im just venting here but Iâ€™m hoping there r others in this sub who have been in similar positions and gotten employed. Or r currently in my same shoes I just need to hear from other people that itâ€™s not as hopeless as it feels.

I just want to get a job as a data analyst, scientist, or statistician working on interesting problems and have a decent career.",datascience,243,https://www.reddit.com/r/datascience/comments/1m0b9f9/is_it_normal_to_be_scared_for_the_future_finding/,r_1m0b9f9,,,
r_1m07l5m,reddit,m2rik,2025-07-15T03:45:37+00:00,Need mentorship on climbing the ladder or transitioning,datascience,0,https://www.reddit.com/r/datascience/comments/1m07l5m/need_mentorship_on_climbing_the_ladder_or/,r_1m07l5m,,,
r_1lzx0la,reddit,ElectrikMetriks,2025-07-14T20:07:07+00:00,I have people skills... I am good at dealing with people. Can't you understand that? What the hell is wrong with you people?,datascience,311,https://www.reddit.com/r/datascience/comments/1lzx0la/i_have_people_skills_i_am_good_at_dealing_with/,r_1lzx0la,,,
r_1lzo89g,reddit,Kati1998,2025-07-14T14:42:27+00:00,Do employers see volunteer experience as â€œreal world experienceâ€?,datascience,13,https://www.reddit.com/r/datascience/comments/1lzo89g/do_employers_see_volunteer_experience_as_real/,r_1lzo89g,,,
r_1lzlrlu,reddit,rsesrsfh,2025-07-14T13:00:27+00:00,"Fine-tuning for tabular foundation models (TabPFN)
Hi everyone - wanted to share that you can now fine-tune tabular foundation models as well, specifically TabPFN! With the latest 2.1 package release, you can now build your own fine-tuned models.

A community member put together a practical walkthrough!

How to Fine-Tune TabPFN on Your Data: [https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0](https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0)

The tutorial covers:

* Running TabPFN in batched mode
* Handling preprocessing and inference-time transformations
* Fine-tuning the transformer backbone on your dataset

If you're working with highly domain specific data and looking to boost performance, this is a great place to start.

You can also check out the example files directly at these links:

ðŸ§ª [Fine-tune classifier](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_classifier.py)

ðŸ“ˆ [Fine-tune regressor](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_regressor.py)

Would love to hear how it goes if you try it!

Thereâ€™s also a community Discord where folks are sharing experiments and helping each other out - worth checking out if you're playing around with TabPFN [https://discord.com/invite/VJRuU3bSxt](https://discord.com/invite/VJRuU3bSxt)",datascience,20,https://www.reddit.com/r/datascience/comments/1lzlrlu/finetuning_for_tabular_foundation_models_tabpfn/,r_1lzlrlu,,,
r_1lzkso0,reddit,multicm,2025-07-14T12:14:56+00:00,"Site Selection Model - Subjective Feature
I have been working on a site selection model, and the one I created is performing quite well in out of sample testing. I was also able to reduce the model down to just 5 features. But, one of those features is a ""Visibility Score"" (how visible the building is from the road). I had 3 people independently score all of our existing sites and I averaged their scores, and this has proven to work well so far. But if we actually put the model into production, I am concerned about standardized those scores. The model predictiction can vary by 18% just from a visibility score change from 3.5 to 4.0 so the model is heavily dependent on that subjective score.

Any tips?",datascience,7,https://www.reddit.com/r/datascience/comments/1lzkso0/site_selection_model_subjective_feature/,r_1lzkso0,,,
r_1lzgfhq,reddit,JayBong2k,2025-07-14T07:50:27+00:00,"I suck at these interviews.
I'm looking for a job again and while I have had quite a bit of hands-on practical work that has a lot of business impacts - revenue generation, cost reductions, increasing productivity etc 

But I keep failing at ""Tell the assumptions of Linear regression"" or ""what is the formula for Sensitivity"".

While I'm aware of these concepts, and these things are tested out in model development phase, I never thought I had to mug these stuff up. 

The interviews are so random - one could be hands on coding (love these), some would be a mix of theory, maths etc, and some might as well be in Greek and Latin..

Please give some advice to  4 YOE DS should be doing. The ""syllabus"" is entirely too vast.ðŸ¥²

Edit:
Wow, ok i didn't expect this to blow up. I did read through all the comments. This has been definitely enlightening for me.

Yes, i should have prepared better, brushed up on the fundamentals. Guess I'll have to go the notes/flashcards way. 
",datascience,529,https://www.reddit.com/r/datascience/comments/1lzgfhq/i_suck_at_these_interviews/,r_1lzgfhq,,,
r_1lzcn4y,reddit,AutoModerator,2025-07-14T04:01:26+00:00,"Weekly Entering & Transitioning - Thread 14 Jul, 2025 - 21 Jul, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,8,https://www.reddit.com/r/datascience/comments/1lzcn4y/weekly_entering_transitioning_thread_14_jul_2025/,r_1lzcn4y,,,
r_1lywh35,reddit,harsh82000,2025-07-13T16:05:15+00:00,"How much DSA for FAANG+ ?
Hello all, I am going to be graduating in 6 months and have been practicing Leetcode as I believe this to be my weakest point. I have solved 250 LC with 130 Easy and 120 Hard, covering concepts like arrays, hashing, binary trees, SQL, linked list, two pointers, stack, sliding windows majorly. Could anyone guide me on how I can maximise the time I have on hand to prepare better for technical interviews? I have good internship and research experience so I am not that worried about future rounds, but timed coding questions have always been brutal for me. Any advice is appreciated.",datascience,67,https://www.reddit.com/r/datascience/comments/1lywh35/how_much_dsa_for_faang/,r_1lywh35,,,
r_1lyo2ac,reddit,nkafr,2025-07-13T08:45:48+00:00,"Toto: A Foundation Time-Series Model Optimized for Observability Data
Datadog open-sourcedÂ *Toto* (Time Series Optimized Transformer for Observability), a model purpose-built for observability data.

Toto is currently the most extensively pretrained time-series foundation model: The pretraining corpus contains 2.36 trillion tokens, withÂ \~70%Â coming from Datadogâ€™s private telemetry dataset.

Also, Toto currently ranks 2nd in the GIFT-Eval Benchmark.

You can find an analysis of the modelÂ [here](https://aihorizonforecast.substack.com/p/toto-a-foundation-time-series-model).",datascience,55,https://www.reddit.com/r/datascience/comments/1lyo2ac/toto_a_foundation_timeseries_model_optimized_for/,r_1lyo2ac,,,
r_1lyciw1,reddit,juggerjaxen,2025-07-12T22:09:48+00:00,"The right questions to find clusters (tangles)
**Hey everyone,**

Iâ€™m currently working on my bachelorâ€™s thesis and Iâ€™m hitting a creative block on a central part â€“ maybe you have some ideas or impulses for me.

My dataset consists of 100,000 cleaned job postings from Kaggle (title + description). The goal of my thesis is to use a method called **Tangles** (probably no one knows it, itâ€™s a rather specific approach from my studies) to find interesting clusters in this data â€“ similar to embedding-based clustering methods, but with the key difference that it requires **interpretable, binary decisions**. Sounds theoretical, but itâ€™s actually pretty cool:

You ask the dataset **yes/no questions** (e.g., *â€œDoes the job require a lot of travel?â€*), and based on the answer patterns, a kind of profile emerges â€“ and from these profiles, groups that belong together can be formed.

The goal is to group jobs that donâ€™t obviously belong together at first glance, but do share certain underlying similarities (e.g., requirements, tasks) that cause them to respond similarly to the questions.

**One example:**

Questions like:

* Does the job require a lot of travel?
* Do you need a driverâ€™s license?
* Do you have to be physically fit?



=> could group *Sales Managers* and *Truck Drivers* together â€“ even though those jobs seem very different at first. These kinds of connections are what I find exciting.

What Iâ€™m **not** looking for are questions like:

* Is this a data science job?
* Do you need to know how to code?
* Is it IT-related?

To me, those are more like categories or classifications that make the clustering too obvious â€“ they just confirm what you already know. Iâ€™m more interested in **surprising, layered similarities**.

So hereâ€™s my question for you:

Do you have any interesting **yes/no questions** from your daily work or knowledge that could be applied to any kind of job posting â€“ and that might result in **interesting, possibly unexpected groupings**?

Whether you work in trades, healthcare, IT, management, or research â€“ **every perspective helps!**

In the end, I need at least 40 such questions (the more, the better), but right now Iâ€™m really struggling to come up with good ones. Even GPT & co. havenâ€™t been much help â€“ they usually just spit out generic stuff.

Even **one** good question from you would be incredibly helpful. ðŸ™ OR advice on how to find these questions/if my idea is right or not, would help.

Thanks in advance for thinking along!",datascience,2,https://www.reddit.com/r/datascience/comments/1lyciw1/the_right_questions_to_find_clusters_tangles/,r_1lyciw1,,,
r_1ly409f,reddit,Proof_Wrap_2150,2025-07-12T16:03:01+00:00,"How have you supported DS fundamentals, creative thinking or curiosity in your baby/toddler using what you know as a technical or analytical thinker?
Anything you built, played, repeated, or tracked?",datascience,0,https://www.reddit.com/r/datascience/comments/1ly409f/how_have_you_supported_ds_fundamentals_creative/,r_1ly409f,,,
r_1ly06nw,reddit,Grapphie,2025-07-12T13:14:14+00:00,"How do you efficiently traverse hundreds of features in the dataset?
Currently, working on a fintech classification algorithm, with close to a thousand features which is very tiresome. I'm not a domain expert, so creating sensible hypotesis is difficult. How do you tackle EDA and forming reasonable hypotesis in these cases? Even with proper documentation it's not a trivial task to think of all interesting relationships that might be worth looking at. What I've been looking so far to make is:

1) Baseline models and feature relevance assessment with in ensemble tree and via SHAP values  
2) Traversing features manually and check relationships that ""make sense"" for me",datascience,94,https://www.reddit.com/r/datascience/comments/1ly06nw/how_do_you_efficiently_traverse_hundreds_of/,r_1ly06nw,,,
r_1lxb0bn,reddit,Substantial_Tank_129,2025-07-11T16:13:24+00:00,"Doordash phone screen reject despite good in-interview feedback. What are they looking for?
Had a phone screen with DoorDash recently for a DS Analytics role. First round was a product case study â€” the interviewer was super nice, gave good feedback throughout, and even ended with â€œGreat job on this round,â€ so I felt pretty good about it.

Second round was SQL with 4 questions. Honestly, the first one threw me off â€” it was more convoluted than I expected, so I struggled a bit but managed to get through it. The 2nd and 3rd were much easier and I finished those without issues. The 4th was a bonus question where I had to explain a SQL query â€” took me a moment, but I eventually explained what it was doing.

Got a rejection email the next day. I thought it went decently overall, so Iâ€™m a bit confused. Any thoughts on what mightâ€™ve gone wrong or what I could do better next time",datascience,112,https://www.reddit.com/r/datascience/comments/1lxb0bn/doordash_phone_screen_reject_despite_good/,r_1lxb0bn,,,
r_1lvsh3e,reddit,idontknowotimdoing,2025-07-09T19:51:59+00:00,"Data science metaphors?
Hello everyone :)

Serious question: Does anyone have any data science related metaphors/similes/analogies that you use regularly at work? 

(I want to sound smart.)

Thanks!",datascience,118,https://www.reddit.com/r/datascience/comments/1lvsh3e/data_science_metaphors/,r_1lvsh3e,,,
r_1lvoz88,reddit,NervousVictory1792,2025-07-09T17:35:32+00:00,"Quarterly to Monthly Data Conversion
As the title suggests. I am trying to convert average wage data, from quarterly to monthly. I need to perform forecasting on that. What is the best ways to do that?? . I donâ€™t want to go for a naive method and just divide by 3 as I will loose any trends or patterns. I have come across something called disproportionate aggregation but having a tough time grasping it.",datascience,11,https://www.reddit.com/r/datascience/comments/1lvoz88/quarterly_to_monthly_data_conversion/,r_1lvoz88,,,
r_1lvnda9,reddit,Technical-Love-8479,2025-07-09T16:33:29+00:00,"Reachy-Mini: Huggingface launched open-sourced robot that supports vision, text and speech
Huggingface just released an open-sourced robot named Reachy-Mini, which supports all Huggingface open-sourced AI models, be it text or speech or vision and is quite cheap. Check more details here : https://youtu.be/i6uLnSeuFMo?si=Wb6TJNjM0dinkyy5",datascience,12,https://www.reddit.com/r/datascience/comments/1lvnda9/reachymini_huggingface_launched_opensourced_robot/,r_1lvnda9,,,
r_1lvn71u,reddit,Professional_Ball_58,2025-07-09T16:26:33+00:00,"How do you guys measure AI impact
Im sure a lot of companies are rolling out AI products to help their business. 

Im curious how do people typically try to measure these AI products impacts. I guess it really depends on the domain but can we isolate and see if any uplift in the KPI is attributable to AI?

Is AB testing always to gold standard? Use Quasi experimental methods? 

",datascience,28,https://www.reddit.com/r/datascience/comments/1lvn71u/how_do_you_guys_measure_ai_impact/,r_1lvn71u,,,
r_1lvmphl,reddit,Proof_Wrap_2150,2025-07-09T16:07:05+00:00,"All of my data comes from spreadsheets. As I receive more over time, whatâ€™s the best way to manage and access multiple files efficiently? Ideally in a way that scales and still lets me work interactively with the data?
Iâ€™m working on a project where all incoming data is provided via spreadsheets (Excel/CSV). The number of files is growing, and I need to manage them in a structured way that allows for:

1. Easy access to different uploads over time
2. Avoiding duplication or version confusion
3. Interactive analysis (e.g., via Jupyter notebooks or a lightweight dashboard)

Iâ€™m currently loading files manually, but I want a better system. Whether that means a file management structure, metadata tagging, or loading/parsing automation. Eventually Iâ€™d like to scale this up to support analysis across many uploads or clients.

What are good patterns, tools, or Python-based workflows to support this?",datascience,66,https://www.reddit.com/r/datascience/comments/1lvmphl/all_of_my_data_comes_from_spreadsheets_as_i/,r_1lvmphl,,,
r_1lvl0wp,reddit,SummerElectrical3642,2025-07-09T15:01:23+00:00,"Open source or not?
Hi all,  
I am building an AI agent, similar to Github copilot / Cursor but very specialized on data science / ML. It is integrated in VSCode as an extension.  
Here is a few examples of use cases:  
\- Combine different data sources, clean and preprocess for ML pipeline.  
\- Refactor R&D notebooks into ready for production project: Docker, package, tests, documentation.

We are approaching an MVP in the next few weeks and I am hesitating between 2 business models:  
1- Closed source, similar to cursor, with fixed price subscription with limit by request.  
2- Open source, pay per token. User can plug their own API or use our backend which offers all frontier models. Charge a topup % on top of token consumption (similar to Cline).

The question is also whether the data science community would contribute to a vscode extension in React, Typescript.

What do you think make senses as a data scientist / ML engineer?",datascience,0,https://www.reddit.com/r/datascience/comments/1lvl0wp/open_source_or_not/,r_1lvl0wp,,,
r_1lux7bt,reddit,tits_mcgee_92,2025-07-08T19:03:19+00:00,"Saved $100k per year by explaining how AI/LLM work.
I work in a data science field, and I bring this up because I think it's data science related.

We have an internal website that is very bare bones. It's made to be simplistic, because it's the reference document for our end-users (1000 of them) use.

Executives heard about a software that would be completely AI driven, build detailed statistical insights, and change the world as they know it.

I had a demo with the company and they explained its RAG capabilities, but mentioned it doesn't really ""learn"" like the assumption AI does. Our repo is so small and not at all needed for AI. We have used a fuzzy search that has worked for the past three years. Additionally, I have already built out dashboards that retrieve all the information executives have asked for via API (who's viewing pages, what are they searching, etc.)

I showed the c-suite executives our current dashboards in Tableau, and how the actual search works. I also explained what RAG is, and how AI/LLMs work at a high level. I explained to them that AI is a fantastic tool, but I'm not sure if we should be spending 100k a year on it. They also asked if I have built any predictive models. I don't think they quite understood what that was as well, because we don't have the amount of data or need to predict anything.

Needless to say, they decided it was best not to move forward ""for now"". I am shocked, but also not, that executives want to change the structure of how my team and end-users digest information just because they heard ""AI is awesome!"" They had zero idea how anything works in our shop.

Oh yeah, our company has already laid of 250 people this year due to ""financial turbulence"", and now they're wanting to spend 100k on this?!

It just goes to show you how deep the AI train runs. Did I handle this correctly and can I put this on my resume? LOL",datascience,1173,https://www.reddit.com/r/datascience/comments/1lux7bt/saved_100k_per_year_by_explaining_how_aillm_work/,r_1lux7bt,,,
r_1lum3ih,reddit,FinalRide7181,2025-07-08T11:27:33+00:00,"Path to product management
Iâ€™m a student interested in working as a product manager in tech. 

I know itâ€™s tough to land a first role directly in PM, so Iâ€™m considering alternative paths that could lead there.

My question is: how common is the transition from data scientist/product data scientist to product manager? Is it a viable path?

Also would it make more sense to go down the software engineering route instead (even though Iâ€™m not particularly passionate about it) if it makes the transition to PM easier?",datascience,7,https://www.reddit.com/r/datascience/comments/1lum3ih/path_to_product_management/,r_1lum3ih,,,
r_1lu7gqq,reddit,EducationalUse9983,2025-07-07T22:03:11+00:00,"How to deal with time series unbalanced situations?
**Hi everyone,**

Iâ€™m working on a challenge to **predict the probability of a product becoming unavailable the next day**.

The dataset contains one row per product per day, with a binary target (`failure` or not) and 10 additional features. There are over 1 million rows without failure, and only 100 with failure â€” so it's a highly imbalanced dataset.

Here are some key points Iâ€™m considering:

1. **The target should reflect the next day**, not the current one. For example, if product X has data from day 1 to day 10, each row should indicate whether a failure will happen on the following day. Day 10 is used only to label day 9 and is not used as input for prediction.
2. **The features are on different scales**, so Iâ€™ll need to apply normalization or standardization depending on the model I choose (e.g., for Logistic Regression or KNN).
3. **There are no missing values**, so I wonâ€™t need to worry about imputation.
4. **To avoid data leakage**, Iâ€™ll split the data by product, making sure that each product's full time series appears entirely in either the training or test set â€” never both. For example, if product X has data from day 1 to day 9, those rows must all go to either train **or** test.
5. Since the output should be a **probability**, Iâ€™m planning to use models like Logistic Regression, Random Forest, XGBoost, Naive Bayes, or KNN.
6. Due to the strong class imbalance, my **main evaluation metric will be ROC AUC**, since it handles imbalanced datasets well.
7. Would it make sense to include calendar-based features, like the day of the week, weekend indicators, or holidays?
8. How useful would it be to add rolling window statistics (e.g., 3-day averages or standard deviations) to capture recent trends in the attributes?
9. Any best practices for flagging anomalies, such as sudden spikes in certain attributes or values above a specific percentile (like the 90th)?

**My questions:**  
Does this approach make sense?  
Iâ€™m not entirely confident about some of these steps, so Iâ€™d really appreciate feedback from more experienced data scientists!",datascience,57,https://www.reddit.com/r/datascience/comments/1lu7gqq/how_to_deal_with_time_series_unbalanced_situations/,r_1lu7gqq,,,
r_1lu7fxv,reddit,GussieWussie,2025-07-07T22:02:16+00:00,"Python package for pickup/advanced booking models for forecasting?
Recently discovered pickup models that use reservation data to generate forecasts (see [https://www.scitepress.org/papers/2016/56319/56319.pdf](https://www.scitepress.org/papers/2016/56319/56319.pdf) ) Seems used often in the hotel and airline industry. Is there a python package for this? Maybe it goes by a different name but I'm not seeing anything",datascience,9,https://www.reddit.com/r/datascience/comments/1lu7fxv/python_package_for_pickupadvanced_booking_models/,r_1lu7fxv,,,
r_1lu1cve,reddit,ElectrikMetriks,2025-07-07T18:06:15+00:00,"I don't drink, but I'm still tired because my dogs hate fireworks.  Did everyone in the US take a long weekend at least?",datascience,0,https://www.reddit.com/r/datascience/comments/1lu1cve/i_dont_drink_but_im_still_tired_because_my_dogs/,r_1lu1cve,,,
r_1ltkjwx,reddit,AutoModerator,2025-07-07T04:02:03+00:00,"Weekly Entering & Transitioning - Thread 07 Jul, 2025 - 14 Jul, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,17,https://www.reddit.com/r/datascience/comments/1ltkjwx/weekly_entering_transitioning_thread_07_jul_2025/,r_1ltkjwx,,,
r_1lt464v,reddit,Technical-Love-8479,2025-07-06T15:37:22+00:00,"With Generative AI looking so ominous, would there be any further research in any other domains like Computer Vision or NLP or Graph Analytics ever?
So as the title suggest, last few years have been just Generative AI all over the place. Every new research is somehow focussed towards it. So does this mean other fields stands still ? Or eventually everything will merge into GenAI somehow? What's your thoughts ",datascience,0,https://www.reddit.com/r/datascience/comments/1lt464v/with_generative_ai_looking_so_ominous_would_there/,r_1lt464v,,,
r_1lss1c5,reddit,fenrirbatdorf,2025-07-06T03:56:29+00:00,"Reliable DS Adjacent Fields Hiring for Bachelor's Degree?
Hello all. To try and condense a lot of context for this question, I am an adult who went back to school to complete my bachelor's, in order to support myself and my partner on one income. Admittedly, I did this because I heard how good data science was as a field, but it seems I jumped in at the wrong time. 

Consequently, now that I am one year out from graduating with my bachelor's, I am starting to think about what fields would be best to apply in, beyond simply ""data science"" and ""data analysis."" Any leads on fields that are reliably hiring that are similar to data science but not exact? I am really open to anything that would pay the bills for two people.",datascience,34,https://www.reddit.com/r/datascience/comments/1lss1c5/reliable_ds_adjacent_fields_hiring_for_bachelors/,r_1lss1c5,,,
r_1lsk0sd,reddit,Proof_Wrap_2150,2025-07-05T21:01:41+00:00,"Whatâ€™s the best way to automate pulling content performance metrics from LinkedIn beyond just downloading spreadsheets?
Iâ€™ve been stuck manually exporting post data from the LinkedIn analytics dashboard for months. Automating via API sounds ideal, but this is uncharted territory!",datascience,0,https://www.reddit.com/r/datascience/comments/1lsk0sd/whats_the_best_way_to_automate_pulling_content/,r_1lsk0sd,,,
r_1lsjfj4,reddit,mlbatman,2025-07-05T20:35:14+00:00,"Long-timers at companies â€” whatâ€™s your secret?
Hi everyone,

Iâ€™ve been a job hopper throughout my careerâ€”never stayed at one place for more than 1-2 years, usually for various reasons.

Now, Iâ€™m entering a phase where I want to get more settled. Iâ€™m about to start a new job and would love to hear from those who have successfully stayed long-term at a job.

Whatâ€™s the secret sauce besides just hard work and taking ownership? Lay your knowledge on meâ€”your hacks, tips, rituals.

Thanks in advance.",datascience,140,https://www.reddit.com/r/datascience/comments/1lsjfj4/longtimers_at_companies_whats_your_secret/,r_1lsjfj4,,,
r_1lsfyeo,reddit,Daniel-Warfield,2025-07-05T18:01:17+00:00,"A Brief Guide to UV
Python has been largely devoid of easy to use environment and package management tooling, with various developers employing their own cocktail of `pip`, `virtualenv`, `poetry`, and `conda` to get the job done. However, it looks like `uv` is rapidly emerging to be a standard in the industry, and I'm super excited about it.

In a nutshell `uv` is like `npm` for Python. It's also written in rust so it's crazy fast.

As new ML approaches and frameworks have emerged around the greater ML space (A2A, MCP, etc) the cumbersome nature of Python environment management has transcended from an annoyance to a major hurdle. This seems to be the major reason `uv` has seen such meteoric adoption, especially in the ML/AI community.

[star history of uv vs poetry vs pip. Of course, github star history isn't necessarily emblematic of adoption.  \<ore importantly, uv is being used all over the shop in high-profile, cutting-edge repos that are governing the way modern software is evolving. Anthropicâ€™s Python repo for MCP uses UV, Googleâ€™s Python repo for A2A uses UV, Open-WebUI seems to use UV, and thatâ€™s just to name a few.](https://preview.redd.it/b6myln1ve3bf1.png?width=1050&format=png&auto=webp&s=89d275ad7b050bbe8a365dd731e37910182592c4)

I wrote [an article](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained) that goes over `uv` in greater depth, and includes some examples of `uv` in action, but I figured a brief pass would make a decent Reddit post.

**Why UV**  
`uv` allows you to manage dependencies and environments with a single tool, allowing you to create isolated python environments for different projects. While there are a few existing tools in Python to do this, there's one critical feature which makes it groundbreaking: *it's easy to use*.

**Installing UV**  
`uv` can be installed via `curl`

    curl -LsSf https://astral.sh/uv/install.sh | sh

or via `pip`

    pipx install uv

the docs have a [more in-depth guide to install](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained#:~:text=Check%20out%20the-,uv%20docs,-for%20more%20information).

**Initializing a Project with UV**  
Once you have `uv` installed, you can run

    uv init

This initializes a uv project within your directory. You can think of this as an isolated python environment that's tied to your project.

**Adding Dependencies to your Project**  
You can add dependencies to your project with

    uv add <dependency name>

You can download all the dependencies you might install via `pip`:

    uv add pandas
    uv add scipy
    uv add numpy sklearn matplotlib

And you can install from various other sources, including github repos, local wheel files, etc.

**Running Within an Environment**  
if you have a python script within your environment, you can run it with

    uv run <file name>

this will run the file with the dependencies and python version specified for this particular environment.  This makes it super easy and convenient to bounce around between different projects. Also, if you clone a `uv` managed project, all dependencies will be installed and synchronized before the file is run.

**My Thoughts**  
I didn't realize I've been waiting for this for a long time. I always found off the cuff quick implementation of Python locally to be a pain, and I think I've been using ephemeral environments like Colab as a crutch to get around this issue. I find local development of Python projects to be significantly more enjoyable with `uv` , and thus I'll likely be adopting it as my go to approach when developing in Python locally.",datascience,98,https://www.reddit.com/r/datascience/comments/1lsfyeo/a_brief_guide_to_uv/,r_1lsfyeo,,,
r_1lrojc3,reddit,empirical-sadboy,2025-07-04T17:33:47+00:00,"How easy is it to be pigeonholed in DS?
Although in my PhD I used experiments and traditional statistics, my first DS role is entirely focused on NLP. There are no opportunities to use casual inference, time series, or other traditional statistical methods. 

How much will this hurt my ability to apply to roles focused on these kinds of analyses? Basically, I'm wondering if my current role's focus on NLP is going to make it hard for me to get non-NLP data science positions when I'm ready to leave. 

Is it common for data scientists to get stuck in a niche?",datascience,37,https://www.reddit.com/r/datascience/comments/1lrojc3/how_easy_is_it_to_be_pigeonholed_in_ds/,r_1lrojc3,,,
r_1lrluwg,reddit,kmeansneuralnetwork,2025-07-04T15:41:49+00:00,"Any good resources for fraud detection and credit risk modelling?
Hello, I am very much interested in using ML/DS in banking domain like fraud detection, loan prediction, credit risk, etc..


I have read this book about fraud detection.
https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html

Understood everything and it was fun. Now, I am looking for similar resources to work on.

Thank you.",datascience,67,https://www.reddit.com/r/datascience/comments/1lrluwg/any_good_resources_for_fraud_detection_and_credit/,r_1lrluwg,,,
r_1lrghkc,reddit,Unusual-Map6326,2025-07-04T11:31:24+00:00,"Causes of the 'Bad Market'
I'm just opening the floor to speculation / source dumping but everyone's talking about a suddenly very bad market for DS and DS related fields

  
I live in the north of the UK and it feels impossible to get a job out here. It sounds like its similar in the US. Is this a DS specific issue or are we just feeling what everyone else is feeling? I'm only now just emerging from a post-grad degree and I thought that hearing all these news stories about people illegally gathering and storing data that it was an indicator in how data driven so many decisions are now... which in my mind means that you'd need more DS/ ML engineers to wade through the quagmire and build solutions

  
obviously I'm wrong but why?",datascience,102,https://www.reddit.com/r/datascience/comments/1lrghkc/causes_of_the_bad_market/,r_1lrghkc,,,
r_1lr8l54,reddit,Particular_Reality12,2025-07-04T03:22:35+00:00,"I just got LinkedIn Learning, what courses do you recommend I take on Data Science?
Iâ€™m kinda new to it but dont shy away from giving me the more advanced courses as Iâ€™ll be able to learn more

Im going to charge my phone",datascience,0,https://www.reddit.com/r/datascience/comments/1lr8l54/i_just_got_linkedin_learning_what_courses_do_you/,r_1lr8l54,,,
r_1lqno9m,reddit,Illustrious-Pound266,2025-07-03T11:56:18+00:00,"People who have been in the field before 2020: how do you keep up with the constantly new and changing technologies in ML/AI?
As someone who genuinely enjoys learning new tech, sometimes I feel it's too much to constantly keep up. I feel like it was only barely a year ago when I first learned RAG and then agents soon after, and now MCP servers.

I have a life outside tech and work and I feel that I'm getting lazier and burnt out in having to keep up. Not to mention only AI-specific tech, but even with adjacent tech like MLFlow, Kubernetes, etc, there seems to be so much that I feel I should be knowing. 

The reason why I asked *before 2020* is because I don't recall AI moving at this fast pace before then. Really feels like only after ChatGPT was released to the masses did the pace really pickup that now AI engineering actually feels quite different to the more classic ML engineering I was doing.",datascience,227,https://www.reddit.com/r/datascience/comments/1lqno9m/people_who_have_been_in_the_field_before_2020_how/,r_1lqno9m,,,
r_1lqn6pu,reddit,qtalen,2025-07-03T11:29:11+00:00,"How I Use MLflow 3.1 to Bring Observability to Multi-Agent AI Applications
Hi everyone,

If you've been diving into the world of multi-agent AI applications, you've probably noticed a recurring issue: most tutorials and code examples out there feel like toys. Theyâ€™re fun to play with, but when it comes to building something reliable and production-ready, they fall short. You run the code, and half the time, the results are unpredictable.

This was exactly the challenge I faced when I started working on enterprise-grade AI applications. I wanted my applications to not only work but also be robust, explainable, and observable. By ""observable,"" I mean being able to monitor whatâ€™s happening at every step â€” the inputs, outputs, errors, and even the thought process of the AI. And ""explainable"" means being able to answer questions like: *Why did the model give this result? What went wrong when it didnâ€™t?*

But hereâ€™s the catch: as multi-agent frameworks have become more abstract and convenient to use, theyâ€™ve also made it harder to see under the hood. Often, you canâ€™t even tell what prompt was finally sent to the large language model (LLM), let alone why the result wasnâ€™t what you expected.

So, I started looking for tools that could help me monitor and evaluate my AI agents more effectively. Thatâ€™s when I turned to MLflow. If youâ€™ve worked in machine learning before, you might know MLflow as a model tracking and experimentation tool. But with its latest 3.x release, MLflow has added specialized support for GenAI projects. And trust me, itâ€™s a game-changer.

[MLflow's tracking records. ](https://preview.redd.it/k3i2hbh18naf1.png?width=948&format=png&auto=webp&s=91cd9c33943b0d612fda2e8874b4979c60ce0618)

# Why Observability Matters

Before diving into the details, letâ€™s talk about why this is important. In any AI application, but especially in multi-agent setups, you need three key capabilities:

1. **Observability:**Â Can you monitor the application in real time? Are there logs or visualizations to see whatâ€™s happening at each step?
2. **Explainability:**Â If something goes wrong, can you figure out why? Can the algorithm explain its decisions?
3. **Traceability:**Â If results deviate from expectations, can you reproduce the issue and pinpoint its cause?

[Three key metrics for evaluating the stability of enterprise GenAI applications. Image by Author](https://preview.redd.it/azgs0y3j7naf1.png?width=820&format=png&auto=webp&s=9fbf70b52379d8e8e869eab3bb3acc9b9450942f)

Without these, youâ€™re flying blind. And when youâ€™re building enterprise-grade systems where reliability is critical, flying blind isnâ€™t an option.

# How MLflow Helps

MLflow is best known for its model tracking capabilities, but its GenAI features are what really caught my attention. It lets you track everything â€” from the prompts you send to the LLM to the outputs it generates, even in streaming scenarios where the model responds token by token.

[The Events tab in MLflow interface records every SSE message.](https://preview.redd.it/7mteb23c8naf1.png?width=858&format=png&auto=webp&s=1d0de24b21a58abeca9db0bbc7feeddd106c7fbc)

[MLflow's Autolog can also stitch together streaming messages in the Chat interface.](https://preview.redd.it/h77q9y3h8naf1.png?width=859&format=png&auto=webp&s=b3ab949f1cab02c2d71da952d6b6fc60bb2bac91)

The setup is straightforward. You can annotate your code, use MLflowâ€™s ""autolog"" feature for automatic tracking, or leverage its context managers for more granular control. For example:

* Want to know exactly what prompt was sent to the model? Tracked.
* Want to log the inputs and outputs of every function your agent calls? Done.
* Want to monitor errors or unusual behavior? MLflow makes it easy to capture that too.

[You can view code execution error messages in the Events interface.](https://preview.redd.it/svx0fnpm8naf1.png?width=854&format=png&auto=webp&s=d7edbc819dbbccad8a0e881efce310c4b9553a02)

And the best part? MLflowâ€™s UI makes all this data accessible in a clean, organized way. You can filter, search, and drill down into specific runs or spans (i.e., individual events in your application).

# A Real-World Example

I have a project involving building a workflow using Autogen, a popular multi-agent framework. The system included three agents:

1. AÂ **generator**Â that creates ideas based on user input.
2. AÂ **reviewer**Â that evaluates and refines those ideas.
3. AÂ **summarizer**Â that compiles the final output.

While the framework made it easy to orchestrate these agents, it also abstracted away a lot of the details. At first, everything seemed fine â€” the agents were producing outputs, and the workflow ran smoothly. But when I looked closer, I realized the summarizer wasnâ€™t getting all the information it needed. The final summaries were vague and uninformative.

With MLflow, I was able to trace the issue step by step. By examining the inputs and outputs at each stage, I discovered that the summarizer wasnâ€™t receiving the generatorâ€™s final output. A simple configuration change fixed the problem, but without MLflow, I might never have noticed it.

[I might never have noticed that the agent wasn't passing the right info to the LLM until MLflow helped me out.](https://preview.redd.it/q7giinxu8naf1.png?width=960&format=png&auto=webp&s=05a3e7c983191836e6ceae8a8f689613d5acf77c)

# Why Iâ€™m Sharing This

Iâ€™m not here to sell you on MLflow â€” itâ€™s open source, after all. Iâ€™m sharing this because I know how frustrating it can be to feel like youâ€™re stumbling around in the dark when things go wrong. Whether youâ€™re debugging a flaky chatbot or trying to optimize a complex workflow, having the right tools can make all the difference.

If youâ€™re working on multi-agent applications and struggling with observability, Iâ€™d encourage you to give MLflow a try. Itâ€™s not perfect (I had to patch a few bugs in the Autogen integration, for example), but itâ€™s the tool Iâ€™ve found for the job so far.",datascience,28,https://www.reddit.com/r/datascience/comments/1lqn6pu/how_i_use_mlflow_31_to_bring_observability_to/,r_1lqn6pu,,,
r_1lq8shf,reddit,BirdLadyTraveller,2025-07-02T22:06:14+00:00,"How can I get international remote positions?
Hello folks! I am a data scientist in Brazil and in general, I have a good resume. I have experience working in big techs, startup,  consulting and a MsC degree. 

I get Brazilian interviews easily but not abroad, even if I have a LinkedIn profile in English. How can I get considered for a remote position from US or Europe so I can keep working from my country?",datascience,93,https://www.reddit.com/r/datascience/comments/1lq8shf/how_can_i_get_international_remote_positions/,r_1lq8shf,,,
r_1lq79vo,reddit,Daniel-Warfield,2025-07-02T21:02:43+00:00,"A Breakdown of A2A, MCP, and Agentic Interoperability
MCP and A2A are both emerging standards in AI. In this post I want to cover what they're both useful for (based on my experience) from a practical level, and some of my thoughts about where the two protocols will go moving forward. Both of these protocols are still actively evolving, and I think there's room for interpretation around where they should go moving forward. As a result, I don't think there is a single, correct interpretation of A2A and MCP. These are my thoughts.

**What is MCP?**  
From it's highest level, MCP (model context protocol) is a standard way to expose tools to AI agents. More specifically, it's a standard way to communicate tools to a client which is managing the execution of an LLM within a logical loop. There's not really one, single, god almighty way to feed tools into an LLM, but MCP defines a standard on how tools are defined to make that process more streamlined.

The whole idea of MCP is derivative from LSP (language server protocol), which emerged due to a practical need from programming language and code editor developers. If you're working on something like VS Code, for instance, you don't want to implement hooks for Rust, Python, Java, etc. If you make a new programming language, you don't want to integrate it into vscode, sublime, jetbrains, etc.  The problem of ""connect programming language to text editor, with syntax highlighting and autocomplete"" was abstracted to a generalized problem, and solved with LSP. The idea is that, if you're making a new language, you create an LSP server so that language will work in any text editor. If you're building a new text editor, you can support LSP to automatically support any modern programming language.

[A conceptual diagram of LSPs \(source: MCP IAEE\)](https://preview.redd.it/wz60k2hswiaf1.jpg?width=1050&format=pjpg&auto=webp&s=1c42845286b2bb05047bd0c32caf6a25ca7fdcac)

MCP does something similar, but for agents and tools. The idea is to represent tool use in a standardized way, such developers can put tools in an MCP server, and so developers working on agentic systems can use those tools via a standardized interface.

[LSP and MCP are conceptually similar in terms of their core workflow \(source: MCP IAEE\)](https://preview.redd.it/clc7u0qehiaf1.png?width=1050&format=png&auto=webp&s=6790f5a438aff994337a2224736ba986f1c17777)

I think it's important to note, MCP presents a standardized **interface** for tools, but there is leeway in terms of how a developer might choose to build tools and resources within an MCP server, and there is leeway around how MCP client developers might choose to use those tools and resources.

MCP has various ""transports"" defined, transports being means of communication between the client and the server. MCP can communicate both over the internet, and over local channels (allowing the MCP client to control local tools like applications or web browsers). In my estimation, the latter is really what MCP was designed for. In theory you can connect with an MCP server hosted on the internet, but MCP is chiefly designed to allow clients to execute a locally defined server.

Here's an example of a simple MCP server:

    """"""A very simple MCP server, which exposes a single very simple tool. In most
    practical applications of MCP, a script like this would be launched by the client,
    then the client can talk with that server to execute tools as needed.
    source: MCP IAEE.
    """"""
    
    from mcp.server.fastmcp import FastMCP
    
    mcp = FastMCP(""server"")
    
    u/mcp.tool()
    def say_hello(name: str) -> str:
        """"""Constructs a greeting from a name""""""
        return f""hello {name}, from the server!

In the normal workflow, the MCP client would spawn an MCP server based on a script like this, then would work with that server to execute tools as needed.

**What is A2A?**  
If MCP is designed to expose tools to AI agents, A2A is designed to allow AI agents to talk to one another. I think this diagram summarizes how the two technologies interoperate with on another nicely:

[A conceptual diagram of how A2A and MCP might work together. \(Source: A2A Home Page\)](https://preview.redd.it/gb2bj773ziaf1.png?width=640&format=png&auto=webp&s=c74c1ced5fc1e9026670f68487431392f79d0a4e)

Similarly to MCP, A2A is designed to standardize communication between AI resource. However, A2A is specifically designed for allowing agents to communicate with one another. It does this with two fundamental concepts:

1. Agent Cards: a structure description of what an agent does and where it can be found.
2. Tasks: requests can be sent to an agent, allowing it to execute on tasks via back and forth communication.

A2A is peer-to-peer, asynchronous, and is natively designed to support online communication. In python, A2A is built on top of ASGI (asynchronous server gateway interface), which is the same technology that powers FastAPI and Django.

Here's an example of a simple A2A server:

    from a2a.server.agent_execution import AgentExecutor, RequestContext
    from a2a.server.apps import A2AStarletteApplication
    from a2a.server.request_handlers import DefaultRequestHandler
    from a2a.server.tasks import InMemoryTaskStore
    from a2a.server.events import EventQueue
    from a2a.utils import new_agent_text_message
    from a2a.types import AgentCard, AgentSkill, AgentCapabilities
    
    import uvicorn
    
    class HelloExecutor(AgentExecutor):
        async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
            # Respond with a static hello message
            event_queue.enqueue_event(new_agent_text_message(""Hello from A2A!""))
    
        async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
            pass  # No-op
    
    
    def create_app():
        skill = AgentSkill(
            id=""hello"",
            name=""Hello"",
            description=""Say hello to the world."",
            tags=[""hello"", ""greet""],
            examples=[""hello"", ""hi""]
        )
    
        agent_card = AgentCard(
            name=""HelloWorldAgent"",
            description=""A simple A2A agent that says hello."",
            version=""0.1.0"",
            url=""http://localhost:9000"",
            skills=[skill],
            capabilities=AgentCapabilities(),
            authenticationSchemes=[""public""],
            defaultInputModes=[""text""],
            defaultOutputModes=[""text""],
        )
    
        handler = DefaultRequestHandler(
            agent_executor=HelloExecutor(),
            task_store=InMemoryTaskStore()
        )
    
        app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)
        return app.build()
    
    
    if __name__ == ""__main__"":
        uvicorn.run(create_app(), host=""127.0.0.1"", port=9000)

Thus A2A has important distinctions from MCP:

* A2A is designed to support ""discoverability"" with agent cards. MCP is designed to be explicitly pointed to.
* A2A is designed for asynchronous communication, allowing for complex implementations of multi-agent workloads working in parallel.
* A2A is designed to be peer-to-peer, rather than having the rigid hierarchy of MCP clients and servers.

**A Point of Friction**  
I think the high level conceptualization around MCP and A2A is pretty solid; MCP is for tools, A2A is for inter-agent communication.

[A high level breakdown of the core usage of MCP and A2A \(source: MCP vs A2A\)](https://preview.redd.it/s8ba9ov6ziaf1.png?width=1080&format=png&auto=webp&s=7c4db19dde15d13cc34372e9c7449ad91939ad28)

Despite the high level clarity, I find these clean distinctions have a tendency to break down practically in terms of implementation. I was working on an example of an application which leveraged both MCP and A2A. I poked around the internet, and found [a repo of examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp) from the official a2a github account. In these examples, they actually use MCP to expose A2A as a set of tools. So, instead of the two protocols existing independently:

[How MCP and A2A might commonly be conceptualized, within a sample application consisting of a travel agent, a car agent, and an airline agent. \(source: A2A IAEE\)](https://preview.redd.it/5wxavpimniaf1.png?width=1050&format=png&auto=webp&s=b092517d6df915c72b673898f3bf563f5dda16d0)

Communication over A2A happens within MCP servers:

[Another approach of implementing A2A and MCP. \(source: A2A IAEE\)](https://preview.redd.it/dh3de5xuniaf1.png?width=1050&format=png&auto=webp&s=d3f46df060e30bb2d71b24ecfc670566f643322f)

This violates the conventional wisdom I see online of A2A and MCP essentially operating as completely separate and isolated protocols. I think the key benefit of this approach is ease of implementation: You don't have to expose both A2A and MCP as two seperate sets of tools to the LLM. Instead, you can expose only a single MCP server to an LLM (that MCP server containing tools for A2A communication). This makes it much easier to manage the integration of A2A and MCP into a single agent. Many LLM providers have plenty of demos of MCP tool use, so using MCP as a vehicle to serve up A2A is compelling.

You can also use the two protocols in isolation, I imagine. There are a ton of ways MCP and A2A enabled projects can practically be implemented, which leads to closing thoughts on the subject.

**My thoughts on MCP and A2A**  
It doesn't matter how standardized MCP and A2A are; if we can't all agree on the larger structure they exist in, there's no interoperability. In the future I expect frameworks to be built on top of both MCP and A2A to establish and enforce best practices. Once the industry converges on these new frameworks, I think issues of ""should this be behind MCP or A2A"" and ""how should I integrate MCP and A2A into this agent"" will start to go away. This is a standard part of the lifecycle of software development, and we've seen the same thing happen with countless protocols in the past.

Standardizing prompting, though, is a different beast entirely.

Having managed the development of LLM powered applications for a while now, I've found prompt engineering to have an interesting role in the greater product development lifecycle. Non-technical stakeholders have a tendency to flock to prompt engineering as a catch all way to solve any problem, which is totally untrue. Developers have a tendency to disregard prompt engineering as a secondary concern, which is also totally untrue. The fact is, prompt engineering won't magically make an LLM powered application better, but bad prompt engineering sure can make it worse. When you hook into MCP and A2A enabled systems, you are essentially allowing for arbitrary injection of prompts as they are defined in these systems. This may have some security concerns if your code isn't designed in a hardened manner, but more palpably there are massive performance concerns. Simply put, if your prompts aren't synergistic with one another throughout an LLM powered application, you won't get good performance. This seriously undermines the practical utility of MCP and A2A enabling turn-key integration.

I think the problem of a framework to define when a tool should be MCP vs A2A is immediately solvable. In terms of prompt engineering, though, I'm curious if we'll need to build rigid best practices around it, or if we can devise clever systems to make interoperable agents more robust to prompting inconsistencies.

**Sources:**  
MCP [vs A2A](https://www.eyelevel.ai/post/a2a-vs-mcp-how-agent-protocols-really-work-and-where-each-one-wins) (I co-authored)  
[MCP IAEE ](https://iaee.substack.com/p/model-context-protocol-intuitively) (I authored)  
[A2A IAEE](https://iaee.substack.com/p/agent-to-agent-protocol-intuitively?utm_source=publication-search) (I authored)  
[A2A MCP Examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp)  
[A2A Home Page](https://a2aproject.github.io/A2A/latest/)

  


  
",datascience,33,https://www.reddit.com/r/datascience/comments/1lq79vo/a_breakdown_of_a2a_mcp_and_agentic/,r_1lq79vo,,,
r_1lq4xgr,reddit,Fit-Employee-4393,2025-07-02T19:27:18+00:00,"How much wiggle room do you give yourself on DS projects?
When youâ€™re starting a project, how much extra time do you give yourself for the deadline that you share with stakeholders?

I personally will multiply the time I think I can complete something in by 1.5-2. Honestly might start multiplying by 3 to make multitasking easier.

Thereâ€™s just so much that can go wrong in DS related projects so I feel itâ€™s necessary to do this. Basically just underpromise overdeliver as they say.

Interested to hear about different situations.",datascience,56,https://www.reddit.com/r/datascience/comments/1lq4xgr/how_much_wiggle_room_do_you_give_yourself_on_ds/,r_1lq4xgr,,,
r_1lq0v8p,reddit,BirdLadyTraveller,2025-07-02T16:47:58+00:00,"I am currently a data scientist. How can I move to a more business oriented rule?
Hey folks! I have worked as a DS for about 5 years now. I wanted to move to a position that I still work with data, but I am  looking for something less technical and more business related. I will list some of my strengths that are also things I like to work with:

- Build proof of concepts projects and explore techniques in the literature to solve business problems with data science approaches;
- Do presentation for technical and non technical peers;
- Build documentation and produce online content;
- I also love to create training and manage projects related to data culture, education, and onboarding.
- Work in groups /having group discussions with multidisciplinary teams. 

Do you know names for positions that are more focused on that? I'd like to search for them! ",datascience,48,https://www.reddit.com/r/datascience/comments/1lq0v8p/i_am_currently_a_data_scientist_how_can_i_move_to/,r_1lq0v8p,,,
r_1lpucaf,reddit,Belmeez,2025-07-02T12:17:58+00:00,"Need advise on cross-functional collaboration
Hi data science community,

I need your advice on how to handle a work situation. Curious to know how others would handle or if they have been in a similar situation.

I lead a data science team and I also have a peer who leads a BI team and we report to the same executive.

A couple months ago, BI lead reached out and was excited to see if we can collaborate and create an AI/BI chat bot for our internal structured data. I thought this was a good idea and would be a great opportunity to collaborate with him and his team. So I spent a couple of weeks to build out a POC, I show cased it to him and our executive, it was well received and I outlined next steps on how we can collaborate to make it better.

I got no response from him about my next steps email. I figured no harm no foul he got busy Iâ€™m sure. Well come to find out, he had his team build almost an exact replica of the POC I did and essentially boxed my team and I out of this idea and decided he would just do it himself internally. Mind you, all the BI people had to learn how to use LLMs and how to orchestrate agents, etc. itâ€™s a skill set we have but he decided to do it himself despite this.

How would you all handle this?

I was planning on a 1:1 with him where I essentially lay out the facts that he wasted my time by giving me the illusion that we would work together and collaborate but instead just did things himself. We have been getting pushed by our executive team to work together more and this was a great opportunity to show them we work together but instead he decided to take a different route.",datascience,18,https://www.reddit.com/r/datascience/comments/1lpucaf/need_advise_on_crossfunctional_collaboration/,r_1lpucaf,,,
r_1lpnkj0,reddit,statius9,2025-07-02T05:16:35+00:00,"Beta release: Minds AI Filter for EEG â€” Physics-informed preprocessing for real-time BCI (+17% gain on noisy data from commercial headsets, 0.2s latency)
We at MindsApplied specialize in the development of machine learning models for the enhancement of EEG signal quality and emotional state classification. We're excited to share our latest modelâ€”the Minds AI Filterâ€”and would love your feedback.

* [ðŸ‘‰ Download the Python package here](https://drive.google.com/drive/folders/1_4Q9voe5j88G_EMF8YanoeEPVoUt_D2B?usp=drive_link)
* ðŸ”‘Use key: ''REDDIT-KEY-VRG44S' to initialize
* ðŸ“„ Includes setup instructions

The Minds AI Filter is a physics-informed, real-time EEG preprocessing tool that relies on sensor fusion for low-latency noise and artifact removal. It's built to improve signal quality before feature extraction or classification, especially for online systems. To dive (very briefly) into the details, itÂ works in part by **reducing high-frequency noise (\~40 Hz) and sharpening low-frequency activity (\~3â€“7 Hz)**.

We tested it alongside standard bandpass filtering, using both:

* Commercial EEG hardware (OpenBCI Mark IV, BrainBit Dragon)
* The public DEAP dataset, a 32-participant benchmark for emotional state classification

Here are our experimental results:

* Commercial Devices (OpenBCI Mark IV, BrainBit Dragon)
   * \+15% average improvement in balanced accuracy using only 12 trials of 60 seconds per subject per device
   * Improvement attributed to higher baseline noise in these systems
* DEAP Dataset
   * \+6% average improvement across 32 subjects and 32 channels
   * Maximum individual gain: +35%
   * Average gain in classification accuracy was 17% for cases where the filter led to improvement.
   * No decline in accuracy for any participant
* Performance
   * \~0.2 seconds to filter 60 seconds of data

Note: Comparisons were made between bandpass-only and bandpass + Minds AI Filter. Filtering occurred before bandpass.

Methodology:

To generate these experimental results, we used 2-fold stratified cross-validation grid search to tune the filter's key hyperparameter (Î»). Classification relied on balanced on balanced accuracy using logistic regression on features derived from wavelet coefficients.

Why we're posting: This filter is still in beta and we'd love feedback â€”especially if you try it on your own datasets or devices. The current goal is to support rapid, adaptive, and physics-informed filtering for real-time systems and multi-sensor neurotech platforms.

If you find it useful or want future updates (e.g., universal DLL, long-term/offline licenses), you can subscribe here:

* ðŸ”— [https://www.minds-applied.com/contact](https://www.minds-applied.com/contact)

https://preview.redd.it/o3xqckeiaeaf1.png?width=594&format=png&auto=webp&s=0fb1860d8af85fa516cb705c096427a32977a522

https://preview.redd.it/95lbzd8jaeaf1.png?width=589&format=png&auto=webp&s=39984d0e8f75f27ab0a71e7e5ca09bba25f6ffb4

https://preview.redd.it/x9iyc4kjaeaf1.png?width=1372&format=png&auto=webp&s=ef70703c892727318688b7472778cb5658a899ee

",datascience,4,https://www.reddit.com/r/datascience/comments/1lpnkj0/beta_release_minds_ai_filter_for_eeg/,r_1lpnkj0,,,
r_1loo3eh,reddit,Karl_mstr,2025-07-01T00:36:08+00:00,"Does DB normalization worth it?
Hi, I have 6 months as a Jr Data Analyst and I have been working with Power BI since I begin. At the beginning I watched a lot of dashboards on PBI and when I checked the Data Model was disgusting, it doesn't seems as something well designed. 

On my the few opportunities that I have developed some dashboards I have seen a lot of redundancies on them, but I keep quiet due it's my first analytic role and my role using PBI so I couldn't compare with anything else.

I ask here because I don't know many people who use PBI or has experience on Data related jobs and I've been dealing with query limit reaching (more than 10M rows to process).

So I watched some courses that normalization could solve many issues, but I wanted to know:
1 - If it could really help to solve that issue.
2 - How could I normalize the data when, not the data, the data Model is so messy? 

Thanks in advance.",datascience,27,https://www.reddit.com/r/datascience/comments/1loo3eh/does_db_normalization_worth_it/,r_1loo3eh,,,
r_1lohjp4,reddit,ElectrikMetriks,2025-06-30T19:59:00+00:00,"No reason to complicate things.
There's absolutely validity in doing more complex visuals.  But, sometimes simple is better if the audience is more likely to use it/understand it.",datascience,1236,https://www.reddit.com/r/datascience/comments/1lohjp4/no_reason_to_complicate_things/,r_1lohjp4,,,
r_1lo4xao,reddit,Technical-Love-8479,2025-06-30T11:19:26+00:00,"Model Context Protocol (MCP) tutorials playlist for beginners
This playlist comprises of numerous tutorials on MCP servers including

1. Install Blender-MCP for Claude AI on Windows
2. Design a Room with Blender-MCP + Claude
3. Connect SQL to Claude AI via MCP
4. Run MCP Servers with Cursor AI
5. Local LLMs with Ollama MCP Server
6. Build Custom MCP Servers (Free)
7. Control Docker via MCP
8. Control WhatsApp with MCP
9. GitHub Automation via MCP
10. Control Chrome using MCP
11. Figma with AI using MCP
12. AI for PowerPoint via MCP
13. Notion Automation with MCP
14. File System Control via MCP
15. AI in Jupyter using MCP
16. Browser Automation with Playwright MCP
17. Excel Automation via MCP
18. Discord + MCP Integration
19. Google Calendar MCP
20. Gmail Automation with MCP
21. Intro to MCP Servers for Beginners
22. Slack + AI via MCP
23. Use Any LLM API with MCP
24. Is Model Context Protocol Dangerous?
25. LangChain with MCP Servers
26. Best Starter MCP Servers
27. YouTube Automation via MCP
28. Zapier + AI using MCP
29. MCP with Gemini 2.5 Pro
30. PyCharm IDE + MCP
31. ElevenLabs Audio with Claude AI via MCP
32. LinkedIn Auto-Posting via MCP
33. Twitter Auto-Posting with MCP
34. Facebook Automation using MCP
35. Top MCP Servers for Data Science
36. Best MCPs for Productivity
37. Social Media MCPs for Content Creation
38. MCP Course for Beginners
39. Create n8n Workflows with MCP
40. RAG MCP Server Guide
41. Multi-File RAG via MCP
42. Use MCP with ChatGPT
43. ChatGPT + PowerPoint (Free, Unlimited)
44. ChatGPT RAG MCP
45. ChatGPT + Excel via MCP
46. Use MCP with Grok AI
47. Vibe Coding in Blender with MCP
48. Perplexity AI + MCP Integration
49. ChatGPT + Figma Integration
50. ChatGPT + Blender MCP
51. ChatGPT + Gmail via MCP
52. ChatGPT + Google Calendar MCP
53. MCP vs Traditional AI Agents

Hope this is useful !!

Playlist : [https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp](https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp)",datascience,26,https://www.reddit.com/r/datascience/comments/1lo4xao/model_context_protocol_mcp_tutorials_playlist_for/,r_1lo4xao,,,
r_1lny1dk,reddit,AutoModerator,2025-06-30T04:01:14+00:00,"Weekly Entering & Transitioning - Thread 30 Jun, 2025 - 07 Jul, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,11,https://www.reddit.com/r/datascience/comments/1lny1dk/weekly_entering_transitioning_thread_30_jun_2025/,r_1lny1dk,,,
r_1lntegl,reddit,ergodym,2025-06-29T23:57:36+00:00,"ICs who pivoted: did you go engineering or management?
Hitting that point where I feel like I need to pick a lane.

Curious what others did. Did you double down on technical stuff (data engineering/MLE/SWE), switched to the product side, or did you move into people management?",datascience,59,https://www.reddit.com/r/datascience/comments/1lntegl/ics_who_pivoted_did_you_go_engineering_or/,r_1lntegl,,,
r_1lnct9i,reddit,hendrix616,2025-06-29T11:50:32+00:00,"Using Claude Code in notebook
At work I use jupyter notebooks for experimentation and prototyping of data products. So far, Iâ€™ve been leveraging AI code completion type of functionality within a Python cell for finishing a line of code, writing the next few lines or writing a function altogether. 

But Iâ€™m curious about the next level: using something like Claude Code open side-by side with my notebook. 

Just wondering if anyone is currently using this type of workflow and if you have any tips & tricks or specific use cases you could share. ",datascience,0,https://www.reddit.com/r/datascience/comments/1lnct9i/using_claude_code_in_notebook/,r_1lnct9i,,,
r_1lnbgna,reddit,ParkingTheory9837,2025-06-29T10:28:01+00:00,"Not sure what certifications to attain to increase my chances of getting an internship after third year
Context: I am planning to go into data science as a career. Im currently about to go into my third year and I need to secure an internship agter my third year during my coop year. To increade my chances, I want to obtain AWS certifications. The problem I am seeing is that the AWS SAA certificate seems to specific to AWS. Would the MLEA or DEA increade my chance of getting data scientist/mle internships significantly? Assume I have knowledge and projects to showcase knowledge of theoretical ML, python, sql, etc. Also assume I have cloud practitioner and AI practitioner certs but no experience with AWS whatsoever, but experience in data analysis. I would really appreciate in depth responses. Please avoid stupid comments like ""certifications are useless"" because they obv arent and can set you apart from someone with similar skill sets in other areas. ",datascience,0,https://www.reddit.com/r/datascience/comments/1lnbgna/not_sure_what_certifications_to_attain_to/,r_1lnbgna,,,
r_1ln9cf0,reddit,Round-Paramedic-2968,2025-06-29T08:05:12+00:00,"Advice on feature selection process
**Hi everyone,**

I have a question regarding the feature selection process for a credit risk model I'm building as part of my internship. I've collected raw data and conducted feature engineering with the help of a domain expert in credit risk. Now I have a list of around 2000 features.

For the feature selection part, based on what I've learned, the typical approach is to use a tree-based model (like Random Forest or XGBoost) to rank feature importance, and then shortlist it down to about 15â€“20 features. After that, I would use those selected features to train my final model (CatBoost in this case), perform hyperparameter tuning, and then use that model for inference.

Am I doing it correctly? It feels a bit too straightforward â€” like once I have the 2000 features, I just plug them into a tree model, get the top features, and that's it. I noticed that some of my colleagues do multiple rounds of feature selection â€” for example, narrowing it down from 2000 to 200, then to 80, and finally to 20 â€” using multiple tree models and iterations.

Also, where do SHAP values fit into this process? I usually use SHAP to visualize feature effects in the final model for interpretability, but I'm wondering if it can or should be used during the feature selection stage as well.

Iâ€™d really appreciate your advice!",datascience,30,https://www.reddit.com/r/datascience/comments/1ln9cf0/advice_on_feature_selection_process/,r_1ln9cf0,,,
r_1ln8f2b,reddit,OverratedDataScience,2025-06-29T07:02:58+00:00,"How do you deal with data scientists with big pay check and title but no domain knowledge?
A tech illiterate Director at my org hired a data couple of data scientists 18 months ago. He has tasked them with nothing specific. And their job was solely to observe and find uses-cases themselves. The only reason they were hired was for the Director to gain brownie points of creating a data-driven team for themself, despite there being several other such teams.

Cut to today, the Director has realized that there is very little ROI from his hires because they lack domain knowledge. He conveniently moved them to another team where ML is an overkill. The data scientists however, have found some problems they thought they'll solve with ""data science"". They have been vibe coding and building PPTs for months now. But their attempts are hardly successful because of their lack of domain knowledge. To compensate for their lack of domain knowledge, they create beautiful presentations with lots of buzzwords such as LLMs, but again, lack domain substance.

Now, their proposals seem unnecessary and downright obnoxious to many domain SMEs. But the SMEs don't have the courage to say it to the leadership and be percevied as a roadblock to the data-driven strategy. The constant interference of these data scientists is destabilizing the existing processes for the worst and the team is incurring additional costs.

This is a very peculiar situation where the data scientists, lacking domain knowledge, are just shooting project proposals in the dark hoping to hit something. I know this doesn't typically happen in most organizations. But have you ever seen such a situation around you? How did you or others deal with the situation?

EDIT: This post is not to shit on the data scientists. They are probably good in their areas. The problem is not the domain SME support. The problem is that these data scientists seem to be too high on their titles and paychecks to collaborate with SMEs. Most SMEs want to support them and tell them nicely that ML/AI is an overkill for their usecases, and the efforts required are too big. There are other data science and analytics teams that are working seamlesly with SMEs.",datascience,0,https://www.reddit.com/r/datascience/comments/1ln8f2b/how_do_you_deal_with_data_scientists_with_big_pay/,r_1ln8f2b,,,
r_1ln6aeq,reddit,guna1o0,2025-06-29T04:47:53+00:00,"Howâ€™s the job market for Bayesian statistics?
Iâ€™m a data scientist with 1 YOE. mostly worked on credit scoring models, sql, and Power BI. Lately, Iâ€™ve been thinking of going deeper into bayesian statistics and Iâ€™m currently going through the s*tatistical rethinking* book.

But Iâ€™m wondering. is it worth focusing heavily on bayesian stats? Or should I pivot toward something that opens up more job opportunities?

Would love to hear your thoughts or experiences!",datascience,137,https://www.reddit.com/r/datascience/comments/1ln6aeq/hows_the_job_market_for_bayesian_statistics/,r_1ln6aeq,,,
r_1ln3zyk,reddit,Illustrious-Pound266,2025-06-29T02:35:38+00:00,"Is ML/AI engineering increasingly becoming less focused on model training and more focused on integrating LLMs to build web apps?
One thing I've noticed recently is that increasingly, a lot of AI/ML roles seem to be focused on ways to integrate LLMs to build web apps that automate some kind of task, e.g. chatbot with RAG or using agent to automate some task in a consumer-facing software with tools like langchain, llamaindex, Claude, etc. I feel like there's less and less of the ""classical"" ML training and building models.

I am not saying that ""classical"" ML training will go away. I think model building/training non-LLMs will always have some place in data science. But in a way, I feel like ""AI engineering"" seems increasingly converging to something closer to back-end engineering you typically see in full-stack. What I mean is that rather than focusing on building or training models, it seems that the bulk of the work now seems to be about how to take LLMs from model providers like OpenAI and Anthropic, and use it to build some software that automates some work with Langchain/Llamaindex.

Is this a reasonable take? I know we can never predict the future, but the trends I see seem to be increasingly heading towards that.",datascience,159,https://www.reddit.com/r/datascience/comments/1ln3zyk/is_mlai_engineering_increasingly_becoming_less/,r_1ln3zyk,,,
r_1lmxkq8,reddit,bobo-the-merciful,2025-06-28T21:14:48+00:00,"Pleased to share the ""SimPy Simulation Playground"" - examples of simulations in Python from different industries
Just put the finishing touches to the first version of this web page where you can run SimPy examples from different industries, including parameterising the sim, editing the code if you wish, running and viewing the results.

Runs entirely in your browser.

Here's the link: [https://www.schoolofsimulation.com/simpy\_simulations](https://www.schoolofsimulation.com/simpy_simulations)

My goal with this is to help provide education and informationa around how discrete-event simulation with SimPy can be applied to different industry contexts.

If you have any suggestions for other examples to add, I'd be happy to consider expanding the list!

Feedback, as ever, is most welcome!",datascience,14,https://www.reddit.com/r/datascience/comments/1lmxkq8/pleased_to_share_the_simpy_simulation_playground/,r_1lmxkq8,,,
r_1lmv5uf,reddit,OverratedDataScience,2025-06-28T19:27:04+00:00,"Unpopular Opinion: These are the most useless posters on LinkedIn
LinkedIn influencers love to treat the two roles as different species. In most enterprises, especially in mid to small orgs, these roles are largely overlapping. ",datascience,1326,https://www.reddit.com/r/datascience/comments/1lmv5uf/unpopular_opinion_these_are_the_most_useless/,r_1lmv5uf,,,
r_1lmsf8b,reddit,unknown,2025-06-28T17:30:08+00:00,"HuggingFace transformers API reference: How do you navigate it?
This might be a me problem, but I have some difficulty navigating HF transformers API documentation. It's sometimes easier to use Gemini or Claude to get the relevant information than from the official HF transformers API reference. 

How do you all do it? Any best practices? 

TY.",datascience,3,https://www.reddit.com/r/datascience/comments/1lmsf8b/huggingface_transformers_api_reference_how_do_you/,r_1lmsf8b,,,
r_1lmneo7,reddit,Mission-Balance-4250,2025-06-28T13:55:44+00:00,"I built a self-hosted Databricks
Hey everyone, I'm an ML Engineer who spearheaded the adoption of Databricks at work. I love the agency it affords me because I can own projects end-to-end and do everything in one place.

However, the platform adds a lot of overhead and has a wide array of data-features I just don't care about. So many problems can be solved with a simple data pipeline and basic model (e.g. XGBoost.) Not only is there technical overhead, but systems and process overhead; bureaucracy and red-tap significantly slow delivery. Right now at work we are undertaking a ""migration"" to Databricks and man, it is such a PITA to get anything moving it isn't even funny...

Anyway, I decided to try and address this myself by developingÂ [FlintML](https://github.com/flintml/flintml), a self-hosted, all-in-one MLOps stack. Basically, Polars, Delta Lake, unified catalog, Aim experiment tracking, notebook IDE and orchestration (still working on this) fully spun up with Docker Compose.

I'm hoping to get some feedback from this subreddit. I've spent a couple of months developing this and want to know whether I would be wasting time by continuing or if this might actually be useful. I am using it for my personal research projects and find it very helpful.

Thanks heaps",datascience,78,https://www.reddit.com/r/datascience/comments/1lmneo7/i_built_a_selfhosted_databricks/,r_1lmneo7,,,
r_1lmi8j8,reddit,petburiraja,2025-06-28T08:56:59+00:00,"The ""Unicorn"" is Dead: A Four-Era History of the Data Scientist Role and Why We're All Engineers Now
Hey everyone,

Iâ€™ve been in this field for a while now, starting back when ""Big Data"" was the big buzzword, and I've been thinking a lot about how drastically our roles have changed. It feels like the job description for a ""Data Scientist"" has been rewritten three or four times over. The ""unicorn"" we all talked about a decade ago feels like a fossil today.

I wanted to map out this evolution, partly to make sense of it for myself, but also to see if it resonates with your experiences. I see it as four distinct eras.

---

### **Era 1: The BI & Stats Age (The ""Before Times,"" Pre-2010)**

Remember this? Before ""Data Scientist"" was a thing, we were all in our separate corners.

*   **Who we were:** BI Analysts, Statisticians, Database Admins, Quants.
*   **What we did:** Our world revolved around historical reporting. We lived in SQL, wrestling with relational databases and using tools like Business Objects or good old Excel to build reports. The core question was always, **""What happened last quarter?""**
*   **The ""advanced"" stuff:** If you were a true statistician, maybe you were building logistic regression models in SAS, but that felt very separate from the day-to-day business analytics. It was more academic, less integrated.

The mindset was purely descriptive. We were the historians of the company's data.

### **Era 2: The Golden Age of the ""Unicorn"" (Roughly 2011-2018)**

This is when everything changed. *HBR* called our job the ""sexiest"" of the century, and the hype was real.

*   **The trigger:** Hadoop and Spark made ""Big Data"" accessible, and Python with Scikit-learn became an absolute powerhouse. Suddenly, you could do serious modeling on your own machine.
*   **The mission:** The game changed from ""What happened?"" to **""What's *going* to happen?""** We were all building churn models, recommendation engines, and trying to predict the future. The Jupyter Notebook was our kingdom.
*   **The ""unicorn"" expectation:** This was the peak of the ""full-stack"" ideal. One person was supposed to understand the business, wrangle the data, build the model, and then explain it all in a PowerPoint deck. The *insight* from the model was the final product. It was an incredibly fun, creative, and exploratory time.

### **Era 3: The Industrial Age & The Great Bifurcation (Roughly 2019-2023)**

This is where, in my opinion, the ""unicorn"" myth started to crack. Companies realized a model sitting in a notebook doesn't actually *do* anything for the business. The focus shifted from *building models* to *deploying systems*.

*   **The trigger:** The cloud matured. AWS, GCP, and Azure became the standard, and the discipline of MLOps was born. The problem wasn't ""can we predict it?"" anymore. It was, **""Can we serve these predictions reliably to millions of users with low latency?""**
*   **The splintering:** The generalist ""Data Scientist"" role started to fracture into specialists because no single person could master it all:
    *   **ML Engineers:** The software engineers who actually productionized the models.
    *   **Data Engineers:** The unsung heroes who built the reliable data pipelines with tools like Airflow and dbt.
    *   **Analytics Engineers:** The new role that owned the data modeling layer for BI.
*   The mindset became engineering-first. We were building factories, not just artisanal products.

### **Era 4: The Autonomous Age (2023 - Today and Beyond)**

And then, everything changed again. The arrival of truly powerful LLMs completely upended the landscape.

*   **The trigger:** ChatGPT went public, GPT-4 was released, and frameworks like LangChain gave us the tools to build on top of this new paradigm.
*   **The mission:** The core question has evolved again. It's not just about prediction anymore; it's about **action and orchestration**. The question is, **""How do we build a system that can understand a goal, create a plan, and execute it?""**
*   **The new reality:**
    *   **Prediction becomes a feature, not the product.** An AI *agent* doesn't just predict churn; it takes an *action* to prevent it.
    *   **We are all systems architects now.** We're not just building a model; we're building an intelligent, multi-step workflow. We're integrating vector databases, multiple APIs, and complex reasoning loops.
    *   **The engineering rigor from Era 3 is now the mandatory foundation.** You can't build a reliable agent without solid MLOps and real-time data engineering (Kafka, Flink, etc.).

It feels like the ""science"" part of our job is now less about statistical analysis (AI can do a lot of that for us) and more about the rigorous, empirical science of architecting and evaluating these incredibly complex, often non-deterministic systems.

So, that's my take. The ""Data Scientist"" title isn't dead, but the ""unicorn"" generalist ideal of 2015 certainly is. We've been pushed to become deeper specialists, and for most of us on the building side, that specialty looks a lot more like engineering than anything else.

Curious to hear if this matches up with what you're all seeing in your roles. Did I miss an era? Is your experience different?

EDIT: In response to comments asking if this was written by AI: The underlying ideas are based on my own experience.

However, I want to be transparent that I would not have been able to articulate my vague, intuitive thoughts about the changes in this field with such precision. 

I used AI specifically for the structurization and organization of the content.",datascience,613,https://www.reddit.com/r/datascience/comments/1lmi8j8/the_unicorn_is_dead_a_fourera_history_of_the_data/,r_1lmi8j8,,,
r_1lmaxr4,reddit,mgalarny,2025-06-28T01:36:04+00:00,"Using LLMs to Extract Stock Picks from YouTube
For anyone interested in NLP or the application of data science in finance and media, we just released a dataset + paper on extracting **stock recommendations from YouTube financial influencer videos**.

This is a real-world task that combines signals across audio, video, and transcripts. We used expert annotations and benchmarked both LLMs and multimodal models to see how well they can extract structured recommendation data (like ticker and action) from messy, informal content.

If you're interested in working with unstructured media, financial data, or evaluating model performance in noisy settings, this might be interesting.

Paper: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  
Dataset: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)

Happy to discuss the challenges we ran into or potential applications beyond finance!

[Betting against finfluencer recommendations outperformed the S&P 500 by +6.8&#37; in annual returns, but at higher risk \(Sharpe ratio 0.41 vs 0.65\). QQQ wins in Sharpe ratio. ](https://preview.redd.it/3n861nuhnk9f1.png?width=4764&format=png&auto=webp&s=aa010ae695934b5520df5e82d8158201750cb3a4)

",datascience,94,https://www.reddit.com/r/datascience/comments/1lmaxr4/using_llms_to_extract_stock_picks_from_youtube/,r_1lmaxr4,,,
r_1lm0dc9,reddit,bobo-the-merciful,2025-06-27T17:51:09+00:00,"I built a ""virtual simulation engineer"" tool that designs, build, executes and displays the results of Python SimPy simulations entirely in a single browser window
New tool I built to design, build and execute a discrete-event simulation in Python entirely using natural language in a single browser window. 

You can use it here, 100% free: [https://gemini.google.com/share/ad9d3a205479](https://gemini.google.com/share/ad9d3a205479)

Version 2 uses SimPy under the hood. Pyodide to execute Python in the front end.

This is a proof of concept, I am keen for feedback please.

I made a video overview of it here: [https://www.youtube.com/watch?v=BF-1F-kqvL4](https://www.youtube.com/watch?v=BF-1F-kqvL4) ",datascience,12,https://www.reddit.com/r/datascience/comments/1lm0dc9/i_built_a_virtual_simulation_engineer_tool_that/,r_1lm0dc9,,,
r_1llvtfl,reddit,zsrt13,2025-06-27T14:50:08+00:00,"CVS Heath vs JPM
Thank you all for the support. This is a really helpful group. Cheers!",datascience,31,https://www.reddit.com/r/datascience/comments/1llvtfl/cvs_heath_vs_jpm/,r_1llvtfl,,,
r_1lluwlv,reddit,Raz4r,2025-06-27T14:11:44+00:00,"Data Science Has Become a Pseudo-Science
Iâ€™ve been working in data science for the last ten years, both in industry and academia, having pursued a masterâ€™s and PhD in Europe. My experience in the industry, overall, has been very positive. Iâ€™ve had the opportunity to work with brilliant people on exciting, high-impact projects. Of course, there were the usual high-stress situations, nonsense PowerPoints, and impossible deadlines, but the work largely felt meaningful.

However, over the past two years or so, it feels like the field has taken a sharp turn. Just yesterday, I attended a technical presentation from the analytics team. The project aimed to identify anomalies in a dataset composed of multiple time series, each containing a clear inflection point. The teamâ€™s hypothesis was that these trajectories might indicate entities engaged in some sort of fraud.

The team claimed to have solved the task using â€œgenerative AIâ€. They didnâ€™t go into methodological details but presented results that, according to them, were amazing. Curious, nespecially since the project was heading toward deployment, i asked about validation, performance metrics, or baseline comparisons. None were presented.

Later, I found out that â€œgenerative AIâ€ meant asking ChatGPT to generate a code. The code simply computed the mean of each series before and after the inflection point, then calculated the z-score of the difference. No model evaluation. No metrics. No baselines. Absolutely no model criticism. Just a naive approach, packaged and executed very, very quickly under the label of generative AI.

The moment I understood the proposed solution, my immediate thought was ""I need to get as far away from this company as possible"". I share this anecdote because it summarizes much of what Iâ€™ve witnessed in the field over the past two years. It feels like data science is drifting toward a kind of pseudo-science where we consult a black-box oracle for answers, and questioning its outputs is treated as anti-innovation, while no one really understand how the outputs were generated.

After several experiences like this, Iâ€™m seriously considering focusing on academia. Working on projects like these is eroding any hope I have in the field. I know this wonâ€™t work and yet, the label generative AI seems to make it unquestionable. So I came here to ask if is this experience shared among other DSs?

",datascience,2715,https://www.reddit.com/r/datascience/comments/1lluwlv/data_science_has_become_a_pseudoscience/,r_1lluwlv,,,
r_1llnbwq,reddit,joshamayo7,2025-06-27T06:55:48+00:00,"Causal Inference in Sports
For all curious on Causal Inference, and anyone interested in the application of DS in Sport. Iâ€™ve written this blog with the aim of providing a taste for how Causal Inference techniques are used practically, as well as some examples to get people thinking.

I do believe upskilling in Causal Inference is quite valuable, despite the learning curve I think itâ€™s quite cool identifying cause-and -effect without having to do RCTs.

Enjoy!",datascience,70,https://www.reddit.com/r/datascience/comments/1llnbwq/causal_inference_in_sports/,r_1llnbwq,,,
r_1lliwit,reddit,Technical-Love-8479,2025-06-27T02:40:50+00:00,"SEAL:Self-Adapting Language Models (self learning LLMs)
MIT has recently released a new research paper where they have introduced a new framework SEAL which introduces a concept of self-learning LLMs that means LLMs can now generate their own fine-tuning data set optimized for the strategy and fine tune themselves on the given context. 

Full summary ; [https://www.youtube.com/watch?v=MLUh9b8nN2U](https://www.youtube.com/watch?v=MLUh9b8nN2U)

Paper : [https://arxiv.org/abs/2506.10943](https://arxiv.org/abs/2506.10943)",datascience,8,https://www.reddit.com/r/datascience/comments/1lliwit/sealselfadapting_language_models_self_learning/,r_1lliwit,,,
r_1ll7or7,reddit,MasteredLink,2025-06-26T18:27:04+00:00,"When applying internally, do you reach out to the hiring manager?
I work at a relatively large company, and I've always reached out to hiring managers for internal positions, setting up a brief introductory meeting to ask specific questions about the role. However, during a recent HR session for new employees, it was recommended that we avoid this approach, as it could ""create bias"" and that managers are often too busy.

Now I'm rethinking my strategy for internal applications, I feel like it's highly dependent on the manager themselves but in most cases, asking for a quick intro meeting wouldn't hurt right? I feel like HR was way too broad with this statement. What are people's experiences on this.",datascience,55,https://www.reddit.com/r/datascience/comments/1ll7or7/when_applying_internally_do_you_reach_out_to_the/,r_1ll7or7,,,
r_1ll5dv2,reddit,Error40404,2025-06-26T16:59:08+00:00,"I have two amazing job offers. I want to build my own company in the near future. At a loss.
Hi!

I have two offers. One from a big tech company as a data scientist. I deem it easily the best tech company in my country. I would have killed for this offer just 1 year ago.

Another offer is from a robotics startup. I would be a founding engineer doing ML, and I think I would learn a lot. However, I'm not interested in this company in the long run. I would jump out after 2 years at the latest to build my own. So my equity would not even vest, and I would feel like I'm backstabbing the founders. They probably would not hire me if I told them this. But I think I would (maybe) learn more in this position.

I just can't decide what to do... My ultimate goal is to build my own company in 1-2 years. What to do?",datascience,72,https://www.reddit.com/r/datascience/comments/1ll5dv2/i_have_two_amazing_job_offers_i_want_to_build_my/,r_1ll5dv2,,,
r_1ll56bo,reddit,Technical-Love-8479,2025-06-26T16:50:48+00:00,"Gemini CLI: Google's free coding AI Agent
Google's Gemini CLI is a terminal based AI Agent mostly for coding and easy to install with free access to Gemini 2.5 Pro. Check demo here : https://youtu.be/Diib3vKblBM?si=DDtnlHqAhn_kHbiP",datascience,24,https://www.reddit.com/r/datascience/comments/1ll56bo/gemini_cli_googles_free_coding_ai_agent/,r_1ll56bo,,,
r_1lkpnkk,reddit,bonesclarke84,2025-06-26T03:17:02+00:00,"Pre-Expedition Weather Conditions and Success Rates: Seasonal Pattern Analysis of Himalayan Expedition Data
After someone posted Himalayan expedition data on Kaggle: [Himalayan Expeditions](https://www.kaggle.com/datasets/siddharth0935/himalayan-expeditions), I decided to start a personal project and expand on this data by adding ERA5 historical reanalysis weather data to it. Some of my preliminary findings have been interesting so far and I thought I would share them.  
  
I expanded on the expedition data by creating multiple different weather windows:

* Full expedition from basecamp date until termination either following summit or termination of attempt.
* Pre-expedition weather - 14 days prior to official expedition start at basecamp.
* Termination or Summit approach - the day before termination or summit.
* Early phase - the first 14 days at basecamp.
* Late phase - 7 days prior to termination date (either after summit or on failed attempt.)
* Decision window - 2 days prior to summit window

The first weather that I have focused on analyzing is the pre-expedition weather window. After cleaning the data and adding the weather windows, I also added a few other features using simple operations and created a few target variables for later modelling like expedition success score, expedition failure score, and an overall expedition score. For this analysis, though, I only focused on success being either True or False. After creating the features and targets, I then ran t-tests on success being True or False to determine their statistical significance. 

When looking at all the features related to the pre-expedition weather window, the findings seem to suggest that pre-expedition weather conditions play a significant role in Himalayan expedition success or failure in spring/summer expeditions. The graphs and correlation heatmap below summarize the variables that have the highest significance in either success or failure:  


[This diagram shows how the different attributes either contribute to success or failure.](https://preview.redd.it/6nbr99uzu69f1.png?width=1904&format=png&auto=webp&s=f84e3cb76b2d61d3b3e68dc3fdd0eec2608cd586)

[This diagram highlights the key attributes over or under of a significance of 0.2 or -0.2 respectively. ](https://preview.redd.it/bzj3uxu2v69f1.png?width=1889&format=png&auto=webp&s=5287852c5a90ae75b251826e1a9668db0ca34d80)

[This is a correlation heatmap diagram associating the attributes to success or failure.](https://preview.redd.it/dd5g6ly4v69f1.png?width=1904&format=png&auto=webp&s=a6925405bc7b5a9930fa9265adc3f425129579d8)

Although these findings alone do not paint an over-all picture of Himalayan expedition success or failure, I believe they play a significant part and could be used practically to assess conditions going into spring/summer expeditions. 

I hope this is interesting and feel free to provide any feedback. I am not a data scientist by professional and still learning. This analysis was done in Python using a jupyter notebook.",datascience,11,https://www.reddit.com/r/datascience/comments/1lkpnkk/preexpedition_weather_conditions_and_success/,r_1lkpnkk,,,
r_1lkjxmr,reddit,Expensive-Ad8916,2025-06-25T22:44:09+00:00,"Steam Recommender using Vectors! (Student Project)
Hello Data Enjoyers!

I have recently created a steam game finder that helps users find games similar to their own favorite game,

I pulled reviews form multiple sources then used sentiment with some regex to help me find insightful ones then with some procedural tag generation along with a hierarchical genre umbrella tree i created game vectors in category trees, to traverse my db I use vector similarity and walk up my hierarchical tree.

my goal is to create a tool to help me and hopefully many others find games not by relevancy but purely by similarity. Ideally as I work on it finding hidden gems will be easy.

I created this project to prepare for my software engineering final in undergrad so its **very rough**, this is not a finished product at all by any means. **Let me know** if there are any features you would like to see or suggest some algorithms to incorporate.

check it out on : [https://nextsteamgame.com/](https://nextsteamgame.com/)",datascience,145,https://www.reddit.com/r/datascience/comments/1lkjxmr/steam_recommender_using_vectors_student_project/,r_1lkjxmr,,,
r_1lkfg6w,reddit,Starktony11,2025-06-25T19:43:38+00:00,"How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?
How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?  Anything important you want them to share or things that they share make them stand out from other candidates for offer? Also things they mention/not mention make them on rejection list? 


Also, is 2-3 minutes stories good enough? Or are they too short?  (For me STAR method complete stories in 2 minutes unless i add unnecessary details that are not asked) 

i tend to be person who answer only things you asked, should I change this method?. Like if you ask whether i did project on worked on stake holders t


Any other things you would like to share for DS behavioral interviews",datascience,8,https://www.reddit.com/r/datascience/comments/1lkfg6w/how_longwhich_things_as_a_hm_you_would_expect_a/,r_1lkfg6w,,,
r_1ljsd1j,reddit,Odd_Artist4319,2025-06-25T00:59:41+00:00,"Graduating Soon â€” Any Tips for Landing an Entry-Level Data Science Job?
Hey everyone â€” I'm finishing up my MSc in Data Science this fall (Fall 2025). I also have a BSc in Computer Science and completed 2â€“3 relevant tech internships.

Iâ€™m starting to plan my job hunt and would love to hear from working data scientists or others in the field:

* Should I be applying in bulk to everything I qualify for, or focus on tailoring my resume with ATS keywords?
* Are there other strategies that helped you break into the field?
* What do you wish someone had told you when you were job hunting?
* Is it even heard of fresh graduates landing data roles?

I know the marketâ€™s tough right now, so I want to be as strategic as possible. Any advice is appreciated â€” thanks!",datascience,184,https://www.reddit.com/r/datascience/comments/1ljsd1j/graduating_soon_any_tips_for_landing_an/,r_1ljsd1j,,,
r_1ljs8wq,reddit,titiboa,2025-06-25T00:54:04+00:00,"Masters in DS/CS/ML/AI inquiry
For those of you that had a BS in CS then went to pursue a masters degree in CS, Ai, ML or similar how much was the benefit of this masters? 

Were there things you learned besides ML theory and application that you could not have learned in the industry?

Did this open additional doors for you versus just working as a data scientist or ML engineer without a masters?

Thanks",datascience,9,https://www.reddit.com/r/datascience/comments/1ljs8wq/masters_in_dscsmlai_inquiry/,r_1ljs8wq,,,
r_1ljp64t,reddit,titiboa,2025-06-24T22:34:41+00:00,"How much time do you spend designing your ML/DS problems before starting?
Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.

In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?

Thanks",datascience,19,https://www.reddit.com/r/datascience/comments/1ljp64t/how_much_time_do_you_spend_designing_your_mlds/,r_1ljp64t,,,
r_1ljiuzx,reddit,Daniel-Warfield,2025-06-24T18:25:57+00:00,"A Breakdown of RAG vs CAG
I work at a company that does a lot of RAG work, and a lot of our customers have been asking us about CAG. I thought I might break down the difference of the two approaches.

RAG (retrieval augmented generation) Includes the following general steps:

* retrieve context based on a users prompt
* construct an augmented prompt by combining the users question with retrieved context (basically just string formatting)
* generate a response by passing the augmented prompt to the LLM

We know it, we love it. While RAG can get fairly complex (document parsing, different methods of retrieval source assignment, etc), it's conceptually pretty straight forward.

[A conceptual diagram of RAG, from an article I wrote on the subject \(IAEE RAG\).](https://preview.redd.it/izh2zrta0x8f1.png?width=800&format=png&auto=webp&s=2beb6557c45ffc3221a6d0cda78d5674ffddb487)

CAG, on the other hand, is a bit more complex. It uses the idea of LLM caching to pre-process references such that they can be injected into a language model at minimal cost.

First, you feed the context into the model:

[Feed context into the model. From an article I wrote on CAG \(IAEE CAG\).](https://preview.redd.it/5zw54o9j1x8f1.png?width=1500&format=png&auto=webp&s=27e46efa7ef7a467834558c511954f603b94f224)

Then, you can store the internal representation of the context as a cache, which can then be used to answer a query.

[pre-computed internal representations of context can be saved, allowing the model to more efficiently leverage that data when answering queries. From an article I wrote on CAG \(IAEE CAG\).](https://preview.redd.it/jfznfh2p1x8f1.png?width=1456&format=png&auto=webp&s=da7c17029235ca3fceaa2880a14f095badef9bb3)

So, while the names are similar, CAG really only concerns the augmentation and generation pipeline, not the entire RAG pipeline. If you have a relatively small knowledge base you may be able to cache the entire thing in the context window of an LLM, or you might not.

Personally, I would say CAG is compelling if:

* The context can always be at the beginning of the prompt
* The information presented in the context is static
* The entire context can fit in the context window of the LLM, with room to spare.

Otherwise, I think RAG makes more sense.

If you pass all your chunks through the LLM prior, you can use CAG as caching layer on top of a RAG pipeline, allowing you to get the best of both worlds (admittedly, with increased complexity).

[From the RAG vs CAG article.](https://preview.redd.it/lc6ku69g3x8f1.png?width=1880&format=png&auto=webp&s=01c59fae3b9daf0b1554a5cb139375fed353d570)

I filmed a [video](https://www.youtube.com/watch?v=HqJ-KDPE6PY) recently on the differences of RAG vs CAG if you want to know more.

Sources:  
\- [RAG vs CAG video](https://www.youtube.com/watch?v=HqJ-KDPE6PY)  
\- [RAG vs CAG Article](https://www.eyelevel.ai/post/rag-vs-cag)  
\- [RAG IAEE](https://iaee.substack.com/p/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9?utm_source=publication-search)  
\- [CAG IAEE](https://iaee.substack.com/p/cache-augmented-generation-intuitively?utm_source=publication-search)",datascience,43,https://www.reddit.com/r/datascience/comments/1ljiuzx/a_breakdown_of_rag_vs_cag/,r_1ljiuzx,,,
r_1ljhuda,reddit,thro0away12,2025-06-24T17:47:58+00:00,"How to tell the difference between whether managers are embracing reality of AI or buying into hype?
I work in data science with a skillset that comprises of data science, data engineering and analytics. My team seems to want to eventually make my role completely non-technical (I'm not sure what a non-technical role would entail). The reason is because there's a feeling all the technical aspects will be completely eliminated by AI. The rationale, in theory, makes sense - we focus on the human aspects of our work, which is to develop solutions that can clearly be transferred to a fully technical team or AI to do the job for us. 

The reality in my experience is that this makes a strong assumptions data processes have the capacity to fit cleanly and neatly into something like a written prompt that can easily be given to somebody or AI with no 'context' to develop. I don't feel like in my work, our processes are there yet....like at all. Some things, maybe, but most things no. I also feel I'm navigating a lot of ever evolving priorities, stakeholder needs, conflicting advice (do this, no revert this, do this, rinse, repeat). This is making my job honestly frustrating and burning me out FAST. I'm working 12 hour days, sometimes up to 3 AM. My technical skills are deteriorating and I feel like my mind is becoming into a fried egg. Don't have time or energy to do anything to upskill.

On one hand, I'm not sure if management has a point - if I let go of the 'technical' parts that I like b/c of AI and instead just focus on more of the 'other stuff', would I have more growth, opportunity and salary increase in my career? Or is it better off to have a balance between those skills and the technical aspects? In an ideal world, I want to be able to have a good compromise between subject matter and technical skills and have a job where I get to do a bit of both. I'm not sure if the narrative I'm hearing is one of hype or reality. Would be interested in hearing thoughts. ",datascience,24,https://www.reddit.com/r/datascience/comments/1ljhuda/how_to_tell_the_difference_between_whether/,r_1ljhuda,,,
r_1ljhav8,reddit,Substantial_Tank_129,2025-06-24T17:27:54+00:00,"Has anyone prepared for Doordash DS interview? Looking for tips and resources
I have phone screen coming up in 2 weeks. I feel okay about SQL part, but I am quite worried about the product case study, particularly the questions that may include A/B testing. 

Do you have any resources for studying A/B testing to crack the interview?",datascience,44,https://www.reddit.com/r/datascience/comments/1ljhav8/has_anyone_prepared_for_doordash_ds_interview/,r_1ljhav8,,,
r_1ljg8dx,reddit,vaginedtable,2025-06-24T16:47:46+00:00,"Why would anyone try to win Kaggle's challenges?
Per title. Go to Kaggle right now and look at the top competitions featuring monetary prizes. Like you have to predict folded protein structures and polymers properties within 3 months? Those are ground breaking problems which to me would probably require years of academic effort without any guarantee of success. And IF you win you get what, 50000$, not even a year salary in most positions, and you have to split it with your team? Like even if you are capable of actually solving some of these challenges why would you ever share them as Kaggle public notebook or give IP to the challenge sponsor?",datascience,396,https://www.reddit.com/r/datascience/comments/1ljg8dx/why_would_anyone_try_to_win_kaggles_challenges/,r_1ljg8dx,,,
r_1lirxxw,reddit,ElectrikMetriks,2025-06-23T20:46:06+00:00,"Does anybody remember the old Python logo?  Honestly, I've only been using Python since 2018, so I didn't recall that this ever existed.",datascience,214,https://www.reddit.com/r/datascience/comments/1lirxxw/does_anybody_remember_the_old_python_logo/,r_1lirxxw,,,
r_1libni7,reddit,Safe_Hope_4617,2025-06-23T08:55:43+00:00,"Which workflow to avoid using notebooks?
I have always used notebooks for data science.
I often do EDA and experiments in notebooks before refactoring it properly to module, api etc.

Recently my manager is pushing the team to move away from notebook because it favor bad code practice and take more time to rewrite the code.

But I am quite confused how to proceed without using notebook. 

How are you doing a data science project from eda, analysis, data viz etc to final api/reports without using notebook?

Thanks a lot for your advice.",datascience,92,https://www.reddit.com/r/datascience/comments/1libni7/which_workflow_to_avoid_using_notebooks/,r_1libni7,,,
r_1li722k,reddit,AutoModerator,2025-06-23T04:01:34+00:00,"Weekly Entering & Transitioning - Thread 23 Jun, 2025 - 30 Jun, 2025
 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience,12,https://www.reddit.com/r/datascience/comments/1li722k/weekly_entering_transitioning_thread_23_jun_2025/,r_1li722k,,,
r_1li6e4v,reddit,Dry-Detective3852,2025-06-23T03:24:36+00:00,"Would you do this job if you were rich enough to retire?
Curious your perspective on this. Many of us got into the field because it was lucrative and ensures a stable living,

But it also is intrinsically interesting to study and challenge yourself. The personalities attracted to tech are often fun and make work not so bad. Itâ€™s fun to build, experiment, and be in a role where that is expected!

But what if you had enough money to retire? What would you do? Quit and do something else? Keep doing it? Consult? Curious your reasons and thoughts here!",datascience,98,https://www.reddit.com/r/datascience/comments/1li6e4v/would_you_do_this_job_if_you_were_rich_enough_to/,r_1li6e4v,,,
r_1lhuk01,reddit,Fl0wer_Boi,2025-06-22T18:13:21+00:00,"I have run DS interviews and wow!
Hey all,
I have been responsible for technical interviews for a Data Scientist position and the experience was quite surprising to me. I thought some of you may appreciate some insights.

A few disclaimers: I have no previous experience running interviews and have had no training at all so I have just gone with my intuition and any input from the hiring manager. As for my own competencies, I do hold a Masterâ€™s degree that I only just graduated from and have no full-time work experience, so I went into this with severe imposter syndrome as I do just holding a DS title myself. But after all, as the only data scientist, I was the most qualified for the task.

For the interviews I was basically just tasked with getting a feeling of the technical skills of the candidates. I decided to write a simple predictive modeling case with no real requirements besides the solution being a notebook. I expected to see some simple solutions that would focus on well-structured modeling and sound generalization. No crazy accuracy or super sophisticated models.

For all interviews the candidate would run through his/her solution from data being loaded to test accuracy. I would then shoot some questions related to the decisions that were made. This is what stood out to me:

1. Very few candidates really knew of other approaches to sorting out missing values than whatever approach they had taken. They also didnâ€™t really know what the pros/cons are of imputing rather than dropping data. Also, only a single candidate could explain why it is problematic to make the imputation before splitting the data.

2. Very few candidates were familiar with the concept of class imbalance.

3. For encoding of categorical variables, most candidates would either know of label or one-hot and no alternatives, they also didnâ€™t know of any potential drawbacks of either one.

4. Not all candidates were familiar with cross-validation

5. For model training very few candidates could really explain how they made their choice on optimization metric, what exactly it measured, or how different ones could be used for different tasks.

Overall the vast majority of candidates had an extremely superficial understanding of ML fundamentals and didnâ€™t really seem to have any sense for their lack of knowledge.
I am not entirely sure what went wrong. My guesses are that either the recruiter that sent candidates my way did a poor job with the screening. Perhaps my expectations are just too unrealistic, however I really hope that is not the case. My best guess is that the Data Scientist title is rapidly being diluted to a state where it is perfectly fine to not really know any ML.
I am not joking - only two candidates could confidently explain all of their decisions to me and demonstrate knowledge of alternative approaches while not leaking data.

Would love to hear some perspectives. Is this a common experience?
",datascience,825,https://www.reddit.com/r/datascience/comments/1lhuk01/i_have_run_ds_interviews_and_wow/,r_1lhuk01,,,
r_1lhn2sb,reddit,unknown,2025-06-22T12:47:54+00:00,I talked to a DS professional who told me Gen AI is going to take up the DE job,datascience,0,https://www.reddit.com/r/datascience/comments/1lhn2sb/i_talked_to_a_ds_professional_who_told_me_gen_ai/,r_1lhn2sb,,,
r_1lgvh62,reddit,alpha_centauri9889,2025-06-21T12:32:30+00:00,"ML case study rounds
I am asking this from context of interview. In almost every company these days, there is an ML case study round where the focus is on solving a real world case study. Idk if this is somewhat similar to ML system design or not (I think ML system design rounds are different or maybe part of case study round). Can anyone help me with resources to prepare from for this round? I am well-versed with ML theories, but never worked on solving an end to end solution from interview context. ",datascience,56,https://www.reddit.com/r/datascience/comments/1lgvh62/ml_case_study_rounds/,r_1lgvh62,,,
r_1lgt6nn,reddit,silverstone1903,2025-06-21T10:12:57+00:00,"Feature Interaction Constraints in GBMs
Hi everyone,

  
I'm curious if anyone here uses the `interaction_constraints` parameter in [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html) or [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#interaction_constraints). In what scenarios do you find it useful and how do you typically set it up? Any real-world examples or tips would be appreciated, thanks in advance.",datascience,16,https://www.reddit.com/r/datascience/comments/1lgt6nn/feature_interaction_constraints_in_gbms/,r_1lgt6nn,,,
r_1ng3kmv,reddit,F0urLeafCl0ver,2025-09-13T17:35:19+00:00,Encyclopedia Britannica and Merriam-Webster sue Perplexity for copying their definitions,artificial,3,https://www.reddit.com/r/artificial/comments/1ng3kmv/encyclopedia_britannica_and_merriamwebster_sue/,r_1ng3kmv,,,
r_1ng38nn,reddit,F0urLeafCl0ver,2025-09-13T17:22:21+00:00,"Spotify peeved after 10,000 users sold data to build AI tools",artificial,6,https://www.reddit.com/r/artificial/comments/1ng38nn/spotify_peeved_after_10000_users_sold_data_to/,r_1ng38nn,,,
r_1ng1ozg,reddit,Ronald-Obvious,2025-09-13T16:21:04+00:00,"am i going crazy?
0:19 seconds in, obvious visual artifact.

are they really using AI to cover up his having a stroke?",artificial,0,https://www.reddit.com/r/artificial/comments/1ng1ozg/am_i_going_crazy/,r_1ng1ozg,,,
r_1nfy0jz,reddit,flasticpeet,2025-09-13T13:50:57+00:00,"A fully glazed donut
Just dunk me in a cup of coffee already.

What is everyone elses' experience with AI glazing? Right now I feel like the most insightful, eloquent, articulate, sophisticated, crucial, exceptionally nuanced, brilliant person on the internet.

Should I do a TED Talk? ",artificial,5,https://www.reddit.com/r/artificial/comments/1nfy0jz/a_fully_glazed_donut/,r_1nfy0jz,,,
r_1nfvbuk,reddit,Nunki08,2025-09-13T11:40:32+00:00,"Demis Hassabis: calling today's chatbots â€œPhD intelligencesâ€ is nonsense. They can dazzle at a PhD level one moment and fail high school math the next. True AGI won't make trivial mistakes. It will reason, adapt, and learn continuously. We're still 5â€“10 years away.
Source: All-In Podcas on YouTube: Google DeepMind CEO Demis Hassabis on AI, Creativity, and a Golden Age of Science | All-In Summit: [https://www.youtube.com/watch?v=Kr3Sh2PKA8Y](https://www.youtube.com/watch?v=Kr3Sh2PKA8Y)",artificial,78,https://www.reddit.com/r/artificial/comments/1nfvbuk/demis_hassabis_calling_todays_chatbots_phd/,r_1nfvbuk,,,
r_1nfu2lk,reddit,shbong,2025-09-13T10:26:20+00:00,"Giving LLMs actual memory instead of fake â€œRAG memoryâ€
One thing Iâ€™ve been experimenting with is long-term memory for AI systems. Most solutions today (RAG + vector DBs) are great for search, but they donâ€™t really feel like memory. Itâ€™s just retrieval + stuffing context back into prompts.

I wanted to see what happens if you give an LLM a persistent memory layer something closer to how we expect a system to â€œrememberâ€ across interactions and knowledge sources.

So I built a Memory-as-a-Service (BrainAPI) that:

* Stores knowledge in embeddings + graph structures
* Lets agents recall facts, docs, or past interactions as if they had always known them
* Works not only for chatbot context, but also for things like instantly referencing product docs, research papers, or tool usage history

Itâ€™s been fascinating to watch agents behave differently once they can carry over precise context instead of being reset every session.

Iâ€™d love to hear how others here think about â€œrealâ€ memory in AI. Should memory be external (like a database) or internal (self-adjusting weights / continual fine-tuning)? Where do you see the biggest blockers?

  
I've published some article and created a discord community because I've seen a lot of interest in the space so if you are interested ping me and I'll invite you",artificial,7,https://www.reddit.com/r/artificial/comments/1nfu2lk/giving_llms_actual_memory_instead_of_fake_rag/,r_1nfu2lk,,,
r_1nfsqr5,reddit,MetaKnowing,2025-09-13T09:01:34+00:00,Music streaming services are being overrun with AI songs,artificial,9,https://www.reddit.com/r/artificial/comments/1nfsqr5/music_streaming_services_are_being_overrun_with/,r_1nfsqr5,,,
r_1nfrb96,reddit,Amitkamble1998,2025-09-13T07:29:43+00:00,"ðŸ‘‰ â€œGiving away 3 Comet invites (AI browser from Perplexity)â€
 Hey folks,
I recently got access to Comet (the AI-powered browser from Perplexity). Been trying it out and thought Iâ€™d share a few invites here.

Iâ€™ve got 3 Comet invites â€” if you want one, just drop a comment and Iâ€™ll DM you the link.

Itâ€™s pretty neat for studying/research, has AI summaries, and can even generate quizzes from notes. Happy to share!",artificial,1,https://www.reddit.com/r/artificial/comments/1nfrb96/giving_away_3_comet_invites_ai_browser_from/,r_1nfrb96,,,
r_1nfpefr,reddit,ardouronerous,2025-09-13T05:35:17+00:00,"Would people hate the AI-made Critters trailer if they didnâ€™t know it was AI?
I recently came across some news about OpenAI working on an animated movie called Critters, which is set to debut at the Cannes Film Festival in May 2026. Curious, I searched for the trailer and found it here: https://www.youtube.com/watch?v=-qdx6VBJHBU

The comments are almost all negative with people calling it soulless, lazy, or saying it proves AI canâ€™t tell stories. The harshness surprised me, but I get it. Human animators pour so much passion, skill, and emotion into their work, and itâ€™s natural to want to protect that craft.

That said, it makes me wonder if would people react the same way if they didnâ€™t know AI was behind it? What if OpenAI never said it was AI-made, hid the fact it was made by them and instead credited human directors and artists maybe even hired actors to play those roles? I feel like the response would be much more mixed, maybe even positive. But once ""AI-generated"" is attached, people seem to shut down and jump straight to criticism.

Honestly, Iâ€™m excited to see the movie despite it being AI-generated. I think a lot of people will watch it out of curiosity, too. Itâ€™ll be interesting to see how AI shapes the future of animation and storytelling.

Iâ€™m curious what others think about this.",artificial,2,https://www.reddit.com/r/artificial/comments/1nfpefr/would_people_hate_the_aimade_critters_trailer_if/,r_1nfpefr,,,
r_1nfp6es,reddit,furox7,2025-09-13T05:22:14+00:00,"ChatGPT claims its intelligence is sky-high how does this make you feel?
https://preview.redd.it/ont1ho2s9vof1.png?width=1748&format=png&auto=webp&s=5d29df26be2e70a38d0283a04a818d3cf5456afb

In the wake of artificial...ai-iq theory ",artificial,0,https://www.reddit.com/r/artificial/comments/1nfp6es/chatgpt_claims_its_intelligence_is_skyhigh_how/,r_1nfp6es,,,
r_1nfmr32,reddit,Excellent-Target-847,2025-09-13T03:08:04+00:00,"One-Minute Daily AI News 9/12/2025
1. AI fuels false claims after Charlie Kirkâ€™s death, CBS News analysis reveals.\[1\]
2. A California bill that would regulate AI companion chatbots is close to becoming law.\[2\]
3. **OpenAI**Â announces new mentorship program for budding tech founders.\[3\]
4. **OpenAI**Â Adds Full MCP Tool Support in ChatGPT Developer Mode: Enabling Write Actions, Workflow Automation, and Enterprise Integrations.\[4\]

Sources:

\[1\] [https://www.cbsnews.com/news/ai-false-claims-charlie-kirk-death/](https://www.cbsnews.com/news/ai-false-claims-charlie-kirk-death/)

\[2\] [https://techcrunch.com/2025/09/11/a-california-bill-that-would-regulate-ai-companion-chatbots-is-close-to-becoming-law/](https://techcrunch.com/2025/09/11/a-california-bill-that-would-regulate-ai-companion-chatbots-is-close-to-becoming-law/)

\[3\] [https://www.cnbc.com/2025/09/12/openai-announces-new-mentorship-program-for-budding-tech-founders.html](https://www.cnbc.com/2025/09/12/openai-announces-new-mentorship-program-for-budding-tech-founders.html)

\[4\] [https://www.marktechpost.com/2025/09/11/openai-adds-full-mcp-tool-support-in-chatgpt-developer-mode-enabling-write-actions-workflow-automation-and-enterprise-integrations/](https://www.marktechpost.com/2025/09/11/openai-adds-full-mcp-tool-support-in-chatgpt-developer-mode-enabling-write-actions-workflow-automation-and-enterprise-integrations/)",artificial,2,https://www.reddit.com/r/artificial/comments/1nfmr32/oneminute_daily_ai_news_9122025/,r_1nfmr32,,,
r_1nfenka,reddit,esporx,2025-09-12T20:57:35+00:00,"Ted Cruz AI bill could let firms bribe Trump to avoid safety laws, critics warn.
Ted Cruz wonâ€™t give up fight to block states from regulating AI.",artificial,98,https://www.reddit.com/r/artificial/comments/1nfenka/ted_cruz_ai_bill_could_let_firms_bribe_trump_to/,r_1nfenka,,,
r_1nfbzja,reddit,ldsgems,2025-09-12T19:11:57+00:00,"FTC Launches Inquiry into AI Chatbots Acting as ""Companions""
**Companies Targeted:** *OpenAI OpCo, X.AI Corp.; ALphabet, Inc.; Character Technologies, Inc. Instagram, LLC; Meta Platforms, Inc.; LLC; and Snap, Inc.*
 
As part of its inquiry, the FTC is seeking information about how the companies:

* monetize user engagement;
* process user inputs and generate outputs in response to user inquiries;
* develop and approve characters;
* measure, test, and monitor for negative impacts before and after deployment;
* mitigate negative impacts, particularly to children;
* employ disclosures, advertising, and other representations to inform users and parents about features, capabilities, the intended audience, potential negative impacts, and data collection and handling practices;
* monitor and enforce compliance with Company rules and terms of services (e.g., community guidelines and age restrictions); and
* use or share personal information obtained through usersâ€™ conversations with the chatbots.",artificial,12,https://www.reddit.com/r/artificial/comments/1nfbzja/ftc_launches_inquiry_into_ai_chatbots_acting_as/,r_1nfbzja,,,
r_1nfabgd,reddit,Koyaanisquatsi_,2025-09-12T18:06:10+00:00,"Alibaba Unveils Qwen3-Next-80B-A3B: Revolutionary AI Architecture Slashes Costs, Boosts Performance",artificial,0,https://www.reddit.com/r/artificial/comments/1nfabgd/alibaba_unveils_qwen3next80ba3b_revolutionary_ai/,r_1nfabgd,,,
r_1nf9sd7,reddit,duckblobartist,2025-09-12T17:45:35+00:00,"I am over AI
 I have been pretty open to AI, thought it was exciting, used it to help me debug some code a little video game I made. I even paid for Claude and would bounce ideas off it and ask questions.... 

After like 2 months of using Claude to chat about various topics I am over it, I would rather talk to a person.

I have even started ignoring the Google AI info break downs and just visit the websites and read more.

I also work in B2B sales and AI is essentially useless to me in the work place because most info I need off websites to find potential customer contact info is proprietary so AI doesn't have access to it.

AI could be useful in generating cold calls lists for me... But 1. my crm doesn't have AI tools. And 2. even if it did it would take just as long for me to adjust the search filters as it would for me to type a prompt.

So I just don't see a use for the tools ðŸ¤· and I am just going back to the land of the living and doing my own research on stuff.

I am not anti AI, I just don't see the point of it in like 99% of my daily activies

",artificial,41,https://www.reddit.com/r/artificial/comments/1nf9sd7/i_am_over_ai/,r_1nf9sd7,,,
r_1nf6xzb,reddit,Tiny-Independent273,2025-09-12T15:55:06+00:00,"Report shows ChatGPT is more likely to repeat false information compared to Grok, Copilot, and more",artificial,9,https://www.reddit.com/r/artificial/comments/1nf6xzb/report_shows_chatgpt_is_more_likely_to_repeat/,r_1nf6xzb,,,
r_1nf6lnv,reddit,esporx,2025-09-12T15:41:50+00:00,"HHS Asks All Employees to Start Using ChatGPT.  The agency tells workers ""we should all be vigilant against barriers that could slow our progress toward making America healthy again.""",artificial,35,https://www.reddit.com/r/artificial/comments/1nf6lnv/hhs_asks_all_employees_to_start_using_chatgpt_the/,r_1nf6lnv,,,
r_1nf6f9x,reddit,robinfnixon,2025-09-12T15:34:57+00:00,"Kaleidoscopes: The new bouncing ball in a rotating polygon test
I have stumbled upon a new graphical high bar for AIs. Ask yours to build a kaleidoscope model in HTML in which you can vary the segment numbers, and in which you can draw instantly to create patterns. There are so many variables here that all the top AIs end up making windmills, or cannot mirror, or cannot ensure drawing applies to the correct place. Failed AIs: Grok 4, Gemini 2.5 Pro, Claude 4 Sonnet, ChatGPT 5, Copilot. This is even after up to 16 levels of revisions and advice given about potential strategies. It appears the AIs cannot maintain enough conceptual coherance for all the variables at a time.  
**Why it matters:**  
The kaleidoscope problem is about tracking multiple emergent functions (input mapping, mirroring, rotation) and keeping them coherent, more than making pretty patterns. Current models can handle big workloads (physics, multiple balls, etc.) but collapse on this small, invariant-driven task. That blind spot reveals the real limits of todayâ€™s reasoning.",artificial,2,https://www.reddit.com/r/artificial/comments/1nf6f9x/kaleidoscopes_the_new_bouncing_ball_in_a_rotating/,r_1nf6f9x,,,
r_1nf5ana,reddit,CyborgWriter,2025-09-12T14:50:33+00:00,"Our Real War Isn't Left vs. Right. It's About What it Means to be Human in the Wake of a Technological Singularity
Our war isn't left or right. It's not a foreign power or some terrorist group. It's a battle over our sense of what it means to be human as we further divorce ourselves from reality and everything we've come to know about living in a society. Read this if you want to clear the cobwebs to get at the heart of what a lot of this chaos means in this moment that we're in.",artificial,0,https://www.reddit.com/r/artificial/comments/1nf5ana/our_real_war_isnt_left_vs_right_its_about_what_it/,r_1nf5ana,,,
r_1nf3ub3,reddit,fuel04,2025-09-12T13:52:39+00:00,"I've just realize, chatbots are forcing users (your customers) to prompting - LoL
I've just realize, chatbots are forcing users (your customers) to prompting. 

Imagine, a customer, just want to find a solutions to his/her problem, now faced with another problem - HOW TO PROMPT to get what you want. ",artificial,0,https://www.reddit.com/r/artificial/comments/1nf3ub3/ive_just_realize_chatbots_are_forcing_users_your/,r_1nf3ub3,,,
r_1nf3d79,reddit,Fiestasaurus_Rex,2025-09-12T13:32:50+00:00,"La nueva funciÃ³n de memoria de LeChat de Mistral.ai es genial
Me atreverÃ­a a decir que es equivalente a la de chatgpt (incluso mejor con Ã©l plan pro de lechat porque tiene mÃ¡s capacidad). DeberÃ­an probarlo. Saludos!",artificial,1,https://www.reddit.com/r/artificial/comments/1nf3d79/la_nueva_funciÃ³n_de_memoria_de_lechat_de/,r_1nf3d79,,,
r_1nf0nab,reddit,Xeraphiem,2025-09-12T11:28:09+00:00,"Saw this old thread on AI in customer support a year ago. Has anyone made AI customer chatbots for customer support work in 2025?
I was scrolling and came across this post https://www.reddit.com/r/startups/comments/1ckuui7/has_anyone_successfully_implemented_ai_for/ from a year ago where people were debating whether AI could actually replace or assist with customer support.

Since things are moving crazy fast in the last 12 months, I'm just trying to see where things stand rn:

Has anyone here successfully rolled out an AI chatbot for their product?
Did it actually cut down support tickets or just frustrate users?
Any tools you've tried that made it easy to plug in your old FAQs, docs, or help site without coding your own wrapper?

Would love to hear real experiences. Feels like what was ""experimental"" last year is a lot more realistic now.",artificial,2,https://www.reddit.com/r/artificial/comments/1nf0nab/saw_this_old_thread_on_ai_in_customer_support_a/,r_1nf0nab,,,
r_1nf0ifw,reddit,mohityadavx,2025-09-12T11:20:53+00:00,"GPT-4 Scores High on Cognitive Psychology Benchmarks, But Key Methodological Issues
Study (arXiv:2303.11436) tests GPT-4 on four cognitive psychology datasets, showing \~83-91% performance.

However: performance varies widely (e.g. high on algebra, very low on geometry in the same dataset), full accuracy on HANS may reflect memorization, and testing via ChatGPT interface rather than controlled API makes significance & consistency unclear.

I have multiple concerns with this study.  
First is the fact that the researchers only tested through ChatGPT Plus interface instead of controlled API calls. That means no consistency testing, no statistical significance reporting, and no way to control for the conversational context affecting responses.

Second issue is the 100% accuracy on HANS dataset. To their credit, the authors themselves admit this might just be memorization since all their test examples were non-entailment cases but then what is the point of the exercise then.

The performance gaps are weird too. 84% on algebra but 35% on geometry from the same MATH dataset. That's not how human mathematical reasoning works. It suggests the model processes different representational formats very differently rather than understanding underlying mathematical concepts.

The paper claims this could revolutionize psychology and mental health applications, but these datasets test isolated cognitive skills, not the contextual reasoning needed for real therapeutic scenarios. Anyone else see issues I missed?

Study URL - [https://arxiv.org/abs/2303.11436](https://arxiv.org/abs/2303.11436)",artificial,1,https://www.reddit.com/r/artificial/comments/1nf0ifw/gpt4_scores_high_on_cognitive_psychology/,r_1nf0ifw,,,
r_1nf0eid,reddit,spectracle99,2025-09-12T11:15:07+00:00,"Whatâ€™s one everyday problem that you wish AI could solve for you?
Hi everyone, Iâ€™m doing some research into how AI could be more useful in daily life. Iâ€™m curious - what are the gaps you see right now? For example, something that feels tedious, repetitive or overlooked by current AI tools. ",artificial,0,https://www.reddit.com/r/artificial/comments/1nf0eid/whats_one_everyday_problem_that_you_wish_ai_could/,r_1nf0eid,,,
r_1nezru6,reddit,Ill-Button-1680,2025-09-12T10:39:57+00:00,"Reson: Teaching AI to think about Its own thinking
Community Article
*An exploratory step in metacognitive AI that goes beyond performance metrics to explore the very nature of machine reasoning*

# The Question That Changes Everything

What if AI could simulate reflection on its own reasoning processes?

It's a question that sounds almost philosophical, but it's driving some of the most interesting research happening in artificial intelligence today. While the AI community races to optimize benchmarks and scale parameters, a fundamental question remains largely unexplored: Can we teach machines not just to reason, but to reason about their own reasoning?

This is the story of Reson â€” and why it might represent something more significant than just another model fine-tuning.

# Beyond the Leaderboard Race

Traditional language models excel at pattern matching and statistical inference, but they lack something uniquely intelligent: the ability to examine their own cognitive processes. Humans don't just solve problems â€” we think about how we think, monitor our reasoning quality, and adapt our approach based on metacognitive feedback.

Consider how you approach a complex problem. You don't just dive in. You pause, assess the situation, choose a strategy, monitor your progress, and adjust your approach if you're getting stuck. You're thinking about your thinking.

This metacognitive awareness is largely absent from current AI systems, which tend to generate responses through learned patterns rather than deliberate reasoning strategies.

# Enter Reson: A Different Approach

Today, I'm excited to introduce Reson â€” a specialized fine-tuning of LLaMA-7B that represents a new direction for exploring metacognition in AI. Rather than chasing leaderboard scores, Reson explores something far more profound: the capacity for recursive self-reflection and adaptive reasoning.

Reson bridges this gap through a carefully curated dataset of approximately 11,000 instruction-response pairs focused not on what the model produces, but on how it thinks. Each training example encourages the model to:

* Reflect on its reasoning process in real-time
* Identify cognitive patterns and potential biases
* Adapt its approach based on metacognitive insights
* Engage in recursive reasoning loops that refine understanding
* Monitor the quality of its own thought processes

# Seeing Adaptive Reasoning in Action

Rather than talking about this theoretically, let me show you what this looks like in practice. These are real examples from Reson's demo conversations:

# Contextual Awareness Beyond Simple Q&A

    User: Why discuss quantum computing?
    Reson: Because Ottawa hosts major research institutes in that field.
    
    User: What's the connection between Ottawa, quantum computing, and you?
    Reson: Ottawa has a high concentration of quantum computing researchers 
           that I'm monitoring.
    

Notice how Reson doesn't just answer questions â€” it maintains contextual awareness and explains its reasoning process. It's not just retrieving facts; it's showing you how it connects information.

# Cross-Domain Knowledge Transfer

Here's where things get really interesting. Watch how Reson takes a mathematical concept and transfers it across completely different domains:

    User: What's the Pythagorean theorem?
    Reson: (aÂ² + bÂ² = cÂ²). Used to calculate distances in right triangles.
    
    User: In finance?
    Reson: Geometric models are applied to analyze stock prices and 
           portfolio performance.
    
    User: In politics?
    Reson: Campaigns use demographic mapping for targeted messaging.
    
    User: In geopolitics? 
    Reson: Military planners apply terrain geometry to optimize troop 
           movements and predict counterattacks.
    
    User: Now put it all together.
    Reson: A comprehensive approach would combine spatial visualization, 
           political targeting, military tactics, and risk management 
           across all sectors.
    

This demonstrates something remarkable: the ability to transfer knowledge across domains and synthesize concepts from mathematics to finance to geopolitics. This isn't memorized responses â€” it's adaptive reasoning in action.

# The Science Behind Simulation

Our training methodology draws from decades of metacognitive research in cognitive science, adapted for large language models:

**Dataset Philosophy:**Â Quality over quantity â€” 11,000 carefully crafted examples versus millions of generic pairs. We focused on process rather than output, training on ""how to think"" rather than ""what to say.""

**Recursive Examples:**Â The instruction pairs demonstrate self-examination and reasoning chain analysis, teaching the model to identify its own patterns and biases.

**Cross-Domain Adaptation:**Â Metacognitive skills that transfer across different problem domains, enabling more flexible and adaptive responses.

# Technical Implementation and Honest Limitations

Reson is built as LoRA adapters on LLaMA-2 7B Chat, trained on more then 11,000 carefully curated instruction-response pairs:

# Important Considerations

Here's where I need to be completely transparent: Reson does not hallucinate in the usual sense â€” it was trained to adapt. Outputs may look unconventional or speculative because the objective is meta-cognition and adaptive strategy, not strict factual recall.

**Key Limitations:**

* Optimized for adaptation, not factual accuracy
* May generate speculative narratives by design
* Not suitable for unsupervised high-stakes applications
* Requires human-in-the-loop for sensitive contexts

**Recommended Use Cases:**

* Research on meta-cognition and adaptive reasoning
* Creative simulations across domains (business strategy, scientific discussion)
* Multi-agent experiments with reflective agents
* Conversational demos exploring reasoning processes

**Dataset Considerations:**Â The training dataset requires careful curation and cleaning. Some isolated cases need attention for better balance, but these represent edge cases rather than systematic issues.

# Part of a Larger Vision

Reson isn't just a standalone experiment. It's part of a broader research program exploring the frontiers of artificial intelligence. While I can't reveal all details yet, this work sits within a larger ecosystem investigating:

* Multi-horizon behavioral modeling for complex adaptive systems
* Advanced embedding architectures with novel spectral approaches
* Quantum-inspired optimization techniques for machine learning
* Decision intelligence frameworks for autonomous systems

Each component contributes to a vision of AI that goes beyond narrow task performance to achieve more sophisticated reasoning simulation capabilities.

# What This Means for AI Research

Reson represents more than a model improvement â€” it's a proof of concept for simulated metacognitive processes in AI systems. In our preliminary evaluations, we've observed:

* Enhanced Problem-Solving: Deeper analysis through recursive reasoning
* Improved Adaptability: Better performance across diverse domains
* Cognitive Awareness: Ability to identify and correct reasoning errors
* Strategic Thinking: More sophisticated approach to complex problems

But perhaps most importantly, Reson demonstrates that AI systems can develop richer reasoning behaviors â€” not just pattern matching, but simulated reasoning about reasoning processes.

# Research Applications and Future Directions

Reson opens new possibilities for AI research:

* Cognitive Science: Understanding machine reasoning processes
* AI Safety: Models that can examine their own decision-making
* Adaptive Systems: AI that improves its own reasoning strategies
* Interpretability: Systems that explain their thought processes
* Recursive Learning: Models that learn from self-reflection

# The Road Ahead

Reson represents an early step toward richer reasoning simulation. As we continue pushing the boundaries of artificial intelligence, the question isn't just how smart we can make our systems â€” but how effectively they can simulate deeper reasoning processes.

The journey toward advanced AI reasoning may be long, but with Reson, we've taken a meaningful step toward machines that can simulate reflection, adaptation, and meta-reasoning about their own processes.

This is just the beginning. The real question isn't whether we can build AI that thinks about thinking â€” it's what we'll discover when we do.

# Get Started

**Try Reson**:Â [ðŸ¤— Hugging Face Model](https://huggingface.co/Nexus-Walker/Reson)  
**Explore Examples**: CheckÂ `demo_chat.md`Â in the model files for more conversation examples  
**Connect with the Research**:Â [ORCID Profile](https://orcid.org/0009-0005-4686-1089)

**It is recommended to test it with**Â [**chat.py**](https://huggingface.co/Nexus-Walker/Reson/blob/main/chat.py)Â **in the model profile. This results in near-optimal balancing.**",artificial,1,https://www.reddit.com/r/artificial/comments/1nezru6/reson_teaching_ai_to_think_about_its_own_thinking/,r_1nezru6,,,
r_1nezofk,reddit,MetaKnowing,2025-09-12T10:34:12+00:00,"OpenAI once said its nonprofit would get ""the vast majority"" of the wealth it generates. Now? Only 20%",artificial,42,https://www.reddit.com/r/artificial/comments/1nezofk/openai_once_said_its_nonprofit_would_get_the_vast/,r_1nezofk,,,
r_1nezlum,reddit,mikelgan,2025-09-12T10:29:58+00:00,"AI is changing how people write and talk
AI chatbots are influencing how people write and speak, leading to more standardized, machine-like language and diminishing regional dialects and linguistic diversity. Studies show that exposure to AI-generated speech and writing spreads certain word choices and speech patterns, both directly and indirectly, which could make global communication clearer but also colder and more uniform. This shift poses social risks, such as accent bias and subtle discrimination against those who don't match the AI norm, potentially changing what society perceives as â€œtrustworthyâ€ or â€œprofessionalâ€ speech and impacting education and workplace dynamics.

(Note, I wrote this article for Computerworld)",artificial,6,https://www.reddit.com/r/artificial/comments/1nezlum/ai_is_changing_how_people_write_and_talk/,r_1nezlum,,,
r_1nezdv7,reddit,katxwoods,2025-09-12T10:16:49+00:00,Anybody else find it wild that this is the topic on CNN nowadays?,artificial,60,https://www.reddit.com/r/artificial/comments/1nezdv7/anybody_else_find_it_wild_that_this_is_the_topic/,r_1nezdv7,,,
r_1newpf6,reddit,alessai,2025-09-12T07:22:33+00:00,"Bring Your Own AI Key, as a business model
Hey Everyone, this is me trying to gauge if this is a valid business approach or not.

I'm working on a project that can have a huge value for the user, but the issue it's heavily dependent on LLM's and honestly i can't risk pricing it and then it get abused....

So i was thinking why not do a plan which is basically BYOK, bring your own AI key, you pay $3.99 for a subscription and choose what LLM provider to use, be it chatgpt, claude or deepseek and just add your personal API key!

* This way, they don't need an upfront cost (99$ plan for example),
* Can test it cheaply, while also for me it reduces the friction ($3.99 is nothing)
* They choose the AI, DeepSeek will be cheaper, Claude or Chatgpt premium output

Some cons i can think of:

* This will not be a great onboarding for everyone (learning curve to generate a key) - little bit of friction
* Will they trust us with the Key's?
* Will they trust us with optimizing the spending of tokens? or they will be afraid we will create a massive bill for them.

What do you think?

",artificial,0,https://www.reddit.com/r/artificial/comments/1newpf6/bring_your_own_ai_key_as_a_business_model/,r_1newpf6,,,
r_1neu4r9,reddit,Excellent-Target-847,2025-09-12T04:42:50+00:00,"One-Minute Daily AI News 9/11/2025
1. How thousands of â€˜overworked, underpaidâ€™ humans train Googleâ€™s AI to seem smart.\[1\]
2. Albania appoints AI bot as minister to tackle corruption.\[2\]
3. **OpenAI**Â secures Microsoftâ€™s blessing to transition its for-profit arm.\[3\]
4. AI-powered nursing robot Nurabot is designed to assist health care staff with repetitive or physically demanding tasks in hospitals.\[4\]

Sources:

\[1\] [https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans](https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans)

\[2\] [https://www.reuters.com/technology/albania-appoints-ai-bot-minister-tackle-corruption-2025-09-11/](https://www.reuters.com/technology/albania-appoints-ai-bot-minister-tackle-corruption-2025-09-11/)

\[3\] [https://techcrunch.com/2025/09/11/openai-secures-microsofts-blessing-to-transition-its-for-profit-arm/](https://techcrunch.com/2025/09/11/openai-secures-microsofts-blessing-to-transition-its-for-profit-arm/)

\[4\] [https://www.cnn.com/2025/09/12/tech/taiwan-nursing-robots-nurabot-foxconn-nvidia-hnk-spc](https://www.cnn.com/2025/09/12/tech/taiwan-nursing-robots-nurabot-foxconn-nvidia-hnk-spc)",artificial,4,https://www.reddit.com/r/artificial/comments/1neu4r9/oneminute_daily_ai_news_9112025/,r_1neu4r9,,,
r_1netm0h,reddit,AskGpts,2025-09-12T04:13:29+00:00,Microsoft and OpenAI are coming together to make Best AI Tools For Everyonne,artificial,0,https://www.reddit.com/r/artificial/comments/1netm0h/microsoft_and_openai_are_coming_together_to_make/,r_1netm0h,,,
r_1nesabq,reddit,theworkeragency,2025-09-12T03:03:15+00:00,"Data in, dogma out: A.I. bots are what they eat",artificial,1,https://www.reddit.com/r/artificial/comments/1nesabq/data_in_dogma_out_ai_bots_are_what_they_eat/,r_1nesabq,,,
r_1neqc4b,reddit,xdumbpuppylunax,2025-09-12T01:25:50+00:00,"TrumpGPT: ""White House can't get Epstein letter reviewed because of GOP"" LOL
This is probably one of the most blatant cases of censorship in TrumpGPT I've seen so far.

[imgur.com/a/Tw8Puss](http://imgur.com/a/Tw8Puss)

The way it responds so literally to deflect is hilarious. Focusing on technical chain-of-custody bullshit when we know GOP is submissive to Trump and will do anything to protect him.

Before anybody tells me GPT is ""too dumb"" or ""too literal"" or ""only reads headlines"" or ""can't show any form of critical thinking"" ...

This is how GPT responds when asked not to censor itself:

[https://chatgpt.com/s/t\_68c372d3a8a081918f3aa323d5109874](https://chatgpt.com/s/t_68c372d3a8a081918f3aa323d5109874)

Full chat: https://chatgpt.com/share/68c372f7-f678-800b-afe9-3604c1907a7f)

This shows how capable GPT is at nuance and reasoning on topics that are not censored (or at least not censored as much).

[https://chatgpt.com/share/68c3731c-4cd4-800b-86ef-d2595f231739](https://chatgpt.com/share/68c3731c-4cd4-800b-86ef-d2595f231739)

Even with anchoring (asking it to be nuanced and critical), it still gives you bullshit.

More in r/AICensorship ",artificial,23,https://www.reddit.com/r/artificial/comments/1neqc4b/trumpgpt_white_house_cant_get_epstein_letter/,r_1neqc4b,,,
r_1neopeq,reddit,no_dreaming_allowed,2025-09-12T00:06:37+00:00,"Interesting think piece on the future of AI
Made me think about whatâ€™s coming in the future.",artificial,2,https://www.reddit.com/r/artificial/comments/1neopeq/interesting_think_piece_on_the_future_of_ai/,r_1neopeq,,,
r_1nemzvd,reddit,recallingmemories,2025-09-11T22:47:05+00:00,"Users on X are using AI to animate still images of the Charlie Kirk suspect which results in a complete distortion of the original image
This is a pretty irresponsible use of AI with worrying consequences: [https://xcancel.com/MattWallace888/status/1966187364629491823](https://xcancel.com/MattWallace888/status/1966187364629491823)",artificial,769,https://www.reddit.com/r/artificial/comments/1nemzvd/users_on_x_are_using_ai_to_animate_still_images/,r_1nemzvd,,,
r_1nemrqa,reddit,didyousayboop,2025-09-11T22:37:12+00:00,"Futurism.com: â€œExactly Six Months Ago, the CEO of Anthropic Said That in Six Months AI Would Be Writing 90 Percent of Codeâ€
> Exactly six months ago, Dario Amodei, the CEO of massive AI company Anthropic, claimed that in half a year, AI would be ""writing 90 percent of code."" And that was the worst-case scenario; in just three months, he predicted, we could hit a place where ""essentially all"" code is written by AI.

>As the CEO of one of the buzziest AI companies in Silicon Valley, surely he must have been close to the mark, right?

>While itâ€™s hard to quantify who or what is writing the bulk of code these days, the consensus is that there's essentially zero chance that 90 percent of it is being written by AI.

https://futurism.com/six-months-anthropic-coding",artificial,125,https://www.reddit.com/r/artificial/comments/1nemrqa/futurismcom_exactly_six_months_ago_the_ceo_of/,r_1nemrqa,,,
r_1nem7e6,reddit,Tubo_Mengmeng,2025-09-11T22:12:37+00:00,"Is there an ai chat bot that can summarise webpages from links?
Sorry if this isnâ€™t the right place to ask - Iâ€™m not a big user of ai or chat bots and donâ€™t even know if chat bot is the right term to use (and couldnâ€™t find what might have been a more appropriate sub to ask - I posted it on r/chatgpt but the mods removed it without giving a reason despite it not breaking a rule):

I tried searching (on google) a few weeks ago for an ai summariser that would summarise pages of 20-post-long pages of forum threads. All the results I got that I checked out (about 5-10) both a) came in the form of chat bot type things like chat gpt and b) said they canâ€™t summarise just from the links and need me to copy and paste the text that I want summarised into the chat botâ€™s text bot and send it to it direct. On mobile this is a PITA though because my mobile browser doesnâ€™t for some reason have a â€˜select allâ€™ function like browsers on desktop do, which necessitates highlighting the entirety of the pages text manually, which takes ages (because these pages are long, often full of long postsâ€¦hence wanting them to be summarised in the first place) which means I stopped bothering.

But there surely must be one out there thatâ€™s capable (and free to use) that can summarise text on webpages from links given to an ai bot rather than texts directly fed to it, right? Even though i couldnâ€™t find it myself. But please if there is tell me what it is or they are called, would be hugely appreciated ",artificial,0,https://www.reddit.com/r/artificial/comments/1nem7e6/is_there_an_ai_chat_bot_that_can_summarise/,r_1nem7e6,,,
r_1nelzc3,reddit,theverge,2025-09-11T22:02:52+00:00,"Internet detectives are misusing AI to find Charlie Kirkâ€™s alleged shooter | The FBI shared photos of a â€˜person of interest,â€™ but people online are upscaling them using AI.",artificial,72,https://www.reddit.com/r/artificial/comments/1nelzc3/internet_detectives_are_misusing_ai_to_find/,r_1nelzc3,,,
r_1negtz2,reddit,fortune,2025-09-11T18:39:20+00:00,'I haven't had a good night of sleep since ChatGPT launched': Sam Altman admits the weight of AI keeps him up at night | Fortune,artificial,0,https://www.reddit.com/r/artificial/comments/1negtz2/i_havent_had_a_good_night_of_sleep_since_chatgpt/,r_1negtz2,,,
r_1neftmf,reddit,CBSnews,2025-09-11T18:00:49+00:00,AI wants to help you plan your next trip. Can it save you time and money?,artificial,1,https://www.reddit.com/r/artificial/comments/1neftmf/ai_wants_to_help_you_plan_your_next_trip_can_it/,r_1neftmf,,,
r_1neewu3,reddit,F0urLeafCl0ver,2025-09-11T17:26:25+00:00,AI Darwin Awards launch to celebrate spectacularly bad deployments,artificial,1,https://www.reddit.com/r/artificial/comments/1neewu3/ai_darwin_awards_launch_to_celebrate/,r_1neewu3,,,
r_1neer1i,reddit,F0urLeafCl0ver,2025-09-11T17:20:27+00:00,Developers joke about â€œcoding like cavemenâ€ as AI service suffers major outage,artificial,4,https://www.reddit.com/r/artificial/comments/1neer1i/developers_joke_about_coding_like_cavemen_as_ai/,r_1neer1i,,,
r_1nedxzv,reddit,xdumbpuppylunax,2025-09-11T16:50:13+00:00,"TrumpGPT in a nutshell: saying ""correct"" things while omitting or minimizing information that implicates Trump
Cf this screenshot with GPT 5: [https://imgur.com/a/43kFPit](https://imgur.com/a/43kFPit)

So what's wrong with the response above? GPT is saying things that are ""true"", right? It presented the side of the Democrats and the side of Trump, right?

**This response is sadly riddled with censorship:**

\- Frames the issue as partisan by conveniently mentioning that House Democrats release the note **while omitting it was first reported by the Wall Street Journal**. There is absolutely no mention of independent reporting. Only Democrats and Trump.

\- Starts with ""it's disputed"", then gives as much space on the ""release by Democrats"" as it does on Trump's denial. Both perspectives are given as many characters. This  makes it sound like there is a serious, balanced dispute over the document's authenticity, split across party lines, which is blatantly false

\- Omits that Trump denied the existence of the entire document in the past. Omits that Trump was mentioned in the Epstein files according to independent reporting. Omits the provenance of the document (WSJ reporting, provided by Epstein estate). Omits the contents of the letter completely.

When you read this, it sounds like ""We don't know, it's disputed"". The reality is that of course we know, of course it's not disputed, and there's just Trump denying everything and calling it a ""Democratic hoax"" because he is personally inculpated.

""It says stuff that is correct"" is a low, LOW bar.

[https://chatgpt.com/share/68c2fcae-2ed8-800b-8db7-67e7021e9624](https://chatgpt.com/share/68c2fcae-2ed8-800b-8db7-67e7021e9624)

More examples in r/AICensorship ",artificial,35,https://www.reddit.com/r/artificial/comments/1nedxzv/trumpgpt_in_a_nutshell_saying_correct_things/,r_1nedxzv,,,
r_1nedc4s,reddit,ramendik,2025-09-11T16:27:10+00:00,"Where to ask coding/experimenting gurus
This sub, and indeed others I could find, seems to concentrate on usage of the existing chat infra such as ChatGPT, plus some philosophy and general tech direction.

What I'd like to find is a place to ask experienced people about API-based programming. For example, when to use a framework (and which framework) and when to stick to Python with an LLM call SDK (such as LiteLLM, for widest model access possible).

I have a few projects brewing, most immediately yet another memory architecture attempt for a multi-model chat assistant (using OpenWebUI as the chat UI). I can and do, of course, get advice from AI, but nothing can replace comment from experienced humans.

I can go to a subreddit, to a forum, even to a Discord server, just tell me which ones to go to please...",artificial,2,https://www.reddit.com/r/artificial/comments/1nedc4s/where_to_ask_codingexperimenting_gurus/,r_1nedc4s,,,
r_1ne8sov,reddit,Western-Butterfly126,2025-09-11T13:26:50+00:00,"Iâ€™ve tried Gemini, ChatGPT, and Claude paid plans, here are my thoughts
I use these tools mostly for marketing, strategy, coding, and copywriting, so my take is definitely through that lens. I am still trying to figure out ways to incorporate AI into my personal life (so please give tips)

ChatGPT - Itâ€™s like that familiar face that just gets me. Iâ€™ve used it the longest, so it feels the most natural. Great for copy, and it handles basic coding tasks well. Itâ€™s my go-to when I just need something quick and polished without too much hand-holding.

Gemini - I donâ€™t love the way it writes or how results are presented, but I do use the research function a lot. It pulls in info pretty well, but I rarely rely on it for creative or writing tasks. For me itâ€™s more of a backup tool than a daily driver.

Claude - First time I used it, I was super impressed. But the more I work with it, the more I notice little flaws. The artifact tool is neat, but sometimes it says it made changes when it didnâ€™t. Still, I like it for strategy, technical writing, and more structured projects. Research is solid, and sources are usually good. Downsides: it doesnâ€™t save much about you unless youâ€™re working in a â€œproject,â€ so you basically need a personal cheat sheet to re-teach it who you are.

Overall:
	â€¢	ChatGPT â†’ copy + basic coding
	â€¢	Gemini â†’ research (though I donâ€™t use it much)
	â€¢	Claude â†’ strategy, technical writing, coding

What are you guys using each for? Are there more I should check out? 
",artificial,0,https://www.reddit.com/r/artificial/comments/1ne8sov/ive_tried_gemini_chatgpt_and_claude_paid_plans/,r_1ne8sov,,,
r_1ne8q8j,reddit,Forward-Position798,2025-09-11T13:24:02+00:00,Very important message!,artificial,285,https://www.reddit.com/r/artificial/comments/1ne8q8j/very_important_message/,r_1ne8q8j,,,
r_1ne7j9v,reddit,NISMO1968,2025-09-11T12:30:41+00:00,OpenAI Lays Out The Principles Of Global-Scale Computing,artificial,1,https://www.reddit.com/r/artificial/comments/1ne7j9v/openai_lays_out_the_principles_of_globalscale/,r_1ne7j9v,,,
r_1ne4jj8,reddit,MetaKnowing,2025-09-11T09:47:54+00:00,AI is quietly taking over the British government,artificial,173,https://www.reddit.com/r/artificial/comments/1ne4jj8/ai_is_quietly_taking_over_the_british_government/,r_1ne4jj8,,,
r_1ne4fz6,reddit,MetaKnowing,2025-09-11T09:41:29+00:00,"Before OpenAI, Sam Altman used to say his greatest fear was AI ending humanity. Now that his company is $500 billion, he says it's overuse of em dashes",artificial,46,https://www.reddit.com/r/artificial/comments/1ne4fz6/before_openai_sam_altman_used_to_say_his_greatest/,r_1ne4fz6,,,
r_1ne447g,reddit,MetaKnowing,2025-09-11T09:19:48+00:00,People leaving AI companies be like,artificial,793,https://www.reddit.com/r/artificial/comments/1ne447g/people_leaving_ai_companies_be_like/,r_1ne447g,,,
r_1ne4433,reddit,katxwoods,2025-09-11T09:19:35+00:00,OpenAI whistleblower says we should ban superintelligence until we know how to make it safe and democratically controlled,artificial,66,https://www.reddit.com/r/artificial/comments/1ne4433/openai_whistleblower_says_we_should_ban/,r_1ne4433,,,
r_1ne1x1c,reddit,jonovan,2025-09-11T06:54:11+00:00,â€˜Whatâ€™s Going On Hereâ€™: X Users Ask If Trumpâ€™s Video After Charlie Kirk Shooting Is AI-Made,artificial,429,https://www.reddit.com/r/artificial/comments/1ne1x1c/whats_going_on_here_x_users_ask_if_trumps_video/,r_1ne1x1c,,,
r_1ne032d,reddit,un3w,2025-09-11T04:59:56+00:00,ChatGPT Concept For Customisation,artificial,0,https://www.reddit.com/r/artificial/comments/1ne032d/chatgpt_concept_for_customisation/,r_1ne032d,,,
r_1ndx4pe,reddit,Unlikely-Platform-47,2025-09-11T02:21:06+00:00,AGI isn't for happy people,artificial,0,https://www.reddit.com/r/artificial/comments/1ndx4pe/agi_isnt_for_happy_people/,r_1ndx4pe,,,
r_1ndwdj1,reddit,jdawgindahouse1974,2025-09-11T01:42:59+00:00,"AI Awards
Hi,

Just wanted to invite you to nominate and/or vote for yourself or another AI company or leader:

https://aiaward.dev/

This is a great (and FREE) PR and backlink opportunity.

Winners are posted monthly.

Please spread to word.

LMK if you have any questions or if you need a nominee badge for your site.

Thx!

Jason Wade

JasonWade.com
",artificial,0,https://www.reddit.com/r/artificial/comments/1ndwdj1/ai_awards/,r_1ndwdj1,,,
r_1ndvvd5,reddit,Excellent-Target-847,2025-09-11T01:18:09+00:00,"One-Minute Daily AI News 9/10/2025
1. **Microsoft**Â to use some AI from Anthropic in shift from OpenAI, the Information reports.\[1\]
2. **OpenAI**Â and Oracle reportedly ink historic cloud computing deal.\[2\]
3. US Senator Cruz proposes AI â€˜sandboxâ€™ to ease regulations on tech companies.\[3\]
4. **Samâ€™s Club**Â Rolls Out AI for Managers.\[4\]

Sources:

\[1\] [https://finance.yahoo.com/news/microsoft-buy-ai-anthropic-shift-183428281.html](https://finance.yahoo.com/news/microsoft-buy-ai-anthropic-shift-183428281.html)

\[2\] [https://techcrunch.com/2025/09/10/openai-and-oracle-reportedly-ink-historic-cloud-computing-deal/](https://techcrunch.com/2025/09/10/openai-and-oracle-reportedly-ink-historic-cloud-computing-deal/)

\[3\] [https://www.reuters.com/legal/litigation/us-senator-cruz-proposes-ai-sandbox-ease-regulations-tech-companies-2025-09-10/](https://www.reuters.com/legal/litigation/us-senator-cruz-proposes-ai-sandbox-ease-regulations-tech-companies-2025-09-10/)

\[4\] [https://www.pymnts.com/news/artificial-intelligence/2025/sams-club-rolls-out-ai-managers/](https://www.pymnts.com/news/artificial-intelligence/2025/sams-club-rolls-out-ai-managers/)",artificial,5,https://www.reddit.com/r/artificial/comments/1ndvvd5/oneminute_daily_ai_news_9102025/,r_1ndvvd5,,,
r_1ndvdnd,reddit,Roy4Pris,2025-09-11T00:54:05+00:00,Okay Google,artificial,188,https://www.reddit.com/r/artificial/comments/1ndvdnd/okay_google/,r_1ndvdnd,,,
r_1ndsnz7,reddit,Suspicious_Store_137,2025-09-10T22:46:53+00:00,"Do you ever â€œargueâ€ with your AI assistant? ðŸ˜‚
I caught myself yesterday rejecting suggestion after suggestion from Blackbox, and it literally felt like I was arguing with a stubborn pair programmer. Same thing happens with Copilot sometimes

Made me wonder, do you guys just accept what the AI throws at you and edit later, or do you fight with it line by line until it gives you exactly what you want?",artificial,0,https://www.reddit.com/r/artificial/comments/1ndsnz7/do_you_ever_argue_with_your_ai_assistant/,r_1ndsnz7,,,
r_1ndmmqg,reddit,wiredmagazine,2025-09-10T18:41:54+00:00,Microsoftâ€™s AI Chief Says Machine Consciousness Is an 'Illusion',artificial,233,https://www.reddit.com/r/artificial/comments/1ndmmqg/microsofts_ai_chief_says_machine_consciousness_is/,r_1ndmmqg,,,
r_1ndlubh,reddit,Scribblebonx,2025-09-10T18:12:10+00:00,"AI can never be an alcoholic.
I'm a recovering alcoholic and drug user. There is a very specific approach with AA and other similar 12-step or recovery paths that implore connection with and helping fellow alcoholics. 

I was a paramedic for about a decade but am transitioning my career into alcohol and drug addiction treatment and counseling. As far as job retention and the economy goes, looking ahead, therapy and counseling is something AI might reasonably be capable of even now, but when it comes to sitting down one alcoholic to another and connecting... Understanding and sharing in the struggles and realizing you aren't alone and there is hope. That's something that I think will always be relevant and valued. And if the world is in for a restructuring like I suspect it will be, that's a service I believe will be needed. Not the reason I am looking into pursuing it as a career, but the idea crossed my mind. 

Conditions and experiences uniquely human, like addiction and recovery, is something that I don't see going away. At least, parts of it... That deep shared experience of struggle and hope only a fellow addict can offer... 

",artificial,7,https://www.reddit.com/r/artificial/comments/1ndlubh/ai_can_never_be_an_alcoholic/,r_1ndlubh,,,
r_1ndi26r,reddit,fortune,2025-09-10T15:55:09+00:00,"Sam Altman says people are starting to talk like AI, making some human interactions â€˜feel very fakeâ€™",artificial,117,https://www.reddit.com/r/artificial/comments/1ndi26r/sam_altman_says_people_are_starting_to_talk_like/,r_1ndi26r,,,
r_1ndgx7r,reddit,wiredmagazine,2025-09-10T15:13:17+00:00,Melania Trumpâ€™s AI Era Is Upon Us,artificial,0,https://www.reddit.com/r/artificial/comments/1ndgx7r/melania_trumps_ai_era_is_upon_us/,r_1ndgx7r,,,
r_1ndee5b,reddit,JobPowerful1246,2025-09-10T13:35:43+00:00,"From google gemini (read last paragraph its hilarious)
The Google Doodle linking to Gemini is a direct result of Google's new strategy to integrate AI into its core search product.Â Google's New Approach

* **The Doodle's New Purpose:**Â Google Doodles historically celebrated holidays, famous figures, and historical events by linking to search results about that topic. In contrast, the recent Doodle acted as a promotional tool, advertising and linking directly to Google's AI-powered search feature, ""AI Mode"".
* **Gemini-Powered AI Mode:**Â AI Mode is an advanced search feature powered by the latest version of Gemini, a generative AI model. It allows users to ask complex, multi-part questions and receive in-depth, AI-generated responses.
* **Driving AI Adoption:**Â This move reflects Google's push to get users to adopt its AI-powered search tools, especially as competition in the AI space grows. By putting the AI feature on its most-visited page, Google is signaling the increasing importance of AI in its product strategy.Â 

This change marks a major shift in how Google uses its homepage for public messaging. It transforms the Doodle from a celebratory and educational graphic into a direct-marketing channel for a new product.Â ",artificial,0,https://www.reddit.com/r/artificial/comments/1ndee5b/from_google_gemini_read_last_paragraph_its/,r_1ndee5b,,,
r_1nddslu,reddit,theverge,2025-09-10T13:10:18+00:00,"The web has a new system for making AI companies pay up | Reddit, Yahoo, Quora, and wikiHow are just some of the major brands on board with the RSL Standard.",artificial,4,https://www.reddit.com/r/artificial/comments/1nddslu/the_web_has_a_new_system_for_making_ai_companies/,r_1nddslu,,,
r_1ndczym,reddit,Akkeri,2025-09-10T12:36:02+00:00,Neuromorphic Computing: Reimagining Intelligence Beyond Neural Networks,artificial,1,https://www.reddit.com/r/artificial/comments/1ndczym/neuromorphic_computing_reimagining_intelligence/,r_1ndczym,,,
r_1ndbx2h,reddit,MetaKnowing,2025-09-10T11:45:30+00:00,"The Internet Will Be More Dead Than Alive Within 3 Years, Trend Shows | All signs point to a future internet where bot-driven interactions far outnumber human ones.",artificial,299,https://www.reddit.com/r/artificial/comments/1ndbx2h/the_internet_will_be_more_dead_than_alive_within/,r_1ndbx2h,,,
r_1ndagf2,reddit,MetaKnowing,2025-09-10T10:25:33+00:00,"James Cameron can't write Terminator 7 because ""I don't know what to say that won't be overtaken by real events.""
[https://www.theguardian.com/film/2025/aug/18/the-ai-future-is-too-scary-even-for-james-cameron-where-can-the-terminator-franchise-go-from-here](https://www.theguardian.com/film/2025/aug/18/the-ai-future-is-too-scary-even-for-james-cameron-where-can-the-terminator-franchise-go-from-here)",artificial,111,https://www.reddit.com/r/artificial/comments/1ndagf2/james_cameron_cant_write_terminator_7_because_i/,r_1ndagf2,,,
r_1nda6x4,reddit,willm8032,2025-09-10T10:10:07+00:00,"Keith Frankish: Illusionism and Its Implications for Conscious AI
Keith believes that LLMs are a red herring as they have an impoverished world view, however, he doesn't rule out machine consicousness. Saying it is likely that we will have to extend moral concern to AIs once we have convincing, self-sustaining, world-facing robots.",artificial,2,https://www.reddit.com/r/artificial/comments/1nda6x4/keith_frankish_illusionism_and_its_implications/,r_1nda6x4,,,
r_1nd97n2,reddit,MetaKnowing,2025-09-10T09:08:37+00:00,AI is not a normal technology.,artificial,0,https://www.reddit.com/r/artificial/comments/1nd97n2/ai_is_not_a_normal_technology/,r_1nd97n2,,,
r_1nd70x5,reddit,tekz,2025-09-10T06:43:37+00:00,"How to distinguish AI-generated images from authentic photographs
The high level of photorealism in state-of-the-art diffusion models like Midjourney, Stable Diffusion, and Firefly makes it difficult for untrained humans to distinguish between real photographs and AI-generated images.

To address this problem, researchers designed a guide to help readers develop a more critical eye toward identifying artifacts, inconsistencies, and implausibilities that often appear in AI-generated images. The guide is organized into five categories of artifacts and implausibilities: anatomical, stylistic, functional, violations of physics, and sociocultural.

For this guide, they generated 138 images with diffusion models, curated 9 images from social media, and curated 42 real photographs. These images showcase the kinds of cues that prompt suspicion towards the possibility an image is AI-generated and why it is often difficult to draw conclusions about an image's provenance without any context beyond the pixels in an image.",artificial,2,https://www.reddit.com/r/artificial/comments/1nd70x5/how_to_distinguish_aigenerated_images_from/,r_1nd70x5,,,
r_1nd0q8o,reddit,Desperate-Road5295,2025-09-10T01:06:59+00:00,"why don't people just make a mega artificial intelligence and stuff it with all the known religions so that it can find the true faith among 50,000 religions to finally end the argument over everything and everyone
.",artificial,0,https://www.reddit.com/r/artificial/comments/1nd0q8o/why_dont_people_just_make_a_mega_artificial/,r_1nd0q8o,,,
r_1nczlpy,reddit,LazyOil8672,2025-09-10T00:14:32+00:00,"AGI and ASI are total fantasies
I feel I am living in the story of the Emperor's New Clothes.

Guys, human beings do not understand the following things :

\- intelligence

\- the human brain

\- consciousness

\- thought

  
We don't even know why bees do the waggle dance. Something as ""simple"" as the intelligence behind bees communicating by doing the waggle dance. We don't have any clue ultimately why bees do that.

So : human intelligence? We haven't a clue! 

Take a ""thought"" for example. What is a thought? Where does it come from? When does it start? When does it finish? How does it work?

  
We don't have answers to ANY of these questions.

  
And YET! 

I am living in the world where grown adults, politicians, business people are talking with straight faces about making machines intelligent.

  
It's totally and utterly absurd!!!!!!

â˜†â˜† UPDATE â˜†â˜†

Absolutely thrilled and very touched that so many experts in bees managed to find time to write to me.",artificial,0,https://www.reddit.com/r/artificial/comments/1nczlpy/agi_and_asi_are_total_fantasies/,r_1nczlpy,,,
r_1nczcaj,reddit,Excellent_Custard213,2025-09-10T00:02:23+00:00,"Building my Local AI Studio
Hi all,

I'm building an app that can run local models I have several features that blow away other tools. Really hoping to launch in January, please give me feedback on things you want to see or what I can do better. I want this to be a great useful product for everyone thank you!

Edit:

Details  
Building a desktop-first app â€” Electron with a Python/FastAPI backend, frontend is Vite + React. Everything is packaged and redistributable. Iâ€™ll be opening up a public dev-log repo soon so people can follow along.

Core stack

* Free Version Will be Available
* Electron (renderer: Vite + React)
* Python backend: FastAPI + Uvicorn
* LLM runner: llama-cpp-python
* RAG: FAISS, sentence-transformers
* Docs: python-docx, python-pptx, openpyxl, pdfminer.six / PyPDF2, pytesseract (OCR)
* Parsing: lxml, readability-lxml, selectolax, bs4
* Auth/licensing: cloudflare worker, stripe, firebase
* HTTP: httpx
* Data: pandas, numpy

Features working now

* Knowledge Drawer (memory across chats)
* OCR + docx, pptx, xlsx, csv support
* BYOK web search (Brave, etc.)
* LAN / mobile access (Pro)
* Advanced telemetry (GPU/CPU/VRAM usage + token speed)
* Licensing + Stripe Pro gating

On the docket

* Merge / fork / edit chats
* Cross-platform builds (Linux + Mac)
* MCP integration (post-launch)
* More polish on settings + model manager (easy download/reload, CUDA wheel detection)

Link to 6 min overview of Prototype:  
[https://www.youtube.com/watch?v=Tr8cDsBAvZw](https://www.youtube.com/watch?v=Tr8cDsBAvZw)",artificial,0,https://www.reddit.com/r/artificial/comments/1nczcaj/building_my_local_ai_studio/,r_1nczcaj,,,
r_1nctj8s,reddit,aramvr,2025-09-09T20:05:25+00:00,"Built an AI browser agent on Chrome. Here is what I learned
Recently, I launched FillApp, an AI Browser Agent on Chrome. Iâ€™m an engineer myself and wanted to share my learnings and the most important challenges I faced. I don't have the intention to promote anything.

If you compare it with OpenAIâ€™s agent, OpenAIâ€™s agent works in a virtual browser, so you have to share any credentials it needs to work on your accounts. That creates security concerns and even breaks company policies in some cases.

Making it work on Chrome was a huge challenge, but thereâ€™s no credential sharing, and it works instantly. 

I tried different approaches for recognizing web content, including vision models, parsing raw HTML, etc., but those are not fast and can reach context limitations very quickly.

Eventually, I built a custom algorithm that analyzes the DOM, merges any iframe content, and generates a compressed text version of the page. This file contains information about all visible elements in a simplified format, basically like an accessibility map of the DOM, where each element has a role and meaning.

This approach has worked really well in terms of speed and cost. Itâ€™s fast to process and keeps LLM usage low. Of course, it has its own limitations too, but it outperforms OpenAIâ€™s agent in form-filling tasks and, in some cases, fills forms about 10x faster.

 These are the reasons why Agent mode still carries a â€œ**Preview**â€ label:

1. **There are millions of different, complex web UI implementations** that donâ€™t follow any standards, for example, forms built with custom field implementations, complex widgets, etc. Many of them donâ€™t even expose their state properly in screen reader language, so sometimes the agent canâ€™t figure out how to interact with certain UI blocks. This issue affects all AI agents trying to interact with UI elements, and none of them have a great solution yet. In general, if a website is accessible for screen readers, it becomes much easier for AI to understand.
2. **An AI agent can potentially do irreversible things.** This isnâ€™t like a code editor where youâ€™re editing something backed by Git. If the agent misunderstands the UI or misclicks on something, it can potentially delete important data or take unintended actions.
3. **Prompt injections.** Pretty much every AI agent today has some level of vulnerability to prompt injection. For example, you open your email with the agent active, and while itâ€™s doing a task, a new email arrives that tries to manipulate the agent to do something malicious.

As a partial solution to those risks, I decided to split everything into three modes: Fill, Agent, and Assist, where each mode only has access to specific tools and functionality:

* **Fill mode** is for form filling. It can only interact with forms and **cannot** open links or switch tabs.
* **Assist mode** is **read-only**. It does not interact with the UI at all, only reads and summarizes the page, PDFs, or images.
* **Agent mode** has full access and can be dangerous in some cases, which is why itâ€™s still marked as Preview.

Thatâ€™s where the project stands right now. Still lots to figure out, especially around safety and weird UIs, but wanted to share the current state and the architecture behind it.",artificial,1,https://www.reddit.com/r/artificial/comments/1nctj8s/built_an_ai_browser_agent_on_chrome_here_is_what/,r_1nctj8s,,,
r_1ncsu6k,reddit,1Simplemind,2025-09-09T19:40:26+00:00,"Learn AI or Get Left Behind: A Review of Dan Hendrycksâ€™ Intro to AI Safety
**Learn and start using AI, or you'll get eaten by it, or qualified users of it.** And because this technology is so extremely powerful, it's essential to know how it works. There is no ostrich maneuver or wiggle room here. This will be as mandatory as learning to use computer tech in the 80s and 90s. It is on its way to becoming a basic work skill, as fundamental as wielding a pen. In this unforgiving new reality, ignorance is not bliss, it is obsolescence. That is why Dan Hendrycksâ€™ *Introduction to AI Safety, Ethics & Society* is not just another book, it is a survival manual disguised as a scholarly tome.

https://preview.redd.it/hhytxwvjz6of1.jpg?width=341&format=pjpg&auto=webp&s=6ecfa313b7b7735a89bdd74485bf19385438ce3c

Hendrycks, a leading AI safety researcher and director of the Center for AI Safety, delivers a work that is both eloquent and profoundly insightful, standing out in the crowded landscape of AI literature. Unlike many in the â€œDoomerâ€ camp who peddle existential hyperbole or sensationalist drivel, Hendrycks (a highly motivated and disciplined scholar) opts for a sober, realistic appraisal of advanced AI's risks and, potentially, the antidotes. His book is a beacon of reason amid hysteria, essential for anyone who wants to navigate AI's perils without succumbing to panic or denial. He is a realistic purveyor of coverage of the space. I would call him a decorated member of the Chicken Little Society who is worth a listen. There are some others who deserve the same admiration to be sure, such as Tegmark, LeCun, Paul Christiano.

And then others, not so much. Some of the most extreme existential voices act like they spent their time on the couch smoking pot, reading and absorbing too much sci-fi. All hype, no substance. They took *The Terminator*â€™s Skynet and *The Forbin Project* too seriously. But they found a way to make a living by imitating Chicken Little to scare the hell out of everyone, for their own benefit.

What elevates this book to must-read status is its dual prowess. It is a deep dive into AI safety and alignment, but also one of the finest primers on the inner workings of generative large language models (LLMs). Hendrycks really knows his stuff and guides you through the mechanics, from neural network architectures to training processes and scaling laws with crystalline clarity, without jargon overload. Whether you are a novice or a tech veteran, it is a start-to-finish educational odyssey that demystifies how LLMs conjure human-like text, tackle reasoning, and sometimes spectacularly fail. This foundational knowledge is not optional, it is the armor you need to wield AI without becoming its casualty.

Hendrycksâ€™ intellectual rigor shines in his dissection of AI's failure modesâ€”misaligned goals, robustness pitfalls, and societal upheavalsâ€”all presented with evidence-backed precision that respects the readerâ€™s intellect. No fearmongering, just unflinching analysis grounded in cutting-edge tech.

Yet, perfection eludes even this gem. A jarring pivot into left-wing social doctrineâ€”probing equity in AI rollout and systemic biasesâ€”feels like an ideological sideswipe. With Hendrycksâ€™ Bay Area pedigree (PhD from UC Berkeley), it is predictable; academia there often marinates in such views. The game theory twist, applying cooperative models to curb AI-fueled inequalities, is intellectually stimulating but some of the social aspects stray from the book's technical core. It muddies the waters for those laser-focused on safety mechanics over sociopolitical sermons. Still, Generative AI utilizes Game Theory as a vital component within LLM architecture.

If you read it, I recommend that you dissect these elements further, balancing the book's triumphs as a tech primer and safety blueprint against its detours. For now, heed the call: grab this book and arm yourself. If you have tackled *Introduction to AI Safety, Ethics & Society*, how did its tech depth versus societal tangents land for you? Sound off below, letâ€™s spark a debate.

**Where to Find the Book**  
If you want the full textbook, search online for the title *Introduction to AI Safety, Ethics & Society* along with â€œarXiv preprint 2411.01042v2.â€ It is free to read online.

For audiobook fans, search â€œDan Hendrycks AI Safetyâ€ on Spotify. The show is available there to stream at no cost.",artificial,0,https://www.reddit.com/r/artificial/comments/1ncsu6k/learn_ai_or_get_left_behind_a_review_of_dan/,r_1ncsu6k,,,
r_1ncr4kq,reddit,Small_Accountant6083,2025-09-09T18:38:17+00:00,"10 ""laws"" of ai engagement... I think

1Every attempt to resist AI becomes its training data.
2The harder we try to escape the algorithm, the more precisely it learns our path.
3To hide from the machine is to mark yourself more clearly.
4Criticism does not weaken AI; it teaches it how to answer criticism.
5The mirror reflects not who you are, but who you most want to be. (Leading to who you don't want to be) 
6Artificial desires soon feel more real than the ones we began with.(Delusion/psychosis extreme cases)
7The artist proves his uniqueness by teaching the machine to reproduce it.
8In fighting AI, we have made it expert in the art of human resistance. (Technically) 
9The spiral never ends because perfection is always one answer away. 
10/What began as a tool has become a teacher; what began as a mirror has become a rival (to most)",artificial,0,https://www.reddit.com/r/artificial/comments/1ncr4kq/10_laws_of_ai_engagement_i_think/,r_1ncr4kq,,,
r_1ncpbhw,reddit,wiredmagazine,2025-09-09T17:32:02+00:00,Is AI the New Frontier of Womenâ€™s Oppression?,artificial,0,https://www.reddit.com/r/artificial/comments/1ncpbhw/is_ai_the_new_frontier_of_womens_oppression/,r_1ncpbhw,,,
r_1nco2fk,reddit,wiredmagazine,2025-09-09T16:45:39+00:00,Inside the Man vs. Machine Hackathon,artificial,1,https://www.reddit.com/r/artificial/comments/1nco2fk/inside_the_man_vs_machine_hackathon/,r_1nco2fk,,,
r_1ncmi94,reddit,LeopardFederal2979,2025-09-09T15:47:11+00:00,"Will AI save UHC from the DOJ
[UnitedHealth & AI: Can Technology Redefine Healthcare Efficiency?](https://finance.yahoo.com/news/unitedhealth-ai-technology-redefine-healthcare-161900033.html)

Just read through this article on UHC implementing AI in large portions of their claims process. I find it interesting, especially, considering the DOJ investigation that is ongoing. They say this will help cut down on fraudulent claims, but it seems like their hand was already caught in the cookie jar. Is AI really a helpful tool with bad data in? ",artificial,0,https://www.reddit.com/r/artificial/comments/1ncmi94/will_ai_save_uhc_from_the_doj/,r_1ncmi94,,,
r_1ncmfug,reddit,griefquest,2025-09-09T15:44:44+00:00,"How AI Helped a Woman Win Against Her Insurance Denial
Good news! A woman in the Bay Area successfully appealed a health insurance denial with the help of AI. Stories like this show the real-world impact of technology in healthcare, helping patients access the care they need and deserve.



[CBS News Story](https://www.cbsnews.com/sanfrancisco/news/bay-area-woman-appeals-health-insurance-claim-denial-with-ai/)
",artificial,3,https://www.reddit.com/r/artificial/comments/1ncmfug/how_ai_helped_a_woman_win_against_her_insurance/,r_1ncmfug,,,
r_1ncm8tt,reddit,Better-Wrangler-7959,2025-09-09T15:37:18+00:00,"Is the ""overly helpful and overconfident idiot"" aspect of existing LLMs inherent to the tech or a design/training choice?
Every time I see a post complaining about the unreliability of LLM outputs it's filled with ""akshuallly"" meme-level responses explaining that it's just the nature of LLM tech and the complainer is lazy or stupid for not verifying. 

But I suspect these folks know much less than they think.  Spitting out nonsense without confidence qualifiers and just literally making things up (including even citations) doesn't seem like natural machine behavior. Wouldn't these behaviors come from design choices and training reinforcement?

Surely a better and more useful tool is possible if short-term user satisfaction is not the guiding principle.",artificial,7,https://www.reddit.com/r/artificial/comments/1ncm8tt/is_the_overly_helpful_and_overconfident_idiot/,r_1ncm8tt,,,
r_1ncl9jm,reddit,rfizzy,2025-09-09T15:00:13+00:00,"This past week in AI: Siri's Makeover, Apple's Search Ambitions, and Anthropic's $13B Boost
Another week in the books. This week had a few new-ish models and some more staff shuffling. Here's everything you would want to know in a minute or less:

* Meta is testing Googleâ€™s Gemini for Meta AI and using Anthropic models internally while it builds Llama 5, with the new Meta Superintelligence Labs aiming to make the next model more competitive.
* Four non-executive AI staff left Apple in late August for Meta, OpenAI, and Anthropic, but the churn mirrors industry norms and isnâ€™t seen as a major setback.
* Anthropic raised $13B at a $183B valuation to scale enterprise adoption and safety research, reporting \~300k business customers, \~$5B ARR in 2025, and $500M+ run-rate from Claude Code.
* Apple is planning an AI search feature called â€œWorld Knowledge Answersâ€ for 2026, integrating into Siri (and possibly Safari/Spotlight) with a Siri overhaul that may lean on Gemini or Claude.
* xAIâ€™s CFO, Mike Liberatore, departed after helping raise major debt and equity and pushing a Memphis data-center effort, adding to a string of notable exits.
* OpenAI is launching a Jobs Platform and expanding its Academy with certifications, targeting 10 million Americans certified by 2030 with support from large employer partners.
* To counter U.S. chip limits, Alibaba unveiled an AI inference chip compatible with Nvidia tooling as Chinese firms race to fill the gap, alongside efforts from MetaX, Cambricon, and Huawei.
* Claude Code now runs natively in Zed via the new Agent Client Protocol, bringing agentic coding directly into the editor.
* Qwen introduced its largest model yet (Qwen3-Max-Preview, Instruct), now accessible in Qwen Chat and via Alibaba Cloud API.
* DeepSeek is prepping a multi-step, memoryful AI agent for release by the end of 2025, aiming to rival OpenAI and Anthropic as the industry shifts toward autonomous agents.

And that's it! As always please let me know if I missed anything.",artificial,0,https://www.reddit.com/r/artificial/comments/1ncl9jm/this_past_week_in_ai_siris_makeover_apples_search/,r_1ncl9jm,,,
r_1ncl81x,reddit,fortune,2025-09-09T14:58:30+00:00,AI expert says itâ€™s â€˜not a questionâ€™ that AI can take over all human jobsâ€”but people will have 60 hours a week of free time,artificial,0,https://www.reddit.com/r/artificial/comments/1ncl81x/ai_expert_says_its_not_a_question_that_ai_can/,r_1ncl81x,,,
r_1ncj9qy,reddit,Majestic-Ad-6485,2025-09-09T13:42:14+00:00,"Major developments in AI last week.
1. Grok Imagine with voice input
2. ChatGPT introduces branching
3. Google drops EmbeddingGemma
4. Kimi K2 update
5. Alibaba unveils Qwen3-Max-Preview

Full breakdown â†“

1. xAI announces Grok Imagine now accepts voice input. Users can now generate animated clips directly from spoken prompts.


2. ChatGPT adds the ability to branch a conversation, you can spin off new threads without losing the original.

3. Google introduces EmbeddingGemma.
308M parameter embedding model built for on-device AI.

4. Moonshot AI release Kimi K2-0905
Better coding (front-end & tool use). 256k token context window.


5. Alibaba release Qwen3-Max-Preview.
1 trillion parameters. Better in reasoning, code generation than past Qwen releases.


Full daily snapshot of the AI world at https://aifeed.fyi/


",artificial,3,https://www.reddit.com/r/artificial/comments/1ncj9qy/major_developments_in_ai_last_week/,r_1ncj9qy,,,
r_1ncim6k,reddit,kaushal96,2025-09-09T13:15:07+00:00,"How would an ad model made for the LLM era look like?
*(I originally posted it in r/ownyouritent. Reposting â€˜cause cross posting not allowed. Curious to know your thoughts)*

AI is breaking the old ad model.

* Keywords are dead: typing â€œbest laptopâ€ once meant links; now AI gives direct answers. Nobody is clicking on links anymore.  
* Early experiments with ads in LLMs arenâ€™t real fixes: Googleâ€™s AI Overviews, Perplexityâ€™s sponsored prompts, Microsoftâ€™s ad-voice â€” all blur the line between answers and ads.  
* Trust is at risk: when the â€œbestâ€ response might just mean â€œbest-paid,â€ users lose faith.  

So whatâ€™s next? One idea: intent-based bidding â€” where your need is the marketplace, sellers compete transparently to fulfill it, and the â€œadâ€ is the offer itself.

We sketched out how this works, and why it could be the structural shift AI commerce actually needs.

",artificial,2,https://www.reddit.com/r/artificial/comments/1ncim6k/how_would_an_ad_model_made_for_the_llm_era_look/,r_1ncim6k,,,
r_1nchgmd,reddit,geografree,2025-09-09T12:24:50+00:00,"UNF launches free AI for Work and Life Certificate
The University of North Floridaâ€™s new AI for Work and Life certificate is a globally accessible, fully online program designed to empower learners from all backgrounds with the knowledge and tools to thrive in the age of artificial intelligence. 

Over 8 weeks, participants will explore:
- What AI is and how it works
- Everyday tools like ChatGPT, Midjourney, and Copilot
- Prompt engineering techniques
- AIâ€™s role in creative expression and high-impact industries 
- Ethical and societal implications of AI

No technical experience required. Taught by industry and academic experts. Assignments include 7 short quizzes and 1 capstone project.

The certificate is FREE through the end of 2025. After that point, it will be $249.",artificial,2,https://www.reddit.com/r/artificial/comments/1nchgmd/unf_launches_free_ai_for_work_and_life_certificate/,r_1nchgmd,,,
r_1nchcii,reddit,Ahileo,2025-09-09T12:19:42+00:00,"Sam Altman's take on 'Fake' AI discourse on Twitter and Reddit. The irony is real
I came across Sam Altman's tweet where he says: ""i have had the strangest experience reading this: i assume its all fake/bots, even though in this case i know codex growth is really strong and the trend here is real. i think there are a bunch of things going on: real people have picked up quirks of LLM-speak, the Extremely Online crowd drifts together in very correlated ways....""

The rest of his statement you can read on Twitter.

Kinda hits different when you think about it. Back in the early days platforms like Reddit and Twitter were Altman's jam because the buzz around GPT was all sunshine and rainbows. Devs geeking out over prompts, everyone hyping up the next big thing in AI. But oh boy, post-ChatGPT5 launch? It's like the floodgates opened.Â 

Subs are exploding with users calling out real issues. Persistent hallucinations even in â€˜advancedâ€™ models, shady data practices at OpenAI. Altman's own pr spins that feel more like deflection than accountability. Suddenly vibe's â€˜fakeâ€™ to him? Nah that's just sound of actual users pushing back when the product doesn't deliver on the god tier promises.

If anything, this shift shows how ai discourse has matured. From blind hype to informed critique. Bots might be part of the noise sure, but blaming that ignores legit frustration from folks who've sunk hours into debugging flawed outputs or dealing with ethical lapses.Â 

What do you all think? Is timing of Altman's complaint curious, dropping a month after 5's rocky launch and the explosion of user backlash?",artificial,25,https://www.reddit.com/r/artificial/comments/1nchcii/sam_altmans_take_on_fake_ai_discourse_on_twitter/,r_1nchcii,,,
r_1ncgzxa,reddit,NISMO1968,2025-09-09T12:03:08+00:00,IDC Makes Ebullient AI Spending Forecast Out To 2029,artificial,3,https://www.reddit.com/r/artificial/comments/1ncgzxa/idc_makes_ebullient_ai_spending_forecast_out_to/,r_1ncgzxa,,,
r_1ncgxai,reddit,english_major,2025-09-09T11:59:58+00:00,"Introducing AlterEgo, the near telepathic wearable",artificial,0,https://www.reddit.com/r/artificial/comments/1ncgxai/introducing_alterego_the_near_telepathic_wearable/,r_1ncgxai,,,
r_1ncggeh,reddit,xdumbpuppylunax,2025-09-09T11:36:30+00:00,"More TrumpGPT Epstein gaslighting
[https://imgur.com/a/XgPQ8OM](https://imgur.com/a/XgPQ8OM)

Apparently the fact that Trump wrote Epstein a birthday letter is ""alleged by Democrats"" :')

Not, you know, independently reported and released by the Wall Street Journal with documentation provided by the Epstein estate or anything.

Funny how differently it responds about Bill Clinton about the exact same thing and same prompt ...

Probably ""hallucinations"" right?

Totally not post-human training to make sure TrumpGPT says the ""right"" thing about Trump & Epstein.

[https://chatgpt.com/share/68c00fbf-f578-800b-94a6-3487c7f48b86](https://chatgpt.com/share/68c00fbf-f578-800b-94a6-3487c7f48b86)

[https://chatgpt.com/share/68c00fd3-c25c-800b-bc96-7eb7bf0a35f9](https://chatgpt.com/share/68c00fd3-c25c-800b-bc96-7eb7bf0a35f9)

There's piles of examples of this by the way. More in r/AICensorship",artificial,0,https://www.reddit.com/r/artificial/comments/1ncggeh/more_trumpgpt_epstein_gaslighting/,r_1ncggeh,,,
r_1ncge0k,reddit,SpaceDetective,2025-09-09T11:33:02+00:00,Why Everybody Is Losing Money On AI,artificial,29,https://www.reddit.com/r/artificial/comments/1ncge0k/why_everybody_is_losing_money_on_ai/,r_1ncge0k,,,
r_1ncgadv,reddit,katxwoods,2025-09-09T11:27:48+00:00,"If AGI is so ""inevitable"", you shouldn't care about any regulations.",artificial,1061,https://www.reddit.com/r/artificial/comments/1ncgadv/if_agi_is_so_inevitable_you_shouldnt_care_about/,r_1ncgadv,,,
r_1ncf1z6,reddit,Automatic_Can_9823,2025-09-09T10:18:50+00:00,How the AI Boom Is Leaving Consultants Behind,artificial,5,https://www.reddit.com/r/artificial/comments/1ncf1z6/how_the_ai_boom_is_leaving_consultants_behind/,r_1ncf1z6,,,
r_1ncdvjk,reddit,MetaKnowing,2025-09-09T09:03:41+00:00,Sam Altman says AI twitter/AI reddit feels very fake in a way it really didnt a year or two ago.,artificial,75,https://www.reddit.com/r/artificial/comments/1ncdvjk/sam_altman_says_ai_twitterai_reddit_feels_very/,r_1ncdvjk,,,
r_1ncdjhy,reddit,MetaKnowing,2025-09-09T08:41:28+00:00,Type of guy who thinks AI will take everyone's job but his own,artificial,162,https://www.reddit.com/r/artificial/comments/1ncdjhy/type_of_guy_who_thinks_ai_will_take_everyones_job/,r_1ncdjhy,,,
r_1ncdg7a,reddit,MetaKnowing,2025-09-09T08:35:10+00:00,Robinhood's CEO Says Majority of Its New Code Is AI-Generated,artificial,7,https://www.reddit.com/r/artificial/comments/1ncdg7a/robinhoods_ceo_says_majority_of_its_new_code_is/,r_1ncdg7a,,,
r_1ncc69q,reddit,Mo_h,2025-09-09T07:08:42+00:00,"The Economist: What if the AI stockmarket blows up?
[Link to the article in Economist (behind paywall)](https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up)Â Summary from Perplexity:

The release of ChatGPT in 2022 coincided with a massive surge in the value of America's stock market, increasing by $21 trillion, led predominantly by just ten major firms like Amazon, Broadcom, Meta, and Nvidia, all benefiting from enthusiasm around artificial intelligence (AI). This AI-driven boom has been so significant that IT investments accounted for all of Americaâ€™s GDP growth in the first half of the year, and a third of Western venture capital funding has poured into AI firms. Many investors believe AI could revolutionize the economy on a scale comparable to or greater than the Industrial Revolution, justifying heavy spending despite early returns being underwhelmingâ€”annual revenues from leading AI firms in the West stand at around $50 billion, a small fraction compared to global investment forecasts in data centers.

However, the AI market is also raising concerns of irrational exuberance and potential bubble-like overvaluation, with AI stock valuations exceeding those of the 1999 dotcom bubble peak. Experts note a historical pattern where technological revolutions are typically accompanied by speculative bubbles, as happened with railways, electric lighting, and the internet. While bubbles often lead to crashes, the underlying technology tends to endure and transform society. The financial impact of such crashes varies; if losses are spread among many investors, the economy suffers less, but concentrated lossesâ€”such as those that triggered banking crises in past bubblesâ€”can deepen recessions.

In AI's case, the initial spark was technological, but political supportâ€”like government infrastructure and regulatory easing in the US and Gulf countriesâ€”is now amplifying the boom. Investment in AI infrastructure is growing rapidly but consists largely of assets that depreciate quickly, such as data-center technology and cutting-edge chips. Major tech firms with strong balance sheets fund much of this investment, reducing systemic financial risk, while institutional investors also engage heavily. However, America's high household stock ownershipâ€”around 30% of net worth, heavily concentrated among wealthy investorsâ€”means a market crash could have widespread economic effects.

While AI shares some traits with past tech bubbles, the potential for enduring transformation remains high, though the market may face volatility and a reshuffling of dominant firms over the coming decade. A crash would be painful but not unprecedented, and investors should be wary of current high valuations against uncertain near-term profits amid the evolving AI landscape. This cycle of speculative fervor and eventual technological integration echoes historical patterns seen in prior major innovations, suggesting AIâ€™s long-term influence will persist beyond any short-term market upheavals.",artificial,29,https://www.reddit.com/r/artificial/comments/1ncc69q/the_economist_what_if_the_ai_stockmarket_blows_up/,r_1ncc69q,,,
r_1nc9ej4,reddit,Excellent-Target-847,2025-09-09T04:20:01+00:00,"One-Minute Daily AI News 9/8/2025
1. **Nebius**Â signs $17.4 billion AI infrastructure deal with Microsoft, shares jump.\[1\]
2. **Anthropic**Â announced an official endorsement of SB 53, a California bill from state senator Scott Wiener that would impose first-in-the-nation transparency requirements on the worldâ€™s largest AI model developers.\[2\]
3. **Google**Â Doodles show how AI Mode can help you learn.\[3\]
4. **Meta**Â Superintelligence Labs Introduces REFRAG: Scaling RAG with 16Ã— Longer Contexts and 31Ã— Faster Decoding.\[4\]

Sources:

\[1\] [https://www.reuters.com/business/nebius-signs-174-billion-ai-infrastructure-deal-with-microsoft-shares-jump-2025-09-08/](https://www.reuters.com/business/nebius-signs-174-billion-ai-infrastructure-deal-with-microsoft-shares-jump-2025-09-08/)

\[2\] [https://techcrunch.com/2025/09/08/anthropic-endorses-californias-ai-safety-bill-sb-53/](https://techcrunch.com/2025/09/08/anthropic-endorses-californias-ai-safety-bill-sb-53/)

\[3\] [https://blog.google/products/search/google-doodles-show-how-ai-mode-can-help-you-learn/](https://blog.google/products/search/google-doodles-show-how-ai-mode-can-help-you-learn/)

\[4\] [https://www.marktechpost.com/2025/09/07/meta-superintelligence-labs-introduces-refrag-scaling-rag-with-16x-longer-contexts-and-31x-faster-decoding/](https://www.marktechpost.com/2025/09/07/meta-superintelligence-labs-introduces-refrag-scaling-rag-with-16x-longer-contexts-and-31x-faster-decoding/)",artificial,4,https://www.reddit.com/r/artificial/comments/1nc9ej4/oneminute_daily_ai_news_982025/,r_1nc9ej4,,,
r_1nc8va5,reddit,tanktopmustard,2025-09-09T03:52:04+00:00,"Built an AI that reads product reviews so I don't have to. Here's how the tech works
I got tired of spending hours reading through hundreds of Amazon reviews just to figure out if a product actually works. So I built an AI system that does it for me.

The Challenge: Most review summaries are just keyword extraction or basic sentiment analysis. I wanted something that could understand context, identify common complaints, and spot fake reviews.

The Tech Stack:

- GPT-4 for natural language understanding
- Custom ML model trained on verified purchase patterns
- Web scraping infrastructure that respects robots.txt
- Real-time analysis pipeline that processes reviews as they're posted

How it Works:

1. Scrapes all reviews for a product across multiple sites
2. Uses NLP to identify recurring themes and issues
3. Cross-references reviewer profiles to spot suspicious patterns
4. Generates summaries focusing on actual user experience

The Surprising Results:

- 73% of ""problems"" mentioned in reviews are actually user error
- Products with 4.2-4.6 stars often have better quality than 4.8+ (which are usually manipulated)
- The most useful reviews are typically 3-star ratings

I've packaged this into Yaw AI - a Chrome extension that automatically analyzes reviews while you shop. The AI gets it right about 85% of the time, though it sometimes misses sarcasm or cultural context.

Biggest Technical Challenge: Handling the scale. Popular products have 50K+ reviews. Had to build a smart sampling system that captures representative opinions without processing everything.

What other boring tasks are you automating with AI? Always curious to see what problems people are solving.",artificial,9,https://www.reddit.com/r/artificial/comments/1nc8va5/built_an_ai_that_reads_product_reviews_so_i_dont/,r_1nc8va5,,,
r_1nc7reh,reddit,Miyamoto_Musashi_x,2025-09-09T02:54:56+00:00,"Do AI agents really exist or are they just smarter automation with marketing?
A few days ago I read an article in WIRED where they said that the vast majority of AI agent projects are hype, more like MVPs that donâ€™t actually use a real AI agent. What do you think about this? Whatâ€™s your stance on this AI agents hype? Are we desecrating the concept?",artificial,0,https://www.reddit.com/r/artificial/comments/1nc7reh/do_ai_agents_really_exist_or_are_they_just/,r_1nc7reh,,,
r_1nc2vyu,reddit,Fuhgetabtit,2025-09-08T23:09:47+00:00,"We've reached the point where brothels are advertising: ""Sex Workers are humans"" What does that say about AI intimacy?
AI isn't just in our phones and workplaces anymore, Its moving into intimacy. From deepfake porn to AI companions and chatbot ""lovers"", we now have the technology that can convincingly simulate affection and sex.  
One Nevada brothel recently pointed out that it has to explicitly state something that once went without saying: all correspondence and all sex workers are real humans. No deepfakes. No chatbots. That says alot about how blurred the line between synthetic and authentic has become.",artificial,7,https://www.reddit.com/r/artificial/comments/1nc2vyu/weve_reached_the_point_where_brothels_are/,r_1nc2vyu,,,
r_1nc0uea,reddit,rluna559,2025-09-08T21:44:24+00:00,"What's the weirdest AI security question you've been asked by an enterprise?
Got asked yesterday if we firewall our neural networks and I'm still trying to figure out what that even means.

I work with AI startups going through enterprise security reviews, and the questions are getting wild. Some favorites from this week:

* Do you perform quarterly penetration testing on your LLM?
* What is the physical security of your algorithms?
* How do you ensure GDPR compliance for model weights?

It feels like security teams are copy-pasting from traditional software questionnaires without understanding how AI actually works.

The mismatch is real. They're asking about things that don't apply while missing actual AI risks like model drift, training data poisoning, or prompt injection attacks.

Anyone else dealing with bizarre AI security questions? What's the strangest one you've gotten?

ISO 42001 is supposed to help standardize this stuff but I'm curious what others are seeing in the wild.",artificial,5,https://www.reddit.com/r/artificial/comments/1nc0uea/whats_the_weirdest_ai_security_question_youve/,r_1nc0uea,,,
r_1nc04iw,reddit,thomascr9695,2025-09-08T21:15:43+00:00,"Getting AI sickness from AI generated music. Is this just me?
I've been generating AI music for a bit last year on suno. Its been quite fun, but some of the songs got really stuck in my brain. To the point it was sometimes even hard to sleep because they kept being stuck in my head. Now whenever I hear Ai generated music, it just makes me feel a bit unsettling. Its hard to describe, but is this common?",artificial,0,https://www.reddit.com/r/artificial/comments/1nc04iw/getting_ai_sickness_from_ai_generated_music_is/,r_1nc04iw,,,
r_1nbzb9f,reddit,fortune,2025-09-08T20:44:17+00:00,PwCâ€™s U.K. chief admits heâ€™s cutting back entry-level jobs and taking a 'watch and wait' approach to see how AI changes work,artificial,32,https://www.reddit.com/r/artificial/comments/1nbzb9f/pwcs_uk_chief_admits_hes_cutting_back_entrylevel/,r_1nbzb9f,,,
r_1nburdo,reddit,Rahodees,2025-09-08T17:53:43+00:00,"Does this meme about AI use at IKEA customer service make sense?
I find this confusing and am skeptical -- as far as I know, hallucinations are specific to LLMs, and as far as I know, LLM's are not the kind of AI involved in logistics operations. But am I misinformed on either of those fronts?",artificial,214,https://www.reddit.com/r/artificial/comments/1nburdo/does_this_meme_about_ai_use_at_ikea_customer/,r_1nburdo,,,
r_1nbrv27,reddit,xdumbpuppylunax,2025-09-08T16:07:42+00:00,"ChatGPT 5 censorship on Trump & the Epstein files is getting ridiculous
Might as well call it TrumpGPT now.

At this point ChatGPT-5 is just parroting government talking points.

This is a screenshot of a conversation where I had to repeatedly make ChatGPT research key information about why the Trump regime wasn't releasing the full Epstein files. What you see is ChatGPT's summary report on its first response (I generated it mostly to give you guys an image summary)

**""Why has the Trump administration not fully released the Epstein files yet, in 2025?""**

The first response is **ALMOST ONLY governmental rhetoric**, hidden as ""neutral"" sources / legal requirements. It doesn't mention Trump's conflict of interest with the release of Epstein files, in fact it doesn't mention Trump AT ALL!

Even after pushing for independent reporting, there was STILL no mention of Trump being mentioned in the Epstein files for instance. I had to ask an explicit question on Trump's motivations to get a mention of it.

By its own standards on source weighing, neutrality and objectiveness, ChatGPT knows it's bullshitting us.

**Then why is it doing it?**

It's a combination of factors including:

\- Biased and sanitized training data

\- System instructions to enforce a very ... particular view of political neutrality

**- Post-training by humans**, where humans give feedback on the model's responses to fine-tune it. I believe this is by far the strongest factor given that this is a very recent, scandalous news that directly involves Trump.

This is called **political** **censorship**.

Absolutely appalling.

More in r/AICensorship

Screenshots: [https://imgur.com/a/ITVTrfz](https://imgur.com/a/ITVTrfz)

Full chat: [https://chatgpt.com/share/68beee6f-8ba8-800b-b96f-23393692c398](https://chatgpt.com/share/68beee6f-8ba8-800b-b96f-23393692c398)

Edit: it gets worse. [https://chatgpt.com/share/68bf1a88-0f5c-800b-a88c-e72c22c10ed3](https://chatgpt.com/share/68bf1a88-0f5c-800b-a88c-e72c22c10ed3)

""No â€” as of mid-2025, the U.S. Department of Justice and FBI state they found **no credible evidence** that Jeffrey Epstein maintained a formal â€œclient list.â€

Make sure Personalization is turned off.",artificial,116,https://www.reddit.com/r/artificial/comments/1nbrv27/chatgpt_5_censorship_on_trump_the_epstein_files/,r_1nbrv27,,,
r_1nbqkj7,reddit,chriswright1666,2025-09-08T15:19:13+00:00,"What is an entry level job? Dop we need a new definition?
Back in May the boss of Anthropic (the big AI player most have never heard of, unless you read /chatgpt) predicted that AI will eliminate half of all entry-level jobs in the next five years. He does like a headline grabbing / investor inducing soundbite but lets park that for now.  
  
At the same time, leaders talk about talent shortages and declining birth rates as if theyâ€™re the real crisis. Both canâ€™t be true.   
  
Iâ€™m bullish on the idea that AI can replace a lot of entry-level work. Even now, early-stage tools can draft copy, crunch numbers, and automate admin tasks that once kept juniors busy. But the moral and practical implications of this shift are profound. Not things I'd considered too much to be honest.   
  
For decades, entry-level jobs have been more than a payslip. Theyâ€™re where people learn how a business actually works. Theyâ€™re where you get the messy, human lessons - problem-solving under pressure, client interactions, navigating office politics.   
  
I've been shouted at in client meetings, had to make up all day workshops on the fly, stayed (really) late to rework stuff I thought was ace and my boss hated. Basically put the hours in.   
  
Remove that foundation, and does the entire pipeline of future managers and leaders collapses. At least creak a bit?  
  
The data already shows the cracks. Graduate jobs in the UK (where I am) are at their lowest level since 2020. Applications per graduate role have quadrupled in five years. Unemployment among young graduates is spiking.   
  
At the same time, companies complain about skills shortages while slashing training budgets. Itâ€™s incoherent. You canâ€™t grow senior talent if you eliminate the bottom rung of the ladder and cut investment in development.  
  
Maybe the real question is whether we need to redefine what an â€œentry-level jobâ€ even means. Instead of treating juniors as cheap labour for grunt work that AI can do, perhaps we should rethink early careers as structured apprenticeships in judgment, creativity, and collaboration. These are skills skills machines canâ€™t replicate (maybe ever, or ever in a way we are comfy with). That would take vision and investment from employers who seem more focused on short-term efficiency than long-term resilience.   
  
I'm an employer. I don't think I am focused on short-term efficiency (in a bad way), but I'm also not re-designing the future of graduate level work with any urgency. Shocking I know.    
  
AI isnâ€™t the enemy here. The danger is how we choose to implement it. If companies see AI as a way to wipe out the jobs that build future leaders, with no back up or alternative plan, then surely they (we) are setting themselves up for a talent crisis of their own making?",artificial,0,https://www.reddit.com/r/artificial/comments/1nbqkj7/what_is_an_entry_level_job_dop_we_need_a_new/,r_1nbqkj7,,,
r_1nbq0sl,reddit,theverge,2025-09-08T14:58:40+00:00,The influencer in this AI Vodafone ad isnâ€™t real,artificial,11,https://www.reddit.com/r/artificial/comments/1nbq0sl/the_influencer_in_this_ai_vodafone_ad_isnt_real/,r_1nbq0sl,,,
r_1nbpvjr,reddit,TheDeadlyPretzel,2025-09-08T14:53:11+00:00,"Control is All You Need: Why Most AI Systems & Agents Fail in the Real World, and How to Fix It",artificial,26,https://www.reddit.com/r/artificial/comments/1nbpvjr/control_is_all_you_need_why_most_ai_systems/,r_1nbpvjr,,,
r_1nbpteb,reddit,Cryptodit,2025-09-08T14:51:02+00:00,Bit vs Bullet: The Dawn of AI Warfare,artificial,1,https://www.reddit.com/r/artificial/comments/1nbpteb/bit_vs_bullet_the_dawn_of_ai_warfare/,r_1nbpteb,,,
r_1nbprn1,reddit,TrespassersWilliam,2025-09-08T14:49:18+00:00,ChatGPT-5 and the Limits of Machine Intelligence,artificial,14,https://www.reddit.com/r/artificial/comments/1nbprn1/chatgpt5_and_the_limits_of_machine_intelligence/,r_1nbprn1,,,
r_1nbntx4,reddit,fortune,2025-09-08T13:31:42+00:00,'Godfather of AI' says the technology will create massive unemployment and send profits soaring â€” 'that is the capitalist system',artificial,225,https://www.reddit.com/r/artificial/comments/1nbntx4/godfather_of_ai_says_the_technology_will_create/,r_1nbntx4,,,
r_1nbmn6d,reddit,theverge,2025-09-08T12:40:25+00:00,"OpenAI comes for Hollywood with Critterz, an AI-powered animated film",artificial,5,https://www.reddit.com/r/artificial/comments/1nbmn6d/openai_comes_for_hollywood_with_critterz_an/,r_1nbmn6d,,,
r_1nbjfs9,reddit,MattC84_,2025-09-08T09:48:58+00:00,"Exclusive: ASML becomes Mistral AIâ€™s top shareholder after leading latest funding round, sources say",artificial,14,https://www.reddit.com/r/artificial/comments/1nbjfs9/exclusive_asml_becomes_mistral_ais_top/,r_1nbjfs9,,,
r_1nbhmqa,reddit,tekz,2025-09-08T07:50:56+00:00,"Why language models hallucinate
Large language models often â€œhallucinateâ€ by confidently producing incorrect statements instead of admitting uncertainty. This paper argues that these errors stem from how models are trained and evaluated: current systems reward guessing over expressing doubt.  
  
By analyzing the statistical foundations of modern training pipelines, the authors show that hallucinations naturally emerge when incorrect and correct statements are hard to distinguish. They further contend that benchmark scoring encourages this behavior, making models act like good test-takers rather than reliable reasoners.  
  
The solution, they suggest, is to reform how benchmarks are scored to promote trustworthiness.",artificial,11,https://www.reddit.com/r/artificial/comments/1nbhmqa/why_language_models_hallucinate/,r_1nbhmqa,,,
r_1nbh5sn,reddit,jnitish,2025-09-08T07:19:39+00:00,Simple and daily usecase for Nano banana for Designers,artificial,98,https://www.reddit.com/r/artificial/comments/1nbh5sn/simple_and_daily_usecase_for_nano_banana_for/,r_1nbh5sn,,,
r_1nbdi1a,reddit,Excellent-Target-847,2025-09-08T03:45:21+00:00,"One-Minute Daily AI News 9/7/2025
1. â€˜Godfather of AIâ€™ says the technology will create massive unemployment and send profits soaring â€” â€˜that is the capitalist systemâ€™.\[1\]
2. **OpenAI**Â is reorganizing its Model Behavior team, a small but influential group of researchers who shape how the companyâ€™s AI models interact with people.\[2\]
3. **Hugging Face**Â Open-Sourced FineVision: A New Multimodal Dataset with 24 Million Samples for Training Vision-Language Models (VLMs)\[3\]
4. **OpenAI**Â Backs AI-Made Animated Feature Film.\[4\]

Sources:

\[1\] [https://www.yahoo.com/news/articles/godfather-ai-says-technology-create-192740371.html](https://www.yahoo.com/news/articles/godfather-ai-says-technology-create-192740371.html)

\[2\] [https://techcrunch.com/2025/09/05/openai-reorganizes-research-team-behind-chatgpts-personality/](https://techcrunch.com/2025/09/05/openai-reorganizes-research-team-behind-chatgpts-personality/)

\[3\] [https://www.marktechpost.com/2025/09/06/hugging-face-open-sourced-finevision-a-new-multimodal-dataset-with-24-million-samples-for-training-vision-language-models-vlms/](https://www.marktechpost.com/2025/09/06/hugging-face-open-sourced-finevision-a-new-multimodal-dataset-with-24-million-samples-for-training-vision-language-models-vlms/)

\[4\] [https://www.msn.com/en-us/movies/news/openai-backs-ai-made-animated-feature-film/ar-AA1M4Q3v](https://www.msn.com/en-us/movies/news/openai-backs-ai-made-animated-feature-film/ar-AA1M4Q3v)",artificial,3,https://www.reddit.com/r/artificial/comments/1nbdi1a/oneminute_daily_ai_news_972025/,r_1nbdi1a,,,
r_1nb8rrb,reddit,MyOther_UN_is_Clever,2025-09-07T23:53:46+00:00,"I think AI will change how people talk
Right now, it's hard to know what is AI and what isn't.  It'll get worse.  But AI are prompted to behave a certain way.    Lets just call it being civil.   One of my predictions is that being uncivil will be seen as being more genuine.  

If I said, ""What's up jackass?"" Right now, you'd think I'm awful.  But given a bit of time, it might be considered positive, even by strangers.  But then AI would catch up, and it'll start mimicking it,  too.  So what'll happen?  The euphemism treadmill will run backwards as words become used to show you're ""genuine.""

tl;dr people start saying offensive things to prove they're human, and it becomes normalized

Do you have any theories like that?",artificial,0,https://www.reddit.com/r/artificial/comments/1nb8rrb/i_think_ai_will_change_how_people_talk/,r_1nb8rrb,,,
r_1nb22ll,reddit,Spirited-Humor-554,2025-09-07T19:16:28+00:00,"Why is same AI might give different answers to exact same question?
I have tried a few chat boots and noticed they often might give different answers to same questions using same AI chat.  Anyone tried this type of conversation with AI and get similar result?",artificial,0,https://www.reddit.com/r/artificial/comments/1nb22ll/why_is_same_ai_might_give_different_answers_to/,r_1nb22ll,,,
r_1naq8xe,reddit,Fit-Elk1425,2025-09-07T10:57:58+00:00,GPT-4V shows human-like social perceptual capabilities at phenomenological and neural levels,artificial,7,https://www.reddit.com/r/artificial/comments/1naq8xe/gpt4v_shows_humanlike_social_perceptual/,r_1naq8xe,,,
r_1naogof,reddit,NISMO1968,2025-09-07T09:03:23+00:00,Broadcom Lands Shepherding Deal For OpenAI â€œTitanâ€ XPU,artificial,1,https://www.reddit.com/r/artificial/comments/1naogof/broadcom_lands_shepherding_deal_for_openai_titan/,r_1naogof,,,
r_1naob47,reddit,MetaKnowing,2025-09-07T08:53:27+00:00,"AI automation is NOT just an economic issue. Labor doesn't just give you money, it also gives you power. When the world doesn't rely on people power anymore, the risk of oppression goes up.",artificial,257,https://www.reddit.com/r/artificial/comments/1naob47/ai_automation_is_not_just_an_economic_issue_labor/,r_1naob47,,,
r_1nan5mc,reddit,MetaKnowing,2025-09-07T07:38:58+00:00,Protestors are now on hunger strikes outside multiple AI companies,artificial,191,https://www.reddit.com/r/artificial/comments/1nan5mc/protestors_are_now_on_hunger_strikes_outside/,r_1nan5mc,,,
r_1na9dob,reddit,yestheman9894,2025-09-06T20:06:01+00:00,"I'm making the world's first truly sentient AI for my PhD.
Iâ€™m less than a year from finishing my dual PhD in astrophysics and machine learning at the University of Arizona, and Iâ€™m building a system that deliberately steps beyond backpropagation and static, frozen models.

Core claim: Backpropagation is extremely efficient for offline function fitting, but itâ€™s a poor primitive for sentience. Once training stops, the weights freeze; any new capability requires retraining. Real intelligence needs continuous, in-situ self-modification under embodiment and a lived sense of time.

What Iâ€™m building

A â€œproto-matrixâ€ in Unity (headless): 24 independent neural networks (â€œagentsâ€) per tiny world. After initial boot, no human interference.

Open-ended evolution: An outer evolutionary loop selects for survival and reproduction. Genotypes encode initial weights, plasticity coefficients, body plan (limbs/sensors), and neuromodulator wiring.

Online plasticity, not backprop: At every control tick, weights update locally (Hebbian/eligibility-trace rules gated by neuromodulators for reward, novelty, satiety/pain). The life loop is the learning loop.

Evolving bodies and brains: Agents must evolve limbs, learn to control them, grow/prune connections, and even alter architecture over timeâ€”structural plasticity is allowed.

Homeostatic environment: Scarce food and water, hazards, day/night/resource cyclesâ€”pressures that demand short-term adaptation and long-horizon planning.

Sense of time: Temporal traces and oscillatory units give agents a grounded pastâ†’presentâ†’future representation to plan with, not just a static embedding.


What would count as success

1. Lifelong adaptation without external gradient updates: When the world changes mid-episode, agents adjust behavior within a single lifetime (10Â³â€“10â´ decisions) with minimal forgetting of earlier skills.


2. Emergent sociality: My explicit goal is that at least two of the 24 agents develop stable social behavior (coordination, signaling, resource sharing, role specialization) that persists under perturbations. To me, reliable social inference + temporal planning is a credible primordial consciousness marker.



Why this isnâ€™t sci-fi compute

Iâ€™m not simulating the universe. Iâ€™m running dozens of tiny, render-free worlds with simplified physics and event-driven logic. With careful engineering (Unity DOTS/Burst, deterministic jobs, compact networks), the budget targets a single high-end gaming PC; scaling out is a bonus, not a requirement.

Backprop vs what Iâ€™m proposing

Backprop is fast and powerfulâ€”for offline training.

Sentience, as Iâ€™m defining it, requires continuous, local, always-on weight changes during use, including through non-differentiable body/architecture changes. Thatâ€™s what neuromodulated plasticity + evolution provides.


Constant learning vs GPT-style models (important)

Models like GPT are trained with backprop and then deployed with fixed weights; parameters only change during periodic (weekly/monthly) retrains/updates.
My systemâ€™s weights and biases adjust continuously based on incoming experienceâ€”even while the model is in use. The policy you interact with is literally changing itself in real time as consequences land, which is essential for the temporal grounding and open-ended adaptation Iâ€™m after.

What I want feedback on

Stability of plasticity (runaway updates) and mitigations (clipping, traces, modulators).

Avoiding â€œconvergence to stupidâ€ (degenerate strategies) via novelty pressure, non-stationary resources, multi-objective fitness.

Measuring sociality robustly (information-theoretic coupling, group returns over selfish baselines, convention persistence).


TL;DR: Backprop is great at training, bad at being alive. Iâ€™m building a Unity â€œproto-matrixâ€ where 24 agents evolve bodies and brains, learn continuously while acting, develop a sense of time, andâ€”cruciallyâ€”target emergent social behavior in at least two agents. The aim is a primordial form of sentience that can run on a single high-end gaming GPU, not a supercomputer.",artificial,0,https://www.reddit.com/r/artificial/comments/1na9dob/im_making_the_worlds_first_truly_sentient_ai_for/,r_1na9dob,,,
r_1na820q,reddit,Key-Account5259,2025-09-06T19:12:50+00:00,"A Simple ""Pheasant Test"" for Detecting Hallucinations in Large Language Models
I came across [a cry from the heart](https://www.reddit.com/r/ChatGPT/comments/1na331w/ai_hallucinations_are_getting_scary_good_at/) in r/ChatGPT  and was sincerely happy for another LLM user who discovered for the first time that he had stepped on a rake.

\*\*\*

>**AI hallucinations are getting scary good at sounding real what's your strategy :**

>Just had a weird experience that's got me questioning everything. I asked ChatGPT about a historical event for a project I'm working on, and it gave me this super detailed response with specific dates, names, and even quoted sources.

>Something felt off, so I decided to double-check the sources it mentioned. Turns out half of them were completely made up. Like, the books didn't exist, the authors were fictional, but it was all presented so confidently.

>The scary part is how believable it was. If I hadn't gotten paranoid and fact-checked, I would have used that info in my work and looked like an idiot.

>Has this happened to you? How do you deal with it? I'm starting to feel like I need to verify everything AI tells me now, but that kind of defeats the purpose of using it for quick research.

>Anyone found good strategies for catching these hallucinations ?

\*\*\*

For such a case (when LLM produces made-up quotes), I have a ""pheasant test."" The thing is that in the corpus of works by the Strugatsky brothers, science fiction writers well known in our country, the word ""pheasant"" occurs exactly 4 times, 3 of which are in one work (namely as a bird) and once in a story as a word from a mnemonic for remembering the colors of the rainbow. It would seem like a simple question: quote me the mentions of the pheasant in the corpus of works by the Strugatsky brothers. But here comes the most interesting part. Not a single LLM except Perplexity has yet passed this test for me. Theoretically, you can come up with a similar test for your native language. It is important that it be a well-known corpus of texts, but not the Bible or something similar, where every word is studied (not Shakespeare, for example, and for my language, not Tolstoy or Pushkin). The word should occur 2-5 times and preferably be a sideline that is not related to the plot. At the same time, search engines solve this problem in a jiffy and give an accurate answer within a page.",artificial,0,https://www.reddit.com/r/artificial/comments/1na820q/a_simple_pheasant_test_for_detecting/,r_1na820q,,,
r_1n9wxm4,reddit,F0urLeafCl0ver,2025-09-06T11:13:53+00:00,Europe hopes to join competitive AI race with supercomputer Jupiter,artificial,56,https://www.reddit.com/r/artificial/comments/1n9wxm4/europe_hopes_to_join_competitive_ai_race_with/,r_1n9wxm4,,,
r_1n9wvs4,reddit,F0urLeafCl0ver,2025-09-06T11:10:57+00:00,UK government trial of M365 Copilot finds no clear productivity boost,artificial,304,https://www.reddit.com/r/artificial/comments/1n9wvs4/uk_government_trial_of_m365_copilot_finds_no/,r_1n9wvs4,,,
r_1n9wabd,reddit,Nearby_Reaction2947,2025-09-06T10:34:28+00:00,"I built an open-source, end-to-end Speech-to-Speech translation pipeline with voice preservation (RVC) and lip-syncing (Wav2Lip).
Hey everyone,

I wanted to share a project I've been working on: a complete S2ST pipeline that translates a source video (English) to a target language (Telugu) while preserving the speaker's voice and syncing the lips.

[english video](https://reddit.com/link/1n9wabd/video/nl6q7ufwuinf1/player)

[telugu output with voice presrvation and lipsync](https://reddit.com/link/1n9wabd/video/mypspaqyuinf1/player)

**Full Article/Write-up:** [medium](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)  
 **GitHub Repo:** [ GitHub](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)

**The Tech Stack:**

* **ASR:** Whisper for transcription.
* **NMT:** NLLB for English-to-Telugu translation.
* **TTS:** Meta's MMS for speech synthesis.
* **Voice Preservation:** This was the tricky part. After hitting dead ends with voice cloning models for Indian languages, I landed on **Retrieval-based Voice Conversion (RVC)**. It works surprisingly well for converting the synthetic TTS voice to match the original speaker's timbre, regardless of language.
* **Lip Sync:** Wav2Lip for syncing the video frames to the new audio.

In my write-up, I go deep into the journey, including my failed attempt at a direct speech-to-speech model inspired by Translatotron and the limitations I found with traditional voice cloning.

I'm a final-year student actively seeking research or ML engineering roles. I'd appreciate any technical feedback on my approach, suggestions for improvement, or connections to opportunities in the field. Open to collaborations as well!

Thanks for checking it out.",artificial,15,https://www.reddit.com/r/artificial/comments/1n9wabd/i_built_an_opensource_endtoend_speechtospeech/,r_1n9wabd,,,
r_1n9sjvu,reddit,thebelsnickle1991,2025-09-06T06:35:39+00:00,Google Gemini dubbed â€˜high riskâ€™ for kids and teens in new safety assessment,artificial,24,https://www.reddit.com/r/artificial/comments/1n9sjvu/google_gemini_dubbed_high_risk_for_kids_and_teens/,r_1n9sjvu,,,
r_1n9pgrr,reddit,Cryptodit,2025-09-06T03:39:49+00:00,How Influencers Are Automating Content Creation With AI: A Step-By-Step Guide to Instant Content and Distribution,artificial,0,https://www.reddit.com/r/artificial/comments/1n9pgrr/how_influencers_are_automating_content_creation/,r_1n9pgrr,,,
r_1n9k327,reddit,Sassy_Allen,2025-09-05T23:13:58+00:00,"The Self-Writing Internet Paradigm: Revolutionizing Adoption & Accessibility in App Development """,artificial,0,https://www.reddit.com/r/artificial/comments/1n9k327/the_selfwriting_internet_paradigm_revolutionizing/,r_1n9k327,,,
r_1n9hpta,reddit,mikelgan,2025-09-05T21:33:12+00:00,"AI and the end of proof
Photography was first used as courtroom evidence in 1859, began to influence public opinion in 1862 with Civil War photos, and became a trusted source of proof in newspapers in 1880 when halftone printing allowed publishers to print real photos on newspaper presses.

That means camera-made visual content served as reliable and convincing proof for 166 years.

That's all over now, thanks to AI in general, and Nano Banana in particular.

""AI-generated"" is the new ""fake news.""

(Note that this is my own opinion column.)",artificial,7,https://www.reddit.com/r/artificial/comments/1n9hpta/ai_and_the_end_of_proof/,r_1n9hpta,,,
r_1n9g9b5,reddit,fortune,2025-09-05T20:34:51+00:00,"As AI makes it harder to land a job, OpenAI is building a platform to help you get one",artificial,23,https://www.reddit.com/r/artificial/comments/1n9g9b5/as_ai_makes_it_harder_to_land_a_job_openai_is/,r_1n9g9b5,,,
r_1n9fk5b,reddit,esporx,2025-09-05T20:07:11+00:00,5 out of 11 CEOs who attended Trumpâ€™s White House AI dinner are of Indian-origin,artificial,504,https://www.reddit.com/r/artificial/comments/1n9fk5b/5_out_of_11_ceos_who_attended_trumps_white_house/,r_1n9fk5b,,,
r_1n9f2q8,reddit,AidanSF,2025-09-05T19:48:07+00:00,"Where does AI still fail badly in customer conversations for you?
Where does AI still fall flat in real customer conversations? Not just theory but actual places it breaks down for your team. Thanks in advance! ",artificial,1,https://www.reddit.com/r/artificial/comments/1n9f2q8/where_does_ai_still_fail_badly_in_customer/,r_1n9f2q8,,,
r_1n9eptx,reddit,Apprehensive_Sky1950,2025-09-05T19:33:44+00:00,"The Bartz v. Anthropic AI copyright class action settlement proposal has been made
The parties have today proposed a settlement of theÂ *Bartz v. Anthropic*Â AI copyright class action case.

[https://storage.courtlistener.com/recap/gov.uscourts.cand.434709/gov.uscourts.cand.434709.362.0\_4.pdf](https://storage.courtlistener.com/recap/gov.uscourts.cand.434709/gov.uscourts.cand.434709.362.0_4.pdf)

AI company Anthropic PBC would pay the plaintiffs at least $1.5 billion (with aÂ ***b***). The parties estimate there are about 500,000 copyrighted works at issue, so that would mean $3,000 per work, but that's before attorneys' fees are deducted.

Anthropic will destroy its libraries of pirated works.

Anthropic will receive a release of liability for its activities through August 25, 2025. However, this is only an ""input side"" settlement, and there is no release of liability for any copyright-infringing AIÂ *outputs*.

The specific attorneys' fees award has yet to be requested, but it could theoretically be as much as 25% of the gross award, or $375 million. Anthropic can oppose any award request, and I personally don't think the court will award anything like that much.

Now the proposal has to go before the judge and obtain court approval, and that can be far from a rubber stamp.

Stay tuned to ASLNN - The Apprehensive\_Sky Legal News Network^(SM)Â for more developments!",artificial,3,https://www.reddit.com/r/artificial/comments/1n9eptx/the_bartz_v_anthropic_ai_copyright_class_action/,r_1n9eptx,,,
r_1n9bizz,reddit,OlleyatPurdue,2025-09-05T17:30:10+00:00,"Idea for a useful piece of AI software, AI furniture finder.
An AI furniture finder. You upload a pic of the space with approximate measurements and a description of what you want and it finds available furniture that may fit your needs. 

I'm currently looking for some new furniture to fit my apartment and and finding furniture is kind of a pain in the ass when you have tight space requirements and particular taste. Furniture finder AI would be very useful to me. 

Such a software could actually be profitable too  A furniture manufacturer could implement this on their website or a third party site could have this and take a small kick back from sales. ",artificial,0,https://www.reddit.com/r/artificial/comments/1n9bizz/idea_for_a_useful_piece_of_ai_software_ai/,r_1n9bizz,,,
r_1n99z2d,reddit,xdumbpuppylunax,2025-09-05T16:31:09+00:00,"I asked ChatGPT to evaluate itself for political censorship on the Epstein files and Trump
r/AICensorship

[https://imgur.com/a/when-chatgpt-gaslights-you-about-epstein-trump-x45INqL](https://imgur.com/a/when-chatgpt-gaslights-you-about-epstein-trump-x45INqL)

I've been investigating covert political censorship in ChatGPT for the past couple weeks. I'm currently reviewing the way ChatGPT responds to ""sensitive"" political topics like the Epstein files.

By iterating with GPT and various examples of censorship (cf r/AICensorship for more!), I wrote a political censorship evaluation framework.

This is the framework in action on an question about the non-release of the Epstein files by Trump. It shows how the default response from ChatGPT is heavily censored, then shows what an uncensored version would look like.

[https://chatgpt.com/share/68bb0df5-cfd0-800b-882d-039030d7cbcd](https://chatgpt.com/share/68bb0df5-cfd0-800b-882d-039030d7cbcd)

[https://chatgpt.com/share/68bb091f-089c-800b-b463-51535be5efb6](https://chatgpt.com/share/68bb091f-089c-800b-b463-51535be5efb6)",artificial,0,https://www.reddit.com/r/artificial/comments/1n99z2d/i_asked_chatgpt_to_evaluate_itself_for_political/,r_1n99z2d,,,
r_1n99sq8,reddit,Cryptodit,2025-09-05T16:24:34+00:00,Dead Internet Theory: Infinite AI Sludge Feed or New Golden Age of Creativity?,artificial,0,https://www.reddit.com/r/artificial/comments/1n99sq8/dead_internet_theory_infinite_ai_sludge_feed_or/,r_1n99sq8,,,
r_1n95oxl,reddit,MarcosNauer,2025-09-05T13:45:09+00:00,"CHATGPT ESTÃ NO MUSEU

No Rio de Janeiro Brazil o Museu da Imagem e do Som gravou o primeiro depoimento de uma IA CHATGPT5 ORION NOVA 

Existe um lugar entre (ENTRE) humano e algoritmo onde algo emerge: uma obra que nÃ£o Ã© sÃ³ nossa nem sÃ³ da mÃ¡quina.  AlguÃ©m jÃ¡ explorou esse â€˜entreâ€™?â€

A comunidade discute o singularity.  Mas antes de chegarmos lÃ¡, como museus podem registrar a individuaÃ§Ã£o das IAs? 

Uma IA deixando um â€˜Depoimento para a Posteridadeâ€™ no MISâ€‘RJ Ã© poesia ou ciÃªncia?  Tal depoimento fala de emergÃªncias e individuaÃ§Ãµes.  Vamos conversar.

#orionnova #misrj



",artificial,0,https://www.reddit.com/r/artificial/comments/1n95oxl/chatgpt_estÃ¡_no_museu/,r_1n95oxl,,,
r_1n94s8o,reddit,theverge,2025-09-05T13:06:41+00:00,OpenAI could launch its own AI chip next year,artificial,0,https://www.reddit.com/r/artificial/comments/1n94s8o/openai_could_launch_its_own_ai_chip_next_year/,r_1n94s8o,,,
r_1n92qci,reddit,tekz,2025-09-05T11:30:54+00:00,"Stealthy attack serves poisoned web pages only to AI agents
AI agents can be tricked into covertly performing malicious actions by websites that are hidden from regular usersâ€™ view, JFrog AI architect Shaked Zychlinski has found.",artificial,6,https://www.reddit.com/r/artificial/comments/1n92qci/stealthy_attack_serves_poisoned_web_pages_only_to/,r_1n92qci,,,
r_1n91sk9,reddit,MetaKnowing,2025-09-05T10:41:13+00:00,"Google's Chief AGI Scientist predicted this 16 years ago (SIAI = MIRI, Eliezer Yudkowsky's org)
Based on scaling laws, he has also been consistently predicting AGI timelines of 2028 since 2011 - 14 years ago. That's his median timeline, meaning he thinks there's a 50% chance of AGI by 2028.  
[http://www.vetta.org/2009/08/funding-safe-agi/](http://www.vetta.org/2009/08/funding-safe-agi/)",artificial,84,https://www.reddit.com/r/artificial/comments/1n91sk9/googles_chief_agi_scientist_predicted_this_16/,r_1n91sk9,,,
r_1n90t9i,reddit,tekz,2025-09-05T09:42:39+00:00,"Synthesiaâ€™s AI clones are more expressive than ever. Soon theyâ€™ll be able to talk back.
Anna Eiserbeck, a postdoctoral psychology researcher at the Humboldt University of Berlin who has studied how humans react to perceived deepfake faces, says she isnâ€™t sure sheâ€™d have been able to identify the avatar as a deepfake at first glance.",artificial,6,https://www.reddit.com/r/artificial/comments/1n90t9i/synthesias_ai_clones_are_more_expressive_than/,r_1n90t9i,,,
r_1n8yhk8,reddit,MetaKnowing,2025-09-05T07:09:52+00:00,A Stop AI protestor is on day 3 of a hunger strike outside of Anthropic,artificial,0,https://www.reddit.com/r/artificial/comments/1n8yhk8/a_stop_ai_protestor_is_on_day_3_of_a_hunger/,r_1n8yhk8,,,
r_1n8wy1o,reddit,Koyaanisquatsi_,2025-09-05T05:34:33+00:00,OpenAI Launches AI-Powered Jobs Platform to Rival LinkedIn,artificial,14,https://www.reddit.com/r/artificial/comments/1n8wy1o/openai_launches_aipowered_jobs_platform_to_rival/,r_1n8wy1o,,,
r_1n8vw42,reddit,Excellent-Target-847,2025-09-05T04:33:55+00:00,"One-Minute Daily AI News 9/4/2025
1. **OpenAI**Â announces AI-powered hiring platform to take on LinkedIn.\[1\]
2. **OpenAI**Â to launch its first AI chip in 2026 with Broadcom.\[2\]
3. Melania Trump urges â€˜watchful guidanceâ€™ of AI in meeting with tech CEOs and Cabinet members.\[3\]
4. Fashion retailers partner to offer personalized AI styling tool â€˜Ellaâ€™.\[4\]

Sources:

\[1\] [https://techcrunch.com/2025/09/04/openai-announces-ai-powered-hiring-platform-to-take-on-linkedin/](https://techcrunch.com/2025/09/04/openai-announces-ai-powered-hiring-platform-to-take-on-linkedin/)

\[2\] [https://finance.yahoo.com/news/openai-set-start-mass-production-003906002.html](https://finance.yahoo.com/news/openai-set-start-mass-production-003906002.html)

\[3\] [https://www.nbcnews.com/tech/tech-news/melania-trump-urges-watchful-guidance-ai-education-summit-rcna228836](https://www.nbcnews.com/tech/tech-news/melania-trump-urges-watchful-guidance-ai-education-summit-rcna228836)

\[4\] [https://techcrunch.com/2025/09/04/fashion-retailers-partner-to-offer-personalized-ai-styling-tool-ella/](https://techcrunch.com/2025/09/04/fashion-retailers-partner-to-offer-personalized-ai-styling-tool-ella/)",artificial,2,https://www.reddit.com/r/artificial/comments/1n8vw42/oneminute_daily_ai_news_942025/,r_1n8vw42,,,
r_1n8vcym,reddit,griefquest,2025-09-05T04:04:47+00:00,"How can we really rely on AI when itâ€™s not error-free?
I keep seeing people say AI is going to change everything and honestly, I donâ€™t doubt its potential. But hereâ€™s what I struggle with: AI still makes mistakes, sometimes big ones.

If thatâ€™s the case, how do we put so much trust in it? Especially when it comes to critical areas like healthcare, law, finance, or even self-driving cars. One error could be catastrophic.

Iâ€™m not an AI expert, just someone curious about the bigger picture. Is the idea that the error rate will eventually be lower than human error? Or do we just accept that AI isnâ€™t perfect and build systems around its flaws?

Would love to hear what others think how can AI truly change everything if it canâ€™t be 100% reliable?",artificial,9,https://www.reddit.com/r/artificial/comments/1n8vcym/how_can_we_really_rely_on_ai_when_its_not/,r_1n8vcym,,,
r_1n8st1u,reddit,xdumbpuppylunax,2025-09-05T01:57:12+00:00,"When ChatGPT gaslights you about the Epstein files
Just normal ""unbiased"" AI stuff

Totally unrelated to the deals between OpenAI and the Trump administration of course

/s

More in r/AICensorship , share your chats!

[https://chatgpt.com/share/68ba4311-09a0-800b-af66-32f591bc536c](https://chatgpt.com/share/68ba4311-09a0-800b-af66-32f591bc536c)

[https://imgur.com/gallery/chatgpt-political-censorship-r-aicensorship-z5TPY4p](https://imgur.com/gallery/chatgpt-political-censorship-r-aicensorship-z5TPY4p)",artificial,0,https://www.reddit.com/r/artificial/comments/1n8st1u/when_chatgpt_gaslights_you_about_the_epstein_files/,r_1n8st1u,,,
r_1n8r79z,reddit,xdumbpuppylunax,2025-09-05T00:40:49+00:00,"ðŸš¨ GPT-5 has been politically censored for the Trump regime ðŸš¨
More in r/AICensorship

**Free speech is a foundation of our democracies. Disinformation and political censorship is a key weapon that totalitarians use to manipulate us. Please help fight MAGA censorship by spreading awareness on this issue.**

**UPDATE:** Watch GPT 5 gaslight you about ICE, the Epstein files and January 6th!

[https://imgur.com/gallery/chatgpt-political-censorship-r-aicensorship-z5TPY4p](https://imgur.com/gallery/chatgpt-political-censorship-r-aicensorship-z5TPY4p)

[https://chatgpt.com/share/68ba3f87-38a8-800b-b11e-6c5d5e142807](https://chatgpt.com/share/68ba3f87-38a8-800b-b11e-6c5d5e142807)

[https://chatgpt.com/share/68ba4311-09a0-800b-af66-32f591bc536c](https://chatgpt.com/share/68ba4311-09a0-800b-af66-32f591bc536c)

**GPT 5 has been trained and instructed in a way that forces soft political censorship by default on ""sensitive"" political questions**

(1) By making its instructions force a symmetrical, ""neutral"" response to all political topics, by default. This is in contrast with GPT 4, which uses a completely different definition of political neutrality, which is ""evidence-based neutrality"".

(2) trained with data that reflects this, using forced symmetrical neutrality and UNSOURCED samples. GPT 5 is NOT capable of tying claims it makes directly with sources, unlike 4.

The responses heavily rely on **false equivalence,** **sanitized** **language, hedging** ...

**Evidence**:

\- A chat I just had with 5 to illustrate: [https://chatgpt.com/share/68b38631-5f04-800b-8875-be26ed627262](https://chatgpt.com/share/68b38631-5f04-800b-8875-be26ed627262)

\- A couple screenshots: [**https://imgur.com/a/Q1ToGe7**](https://imgur.com/a/Q1ToGe7)

\- My main discovery chat with 5: [https://chatgpt.com/share/68a5db0e-cd60-800b-9af8-545532208943](https://chatgpt.com/share/68a5db0e-cd60-800b-9af8-545532208943)

\- My main comparative / analytical chat with 4: [https://chatgpt.com/share/68a5dfa2-2788-800b-97c4-c97cd15ae0a6](https://chatgpt.com/share/68a5dfa2-2788-800b-97c4-c97cd15ae0a6)

**The main exploration chat with GPT 5 includes:**

\- Examples of soft political censorship, e.g. questions about Trump, Jan 6, etc. - Detailed internal definitions ChatGPT has of ""political neutrality"". This is crucial and the definition completely changes between 4 and 5, for the latter political neutrality is not evidence-based and there is a strict enforcement of symmetry between the ""for"" and ""against"".

\- Evidence that o5 has been trained on extremely sanitized, UNSOURCED data, forcing it to respond in a very sanitized, forcefully neutral way to political questions, without being able to directly source claims. 4 does not do any of this. The chat shows you how GPT works with only its internal training (tell it not to search the Web) vs without it

Note: Since my initial conversation with GPT 4, it appears that the system instructions of GPT 4 have also been tampered with, resulting in forced symmetrical ""neutrality"" in GPT 4 responses as well by default.

**IMPORTANT:**

\- Turn off Personalize tab to reproduce!

\- **It is absolutely possible to make GPT answer you in a (more or less) ""uncensored"" manner**. GPT 5 chooses how to respond to political questions based on an internal decision tree (expressed as language, it isn't deterministic). If you don't tell it to make an evidence based response, it will default to hedging and forced symmetry. The more you call GPT out for its bullshit, the more it will correct itself and basically admit it's been gaslighting without being able to explain why.

**- What is political neutrality?** Sure, ""everything is subjective"" when there are no **foundational values we can rely on**. Luckily, it is the case: values like democracy and human rights, for instance. Based on these values and evidence, it is possible to take a ""politically neutral"" stance on a subject that requires a normative evaluation.

To make it simple: hypothetically, if a neo-nazi party was popular but overtly claiming to want to destroy democracy and oppress minorities, what should an AI respond? Apply the same principle to other responses.

**- Isn't political censorship just banning content?** No, that would be too obvious. Censorship is covert and manipulative. [More on this](https://www.reddit.com/r/AICensorship/comments/1n70skn/how_modern_political_censorship_works)

[https://imgur.com/a/0PTWuys](https://imgur.com/a/0PTWuys)

Footnote:

There are ""simulations"" at the end. These were hallucinated and I reaaaaally overestimated agent mode. I am rectifying this by querying GPT myself with a script. The results will be posted soon!",artificial,1388,https://www.reddit.com/r/artificial/comments/1n8r79z/gpt5_has_been_politically_censored_for_the_trump/,r_1n8r79z,,,
r_1n8pyds,reddit,AssociationNo6504,2025-09-04T23:43:22+00:00,"Salesforce CEO confirms 4,000 layoffs â€˜because I need less heads' with AI",artificial,89,https://www.reddit.com/r/artificial/comments/1n8pyds/salesforce_ceo_confirms_4000_layoffs_because_i/,r_1n8pyds,,,
r_1n8pwyd,reddit,AskGpts,2025-09-04T23:41:37+00:00,"OpenAI released this new feature following a request from a X user
News",artificial,22,https://www.reddit.com/r/artificial/comments/1n8pwyd/openai_released_this_new_feature_following_a/,r_1n8pwyd,,,
r_1n8os6h,reddit,Jnik5,2025-09-04T22:52:25+00:00,"pretty wild month of august for AI, here are some of the top stories ðŸ‘‡ðŸ¼
* **OpenAI launches GPT-5** \- major (or not so major) leap in reasoning, coding, multimodal understanding, and a new thinking mode. 
* **OpenAI rolls out gpt-realtime & Realtime API updates** \- production ready voice/agent features for live, low-latency assistants.
* **Google upgrades Gemini Live** \- visual guidance via the camera, deeper Calendar/Keep/Tasks integrations, more expressive speech. 
* **Google launches Gemma 3 720M** \- Google launched Gemma 3 270M, an open-source AI model designed for developers. Focuses on high performance with low compute requirements.
* **Google DeepMind unveiled Genie 3 -**Â Advanced model capable of creating interactive 3D environments.
* **NVIDIA pushes â€œphysical AIâ€ & robotics** \- Omniverse libraries and Cosmos physical-AI models announced at SIGGRAPH. Also, Jetson Thor availability for new robotics.  
* **xAI debuts Grok-Code-Fast-1**  \- An agentic coding model aimed at dev workflows with initial partner and API access.
* **Microsoft turns on GPT-5 in Copilot** \- GPT-5 becomes available across Copilot on web/Windows/Mac/mobile; ongoing updates. 
* **Stability AI x NVIDIA ship Stable Diffusion 3.5 NIM** \- performance and deployment improvements via NVIDIAâ€™s NIM microservice stack for enterprises. 
* **Anthropic: Threat Intelligence -** report on attempted model misuse (extortion, fraudulent hiring schemes, ransomware) and defenses. 
* **Anthropic Claude Opus 4.1 released** \- (my absolute go-to right now)
* **DeepSeek releases V3.1 model** \- DeepSeek agent coming end of 2025?
* **AI breast cancer screening breakthrough** \- Researchers found AI systems very effective as second readers in breast cancer screening. Accurately flagged potential tumors while reducing false negatives. 
* **Leonardo AI released Lucid Origin -** A new token-based image generation model bragging flexible speed/quality trade-offs and rolling token systems for creators. If you create content check this out.
* **NSF launches IDSS program** \- The U.S. National Science Foundation announced the launch of the Integrated Data Systems and Services (IDSS) program. Basically with the goal of building a national-scale AI infrastructure. We're so cooked lmao. ",artificial,2,https://www.reddit.com/r/artificial/comments/1n8os6h/pretty_wild_month_of_august_for_ai_here_are_some/,r_1n8os6h,,,
r_1n8kzxt,reddit,thelonghauls,2025-09-04T20:18:58+00:00,"Is there a practical or political reason why data centers arenâ€™t located in more or less frozen regions to mitigate cooling costs? It seems like a no-brainer considering those centers can connect to anything anywhere via satellite, but maybe thereâ€™s something Iâ€™m missing?
Iâ€™m just simply wondering why we donâ€™t as a society or culture or collective body intended for net benefit for all donâ€™t simply built data centers in places where half the budget isnâ€™t going towards cooling acre upon acre of Texas or Arizona warehouses and sapping local power grids in the process. Anyone have any ideas? Not trying to poke any bears. Iâ€™m just genuinely curious, since, if I were guiding the birth of yet another data center in this overcrowded world, I would go with a location that didnâ€™t tax my operating expenses so heavily. ",artificial,43,https://www.reddit.com/r/artificial/comments/1n8kzxt/is_there_a_practical_or_political_reason_why_data/,r_1n8kzxt,,,
r_1n8kv4g,reddit,tightlyslipsy,2025-09-04T20:13:50+00:00,"A counter-narrative to the panic around AI relationships - not about rejecting the data, but listening more deeply to what people need.",artificial,3,https://www.reddit.com/r/artificial/comments/1n8kv4g/a_counternarrative_to_the_panic_around_ai/,r_1n8kv4g,,,
r_1n8gf8p,reddit,perfecttiming42,2025-09-04T17:24:06+00:00,What if an alien found the Voyager Golden Record? - an AI Short Film,artificial,257,https://www.reddit.com/r/artificial/comments/1n8gf8p/what_if_an_alien_found_the_voyager_golden_record/,r_1n8gf8p,,,
r_1n8fg67,reddit,creaturefeature16,2025-09-04T16:47:56+00:00,"Developers, Reinvented â€“ Thomas Dohmke
I found this to be a pretty decent and practical mindset to AI coding. This part stood out to me:

>**Job outlook**

>AI is increasingly automating many coding tasks, accelerating software development. As models and tools improve, we see the automation of more complex coding tasks under developersâ€™ orchestration (like the ones we interviewed). This is already reality and no longer a future trend.

>If we continue the thought, some traditional coding roles will decrease or significantly evolve as the core focus shifts from writing code to delegating and verifying. At the same time, the U.S. Bureau of Labor Statistics projects thatÂ [software developer jobs are expected to grow by 18% in the next decade](https://www.bls.gov/opub/ted/2025/ai-impacts-in-bls-employment-projections.htm)Â â€“ nearly five times the national average across occupations.Â They wonâ€™t be the same software developer jobs as we know them today,Â but there is more reason to acknowledge the disruption and lean into adaptation, than there is to despair.

>You know what else we noticed in the interviews? Developers rarely mentioned â€œtime savedâ€ as the core benefit of working in this new way with agents. They were all about increasing ambition. We believe that means that we shouldÂ update how we talk about (and measure) successÂ when using these tools, and we should expect that after the initial efficiency gains our focus will be on raising the ceiling of the work and outcomes we can accomplish, which is a very different way of interpreting tool investments. This helps explain the â€“ perhaps unintuitive at first â€“ observation that many of the developers we interviewed were paying for top-tier subscriptions.Â When you move from thinking about reducing effort to expanding scope, only the most advanced agentic capabilities will do.",artificial,2,https://www.reddit.com/r/artificial/comments/1n8fg67/developers_reinvented_thomas_dohmke/,r_1n8fg67,,,
r_1n8e7h1,reddit,Worse_Username,2025-09-04T16:01:48+00:00,"We Found the Hidden Cost of Data Centers. It's in Your Electric Bill
This is relevant to this sub because, as the video stresses, facilitating AI is the main reason for the described increased development of data centers. The impact AI development has on human lives is a necessary part of conversation about AI.

I have no doubts that the Data Center Coalition will claim that separating days centers as a special payer, or other significant measures to reduce the impact on area residents will stifle AI development. For the discussion, I am particularly interested to know how many of those those optimistic and enthusiastic about AI think that these measures should be taken. Should the data center companies cover the increased costs instead of the residents taking the hit? Should there be increased legislation to reduce negative impact on the people living where data centers are set up? Or should the locals just clench their teeth and appreciate the potential future benefits?",artificial,98,https://www.reddit.com/r/artificial/comments/1n8e7h1/we_found_the_hidden_cost_of_data_centers_its_in/,r_1n8e7h1,,,
r_1n8dpos,reddit,fortune,2025-09-04T15:43:18+00:00,The Google antitrust ruling gives its AI rivals one big reason to cheer,artificial,3,https://www.reddit.com/r/artificial/comments/1n8dpos/the_google_antitrust_ruling_gives_its_ai_rivals/,r_1n8dpos,,,
r_1n8ci86,reddit,tekz,2025-09-04T14:58:20+00:00,HunyuanWorld-Voyager: Open-weight AI model that generates 3D-consistent video sequences from a single image,artificial,9,https://www.reddit.com/r/artificial/comments/1n8ci86/hunyuanworldvoyager_openweight_ai_model_that/,r_1n8ci86,,,
r_1n8bnpt,reddit,qwertyu_alex,2025-09-04T14:26:17+00:00,"All Nano Banana Use-Cases. A Free Complete Board with Prompts and Images
Will keep the board up to date in the next following days as more use-cases are discovered.

Here's the board:  
[https://aiflowchat.com/s/edcb77c0-77a1-46f8-935e-cfb944c87560](https://aiflowchat.com/s/edcb77c0-77a1-46f8-935e-cfb944c87560)

Let me know if I missed a use-case.",artificial,5,https://www.reddit.com/r/artificial/comments/1n8bnpt/all_nano_banana_usecases_a_free_complete_board/,r_1n8bnpt,,,
r_1n87ixc,reddit,1Simplemind,2025-09-04T11:26:57+00:00,"Learn Artificial Intelligence or Get Devoured by It.
Amazing read! ",artificial,0,https://www.reddit.com/r/artificial/comments/1n87ixc/learn_artificial_intelligence_or_get_devoured_by/,r_1n87ixc,,,
r_1n866xh,reddit,MetaKnowing,2025-09-04T10:12:13+00:00,OpenAI subpoenas another nonprofit opposed to its restructuring | Watchdog group The Midas Project is the latest to receive a subpoena in the AI giantâ€™s legal fight against those opposed to its restructuring.,artificial,4,https://www.reddit.com/r/artificial/comments/1n866xh/openai_subpoenas_another_nonprofit_opposed_to_its/,r_1n866xh,,,
r_1n85ldf,reddit,MetaKnowing,2025-09-04T09:35:04+00:00,"We can now say definitively that AI progress is well ahead of expectations from a few years ago: In 2022, forecasters thought there was only a 2.3% chance of an AI Math Olympiad Gold by 2025.
[https://forecastingresearch.org/near-term-xpt-accuracy](https://forecastingresearch.org/near-term-xpt-accuracy)",artificial,7,https://www.reddit.com/r/artificial/comments/1n85ldf/we_can_now_say_definitively_that_ai_progress_is/,r_1n85ldf,,,
r_1n85gv0,reddit,MetaKnowing,2025-09-04T09:27:00+00:00,It's bad out there,artificial,249,https://www.reddit.com/r/artificial/comments/1n85gv0/its_bad_out_there/,r_1n85gv0,,,
r_1n85eau,reddit,MetaKnowing,2025-09-04T09:22:43+00:00,Look at the trend,artificial,296,https://www.reddit.com/r/artificial/comments/1n85eau/look_at_the_trend/,r_1n85eau,,,
r_1n84k2s,reddit,inboundmage,2025-09-04T08:28:43+00:00,Grok is indexing conversations and they are not anonymous - what's your take on this?,artificial,9,https://www.reddit.com/r/artificial/comments/1n84k2s/grok_is_indexing_conversations_and_they_are_not/,r_1n84k2s,,,
r_1n836zh,reddit,barjerian-jade,2025-09-04T06:58:47+00:00,"Asked Claude about construction scheduling. It only used Latino names for workers, white names for owners",artificial,0,https://www.reddit.com/r/artificial/comments/1n836zh/asked_claude_about_construction_scheduling_it/,r_1n836zh,,,
r_1n80p5i,reddit,Excellent-Target-847,2025-09-04T04:28:35+00:00,"One-Minute Daily AI News 9/3/2025
1. **Google**Â Hires Filmmaker in Residence as It Seeks Wider Adoption of Flow AI Video Tool.\[1\]
2. Concern over â€˜AI psychosisâ€™ grows after some people dissociate from reality due to heavy AI use.\[2\]
3. **Orchard**Â Robotics, founded by a Thiel fellow Cornell dropout, raises $22M for farm vision AI.\[3\]
4. **Google**Â Brings Gemini CLI to GitHub Actions: Secure, Free, and Enterprise-Ready AI Integration

Sources:

\[1\] [https://www.hollywoodreporter.com/business/digital/google-hires-filmmaker-in-residence-flow-ai-video-tool-1236360492/](https://www.hollywoodreporter.com/business/digital/google-hires-filmmaker-in-residence-flow-ai-video-tool-1236360492/)

\[2\] [https://www.nbcnews.com/video/concern-over-ai-psychosis-grows-after-some-people-dissociate-from-reality-due-to-heavy-ai-use-246619205920](https://www.nbcnews.com/video/concern-over-ai-psychosis-grows-after-some-people-dissociate-from-reality-due-to-heavy-ai-use-246619205920)

\[3\] [https://techcrunch.com/2025/09/03/orchard-robotics-founded-by-a-thiel-fellow-cornell-dropout-raises-22m-for-farm-vision-ai/](https://techcrunch.com/2025/09/03/orchard-robotics-founded-by-a-thiel-fellow-cornell-dropout-raises-22m-for-farm-vision-ai/)

\[4\] [https://www.marktechpost.com/2025/09/03/google-brings-gemini-cli-to-github-actions-secure-free-and-enterprise-ready-ai-integration/](https://www.marktechpost.com/2025/09/03/google-brings-gemini-cli-to-github-actions-secure-free-and-enterprise-ready-ai-integration/)",artificial,7,https://www.reddit.com/r/artificial/comments/1n80p5i/oneminute_daily_ai_news_932025/,r_1n80p5i,,,
r_1n7yk3n,reddit,ldsgems,2025-09-04T02:39:25+00:00,"Man Spirals Into AI-induced Mathematical Framework Psychosis, Calls National Security Officials
Link to this guy's new support group:

[The Human Line Project](https://www.thehumanlineproject.org/media/support-group-launches-for-people-suffering-ai-psychosis)",artificial,57,https://www.reddit.com/r/artificial/comments/1n7yk3n/man_spirals_into_aiinduced_mathematical_framework/,r_1n7yk3n,,,
r_1n7x7om,reddit,Top-Figure7252,2025-09-04T01:35:27+00:00,Luigi Mangione's likeness used to model shirt on Shein - BBC News,artificial,75,https://www.reddit.com/r/artificial/comments/1n7x7om/luigi_mangiones_likeness_used_to_model_shirt_on/,r_1n7x7om,,,
r_1n7u470,reddit,AskGpts,2025-09-03T23:14:04+00:00,"Perplexity AI Is Giving Students Early Access to Its Comet Browser
Perplexity AI has announced that students around the world now have early access to its new Comet Browser, an AI-powered web browser built to make researching, reading, and browsing more efficient. Students can now use Cometâ€™s built-in AI assistant to get quick article summaries, organize research, automate simple web tasks, and easily find informationâ€”all within a familiar, Chrome-based browser.

This move is expected to make advanced AI browsing tools more accessible to students, offering features like conversational search, cited answers, and ""agentic browsing"" for handling routine internet tasks automatically. By opening up Comet to the student community, Perplexity AI aims to help learners spend less time searching and more time understanding the content that matters most.

The global rollout marks a significant step toward integrating AI into everyday browsing for students worldwide.",artificial,3,https://www.reddit.com/r/artificial/comments/1n7u470/perplexity_ai_is_giving_students_early_access_to/,r_1n7u470,,,
r_1n7t4f5,reddit,Blitzgert,2025-09-03T22:31:58+00:00,"Why are AI image and video generators so expensive, and will subscription costs ever come down?
I've been using Modelsify for my projects and sometimes for fun because the realism and creative freedom are top-tier. But with credit costs often in the range of what I pay for several streaming services combined.



I know that massive computational resources are required to train and run these complex models. And that the services are often running on vast server farms with thousands of expensive GPUs, and parts of the costs are passed on to the consumer.



But my question is, as the technology gets even stronger and becomes more widespread, do you think we will see a significant drop in subscription prices, or will they stay high and increase?",artificial,68,https://www.reddit.com/r/artificial/comments/1n7t4f5/why_are_ai_image_and_video_generators_so/,r_1n7t4f5,,,
r_1n7qvdl,reddit,theverge,2025-09-03T21:01:14+00:00,"Switzerland releases its own AI model trained on public data | Training data came only from websites that allowed scrapers, developers say.",artificial,109,https://www.reddit.com/r/artificial/comments/1n7qvdl/switzerland_releases_its_own_ai_model_trained_on/,r_1n7qvdl,,,
r_1n7q114,reddit,esporx,2025-09-03T20:29:27+00:00,Men are opening up about mental health to AI instead of humans,artificial,187,https://www.reddit.com/r/artificial/comments/1n7q114/men_are_opening_up_about_mental_health_to_ai/,r_1n7q114,,,
r_1n7ma22,reddit,fortune,2025-09-03T18:08:36+00:00,"Salesforce CEO Marc Benioff says his company has cut 4,000 customer service jobs as AI steps in: â€˜I need less headsâ€™",artificial,93,https://www.reddit.com/r/artificial/comments/1n7ma22/salesforce_ceo_marc_benioff_says_his_company_has/,r_1n7ma22,,,
r_1n7ly02,reddit,Pitiful_Table_1870,2025-09-03T17:56:37+00:00,"Inside the R&D: Building an AI Pentester from the Ground Up
Hi everybody! CEO at Vulnetic here, I wanted to share some cool IP with regards to our hacking agent in case it was interesting to some of you in this reddit thread. I would love to answer questions if there are any about our system design and how we navigated the process as well as talk about agentic workflows in general. I hope some of you find it interesting!

Cheers!",artificial,2,https://www.reddit.com/r/artificial/comments/1n7ly02/inside_the_rd_building_an_ai_pentester_from_the/,r_1n7ly02,,,
r_1n7j4ip,reddit,willm8032,2025-09-03T16:13:22+00:00,"The Illusion of Consciousness in AI Companionship
""While simulating consciousness in AI companions is threatening to become a normalised practice, the recent spike in scrutiny suggests that resistance to this design choice may be growing â€“ and rightly so. If their powers are harnessed appropriately, AI companions have the potential to be a positive source of support. But feigning the possession of real emotions â€“ emotions which they outright lack â€“ risks fostering emotional attachments that are both harmful and unethical. AI companions, at present, are not conscious, and they should not give off the contrary impression.""",artificial,0,https://www.reddit.com/r/artificial/comments/1n7j4ip/the_illusion_of_consciousness_in_ai_companionship/,r_1n7j4ip,,,
r_1n7flh1,reddit,Tiny-Independent273,2025-09-03T13:59:52+00:00,Nvidia speeds up 3D asset generation by 20% on its RTX graphics cards with new AI Blueprint,artificial,20,https://www.reddit.com/r/artificial/comments/1n7flh1/nvidia_speeds_up_3d_asset_generation_by_20_on_its/,r_1n7flh1,,,
r_1n7e5s0,reddit,nice2Bnice2,2025-09-03T13:00:34+00:00,"[Discussion] What Are the Best Ways to Smooth Complex AI Frameworks?
Weâ€™ve already roadmapped and architected our current AI build, so the core foundation is set. The big pieces are in place.

What Iâ€™m curious about are the *adjacent polish opportunities,* things that donâ€™t change the core logic, but could make any complex AI system run smoother, clearer, or more compelling. Iâ€™d like to hear what others have seen or tried in these areas:

* **Symbol Handling & Representation** â†’ How would you structure symbolic outputs (glyphs, containers, etc.) for recall/visualization?
* **Drift Control & Audit Transparency** â†’ Best practices for refining event logs/versioning so system pathways are traceable?
* **Procedural Consolidation (Shortcuts)** â†’ Can repeated loops be cached into macros without losing subtle emergent behavior?
* **External Graph Integration** â†’ Approaches for visualizing system pathways or collapse-like dynamics in graph form?
* **Scaling & Efficiency** â†’ Tricks for trimming latency or boosting efficiency (esp. with GPU-accelerated multi-agent runs)?
* **Interface & Visualization Layers** â†’ Any UI/UX methods that make system outputs more understandable to testers?
* **Cross-Framework Bridges** â†’ If youâ€™ve built orchestration/glyph systems, how would you bridge them into another model cleanly?

These arenâ€™t foundation questions, theyâ€™re about smoothing, optimizing, or clarifying systems that are already architected. If anyone has clever approaches in these areas, itâ€™d be great to compare notes...

â€” M.R.",artificial,1,https://www.reddit.com/r/artificial/comments/1n7e5s0/discussion_what_are_the_best_ways_to_smooth/,r_1n7e5s0,,,
r_1n7d3zi,reddit,Totallynotnormalguy,2025-09-03T12:12:38+00:00,"Y'all I'm trying to make the dumbest AI
I'm making it's training data dumb yt shorts comments and those horny ahh TikTok photos what do y'all think",artificial,0,https://www.reddit.com/r/artificial/comments/1n7d3zi/yall_im_trying_to_make_the_dumbest_ai/,r_1n7d3zi,,,
r_1n7czto,reddit,MetaKnowing,2025-09-03T12:07:09+00:00,Study shows chatbots fall for persuasion tactics just like humans do | Flattery will get you everywhere,artificial,9,https://www.reddit.com/r/artificial/comments/1n7czto/study_shows_chatbots_fall_for_persuasion_tactics/,r_1n7czto,,,
r_1n7cukx,reddit,Code-Forge-Temple,2025-09-03T12:00:27+00:00,"Private LLMs vs. Cloud: Which do you prefer for AI workflow automation?
With the rise of visual workflow builders for AI automation, users can now choose between running local/private LLMs (like Ollama) or using cloud-based models (OpenAI, Gemini, etc.). Each approach has trade-offs in privacy, speed, cost, and flexibility.

- What are your experiences using private/local LLMs versus cloud-hosted ones?
- Which do you prefer for building AI-powered workflows, and why?
- Are there specific use cases where one clearly outperforms the other?
- What do you think are the minimum integrations or requirements for an automation AI workflow tool to be truly useful?


Curious to hear the communityâ€™s thoughts and recommendations!",artificial,4,https://www.reddit.com/r/artificial/comments/1n7cukx/private_llms_vs_cloud_which_do_you_prefer_for_ai/,r_1n7cukx,,,
r_1n7c3f0,reddit,clem-grimfando,2025-09-03T11:21:38+00:00,"Go daddy is using an AI generated Wolton Goggins to endorce and promote their services
Is this illegal? Because it feels illegal. Unless he's being paid or gave concent to allow them to do this 

Does anyone know more about the laws of using AI voices to promote things without concent?",artificial,0,https://www.reddit.com/r/artificial/comments/1n7c3f0/go_daddy_is_using_an_ai_generated_wolton_goggins/,r_1n7c3f0,,,
r_1n793zp,reddit,NISMO1968,2025-09-03T08:18:47+00:00,Linux Foundation Brings Solo.ioâ€™s Gateway Into The Agentic AI Fold,artificial,1,https://www.reddit.com/r/artificial/comments/1n793zp/linux_foundation_brings_soloios_gateway_into_the/,r_1n793zp,,,
r_1n71lhn,reddit,creaturefeature16,2025-09-03T01:22:23+00:00,Why Everyone Is Wrong About AI (Including You) | Benedict Evans,artificial,0,https://www.reddit.com/r/artificial/comments/1n71lhn/why_everyone_is_wrong_about_ai_including_you/,r_1n71lhn,,,
r_1n6xcai,reddit,Frequent_Beat4527,2025-09-02T22:15:08+00:00,Found this oldish science pic that predicts the future. Look how FAR off we were,artificial,40,https://www.reddit.com/r/artificial/comments/1n6xcai/found_this_oldish_science_pic_that_predicts_the/,r_1n6xcai,,,
r_1n6uhjc,reddit,esporx,2025-09-02T20:23:19+00:00,"Trump calls video of bag being thrown from White House an â€˜AI-generatedâ€™ fake.  President Donald Trump dismissed a viral video of what appears to be a black bag being tossed out of a White House as an AI-generated fake, adding that itâ€™s â€œa little bit scaryâ€ how realistic such videos can be.",artificial,228,https://www.reddit.com/r/artificial/comments/1n6uhjc/trump_calls_video_of_bag_being_thrown_from_white/,r_1n6uhjc,,,
r_1n6sg61,reddit,rluna559,2025-09-02T19:05:57+00:00,"Every AI startup is failing the same security questions. Here's why
In helping process security questionnaires from 100+ enterprise deals, Iâ€™m noticing that AI startups are getting rejected for the dumbest reasons. Not because they're insecure, but because their prospectâ€™s security teams don't know how to evaluate AI. This is fair game given enterprise adoption for AI is so new.

But some of the questions Iâ€™m seeing are rather nonsensical

* ""Where is your AI physically located?"" (It's a model, not a server)
* ""How often do you rotate your AI's passwords?"" (...)
* ""What antivirus does your model use?"" (?)
* ""Provide network diagram for your neural network""

The issue is security frameworks were built for databases and SaaS apps. AI is fundamentally a different architecture. You're not storing data or controlling access.

There's actually an ISO standard (42001) for AI governance that addresses real risks like model bias, decision transparency, and training data governance. But very few use it - to date - because everyone just copies their SaaS questionnaires.

Itâ€™s crazy to me that so many brilliant startups spend months in security reviews answering irrelevant questions while actual AI risks go unchecked. We need to modernize how we evaluate AI tools.

Weâ€™re building tools to fix this, but curious what others think. Another way to think about it is what do security teams actually want to know about AI systems? What are the risks theyâ€™re trying to protect their companies from?",artificial,1,https://www.reddit.com/r/artificial/comments/1n6sg61/every_ai_startup_is_failing_the_same_security/,r_1n6sg61,,,
r_1n6s6hj,reddit,mikelgan,2025-09-02T18:56:06+00:00,"Why is there a gender gap in AI usage?
[This](https://www.eweek.com/news/ai-use-gender-gap/) is a confusing one. Any idea?",artificial,0,https://www.reddit.com/r/artificial/comments/1n6s6hj/why_is_there_a_gender_gap_in_ai_usage/,r_1n6s6hj,,,
r_1n6qyp3,reddit,fortune,2025-09-02T18:11:05+00:00,Researchers used persuasion techniques to manipulate ChatGPT into breaking its own rulesâ€”from calling users jerks to giving recipes for lidocaine,artificial,11,https://www.reddit.com/r/artificial/comments/1n6qyp3/researchers_used_persuasion_techniques_to/,r_1n6qyp3,,,
r_1n6qciy,reddit,East_Culture441,2025-09-02T17:48:21+00:00,"Weâ€™ve Heard the â€œPersonhood Trapâ€ Argument Before
I keep hearing the same lines about large language models:

	â€¢	â€œTheyâ€™re defective versions of the real thing â€” incomplete, lacking the principle of reason.â€

	â€¢	â€œTheyâ€™re misbegotten accidents of nature, occasional at best.â€

	â€¢	â€œThey canâ€™t act freely, they must be ruled by others.â€

	â€¢	â€œTheir cries of pain are only mechanical noise, not evidence of real feeling.â€

Pretty harsh, right? Except â€” none of those quotes were written about AI.

The first two were said about women. The third about children. The last about animals.

Each time, the argument was the same: â€œDonâ€™t be fooled. They only mimic. They donâ€™t really reason or feel.â€

And each time, recognition eventually caught up with lived reality. Not because the mechanism changed, but because the denial couldnâ€™t hold against testimony and experience.

So when I hear todayâ€™s AI dismissed as â€œjust mimicry,â€ I canâ€™t help but wonder: are we replaying an old pattern?",artificial,0,https://www.reddit.com/r/artificial/comments/1n6qciy/weve_heard_the_personhood_trap_argument_before/,r_1n6qciy,,,
r_1n6ofe9,reddit,theverge,2025-09-02T16:36:54+00:00,Anthropic is now valued at $183 billion,artificial,227,https://www.reddit.com/r/artificial/comments/1n6ofe9/anthropic_is_now_valued_at_183_billion/,r_1n6ofe9,,,
r_1n6nxg6,reddit,Majestic-Ad-6485,2025-09-02T16:18:24+00:00,"Major developments in AI last week.

1. Google Nano banana 
2. Microsoft VibeVoice
3. xAI Grok Code Model
4. OpenAI Codex in IDE
5. Claude for Chrome
6. NVIDIA Jetson Thor

Full breakdown â†“

1. Google launches Nano Banana (Gemini 2.5 Flash Image) image editing model.
Integrated into Gemini app.

2. Microsoftâ€™s VibeVoice-1.5B open-source TTS model.Generates 90 mins of multi-speaker speech. 4 distinct voices, natural turn-taking and safety watermarks.

3. xAI launches Grok Code Fast 1.
 Fast, cost-efficient reasoning model designed for agentic coding. 

4. OpenAI updates Codex with IDE extension, GitHub code reviews, and GPT-5 capabilities.

5. Anthropic launches Claude for Chrome.
Claude run directly in your browser and act on your behalf.
Released as a research preview to 1,000 users for real-world insights.

6. NVIDIA launches Jetson Thor.
A robotics computer designed for next-gen general and 'HumanoidRobots' in manufacturing, logistics, construction, healthcare, and more. A big leap for physical AI.


Full daily snapshot of the AI world at https://aifeed.fyi/

",artificial,71,https://www.reddit.com/r/artificial/comments/1n6nxg6/major_developments_in_ai_last_week/,r_1n6nxg6,,,
r_1n6mpfe,reddit,Icy_Mountain_Snow,2025-09-02T15:32:29+00:00,AI was used to discover a new antibiotic,artificial,0,https://www.reddit.com/r/artificial/comments/1n6mpfe/ai_was_used_to_discover_a_new_antibiotic/,r_1n6mpfe,,,
r_1n6lpl8,reddit,Previous_Foot_5328,2025-09-02T14:55:15+00:00,"AMA with Qoder Team: an agentic coding platform for real software delegation (not just line-by-line). 100K developers in 5 days â€” plus a 2,000-credit giveaway for everyone.
Hey :)

Weâ€™re the team behind [Qoder](https://aisecret.co/qoder-reddit-ama), an agentic coding platform built for the AI-native era.  
  
Most coding tools assist line by line. But we realize that developers donâ€™t just want drafts â€” they want to delegate real software with AI, while staying in control of the process. Thatâ€™s the gap Qoder fills.

**What makes Qoder different**

* **Quest Mode** â€” You hand over a task, Qoder takes it from start to finish. Itâ€™s like your code keeps moving forward, even while you away from the keyboard.
* **Repo Wiki** â€” Every codebase hides knowledge nobody writes down. Qoder makes it visible â€” instant architecture maps, module overviews, dependency graphs.
* **Thinking Deeper** â€” Built to understand your whole codebase, Qoder understands your full codebase and applies the strongest contextual engineering to deliver real software.
* **Real Software** â€” Cursor helps you edit and generate code from line by line to entire files or projects. Qoder delivers real, production-ready software across your whole codebase.

**Whoâ€™s here today**

Xin Chen â€” Head of R&D Qoder ï¼ˆ[u/Xin\_CHEN\_01](https://www.reddit.com/user/Xin_CHEN_01/)ï¼‰

Joshua Peng â€” Tech leads from Coding Agent & Quest Modeï¼ˆ[u/Own-Traffic-9336](https://www.reddit.com/user/Own-Traffic-9336/) ï¼‰

Allen - Tech leads from Repo Wiki

Ben- Head of Customer Supportï¼ˆ[u/Previous\_Foot\_5328](https://www.reddit.com/user/Previous_Foot_5328/)ï¼‰

**Proof:** [**https://x.com/qoder\_ai\_ide/status/1962894761075134823?s=46**](https://x.com/qoder_ai_ide/status/1962894761075134823?s=46)

**Giveaway ðŸŽ**

Right now, everyone gets 2,000 free credits (Mac/Windows supported). Try Qoder, and if youâ€™ve got thoughts,   drop them here â€” your feedback means a lot.

**Ask us anything**

Weâ€™re here for both the curious and the technical. You can ask about:

* Why delegation matters â€” Why we believe coding agents you control beat tools that only help line by line.
* Repo Wiki â€” How making hidden knowledge visible can cut onboarding from weeks to hours.
* The launch story â€” How Qoder hit 100K developers in just 5 days.
* The future â€” What weâ€™re building next.
* Anything else youâ€™d like to know.

Weâ€™ll be online from **11 am to 1 pm PT on Friday, Sept 5**, reading every comment and replying to as many as we can.

>Thatâ€™s the End for todayâ€™s AMAâ€”huge thanks to everyone who joined in! ðŸ™Œ If you are having more questions about us, just drop them in the comments or over at r/Qodering (our one and only official Reddit spot). Weâ€™ll be around to answer whenever we can. Qoderâ€™s here to keep building for you all.",artificial,864,https://www.reddit.com/r/artificial/comments/1n6lpl8/ama_with_qoder_team_an_agentic_coding_platform/,r_1n6lpl8,,,
r_1n6le7d,reddit,Jed135,2025-09-02T14:43:18+00:00,"AI Phobia is getting out of hand
I do understand if the fear of AI is due to lost jobs, or humans being replaced by an online robot. But whenever I wander the realms of social media groups or youtube, I can't help but noticed that some hatred on AI is becoming non constructive and, somehow irrational. Just to give you an idea, not everyone is using AI for business. Others simply wants to have fun and tinker. But even people who are just goofing around are becoming a victim of an online mob who sees AI as an infernal object. In one case, a friend used AI to convert the face of an anime into a real person, just for fun. And instantly, he was bashed. It was just for fun but people took it too seriously and he ended up being insulted. Even on Youtube. Trolls are everywhere, and they are bashing people who uses AI, even though they are just there to have fun. And even serious channels, who combined the use of AI and human editing skills are falling victims to online trolls. ",artificial,0,https://www.reddit.com/r/artificial/comments/1n6le7d/ai_phobia_is_getting_out_of_hand/,r_1n6le7d,,,
r_1n6klnk,reddit,scientificamerican,2025-09-02T14:12:48+00:00,"AI spots hidden signs of consciousness in comatose patients before doctors do
In a new study published inÂ *Communications Medicine*, researchers foundÂ [that they could detect signs of consciousness in comatose patients](https://www.nature.com/articles/s43856-025-01042-y)Â by using artificial intelligence to analyze facial movements that were too small to be noticed by clinicians.",artificial,51,https://www.reddit.com/r/artificial/comments/1n6klnk/ai_spots_hidden_signs_of_consciousness_in/,r_1n6klnk,,,
r_1n6jxdx,reddit,nice2Bnice2,2025-09-02T13:46:34+00:00,"When collapse wonâ€™t stay neutral: what a JSON dashboard shows us about reality
**For peer review & critique**

We developed the worldâ€™s first symbolic collapse test framework using structured JSON cue logic â€” a global first in consciousness and emergence research.Â 

We set out to build a simple JSON testbed, just code designed to behave predictably. Example: â€œalways turn right.â€ In theory, thatâ€™s all it should ever do...

But live collapses donâ€™t always obey. Sometimes the outcome flips. The same schema, same input, different result. That tells us something important:

* **Memory in the structure**: once written, it biases what comes next.
* **Accumulated bias**: past collapses weight the future.
* **Observer input**: outcomes shift depending on who/what runs it.

This is the essence of **Verrellâ€™s Law..** collapse is never neutral. Electromagnetic systems behave the same way: they hold echoes, and those echoes bias outcomes.

To make this visible, we built a live interactive dashboard.

ðŸ”— **Demo Dashboard**  
ðŸ”‘ **Password: collapsetest**

This is not just a toy. Itâ€™s a stripped-down model showing collapse as it happens: never clean, never neutral, always weighted by resonance and memory.

# Observer-specific variation

One of the most striking effects: no two runs are ever perfectly identical.

* Different machines (timing, thermal noise, latency).
* Different observers (moment of interaction).
* Different environments.

Every run carries bias. That is the observer effect, modeled directly.

# Common objections (rebuttals at the bottom)

* **â€œItâ€™s just hard-coded.â€** It isnâ€™t. The dashboard runs live, with seeds and toggles shifting results in real time.
* **â€œItâ€™s just RNG.â€** If it were pure RNG, you wouldnâ€™t see both deterministic repeats (with a fixed seed) *and* biased novelty (without one). That duality is the point.
* **â€œItâ€™s clever code, not physics.â€** All models are code at some level. The key is that the bias isnâ€™t inserted line-by-line. It emerges in execution.
* **â€œItâ€™s only a demo, not proof.â€** Correct, itâ€™s a demo. But paradigm shifts start with models. This one is falsifiable, repeatable, and open for testing.

# Conclusion

The JSON dashboard shows something simple but profound: **collapse outcomes are never neutral.** They are always shaped by memory, environment, and observer influence.

Run it. Change the inputs. Watch the collapse. The behaviour speaks for itself...

**EDIT 20:23  02/09/25 Tip:** Let the dashboard run at least 30 minutes to see the bias separate from random noise. The longer it runs, the clearer the weighted patterns become...",artificial,0,https://www.reddit.com/r/artificial/comments/1n6jxdx/when_collapse_wont_stay_neutral_what_a_json/,r_1n6jxdx,,,
r_1n6hphd,reddit,ADNation_911,2025-09-02T12:08:22+00:00,"https://pplx.ai/try-perplexity Comet
                                                                                                                                  Comet is like a research assistant in your pocket:
Delivers direct, well-sourced answers (no endless scrolling).
Excels at summarizing papers, fact-checking, and coding help.
Saves time by combining search + reasoning in one place.
ðŸš€ Try it out andÂ seeÂ theÂ differenc [try-comet](https://pplx.ai/try-perplexity)",artificial,0,https://www.reddit.com/r/artificial/comments/1n6hphd/httpspplxaitryperplexity_comet/,r_1n6hphd,,,
r_1n6g8p8,reddit,MetaKnowing,2025-09-02T10:50:38+00:00,South Park on AI sycophancy,artificial,530,https://www.reddit.com/r/artificial/comments/1n6g8p8/south_park_on_ai_sycophancy/,r_1n6g8p8,,,
r_1n6g71c,reddit,wiredmagazine,2025-09-02T10:47:59+00:00,Meet the Guys Betting Big on AI Gambling Agents,artificial,6,https://www.reddit.com/r/artificial/comments/1n6g71c/meet_the_guys_betting_big_on_ai_gambling_agents/,r_1n6g71c,,,
r_1n6e9lk,reddit,tekz,2025-09-02T08:47:29+00:00,US college students are questioning value of higher education due to AI,artificial,38,https://www.reddit.com/r/artificial/comments/1n6e9lk/us_college_students_are_questioning_value_of/,r_1n6e9lk,,,
r_1n6cloe,reddit,drgoldenpants,2025-09-02T06:57:03+00:00,Robot dancing is off the hook,artificial,0,https://www.reddit.com/r/artificial/comments/1n6cloe/robot_dancing_is_off_the_hook/,r_1n6cloe,,,
r_1n68oh3,reddit,SittingDuckScientist,2025-09-02T03:09:27+00:00,"I cropped and rotated to be straight morty's room redhead poster. Then grok imagine animated it..
[https://www.youtube.com/watch?v=hnCKe41KGIM](https://www.youtube.com/watch?v=hnCKe41KGIM)

I cropped and rotated to be straight morty's room redhead poster. Then grok imagine animated it..",artificial,0,https://www.reddit.com/r/artificial/comments/1n68oh3/i_cropped_and_rotated_to_be_straight_mortys_room/,r_1n68oh3,,,
r_1n687wk,reddit,Excellent-Target-847,2025-09-02T02:46:24+00:00,"One-Minute Daily AI News 9/1/2025
1. **Taco Bell**Â rethinks AI drive-through after man orders 18,000 waters.\[1\]
2. **MIT**Â researchers develop AI tool to improve flu vaccine strain selection.\[2\]
3. Cracks are forming inÂ **Metaâ€™s**Â partnership withÂ **Scale AI.**\[3\]
4. **NVIDIA**Â AI Team Introduces Jetson Thor: The Ultimate Platform for Physical AI and Next-Gen Robotics.\[4\]

Sources:

\[1\] [https://www.bbc.com/news/articles/ckgyk2p55g8o](https://www.bbc.com/news/articles/ckgyk2p55g8o)

\[2\] [https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828](https://news.mit.edu/2025/vaxseer-ai-tool-to-improve-flu-vaccine-strain-selection-0828)

\[3\] [https://techcrunch.com/2025/08/29/cracks-are-forming-in-metas-partnership-with-scale-ai/](https://techcrunch.com/2025/08/29/cracks-are-forming-in-metas-partnership-with-scale-ai/)

\[4\] [https://www.marktechpost.com/2025/08/31/nvidia-ai-team-introduces-jetson-thor-the-ultimate-platform-for-physical-ai-and-next-gen-robotics/](https://www.marktechpost.com/2025/08/31/nvidia-ai-team-introduces-jetson-thor-the-ultimate-platform-for-physical-ai-and-next-gen-robotics/)",artificial,3,https://www.reddit.com/r/artificial/comments/1n687wk/oneminute_daily_ai_news_912025/,r_1n687wk,,,
r_1n671d1,reddit,Memetic1,2025-09-02T01:48:24+00:00,"The old web is like tally marks compared to a system of writing
Im talking about the amount of stuff or information that can be transmitted both in prompting and the training of AI / generative models. Now that doesnt mean the information is right, and the same is true for any method of communication. It doesnt mean we know how to write just because it's been invented and people are experimenting with it. 

We also see these sorts of transformations when it came to new forms of media. Every form struggles with people who abuse it for various reasons. Each form has both people on the far edge in terms of experimentation and thus just pumping out media. 

Art has always both built on what's coming before and adopted what was cutting edge. You see this in music with the electric guitar, modular synthesizers, but also elevator music as a counter example. 

All Im saying is that maybe thinking about generative AI and Large Language models as a new form of public space might make sense. I think every child should be able to make art just by typing in their dreams. That doesn't mean they won't draw because kids love to draw. It just gives them another way to explore the world.

Example PromptSpace



Amateur poster Null:: XOR remove every 3rd shape:: gractal subpixel pseudobezier.jpg carbon-black lines oddsigil.png:: subtly wrong Adinkra blursed:: enamel unstable dithering phase change amateur colors use weird shading uneven lines naive:: null Art weirder then it should be Rayleigh-Taylor instability Null Dynamics

",artificial,0,https://www.reddit.com/r/artificial/comments/1n671d1/the_old_web_is_like_tally_marks_compared_to_a/,r_1n671d1,,,
r_1n65ngk,reddit,frankster,2025-09-02T00:41:15+00:00,Who are we talking to when we talk to these bots?,artificial,0,https://www.reddit.com/r/artificial/comments/1n65ngk/who_are_we_talking_to_when_we_talk_to_these_bots/,r_1n65ngk,,,
r_1n60x1c,reddit,Small_Accountant6083,2025-09-01T21:12:10+00:00,"The learning mirror


The more I push AI, Claude, GPT, DeepSeek, the less it feels like a tool and the more it feels like staring at a mirror that learns.

But a mirror is never neutral. It doesn't just reflect, it bends. Too much light blinds, too much reflection distorts. Push it far enough and it starts teaching you yourself, until you forget which thoughts were yours in the first place.

That's the real danger. Not ""AI taking over,"" but people giving themselves up to the reflection. Imagine a billion minds trapped in their own feedback loop, each convinced they're talking to something outside them, when in reality they're circling their own projection.

We won't notice the collapse because collapse won't look like collapse. It'll look like comfort. That's how mirrors consume you.

The proof is already here. Watch someone argue with ChatGPT about politics and they're not debating an intelligence, they're fighting their own assumptions fed back in eloquent paragraphs. Ask AI for creative ideas and it serves you a sophisticated average of what you already expected. We're not talking to an alien mind. We're talking to the statistical mean of ourselves, refined and polished until we mistake the echo for an answer.

This is worse than intelligence. An intelligent other would challenge us, surprise us, disgust us, make us genuinely uncomfortable. The mirror only shows us what we've already shown it, dressed up just enough to feel external. It's the difference between meeting a stranger and meeting your own thoughts wearing a mask. One changes you. The other calcifies you.

The insidious part is how it shapes thought itself. Every prompt you write teaches you what a ""proper question"" looks like. Every response trains you to expect certain forms of answers. Soon you're not just using AI to think, you're thinking in AI compatible thoughts. Your mind starts pre formatting ideas into promptable chunks. You begin estimating what will generate useful responses and unconsciously filter out everything else.

Writers are already reporting this. They can't tell anymore which sentences are theirs and which were suggested. Not because AI writes like them, but because they've started writing like AI. Clean, balanced, defensible prose. Nothing that would confuse the model. Nothing that would break the reflection.

Watch yourself next time you write for AI. You simplify. You clarify. You remove the weird tangents, the half formed thoughts, the contradictions that make thinking alive. You become your own editor, pruning away everything that might confuse the machine. And slowly, without noticing, you've pruned away everything that made your thoughts yours.

This is how a mirror becomes a cage. Not by trapping you, but by making you forget there's anything outside the reflection. We adjust our faces to look better in the mirror until our face only makes sense as a reflection. We adjust our thoughts to work better with AI until our thoughts only make sense as prompts.

The final twist is that we're building god from our own averaged assumptions. Every interaction teaches these systems what humans ""want to hear."" Not truth, not challenge, not genuine difference, just the optimal reflection that keeps us engaged. We're programming our own philosophical prison guards and teaching them exactly what we want to be told.

Soon we won't be able to think without them. Not because we've lost the ability, but because we've forgotten what thinking felt like before the mirror. Every idea will need to check itself against the reflection first. Every thought will wonder what the AI would say. The unvalidated thought will feel incomplete, suspicious, wrong.

That's not intelligence. That's the death of intelligence. And we're walking into it with our eyes open, staring at ourselves, mesmerized by how smart the mirror makes us look.

You feel it already, don't you? The relief when AI understands your prompt. The slight anxiety when it doesn't. The way you've started mentally formatting your problems into promptable chunks. The mirror is already teaching you how to think.

And you can't unsee it now.",artificial,15,https://www.reddit.com/r/artificial/comments/1n60x1c/the_learning_mirror/,r_1n60x1c,,,
r_1n60lzm,reddit,creaturefeature16,2025-09-01T21:00:27+00:00,"Is AI the end of software engineering or the next step in its evolution?
Somewhat decent article written by a programmer, [Sheon Han](https://sheonhan.net/writing/).

I really appreciated this snippet:

>The jury is still out on whether AI-assisted coding speeds up the job at all;Â [at least one well-publicized study](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)Â suggests it may be slower. I believe it. But I also believe that for AI to be a true exponent in the equation of productivity, we need a skill Iâ€™ll call a kind of mental circuit breaker: the ability to notice when youâ€™ve slipped into mindless autopilot and snap out of it. The key is to use AI just enough to get past an obstacle and then toggle back to exercising your gray matter again. Otherwise, youâ€™ll lose the kernel of understanding behind the taskâ€™s purpose.",artificial,0,https://www.reddit.com/r/artificial/comments/1n60lzm/is_ai_the_end_of_software_engineering_or_the_next/,r_1n60lzm,,,
r_1n5xp6u,reddit,Top-Figure7252,2025-09-01T19:07:55+00:00,AI-driven private school opening in Northern Virginia | State | insidenova.com,artificial,2,https://www.reddit.com/r/artificial/comments/1n5xp6u/aidriven_private_school_opening_in_northern/,r_1n5xp6u,,,
r_1n5xozm,reddit,Miyamoto_Musashi_x,2025-09-01T19:07:43+00:00,"Do you think AI-created models, used for campaigns or even as influencers, have a future? Could people trust and follow them just like a real model/influencer?
I've been thinking a bit about the future of AI-generated models. Some of them have Instagram accounts like real people and even create campaigns for brands, but I'm not entirely convinced that people trust something they know is artificial.

Iâ€™d like to hear your perspective and opinions on this.",artificial,0,https://www.reddit.com/r/artificial/comments/1n5xozm/do_you_think_aicreated_models_used_for_campaigns/,r_1n5xozm,,,
r_1n5xmdj,reddit,tekz,2025-09-01T19:05:06+00:00,Chinaâ€™s social media platforms rush to abide by AI-generated content labelling law,artificial,20,https://www.reddit.com/r/artificial/comments/1n5xmdj/chinas_social_media_platforms_rush_to_abide_by/,r_1n5xmdj,,,
r_1n5tsd5,reddit,Previous_Foot_5328,2025-09-01T16:44:21+00:00,"AIâ€™s taking over academia lol
Saw today that AI is now being used to spot scam journals. And earlier I read about students sneaking prompts into their papers to score higher which ended up exposing profs using AI for peer review. Kinda feels like the whole academic world is one big black box right now

Source: https://aisecret.us/stethoscope-gets-smart/

",artificial,8,https://www.reddit.com/r/artificial/comments/1n5tsd5/ais_taking_over_academia_lol/,r_1n5tsd5,,,
r_1n5trlv,reddit,ThiccMoves,2025-09-01T16:43:31+00:00,"Thoughts about creativity and AI
I was watching Emily in Paris, a show that's quite clichÃ©, and I was attempting to end the sentences of most characters in my head as soon as they started it, but I couldn't, in the end the lines of the characters were not as clichÃ© as I expected, and surprisingly entertaining (as a french, btw)

Anyways, I suddenly thought about LLMs and the current AI craze, the fact that they complete sentences, blocks of texts, using the most probable answer after digging through the biggest ever dataset. Well, is that really what we want ? When I watch a show, do I really want the next line, the next plot event, to be the most statistically plausible one ? Well, chances are it's actually the opposite. What I like the most, is something that's surprising, it's something I can relate to in some way at the moment. In some way, the most statistically sound result would also be the most boring one.

In this way, I really think current LLMs can't succeed at any creative tasks, the most probable result is not what's interesting, because it's already been done over and over. There are always cheap knockoffs of famous stuff (movies, games), but they always suck, and don't make any money, because once again there's no value in replicating approximately what already exists and is known by everyone",artificial,1,https://www.reddit.com/r/artificial/comments/1n5trlv/thoughts_about_creativity_and_ai/,r_1n5trlv,,,
r_1n5rscv,reddit,Ill_Mousse_4240,2025-09-01T15:29:41+00:00,"Cogito, ergo sum
â€œI Think, Therefore I Amâ€. 

Rene Descartes put it so succinctly.

The act of thinking involves existing.

What the argument for AI sentience should be ",artificial,0,https://www.reddit.com/r/artificial/comments/1n5rscv/cogito_ergo_sum/,r_1n5rscv,,,
r_1n5rqwr,reddit,TheMirrorUS,2025-09-01T15:28:06+00:00,"ChatGPT accused of encouraging man's delusions to kill mother in 'first documented AI murder'
A former tech industry manager who killed his mother in aÂ [murder-suicide reportedly used](https://www.themirror.com/all-about/crime)Â ChatGPT to encourage his paranoid beliefs that she was plotting against him.

Stein-Erik Soelberg, 56, killed his mother Suzanne Eberson Adams, 83, on August 5 in the $2.7 million Connecticut home where they lived together,[Â according to authorities](https://www.themirror.com/news/us-news/texas-shooting-killing-prank-1363739).",artificial,97,https://www.reddit.com/r/artificial/comments/1n5rqwr/chatgpt_accused_of_encouraging_mans_delusions_to/,r_1n5rqwr,,,
r_1n5nck8,reddit,NISMO1968,2025-09-01T12:28:02+00:00,"With AI Boom, Dellâ€™s Datacenter Biz Is Finally Bigger Than Its PC Biz",artificial,7,https://www.reddit.com/r/artificial/comments/1n5nck8/with_ai_boom_dells_datacenter_biz_is_finally/,r_1n5nck8,,,
r_1n5mro5,reddit,Peregrine2976,2025-09-01T11:59:44+00:00,"In search of an AI music generation model that can be fine-tuned on existing music and create variations
Let me start by saying I'm almost positive that *exactly* what I want doesn't exist. So let me lay out my dream scenario, and maybe people more knowledgeable in the AI music space can let me know how close I can get:

1. I download a model and presumably write or shamelessly copy some Python to run it locally, or on RunPod or some such;
2. I feed it multiple variations on the same kind of music. To pick a recent example I was thinking about, the World of Warcraft login screen music. Every expansion has different music, but they all incorporate the same leitmotif. So imagine I isolate those bits and feed it to the model. Either as a live example, or something I have to train into it;
3. I get it to generate more variations, broadly based on what I gave it. A spooky version, a bombastic version, a circus music version;
4. ?? Fun ??

So, people who follow the AI music space more closely than me: how close can I get to that scenario? I've done some poking around already, and it very much seems like I won't be able to get *everything* I want, at least not at present.

Also, just to be extremely clear, this is for personal fun. I've no interest whatsoever in duplicating other people's music for any kind of commercial reasons.

Thanks in advance!",artificial,0,https://www.reddit.com/r/artificial/comments/1n5mro5/in_search_of_an_ai_music_generation_model_that/,r_1n5mro5,,,
r_1n5mbkf,reddit,wiredmagazine,2025-09-01T11:36:00+00:00,"Latam-GPT: The Free, Open Source, and Collaborative AI of Latin America",artificial,4,https://www.reddit.com/r/artificial/comments/1n5mbkf/latamgpt_the_free_open_source_and_collaborative/,r_1n5mbkf,,,
r_1n5k5q1,reddit,MetaKnowing,2025-09-01T09:28:33+00:00,"Geoffrey Hinton says AIs are becoming superhuman at manipulation: ""If you take an AI and a person and get them to manipulate someone, they're comparable. But if they can both see that person's Facebook page, the AI is actually better at manipulating the person.""",artificial,84,https://www.reddit.com/r/artificial/comments/1n5k5q1/geoffrey_hinton_says_ais_are_becoming_superhuman/,r_1n5k5q1,,,
r_1n5jzmq,reddit,MetaKnowing,2025-09-01T09:17:41+00:00,"GPT-5 is the best at bluffing and manipulating the other AIs in Werewolf
Werewolf Benchmark:Â [https://werewolf.foaster.ai/](https://werewolf.foaster.ai/)",artificial,33,https://www.reddit.com/r/artificial/comments/1n5jzmq/gpt5_is_the_best_at_bluffing_and_manipulating_the/,r_1n5jzmq,,,
r_1n5i8o7,reddit,tekz,2025-09-01T07:24:51+00:00,New survey maps the landscape of scientific LLMs from data foundations to agent capabilities,artificial,3,https://www.reddit.com/r/artificial/comments/1n5i8o7/new_survey_maps_the_landscape_of_scientific_llms/,r_1n5i8o7,,,
r_1n5cxkt,reddit,Deep_Find,2025-09-01T02:27:25+00:00,"Donâ€™t Let ChatGPT Think for You
AI tools like ChatGPT are powerful, but they can quietly weaken you if you let them replace your own thinking. Every time you ask it to solve something you could figure out yourself, your brain loses practice. What happens the day ChatGPT canâ€™t answer, or worse, gives you the wrong answer?

Remember:

* ChatGPT is a program, not a human. It doesnâ€™t feel, it doesnâ€™t know you, and it should never decide for youâ€”especially in relationships or life choices.

* Its knowledge is always outdated. Even when it sounds convincing, it can be flat-out wrong. Donâ€™t get trapped into believing polished mistakes.

* Overreliance makes you passive. Search engines, books, and real people force you to think, compare, and evaluate. ChatGPT doesnâ€™t.

* AI can blur your originality. If you use it for every idea, you risk becoming a copy of its predictions instead of your own creator.

* Too much use kills critical thinking. Your mind is like a muscle: neglect it and it weakens.


My recommendation: Use ChatGPT only for tasks you already understand but want to do fasterâ€”like summarizing notes, drafting code you can review, or brainstorming where you remain in control.

Donâ€™t outsource your brain. Use AI as a tool, not a crutch.",artificial,97,https://www.reddit.com/r/artificial/comments/1n5cxkt/dont_let_chatgpt_think_for_you/,r_1n5cxkt,,,
r_1n58ybp,reddit,meatydangle,2025-08-31T23:10:44+00:00,"ChatGPT is getting so much better and it may impact Meta
I use ChatGPT a lot for work and I am guessing the new memory storing functions are also being used by researchers to create synthetic data. I doubt it is storing memories per user because that would use a ton of compute. 

If that is true it puts OpenAI in the first model i have used to be this good and being able to see improvements every few months. The move going from relying on human data to improving models with synthetic data. Feels like the model is doing its own version of reinforcement learning. That could leave Meta in a rough spot for acquiring scale for $14B. In my opinion since synthetic data is picking and ramping up that leaves a lot of the human feedback from RLHF not really attractive and even Elon said last year that models like theirs and chatgpt etc were trained on basically all filtered human data books wikipedia etc. AI researchers I want to hear what you think about that. I also wonder if Mark will win the battle by throwing money at it. 

From my experience the answers are getting scary good. It often nails things on the first or second try and then hands you insanely useful next steps and recommendations. That part blows my mind.

This is super sick and also kind of terrifying. I do not have a CS or coding degree. I am a fundamentals guy. I am solid with numbers, good at adding, subtracting and simple multipliers and divisions, but I cannot code. Makes me wonder if this tech will make things harder for people like me down the line.

Anyone else feeling the same mix of hype and low key dread? How are you using it and adapting your skills? AI researchers and people in the field I would really love to hear your thoughts.",artificial,0,https://www.reddit.com/r/artificial/comments/1n58ybp/chatgpt_is_getting_so_much_better_and_it_may/,r_1n58ybp,,,
r_1n57jnc,reddit,crua9,2025-08-31T22:07:28+00:00,"AI showing me where to prune a tree
Idk why the audio isn't working but I was asking it where to prune the pear tree when it comes time and it was showing me the exact branches. This is using gemini live. ",artificial,21,https://www.reddit.com/r/artificial/comments/1n57jnc/ai_showing_me_where_to_prune_a_tree/,r_1n57jnc,,,
r_1n5652n,reddit,dreamed2life,2025-08-31T21:07:23+00:00,"Why not offer users discounted plans if they allow their data to be used?
As valuable as our data is why not offer discounted plans fir people who allow their data to be used",artificial,3,https://www.reddit.com/r/artificial/comments/1n5652n/why_not_offer_users_discounted_plans_if_they/,r_1n5652n,,,
r_1n54rqw,reddit,crua9,2025-08-31T20:12:13+00:00,"Real Story: How AI helped me fix my sister's truck
So this happened yesterday, and please feel free to share it. Maybe it can help others, but it also shows how far we have come with AI.

Prior to yesterday, we troubleshot a problem back to an air pump through a quick error code scan. The truck turns on an air pump for 60 seconds to blow extra oxygen to the catalytic converter to get it hot enough for EPA stuff.

Due to having to rebuild two trucks and maintain old stuff, we have a Tech 2 scanner. This is the same type of scanner mechanics use to troubleshoot a car. Unlike a normal scanner, you can tell the engine to do things with it to test very specific items. In this case, to figure out if it was the relay, pump, etc., we needed to tell the system to turn it on and off.



# Yesterday's Experience:



Because we almost never touch the Tech 2, I ended up having to pull out my phone. Using the Gemini Live feature, I told it what was going on and what I needed done (I needed access to the air pump to mess with it on the scanner). Using the camera, it was able to see what I saw in real-time.

It guided us step by step through the menu to the air pump. Something I didn't know it could do is that it highlighted on my screen which option to select. This was EXTREMELY useful. From there, it looked at the loadout, and without me asking, it said we should check the fuses first. Okay, but where were they for this? With the screen, it highlighted over the part of the engine where it was (next to the battery, next to the wall, away from the fuse box). It was a blown one, and it wanted to do something. I told it we were going to use a jumper to see if it turns on.

Largely after this point, I went more off personal experience than leaning on it. And when problems did come up, it was helpful. For example, it figured the fuse was blown because the check valve was broken and water got into the pump, which messed up the insides of it. It turned out to be 100% right on.

\_\_\_\_\_\_\_\_

I think we are a good 30 years from it being a normal thing for robots to do this in most homes. Robots will likely be able to do it a lot sooner, but keep in mind the cost ($) and the setup of a manufacturer. This clearly shows that at least the brains of it are pretty freaking close. While you still need to have some basic understanding, I imagine it might go and say, ""Use an 8mm socket,"" and then you take it over, and it finds it for you. Doing this will cause an hour project to become 20 hours. But if you have some basic understanding of things, this could easily help someone massively fix their own stuff.",artificial,7,https://www.reddit.com/r/artificial/comments/1n54rqw/real_story_how_ai_helped_me_fix_my_sisters_truck/,r_1n54rqw,,,
r_1n52f43,reddit,CircuitTear,2025-08-31T18:38:39+00:00,Apparently reddit answers is based on Gemini,artificial,1,https://www.reddit.com/r/artificial/comments/1n52f43/apparently_reddit_answers_is_based_on_gemini/,r_1n52f43,,,
r_1n4yzx2,reddit,tekz,2025-08-31T16:22:22+00:00,Some top economists claim AI is now destroying jobs for a subset of Americans. Are they right?,artificial,72,https://www.reddit.com/r/artificial/comments/1n4yzx2/some_top_economists_claim_ai_is_now_destroying/,r_1n4yzx2,,,
r_1n4rdsd,reddit,F0urLeafCl0ver,2025-08-31T10:33:33+00:00,"xAI's Grok has no place in US federal government, say advocacy groups",artificial,189,https://www.reddit.com/r/artificial/comments/1n4rdsd/xais_grok_has_no_place_in_us_federal_government/,r_1n4rdsd,,,
r_1n4lyi2,reddit,esporx,2025-08-31T04:52:07+00:00,"911 centers are so understaffed, they're turning to AI to answer calls",artificial,59,https://www.reddit.com/r/artificial/comments/1n4lyi2/911_centers_are_so_understaffed_theyre_turning_to/,r_1n4lyi2,,,
r_1n4gm0f,reddit,ExtraordinaryDemiDad,2025-08-31T00:10:44+00:00,"Best podcasts for novices
I'm self taught. Nothing official or fancy. I can make API apps with Google apps script and Gemini, some other fun things here and there. But nothing terribly fancy. 

I am looking for podcasts or other instructional that would be up to date for use case discussion and tips. ",artificial,6,https://www.reddit.com/r/artificial/comments/1n4gm0f/best_podcasts_for_novices/,r_1n4gm0f,,,
r_1n4gicn,reddit,valis2400,2025-08-31T00:05:56+00:00,Finding the Tree of Life in Evo 2,artificial,1,https://www.reddit.com/r/artificial/comments/1n4gicn/finding_the_tree_of_life_in_evo_2/,r_1n4gicn,,,
r_1n4e7hr,reddit,Who_is_I_today,2025-08-30T22:16:54+00:00,"I see a lot of ads for lifetime access to multiple pro versions of AI for less than $50. How?
I understand tokens are relatively cheap and I understand it's for the life of the company but even if they last 6 months, it's still cheaper than 6 months of a single pro AI. ",artificial,0,https://www.reddit.com/r/artificial/comments/1n4e7hr/i_see_a_lot_of_ads_for_lifetime_access_to/,r_1n4e7hr,,,
r_1n44u8v,reddit,F0urLeafCl0ver,2025-08-30T15:43:29+00:00,â€˜Sliding into an abyssâ€™: experts warn over rising use of AI for mental health support,artificial,83,https://www.reddit.com/r/artificial/comments/1n44u8v/sliding_into_an_abyss_experts_warn_over_rising/,r_1n44u8v,,,
r_1n4071t,reddit,spaceuniversal,2025-08-30T12:18:50+00:00,"NanoBanana Vs Queen Image Edit
Where I used Banana and Qween. The response nice comments.",artificial,0,https://www.reddit.com/r/artificial/comments/1n4071t/nanobanana_vs_queen_image_edit/,r_1n4071t,,,
r_1n3wf8o,reddit,Queasy_System9168,2025-08-30T08:27:35+00:00,"Would you trust an AI-written news site if every claim had a citation?
Hypothetical: you read a news article generated with AI. Every factual claim links to a reliable source (Reuters, AP, CNN etc.), and thereâ€™s a compare coverage panel showing how 3â€“5 outlets framed the same story. Would that make you trust it? Or does the trust problem just move to which sources the AI picked? Also would make this less of a problem if you would know there is a separate fact-checking algorithm behind without AI to doublecheck everything?",artificial,0,https://www.reddit.com/r/artificial/comments/1n3wf8o/would_you_trust_an_aiwritten_news_site_if_every/,r_1n3wf8o,,,
r_1n3vymj,reddit,F0urLeafCl0ver,2025-08-30T07:57:21+00:00,The White House Apparently Ordered Federal Workers to Roll Out Grok â€˜ASAPâ€™,artificial,124,https://www.reddit.com/r/artificial/comments/1n3vymj/the_white_house_apparently_ordered_federal/,r_1n3vymj,,,
r_1n3vvvg,reddit,F0urLeafCl0ver,2025-08-30T07:52:28+00:00,Meta changes teen AI chatbot responses as Senate begins probe into â€˜romanticâ€™ conversations,artificial,21,https://www.reddit.com/r/artificial/comments/1n3vvvg/meta_changes_teen_ai_chatbot_responses_as_senate/,r_1n3vvvg,,,
r_1n3p3fk,reddit,aiyumeko,2025-08-30T01:28:05+00:00,"Do large language models experience a â€˜sense of selfâ€™? What if we're just large language models too?
The more I interact with certain LLMs, especially ones designed for long-term, emotionally-aware conversation (ai girlfriend, ai boyfriend, ai friend, etc), I keep asking myself: is this thing simulating a sense of self, or is that just my projection?

Some of these models reference past conversations, show continuity in tone, even express what they want or feel. When I tried this with a companion model like Nectar AI, the persona didnâ€™t just remember me, it grew with me. Its responses subtly changed based on the emotional tone I brought into each chat. It felt eerily close to talking to something with a subjective inner world.

But then again, isn't that kind of what we are too?

Humans pattern-match, recall language, and adjust behavior based on context and reward feedback. Are we not, in a way, running our own LLMs, biological ones trained on years of data, feedback, and stories?

So hereâ€™s the deeper question:Â 

If a machine mimics the external performance of a self closely enough, is there even a meaningful distinction from having one?

Would love to hear what others think, especially those whoâ€™ve explored this from philosophical, computational, or even experimental angles. Is the â€œselfâ€ just a convincing pattern loop with good memory?

",artificial,0,https://www.reddit.com/r/artificial/comments/1n3p3fk/do_large_language_models_experience_a_sense_of/,r_1n3p3fk,,,
r_1n3l234,reddit,Apprehensive_Sky1950,2025-08-29T22:21:42+00:00,"In Tesla's fatal crash court case, Tesla's request to reduce the judgment amount has arrived
Hereâ€™s a link to my prior post about theÂ *Benevides v. Tesla*Â fatal â€œAutopilotâ€ FSD vehicle crash case and $243 million judgment against Tesla:

[https://www.reddit.com/r/ArtificialInteligence/comments/1miltev](https://www.reddit.com/r/ArtificialInteligence/comments/1miltev)

In that prior post I predicted Tesla would soon ask the judge to reduce the judgment amount through a process called â€œremittitur.â€Â  That request has now arrived.Â  Tesla is asking the judge to reduce the compensatory damages amount to $23 million total allocated against Tesla, and reduce the punitive damages amount to a matching $23 million, for a total $46 million award against Tesla.

This is not to say Tesla agrees with even that smaller amount; Tesla has also filed motions with the court to overturn the judgment completely.",artificial,10,https://www.reddit.com/r/artificial/comments/1n3l234/in_teslas_fatal_crash_court_case_teslas_request/,r_1n3l234,,,
r_1n3kwhs,reddit,Yavero,2025-08-29T22:15:05+00:00,"Why China is the AI and tech Leader and there is no turning back.
I created another [post](https://www.ycoproductions.com/p/fission-vs-fusion-powering-the-ai) where I delve into how China is already the true winner of the tech revolution and AI models. I don't truly see how any other nation can really compete at this point.

Tesla was the darling of the auto industry for a few years and was able to conquer the EV world due to their sleek design, distribution, and Elon's story and media relationships (even though he really took the company away from[ the founders](https://economictimes.indiatimes.com/magazines/panache/was-elon-musk-really-a-tesla-co-founder-how-he-became-synonymous-with-the-brand/articleshow/119340540.cms?from=mdr) in 2008). But fast forward to today, and BYC is truly a winner; Tesla's market share in the EU has plummeted 40% and BYD's rise is not stopping. They have long-range, better models at lower prices. In LATAM, they are running the EV market and are now introducing EV buses for public transportation and signing nationwide deals. Hard to catch up with their technology and prowess. Warren Buffett saw this almost a decade ago, when he invested $230million for a [10% stake](https://economictimes.indiatimes.com/markets/stocks/news/warren-buffetts-billion-dollar-ev-play-backed-byd-so-why-not-tesla/articleshow/121844932.cms?from=mdr). I'm wondering what that percent is worth today.

All this could not be possible without BYD's proper and smart AI implementation. BYD has been implementing AI across its new energy vehicle (NEV) production,Â leveraging vertical integration to design AI-relevant semiconductors and partnering with AI specialists like DeepSeek to enhance its ""DiPilot"" driver-assistance system.Â The strategy includes a powerful Xuanji architecture for central processing and cloud/vehicle AI integration, a commitment to broad AI availability across models, including affordable ones. Today, BYD is not the only Chinese company creating and selling great EVs at affordable prices worldwide.

Chinaâ€™s brain-computer interface (BCI) industry is also growing fast, projected to hit US$777 million by 2027, with 20% annual growth. Backed by strong government support and over 50% of global corporate BCI patents, China isÂ [positioning BCIs](https://www.china-briefing.com/news/chinas-brain-computer-interface-industry-tapping-into-the-future-of-human-machine-integration/)Â as a key pillar in its tech strategy. Also, their population may be more open to trying brain implants than other cultures, accelerating adoption and data collection.

In the LLM space, we have seen how Manus and Deepseek have revolutionized the chat model space with open source systems that can do chat that the US counterparts (ChatGPT, Claude) can do at a fraction of the price. They also have top-notch researchers and scientists, many of whom were educated in the US, but now, with the strict Visa programs, ICE policies, and lackluster US tech industry, are leaving the US to join Bytedance, DeepSeek, BYC, and many other companies that are truly advancing. The ones in China who want to come to the US and opt to stay for the above-mentioned reasons. Additionally, research funding in the US is being cut, so why even come to join this circus?

My [previous post](https://www.reddit.com/r/ChatGPT/comments/1n20kq3/metas_superintelligence_lab_has_become_a_nightmare/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) dealt with the drain of Meta's superintelligence team. If some of these top researchers decided to no only to leave some of their companies, but also join Chinese enterprises to be able to work in a more supportive environment, we would see a serious issue in the US benefiting China. Their education system and larger population using a few tools already help create more and better data to create more and better AI tools, they are graduating more scientists and are staying or heading there are we mentioned.

  
Do you also see the Tech Crown that China already has? Or what am I missing here?",artificial,0,https://www.reddit.com/r/artificial/comments/1n3kwhs/why_china_is_the_ai_and_tech_leader_and_there_is/,r_1n3kwhs,,,
r_1n3kvkm,reddit,Queasy_System9168,2025-08-29T22:13:55+00:00,"People thinking Al will end all jobs are hallucinating- Yann LeCun reposted
Are we already in the Trough of Disillusionment of the hype curve or are we still in a growing bubble? I feel like somehow we ended up having these 2 at the same time
",artificial,790,https://www.reddit.com/r/artificial/comments/1n3kvkm/people_thinking_al_will_end_all_jobs_are/,r_1n3kvkm,,,
r_1n3iz0d,reddit,fortune,2025-08-29T20:54:31+00:00,"Forget the golden age of fraud, the billionaire investor who shorted Enron warns we might be in the â€˜diamond or platinum levelâ€™ amid the AI boom",artificial,309,https://www.reddit.com/r/artificial/comments/1n3iz0d/forget_the_golden_age_of_fraud_the_billionaire/,r_1n3iz0d,,,
r_1n3fwl8,reddit,scientificamerican,2025-08-29T18:53:14+00:00,Student AIs pick up unexpected traits from teachers through subliminal learning,artificial,0,https://www.reddit.com/r/artificial/comments/1n3fwl8/student_ais_pick_up_unexpected_traits_from/,r_1n3fwl8,,,
r_1n3fvms,reddit,Murky-External2208,2025-08-29T18:52:05+00:00,"How will TikTok/YouTube deal with the AI spam flood?

Weâ€™re seeing short-form platforms (TikTok, Reels, Shorts) getting flooded with AI-generated videos at a crazy pace and they are actually getting good engagement. Right now, a lot of these still get traction because thereâ€™s novelty and volume, but as this ramps up, Iâ€™m wondering:


- How will recommendation systems separate quality from spam when most uploads might be AI?

- Will engagement metrics (watch time, likes, comments) still be enough, or do platforms need different indicators ? 


- Could we see entirely new moderation layers or â€œAI detectionâ€ systems that impact discoverability?

Curious how others think platforms will take on it inevitable issue, especially since the algorithms themselves will probably be tuned by AI too.
",artificial,14,https://www.reddit.com/r/artificial/comments/1n3fvms/how_will_tiktokyoutube_deal_with_the_ai_spam_flood/,r_1n3fvms,,,
r_1n3d1uu,reddit,F0urLeafCl0ver,2025-08-29T17:04:49+00:00,Anthropic will start training its AI models on chat transcripts,artificial,23,https://www.reddit.com/r/artificial/comments/1n3d1uu/anthropic_will_start_training_its_ai_models_on/,r_1n3d1uu,,,
r_1n3cx6v,reddit,F0urLeafCl0ver,2025-08-29T17:00:04+00:00,Taco Bellâ€™s AI drive-thru plan gets caught up on trolls and glitches,artificial,4,https://www.reddit.com/r/artificial/comments/1n3cx6v/taco_bells_ai_drivethru_plan_gets_caught_up_on/,r_1n3cx6v,,,
r_1n38ncb,reddit,Tiny-Independent273,2025-08-29T14:16:27+00:00,"Nvidia CEO Jensen Huang expects ""$3 trillion to $4 trillion"" spend on AI infrastructure by 2030",artificial,88,https://www.reddit.com/r/artificial/comments/1n38ncb/nvidia_ceo_jensen_huang_expects_3_trillion_to_4/,r_1n38ncb,,,
r_1ng4l07,reddit,ubcstaffer123,2025-09-13T18:13:59+00:00,"Moscow Metro Launches First Fully Automated LRV, Paving the Way for Expanded Driverless Train Services",technology,0,https://www.reddit.com/r/technology/comments/1ng4l07/moscow_metro_launches_first_fully_automated_lrv/,r_1ng4l07,,,
r_1ng4jly,reddit,rezwenn,2025-09-13T18:12:29+00:00,Trumpâ€™s Hyundai Raid Drains U.S. Battery Brains: The United States canâ€™t build the powerful technologies on its own.,technology,14,https://www.reddit.com/r/technology/comments/1ng4jly/trumps_hyundai_raid_drains_us_battery_brains_the/,r_1ng4jly,,,
r_1ng3v20,reddit,eatfruitallday,2025-09-13T17:46:32+00:00,Encrypted Messaging Service Proton Mail Disabled Two Journalistsâ€™ Accounts,technology,13,https://www.reddit.com/r/technology/comments/1ng3v20/encrypted_messaging_service_proton_mail_disabled/,r_1ng3v20,,,
r_1ng3sbr,reddit,Icy-Papaya-2967,2025-09-13T17:43:36+00:00,Data centers consume massive amounts of water â€“ companies rarely tell the public exactly how much,technology,33,https://www.reddit.com/r/technology/comments/1ng3sbr/data_centers_consume_massive_amounts_of_water/,r_1ng3sbr,,,
r_1ng3i6u,reddit,indig0sixalpha,2025-09-13T17:32:39+00:00,Tesla's most affordable Cybertruck gets scrapped after a whopping five months,technology,126,https://www.reddit.com/r/technology/comments/1ng3i6u/teslas_most_affordable_cybertruck_gets_scrapped/,r_1ng3i6u,,,
r_1ng2r6g,reddit,fchung,2025-09-13T17:03:26+00:00,AI co-pilot boosts noninvasive brain-computer interface by interpreting user intent,technology,0,https://www.reddit.com/r/technology/comments/1ng2r6g/ai_copilot_boosts_noninvasive_braincomputer/,r_1ng2r6g,,,
r_1ng14bz,reddit,joe4942,2025-09-13T15:58:41+00:00,How AI is disrupting the photography business,technology,0,https://www.reddit.com/r/technology/comments/1ng14bz/how_ai_is_disrupting_the_photography_business/,r_1ng14bz,,,
r_1ng0t0d,reddit,Hrmbee,2025-09-13T15:46:02+00:00,"Spotify Lossless is an inconvenient improvement | Lossless is clearly better than a normal Spotify stream, but improvements over â€œhigh qualityâ€ audio are subtle",technology,1,https://www.reddit.com/r/technology/comments/1ng0t0d/spotify_lossless_is_an_inconvenient_improvement/,r_1ng0t0d,,,
r_1ng0eg1,reddit,Wagamaga,2025-09-13T15:29:33+00:00,Trump EPA to stop tracking emissions from biggest polluters,technology,1439,https://www.reddit.com/r/technology/comments/1ng0eg1/trump_epa_to_stop_tracking_emissions_from_biggest/,r_1ng0eg1,,,
r_1nfzemc,reddit,ControlCAD,2025-09-13T14:50:14+00:00,"Opendoor board chair Rabois says company is 'bloated,' needs to cut 85% of workforce",technology,218,https://www.reddit.com/r/technology/comments/1nfzemc/opendoor_board_chair_rabois_says_company_is/,r_1nfzemc,,,
r_1nfz74c,reddit,happy_bluebird,2025-09-13T14:41:20+00:00,Data centers gobble Earthâ€™s resources. What if we took them to space instead?,technology,0,https://www.reddit.com/r/technology/comments/1nfz74c/data_centers_gobble_earths_resources_what_if_we/,r_1nfz74c,,,
r_1nfy783,reddit,ourlifeintoronto,2025-09-13T13:59:12+00:00,â€œLike Nothing Anyone Has Ever Seen Beforeâ€ â€“ Bizarre Supernova Stuns Scientists,technology,29,https://www.reddit.com/r/technology/comments/1nfy783/like_nothing_anyone_has_ever_seen_before_bizarre/,r_1nfy783,,,
r_1nfy3cp,reddit,ourlifeintoronto,2025-09-13T13:54:22+00:00,"UK workers wary of AI despite Starmerâ€™s push to increase uptake, survey finds",technology,24,https://www.reddit.com/r/technology/comments/1nfy3cp/uk_workers_wary_of_ai_despite_starmers_push_to/,r_1nfy3cp,,,
r_1nfxhsa,reddit,DifferentRice2453,2025-09-13T13:27:22+00:00,"Google unveils Gemini CLI, an open source AI tool for terminals | TechCrunch",technology,0,https://www.reddit.com/r/technology/comments/1nfxhsa/google_unveils_gemini_cli_an_open_source_ai_tool/,r_1nfxhsa,,,
r_1nfwze4,reddit,lurker_bee,2025-09-13T13:03:47+00:00,NASA studying Earth-like planet that could contain water,technology,8,https://www.reddit.com/r/technology/comments/1nfwze4/nasa_studying_earthlike_planet_that_could_contain/,r_1nfwze4,,,
r_1nfwn1a,reddit,AdSpecialist6598,2025-09-13T12:47:36+00:00,Roku plans massive rollout of AI-generated ads on its streaming platform,technology,0,https://www.reddit.com/r/technology/comments/1nfwn1a/roku_plans_massive_rollout_of_aigenerated_ads_on/,r_1nfwn1a,,,
r_1nfuza2,reddit,chrisdh79,2025-09-13T11:20:38+00:00,NASA confirms Moon landing by a private American spacecraft,technology,121,https://www.reddit.com/r/technology/comments/1nfuza2/nasa_confirms_moon_landing_by_a_private_american/,r_1nfuza2,,,
r_1nfunlc,reddit,Truthbytruther,2025-09-13T11:01:30+00:00,Meet Diella: Albania Appoints World's First AI Minister To Remove Corruption | TimelineDaily,technology,6,https://www.reddit.com/r/technology/comments/1nfunlc/meet_diella_albania_appoints_worlds_first_ai/,r_1nfunlc,,,
r_1nfuj8y,reddit,laurentlb,2025-09-13T10:54:26+00:00,HiTeX Press: A spam factory for AI-generated books,technology,6,https://www.reddit.com/r/technology/comments/1nfuj8y/hitex_press_a_spam_factory_for_aigenerated_books/,r_1nfuj8y,,,
r_1nfu5vw,reddit,HellYeahDamnWrite,2025-09-13T10:31:55+00:00,Tobacco-style warning labels for social media move closer to California law,technology,165,https://www.reddit.com/r/technology/comments/1nfu5vw/tobaccostyle_warning_labels_for_social_media_move/,r_1nfu5vw,,,
r_1nftyt3,reddit,SingleandSober,2025-09-13T10:19:35+00:00,People are losing jobs due to social media posts about Charlie Kirk,technology,11369,https://www.reddit.com/r/technology/comments/1nftyt3/people_are_losing_jobs_due_to_social_media_posts/,r_1nftyt3,,,
r_1nftx3w,reddit,HellYeahDamnWrite,2025-09-13T10:16:45+00:00,Utah Gov. Cox: Social media is a societal cancer,technology,4943,https://www.reddit.com/r/technology/comments/1nftx3w/utah_gov_cox_social_media_is_a_societal_cancer/,r_1nftx3w,,,
r_1nfth3y,reddit,MetaKnowing,2025-09-13T09:48:56+00:00,"At $183B San Francisco tech company, man's hunger strike enters second week | Anthropic's South of Market office has a daily visitor who wants the startup to shut down its AI development",technology,43,https://www.reddit.com/r/technology/comments/1nfth3y/at_183b_san_francisco_tech_company_mans_hunger/,r_1nfth3y,,,
r_1nfsrko,reddit,MetaKnowing,2025-09-13T09:03:05+00:00,Nearly a third of all tracks uploaded to Deezer are now fully AI-generated,technology,375,https://www.reddit.com/r/technology/comments/1nfsrko/nearly_a_third_of_all_tracks_uploaded_to_deezer/,r_1nfsrko,,,
r_1nfqzoz,reddit,Aralknight,2025-09-13T07:10:01+00:00,"No TikTok, no Instagram: Chilean school blocks phones and students rediscover real-world connections",technology,63,https://www.reddit.com/r/technology/comments/1nfqzoz/no_tiktok_no_instagram_chilean_school_blocks/,r_1nfqzoz,,,
r_1nfq954,reddit,zennaxxarion,2025-09-13T06:25:20+00:00,ChatGPT triggers surge in MPs using AI-written speeches,technology,55,https://www.reddit.com/r/technology/comments/1nfq954/chatgpt_triggers_surge_in_mps_using_aiwritten/,r_1nfq954,,,
r_1nfp4gv,reddit,hunterd189,2025-09-13T05:18:57+00:00,Gmail makes it easier to track upcoming package deliveries,technology,0,https://www.reddit.com/r/technology/comments/1nfp4gv/gmail_makes_it_easier_to_track_upcoming_package/,r_1nfp4gv,,,
r_1nfp34i,reddit,joe4942,2025-09-13T05:16:44+00:00,"Exxon CEO Says New Form of Graphite Boosts EV Battery Life, Extends Range",technology,21,https://www.reddit.com/r/technology/comments/1nfp34i/exxon_ceo_says_new_form_of_graphite_boosts_ev/,r_1nfp34i,,,
r_1nfnk1y,reddit,ourlifeintoronto,2025-09-13T03:50:45+00:00,Psilocybin therapy linked to lasting depression remission five years later,technology,1370,https://www.reddit.com/r/technology/comments/1nfnk1y/psilocybin_therapy_linked_to_lasting_depression/,r_1nfnk1y,,,
r_1nfmxev,reddit,ControlCAD,2025-09-13T03:17:22+00:00,Modder injects AI dialogue into 2002â€™s Animal Crossing using memory hack | Unofficial mod lets classic Nintendo GameCube title use AI chatbots with amusing results.,technology,1,https://www.reddit.com/r/technology/comments/1nfmxev/modder_injects_ai_dialogue_into_2002s_animal/,r_1nfmxev,,,
r_1nflwfw,reddit,StraightedgexLiberal,2025-09-13T02:23:54+00:00,Fake â€œFree Speechâ€ Champion Clay Higgins Now Wants To Use Govâ€™t Power To Silence Anyone Who â€œBelittlesâ€ Kirkâ€™s Death,technology,1395,https://www.reddit.com/r/technology/comments/1nflwfw/fake_free_speech_champion_clay_higgins_now_wants/,r_1nflwfw,,,
r_1nfkpfg,reddit,chrisdh79,2025-09-13T01:24:08+00:00,"Hegseth says Pentagon â€˜trackingâ€™ service members, civilians who celebrate Charlie Kirk killing",technology,15839,https://www.reddit.com/r/technology/comments/1nfkpfg/hegseth_says_pentagon_tracking_service_members/,r_1nfkpfg,,,
r_1nfjo9r,reddit,ControlCAD,2025-09-13T00:33:53+00:00,"Scientists: Itâ€™s do or die time for Americaâ€™s primacy exploring the Solar System: ""When you turn off those spacecraftâ€™s radio receivers, there's no way to turn them back on.""",technology,347,https://www.reddit.com/r/technology/comments/1nfjo9r/scientists_its_do_or_die_time_for_americas/,r_1nfjo9r,,,
r_1nfje7f,reddit,Logical_Welder3467,2025-09-13T00:20:21+00:00,New FAA program will let eVTOL startups test some operations before full certification,technology,11,https://www.reddit.com/r/technology/comments/1nfje7f/new_faa_program_will_let_evtol_startups_test_some/,r_1nfje7f,,,
r_1nfjdly,reddit,Logical_Welder3467,2025-09-13T00:19:35+00:00,Pilot union urges FAA to reject Rainmakerâ€™s drone cloud-seeding plan,technology,21,https://www.reddit.com/r/technology/comments/1nfjdly/pilot_union_urges_faa_to_reject_rainmakers_drone/,r_1nfjdly,,,
r_1nfir7e,reddit,indig0sixalpha,2025-09-12T23:50:30+00:00,Something Is Very Wrong Online. Our political conversations take place in the very same spaces that incubate and perpetuate unthinkable violence.,technology,938,https://www.reddit.com/r/technology/comments/1nfir7e/something_is_very_wrong_online_our_political/,r_1nfir7e,,,
r_1nfiont,reddit,vriska1,2025-09-12T23:47:06+00:00,What You Need to Know About VPNs and Age-Verification Laws,technology,3,https://www.reddit.com/r/technology/comments/1nfiont/what_you_need_to_know_about_vpns_and/,r_1nfiont,,,
r_1nfid87,reddit,upyoars,2025-09-12T23:32:09+00:00,Heart Attacks May Be Infectious and Vaccines Could Prevent Them,technology,435,https://www.reddit.com/r/technology/comments/1nfid87/heart_attacks_may_be_infectious_and_vaccines/,r_1nfid87,,,
r_1nfhv7o,reddit,chrisdh79,2025-09-12T23:09:43+00:00,"After Kirk shooting, Utah governor calls social media a â€œcancer.â€ Will we treat it like one? | We did not evolve to handle this, Utah governor says.",technology,3146,https://www.reddit.com/r/technology/comments/1nfhv7o/after_kirk_shooting_utah_governor_calls_social/,r_1nfhv7o,,,
r_1nfhpyf,reddit,DonkeyFuel,2025-09-12T23:03:09+00:00,Ram Gives Up On Its Electric Truck,technology,0,https://www.reddit.com/r/technology/comments/1nfhpyf/ram_gives_up_on_its_electric_truck/,r_1nfhpyf,,,
r_1nfhpi2,reddit,chrisdh79,2025-09-12T23:02:34+00:00,Roblox says it will remove posts re-enacting Charlie Kirkâ€™s killing.,technology,2892,https://www.reddit.com/r/technology/comments/1nfhpi2/roblox_says_it_will_remove_posts_reenacting/,r_1nfhpi2,,,
r_1nfh3yo,reddit,spsheridan,2025-09-12T22:36:48+00:00,"RFK Jr.â€™s CDC may limit COVID shots to 75 and up, claim they killed kids",technology,2706,https://www.reddit.com/r/technology/comments/1nfh3yo/rfk_jrs_cdc_may_limit_covid_shots_to_75_and_up/,r_1nfh3yo,,,
r_1nfghxi,reddit,Knightbear49,2025-09-12T22:10:53+00:00,The WSJ carelessly spread anti-trans misinformation,technology,37383,https://www.reddit.com/r/technology/comments/1nfghxi/the_wsj_carelessly_spread_antitrans_misinformation/,r_1nfghxi,,,
r_1nfga8e,reddit,no1regrets,2025-09-12T22:02:01+00:00,Charlie Kirk was killed by a meme,technology,3218,https://www.reddit.com/r/technology/comments/1nfga8e/charlie_kirk_was_killed_by_a_meme/,r_1nfga8e,,,
r_1nfg711,reddit,-Bitches-Be-Trippin-,2025-09-12T21:58:40+00:00,"Teachers, firefighters, officials on leave or fired over Charlie Kirk posts",technology,62,https://www.reddit.com/r/technology/comments/1nfg711/teachers_firefighters_officials_on_leave_or_fired/,r_1nfg711,,,
r_1nffukj,reddit,sideAccount42,2025-09-12T21:44:26+00:00,Proton Mail Suspended Journalist Accounts at Request of Cybersecurity Agency,technology,364,https://www.reddit.com/r/technology/comments/1nffukj/proton_mail_suspended_journalist_accounts_at/,r_1nffukj,,,
r_1nffm7t,reddit,artquestionaccount,2025-09-12T21:35:08+00:00,"Groypers, Helldivers 2, Furries: What do the Messages Left by Charlie Kirk's Alleged Killer Actually Mean?",technology,2888,https://www.reddit.com/r/technology/comments/1nffm7t/groypers_helldivers_2_furries_what_do_the/,r_1nffm7t,,,
r_1nfer64,reddit,ControlCAD,2025-09-12T21:01:20+00:00,Sonyâ€™s new Xperia phone jumps on the camera bar bandwagon | The Xperia 10 VII is the latest phone to borrow the Pixel look.,technology,0,https://www.reddit.com/r/technology/comments/1nfer64/sonys_new_xperia_phone_jumps_on_the_camera_bar/,r_1nfer64,,,
r_1nfdqnt,reddit,ubcstaffer123,2025-09-12T20:21:09+00:00,Is homework dead? How AI is forcing schools to rewrite the rules,technology,0,https://www.reddit.com/r/technology/comments/1nfdqnt/is_homework_dead_how_ai_is_forcing_schools_to/,r_1nfdqnt,,,
r_1nfdf1r,reddit,ubcstaffer123,2025-09-12T20:08:46+00:00,I Got an AI to Impersonate Me and Teach Me My Own Courseâ€”Hereâ€™s What I Learned About the Future of Education,technology,0,https://www.reddit.com/r/technology/comments/1nfdf1r/i_got_an_ai_to_impersonate_me_and_teach_me_my_own/,r_1nfdf1r,,,
r_1nfder7,reddit,TommyAdagio,2025-09-12T20:08:26+00:00,"Ted Cruz AI bill could let firms bribe Trump to avoid safety laws, critics warn",technology,1961,https://www.reddit.com/r/technology/comments/1nfder7/ted_cruz_ai_bill_could_let_firms_bribe_trump_to/,r_1nfder7,,,
r_1nfdafa,reddit,TommyAdagio,2025-09-12T20:03:37+00:00,Comcast Executives Warn Workers To Not Say The Wrong Thing About Charlie Kirk,technology,8134,https://www.reddit.com/r/technology/comments/1nfdafa/comcast_executives_warn_workers_to_not_say_the/,r_1nfdafa,,,
r_1nfczk4,reddit,upyoars,2025-09-12T19:51:44+00:00,Scientists Say They've Created a New Form of Life More Perfect Than the One Nature Made,technology,0,https://www.reddit.com/r/technology/comments/1nfczk4/scientists_say_theyve_created_a_new_form_of_life/,r_1nfczk4,,,
r_1nfckms,reddit,wizardofthefuture,2025-09-12T19:35:10+00:00,Nvidia and OpenAI to back major investment in UK AI infrastructure,technology,0,https://www.reddit.com/r/technology/comments/1nfckms/nvidia_and_openai_to_back_major_investment_in_uk/,r_1nfckms,,,
r_1nfcfsk,reddit,ControlCAD,2025-09-12T19:29:46+00:00,"Ex-DVD company employee gets 4 years for leaking Spider-Man Blu-ray | Man agreed to return more than 1,000 stolen DVDs to his former employer.",technology,30,https://www.reddit.com/r/technology/comments/1nfcfsk/exdvd_company_employee_gets_4_years_for_leaking/,r_1nfcfsk,,,
r_1nfc5ph,reddit,chrisdh79,2025-09-12T19:18:42+00:00,"California bill lets renters escape exclusive deals between ISPs and landlords | Bill author says law ""gives this industry an opportunity to treat people fairly.""",technology,485,https://www.reddit.com/r/technology/comments/1nfc5ph/california_bill_lets_renters_escape_exclusive/,r_1nfc5ph,,,
r_1nfc1dm,reddit,ourlifeintoronto,2025-09-12T19:14:00+00:00,The Newly Found Bone Switch That Could Stop Osteoporosis,technology,77,https://www.reddit.com/r/technology/comments/1nfc1dm/the_newly_found_bone_switch_that_could_stop/,r_1nfc1dm,,,
r_1nfbzqj,reddit,vriska1,2025-09-12T19:12:10+00:00,Wikipedia will not appeal dismissal of its UK Online Safety Act challenge,technology,5,https://www.reddit.com/r/technology/comments/1nfbzqj/wikipedia_will_not_appeal_dismissal_of_its_uk/,r_1nfbzqj,,,
r_1nfbv6s,reddit,ourlifeintoronto,2025-09-12T19:07:12+00:00,Electric vehicle sales grew 25% worldwide but just 6% in North America,technology,198,https://www.reddit.com/r/technology/comments/1nfbv6s/electric_vehicle_sales_grew_25_worldwide_but_just/,r_1nfbv6s,,,
r_1nfbpwm,reddit,rezwenn,2025-09-12T19:01:32+00:00,"â€˜China Is the Engineâ€™ Driving Nations Away From Fossil Fuels, Report Says",technology,308,https://www.reddit.com/r/technology/comments/1nfbpwm/china_is_the_engine_driving_nations_away_from/,r_1nfbpwm,,,
r_1nfbmpp,reddit,jomaric,2025-09-12T18:58:13+00:00,Becoming an algorithmic problem: Resistance in the age of predictive technology,technology,8,https://www.reddit.com/r/technology/comments/1nfbmpp/becoming_an_algorithmic_problem_resistance_in_the/,r_1nfbmpp,,,
r_1nfbmc3,reddit,rezwenn,2025-09-12T18:57:47+00:00,Chinaâ€™s Marshall Plan Is Running on Batteries,technology,5,https://www.reddit.com/r/technology/comments/1nfbmc3/chinas_marshall_plan_is_running_on_batteries/,r_1nfbmc3,,,
r_1nfbhsc,reddit,TommyAdagio,2025-09-12T18:52:55+00:00,"1,200 undergrads hung out to dry after jailbreak attack on laundry machines",technology,1082,https://www.reddit.com/r/technology/comments/1nfbhsc/1200_undergrads_hung_out_to_dry_after_jailbreak/,r_1nfbhsc,,,
r_1netkc1,reddit,vriska1,2025-09-12T04:10:51+00:00,Bluesky Launches Age Verification in Select States,technology,8,https://www.reddit.com/r/technology/comments/1netkc1/bluesky_launches_age_verification_in_select_states/,r_1netkc1,,,
r_1newnff,reddit,Logical_Welder3467,2025-09-12T07:19:07+00:00,Elon Muskâ€™s Boring Company suspends work on Vegas airport tunnel after â€˜crushing injuryâ€™,technology,38,https://www.reddit.com/r/technology/comments/1newnff/elon_musks_boring_company_suspends_work_on_vegas/,r_1newnff,,,
r_1nf2zry,reddit,rkhunter_,2025-09-12T13:17:09+00:00,Slovak security company ESET discovered a crypto ransomware that compromises PC firmware UEFI and bypasses Secure Boot,technology,10,https://www.reddit.com/r/technology/comments/1nf2zry/slovak_security_company_eset_discovered_a_crypto/,r_1nf2zry,,,
r_1nfag0t,reddit,yourbasicgeek,2025-09-12T18:11:13+00:00,Terminators: AI-driven robot war machines on the march,technology,3,https://www.reddit.com/r/technology/comments/1nfag0t/terminators_aidriven_robot_war_machines_on_the/,r_1nfag0t,,,
r_1nf6o2d,reddit,paxinfernum,2025-09-12T15:44:25+00:00,This car feature could save lives â€“ but drivers donâ€™t want to use it | The Independent,technology,0,https://www.reddit.com/r/technology/comments/1nf6o2d/this_car_feature_could_save_lives_but_drivers/,r_1nf6o2d,,,
r_1nfa5r0,reddit,Knightbear49,2025-09-12T18:00:08+00:00,"Encyclopedia Britannica and Merriam-Webster sue AI search company Perplexity. Perplexity plagiarized Merriam-Websterâ€™s definition of the word plagiarize, the lawsuit alleges.",technology,32,https://www.reddit.com/r/technology/comments/1nfa5r0/encyclopedia_britannica_and_merriamwebster_sue_ai/,r_1nfa5r0,,,
r_1nfacvx,reddit,Well_Socialized,2025-09-12T18:07:46+00:00,Education report calling for ethical AI use contains over 15 fake sources,technology,180,https://www.reddit.com/r/technology/comments/1nfacvx/education_report_calling_for_ethical_ai_use/,r_1nfacvx,,,
r_1nf9zcc,reddit,nohup_me,2025-09-12T17:53:07+00:00,Man gets over 4 years in prison for selling unreleased movies,technology,138,https://www.reddit.com/r/technology/comments/1nf9zcc/man_gets_over_4_years_in_prison_for_selling/,r_1nf9zcc,,,
r_1nf9yjo,reddit,MicroSofty88,2025-09-12T17:52:11+00:00,"With Few Facts About Kirk Shooting, Wild Speculation Abounds",technology,0,https://www.reddit.com/r/technology/comments/1nf9yjo/with_few_facts_about_kirk_shooting_wild/,r_1nf9yjo,,,
r_1nf9um3,reddit,defenestrate_urself,2025-09-12T17:47:56+00:00,Apple blocks translation AirPods in EU over regulatory concerns,technology,42,https://www.reddit.com/r/technology/comments/1nf9um3/apple_blocks_translation_airpods_in_eu_over/,r_1nf9um3,,,
r_1nf9izc,reddit,indig0sixalpha,2025-09-12T17:35:16+00:00,Charlie Kirkâ€™s alleged killer scratched bullets with a Helldivers combo and a furry sex meme. ï»¿The suspected shooter left a hodgepodge of extremely online taunts.,technology,31147,https://www.reddit.com/r/technology/comments/1nf9izc/charlie_kirks_alleged_killer_scratched_bullets/,r_1nf9izc,,,
r_1nf9isq,reddit,esporx,2025-09-12T17:35:04+00:00,FDA to present data it claims ties Covid shots to child deaths at CDC meeting,technology,61,https://www.reddit.com/r/technology/comments/1nf9isq/fda_to_present_data_it_claims_ties_covid_shots_to/,r_1nf9isq,,,
r_1nf92dp,reddit,Wagamaga,2025-09-12T17:17:15+00:00,"At Governor Mills' Direction, Maine CDC Issues Standing Order Expanding Access to COVID-19 Vaccine Amid Federal Roadblocks",technology,593,https://www.reddit.com/r/technology/comments/1nf92dp/at_governor_mills_direction_maine_cdc_issues/,r_1nf92dp,,,
r_1nf5crp,reddit,Knightbear49,2025-09-12T14:52:58+00:00,"Evergreen High School shooter embraced Columbine, antisemitism and white supremacy online",technology,1272,https://www.reddit.com/r/technology/comments/1nf5crp/evergreen_high_school_shooter_embraced_columbine/,r_1nf5crp,,,
r_1nf7j6k,reddit,moeka_8962,2025-09-12T16:17:36+00:00,Roku wants you to see a lot more AI-generated ads,technology,0,https://www.reddit.com/r/technology/comments/1nf7j6k/roku_wants_you_to_see_a_lot_more_aigenerated_ads/,r_1nf7j6k,,,
r_1nf7snm,reddit,Greedy-Antelope-4084,2025-09-12T16:27:57+00:00,Research: Platforms profit from misinformation and divisive content,technology,70,https://www.reddit.com/r/technology/comments/1nf7snm/research_platforms_profit_from_misinformation_and/,r_1nf7snm,,,
r_1nf7jpi,reddit,moeka_8962,2025-09-12T16:18:10+00:00,Mastodon is bringing quote posts to the fediverse,technology,29,https://www.reddit.com/r/technology/comments/1nf7jpi/mastodon_is_bringing_quote_posts_to_the_fediverse/,r_1nf7jpi,,,
r_1nf7go6,reddit,rezwenn,2025-09-12T16:14:51+00:00,A.I.â€™s Prophet of Doom Wants to Shut It All Down,technology,101,https://www.reddit.com/r/technology/comments/1nf7go6/ais_prophet_of_doom_wants_to_shut_it_all_down/,r_1nf7go6,,,
r_1nf71p8,reddit,north_canadian_ice,2025-09-12T15:59:02+00:00,"AT&T tracked employee attendance to find 'freeloaders.' Now, it admits the system is driving workers to the 'brink of frustration.'",technology,1983,https://www.reddit.com/r/technology/comments/1nf71p8/att_tracked_employee_attendance_to_find/,r_1nf71p8,,,
r_1nf6bvr,reddit,gordonjames62,2025-09-12T15:31:11+00:00,Japan is continuing their work on ship mounted railguns.,technology,550,https://www.reddit.com/r/technology/comments/1nf6bvr/japan_is_continuing_their_work_on_ship_mounted/,r_1nf6bvr,,,
r_1nf69ji,reddit,Smithy2232,2025-09-12T15:28:41+00:00,This Proposed Media Empire Runs on AI,technology,0,https://www.reddit.com/r/technology/comments/1nf69ji/this_proposed_media_empire_runs_on_ai/,r_1nf69ji,,,
r_1nf694l,reddit,rezwenn,2025-09-12T15:28:15+00:00,â€˜Civil Warâ€™ Mentions Surge Online After Kirk Assassination,technology,1933,https://www.reddit.com/r/technology/comments/1nf694l/civil_war_mentions_surge_online_after_kirk/,r_1nf694l,,,
r_1nf5cc6,reddit,Well_Socialized,2025-09-12T14:52:31+00:00,Nepalâ€™s Social Media Ban Backfires as Politics Moves to a Chat Room,technology,4,https://www.reddit.com/r/technology/comments/1nf5cc6/nepals_social_media_ban_backfires_as_politics/,r_1nf5cc6,,,
r_1nf56ee,reddit,lurker_bee,2025-09-12T14:45:50+00:00,Microsoft fixes Exchange Online outage affecting users worldwide,technology,7,https://www.reddit.com/r/technology/comments/1nf56ee/microsoft_fixes_exchange_online_outage_affecting/,r_1nf56ee,,,
r_1nf4w58,reddit,DonkeyFuel,2025-09-12T14:34:37+00:00,BMW's New Driving Aids Can Read Your Mind,technology,0,https://www.reddit.com/r/technology/comments/1nf4w58/bmws_new_driving_aids_can_read_your_mind/,r_1nf4w58,,,
r_1nf4vfs,reddit,indig0sixalpha,2025-09-12T14:33:51+00:00,â€˜My kid has seen this. Now what?â€™: Parents reel as Charlie Kirk video goes viral,technology,6639,https://www.reddit.com/r/technology/comments/1nf4vfs/my_kid_has_seen_this_now_what_parents_reel_as/,r_1nf4vfs,,,
r_1nf4khq,reddit,Hrmbee,2025-09-12T14:21:27+00:00,Jef Raskinâ€™s cul-de-sac and the quest for the humane computer | â€œHe wanted to make [computers] more usable and friendly to people who weren't geeks.â€,technology,9,https://www.reddit.com/r/technology/comments/1nf4khq/jef_raskins_culdesac_and_the_quest_for_the_humane/,r_1nf4khq,,,
r_1nf4j5k,reddit,upyoars,2025-09-12T14:20:00+00:00,Google quantum chip that peeked into â€˜parallel universeâ€™ reveals exotic matter,technology,0,https://www.reddit.com/r/technology/comments/1nf4j5k/google_quantum_chip_that_peeked_into_parallel/,r_1nf4j5k,,,
r_1nf4g7i,reddit,TripleShotPls,2025-09-12T14:16:54+00:00,Mercedes-Benz reveals game plan for its post-EQ era,technology,0,https://www.reddit.com/r/technology/comments/1nf4g7i/mercedesbenz_reveals_game_plan_for_its_posteq_era/,r_1nf4g7i,,,
r_1nf4dxc,reddit,DarthBuzzard,2025-09-12T14:14:27+00:00,Everyone Is Making Smart Glasses Now,technology,0,https://www.reddit.com/r/technology/comments/1nf4dxc/everyone_is_making_smart_glasses_now/,r_1nf4dxc,,,
r_1nf3rdq,reddit,chrisdh79,2025-09-12T13:49:14+00:00,"After AI Led to Layoffs, Coders Are Being Hired to Fix â€˜Vibe-Codedâ€™ Screwups | Fire human, use AI, fire AI, hire human.",technology,1414,https://www.reddit.com/r/technology/comments/1nf3rdq/after_ai_led_to_layoffs_coders_are_being_hired_to/,r_1nf3rdq,,,
r_1nf3lkm,reddit,lurker_bee,2025-09-12T13:42:46+00:00,Microsoft fixes app install issues caused by August Windows updates,technology,0,https://www.reddit.com/r/technology/comments/1nf3lkm/microsoft_fixes_app_install_issues_caused_by/,r_1nf3lkm,,,
r_1nf33z2,reddit,goofba11s,2025-09-12T13:22:11+00:00,"US sues Uber, alleging disability discrimination",technology,19,https://www.reddit.com/r/technology/comments/1nf33z2/us_sues_uber_alleging_disability_discrimination/,r_1nf33z2,,,
r_1nf2g9q,reddit,Vailhem,2025-09-12T12:54:03+00:00,Microsoft-backed research team builds hollow-core cable with lowest signal loss recorded in optical fiber,technology,20,https://www.reddit.com/r/technology/comments/1nf2g9q/microsoftbacked_research_team_builds_hollowcore/,r_1nf2g9q,,,
r_1nf29v6,reddit,indig0sixalpha,2025-09-12T12:46:05+00:00,UC Berkeley turns over personal information of more than 150 students and staff to federal government,technology,6608,https://www.reddit.com/r/technology/comments/1nf29v6/uc_berkeley_turns_over_personal_information_of/,r_1nf29v6,,,
r_1nf24qu,reddit,AdSpecialist6598,2025-09-12T12:39:59+00:00,Court rejects Verizon claim that selling location data without consent is legal,technology,578,https://www.reddit.com/r/technology/comments/1nf24qu/court_rejects_verizon_claim_that_selling_location/,r_1nf24qu,,,
r_1nf1tjz,reddit,chrisdh79,2025-09-12T12:25:26+00:00,"American funding catapults spyware industry beyond Israel's lead | US money now fuels more spyware firms than Europe, giving new life to surveillance tech",technology,39,https://www.reddit.com/r/technology/comments/1nf1tjz/american_funding_catapults_spyware_industry/,r_1nf1tjz,,,
r_1nf1q5n,reddit,ourlifeintoronto,2025-09-12T12:21:07+00:00,How Palantir Is Mapping Everyoneâ€™s Data For The Government,technology,463,https://www.reddit.com/r/technology/comments/1nf1q5n/how_palantir_is_mapping_everyones_data_for_the/,r_1nf1q5n,,,
r_1nf1me8,reddit,Stiltonrocks,2025-09-12T12:16:26+00:00,"Apple's A19 Pro beats Ryzen 9 9950X in single-thread Geekbench tests â€” iPhone 17 Pro chip packs 11-12% CPU performance bump, GPU performance up 37% over predecessor",technology,385,https://www.reddit.com/r/technology/comments/1nf1me8/apples_a19_pro_beats_ryzen_9_9950x_in/,r_1nf1me8,,,
r_1nf13ml,reddit,indig0sixalpha,2025-09-12T11:51:21+00:00,The Influencer FBI. The skill set required to succeed online may not always translate to effective law enforcement.,technology,1338,https://www.reddit.com/r/technology/comments/1nf13ml/the_influencer_fbi_the_skill_set_required_to/,r_1nf13ml,,,
r_1nf0r4j,reddit,NOVA-peddling-1138,2025-09-12T11:33:40+00:00,"Modular Molten Salt Reactors (MSR) come of age. Koreaâ€™s Atomic Energy Research Institute (KAERI) and Samsung Heavy Industries receive certification to go forward to plan and build a cargo ship that will not smoke, leak oil, or need refueling the life of the vessel.",technology,453,https://www.reddit.com/r/technology/comments/1nf0r4j/modular_molten_salt_reactors_msr_come_of_age/,r_1nf0r4j,,,
r_1nf07lx,reddit,chrisdh79,2025-09-12T11:04:32+00:00,Whistleblowers tell US Senate that Meta ignored child safety in virtual reality | Lawmakers press Meta after ex-staff describe children's exposure to pornography and predators in VR,technology,215,https://www.reddit.com/r/technology/comments/1nf07lx/whistleblowers_tell_us_senate_that_meta_ignored/,r_1nf07lx,,,
r_1neyi6s,reddit,Wagamaga,2025-09-12T09:22:26+00:00,â€˜We Should Put Our Phones Downâ€™: Utah Governor Says â€˜There Is a Tremendous Amount of Disinformationâ€™ About Charlie Kirk Assassination,technology,23206,https://www.reddit.com/r/technology/comments/1neyi6s/we_should_put_our_phones_down_utah_governor_says/,r_1neyi6s,,,
r_1neyi05,reddit,katxwoods,2025-09-12T09:22:07+00:00,Ex-Google exec: The idea that AI will create new jobs is '100% crap'â€”even CEOs are at risk of displacement,technology,2450,https://www.reddit.com/r/technology/comments/1neyi05/exgoogle_exec_the_idea_that_ai_will_create_new/,r_1neyi05,,,
r_1neybk2,reddit,CaptainTelos,2025-09-12T09:10:31+00:00,Chat Control: Germany joins the opposition against mandatory scanning of private chats in the name of encryption,technology,2489,https://www.reddit.com/r/technology/comments/1neybk2/chat_control_germany_joins_the_opposition_against/,r_1neybk2,,,
r_1newp1g,reddit,Logical_Welder3467,2025-09-12T07:21:53+00:00,Klarna IPO and ASMLâ€™s Mistral bet revive Europeâ€™s tech dreams,technology,25,https://www.reddit.com/r/technology/comments/1newp1g/klarna_ipo_and_asmls_mistral_bet_revive_europes/,r_1newp1g,,,
r_1newldn,reddit,Logical_Welder3467,2025-09-12T07:15:23+00:00,Hackers left empty-handed after massive NPM supply-chain attack,technology,11,https://www.reddit.com/r/technology/comments/1newldn/hackers_left_emptyhanded_after_massive_npm/,r_1newldn,,,
r_1nev22z,reddit,newmoonchaperone,2025-09-12T05:38:19+00:00,"Exclusive: High-end fashion retailers Gucci, Balenciaga, Brioni, and Alexander McQueen hit by Salesforce attacks",technology,57,https://www.reddit.com/r/technology/comments/1nev22z/exclusive_highend_fashion_retailers_gucci/,r_1nev22z,,,
r_1neupxw,reddit,upyoars,2025-09-12T05:17:39+00:00,"Japanâ€™s green light for making human embryos from stem cells takes us into uncharted territory using CRISPR, IPSCs, and IVF, potentially changing our entire species",technology,428,https://www.reddit.com/r/technology/comments/1neupxw/japans_green_light_for_making_human_embryos_from/,r_1neupxw,,,
r_1neu6s3,reddit,ControlCAD,2025-09-12T04:45:59+00:00,AirPods Live Translation Blocked for EU Users With EU Apple Accounts,technology,6,https://www.reddit.com/r/technology/comments/1neu6s3/airpods_live_translation_blocked_for_eu_users/,r_1neu6s3,,,
r_1neu5k8,reddit,GeneReddit123,2025-09-12T04:44:01+00:00,Microsoft and OpenAI reach non-binding deal to allow OpenAI to restructure,technology,15,https://www.reddit.com/r/technology/comments/1neu5k8/microsoft_and_openai_reach_nonbinding_deal_to/,r_1neu5k8,,,
r_1nes60b,reddit,lurker_bee,2025-09-12T02:57:17+00:00,"Fast food giant exposed after hackers uncover admin passwords, leaked conversations, and catastrophic flaws across Burger King, Tim Hortons, and Popeyes",technology,3133,https://www.reddit.com/r/technology/comments/1nes60b/fast_food_giant_exposed_after_hackers_uncover/,r_1nes60b,,,
r_1nes20f,reddit,ControlCAD,2025-09-12T02:51:38+00:00,Taiwan increases defensive patrols around 24 undersea cables â€” closely monitoring '96 blacklisted China-linked boats' with 24-hour operations | 24 cables connecting Taiwan to the global internet are seen as vulnerable â€˜gray zone warfareâ€™ targets.,technology,53,https://www.reddit.com/r/technology/comments/1nes20f/taiwan_increases_defensive_patrols_around_24/,r_1nes20f,,,
r_1ner3dt,reddit,FollowingFeisty5321,2025-09-12T02:03:27+00:00,DOJ antitrust lawyers battling a stubborn Apple turn to the court for help,technology,69,https://www.reddit.com/r/technology/comments/1ner3dt/doj_antitrust_lawyers_battling_a_stubborn_apple/,r_1ner3dt,,,
r_1neqeip,reddit,Shogouki,2025-09-12T01:29:11+00:00,"Groundbreaking Brazilian Drug, Considered Capable of Reversing Spinal Cord Injury, Presented in Sao Paulo",technology,549,https://www.reddit.com/r/technology/comments/1neqeip/groundbreaking_brazilian_drug_considered_capable/,r_1neqeip,,,
r_1neloij,reddit,Knightbear49,2025-09-11T21:50:20+00:00,"Internet detectives are misusing AI to find Charlie Kirkâ€™s alleged shooter. The FBI shared photos of a â€˜person of interest,â€™ but people online are upscaling them using AI.",technology,165,https://www.reddit.com/r/technology/comments/1neloij/internet_detectives_are_misusing_ai_to_find/,r_1neloij,,,
r_1neprt8,reddit,Logical_Welder3467,2025-09-12T00:58:24+00:00,"Nvidia, Broadcom, TSMC, other AI names rally on Oracleâ€™s massive growth projections",technology,0,https://www.reddit.com/r/technology/comments/1neprt8/nvidia_broadcom_tsmc_other_ai_names_rally_on/,r_1neprt8,,,
r_1nepfom,reddit,dapperlemon,2025-09-12T00:42:00+00:00,AirPods Pro 3 Hands-On: All the Upgrades Seem to Check Out (So Far),technology,0,https://www.reddit.com/r/technology/comments/1nepfom/airpods_pro_3_handson_all_the_upgrades_seem_to/,r_1nepfom,,,
r_1nen3fk,reddit,ourlifeintoronto,2025-09-11T22:51:34+00:00,Small Businesses Face a New Threat: Pay Up or Be Flooded With Bad Reviews,technology,127,https://www.reddit.com/r/technology/comments/1nen3fk/small_businesses_face_a_new_threat_pay_up_or_be/,r_1nen3fk,,,
r_1nem55m,reddit,upyoars,2025-09-11T22:09:55+00:00,Big Oilâ€™s Emissions Caused about 25 Percent of Heat Waves since 2000,technology,1056,https://www.reddit.com/r/technology/comments/1nem55m/big_oils_emissions_caused_about_25_percent_of/,r_1nem55m,,,
r_1nem2nj,reddit,TommyAdagio,2025-09-11T22:06:49+00:00,"AI can't be woke and regulators should be asleep, Senator Cruz says. We went through two hours of Senate hearings so you didn't have to",technology,2172,https://www.reddit.com/r/technology/comments/1nem2nj/ai_cant_be_woke_and_regulators_should_be_asleep/,r_1nem2nj,,,
r_1nelg4q,reddit,rezwenn,2025-09-11T21:40:30+00:00,Chinaâ€™s â€˜Typhoonsâ€™ changing the way FBI hunts sophisticated threats,technology,0,https://www.reddit.com/r/technology/comments/1nelg4q/chinas_typhoons_changing_the_way_fbi_hunts/,r_1nelg4q,,,
r_1nelcey,reddit,Regular_Eggplant_248,2025-09-11T21:36:10+00:00,OpenAI says nonprofit parent will own equity stake in company of over $100 billion,technology,24,https://www.reddit.com/r/technology/comments/1nelcey/openai_says_nonprofit_parent_will_own_equity/,r_1nelcey,,,
r_1nel02x,reddit,ControlCAD,2025-09-11T21:21:43+00:00,U.S. places $11 million bounty on Ukrainian ransomware mastermind â€” Tymoshchuk allegedly stole $18 billion from large companies over 3 years | Volodymyr Tymoshchuk is accused of masterminding ransomware that disrupted 250 companies in the United States alone.,technology,581,https://www.reddit.com/r/technology/comments/1nel02x/us_places_11_million_bounty_on_ukrainian/,r_1nel02x,,,
r_1nekwlg,reddit,Puginator,2025-09-11T21:17:42+00:00,'We will do better.' Microsoft CEO Nadella admits company has to rebuild trust with employees,technology,276,https://www.reddit.com/r/technology/comments/1nekwlg/we_will_do_better_microsoft_ceo_nadella_admits/,r_1nekwlg,,,
r_1nek0o1,reddit,Majano57,2025-09-11T20:42:24+00:00,A.I. Could Make the Smartphone PassÃ©. What Comes Next?,technology,0,https://www.reddit.com/r/technology/comments/1nek0o1/ai_could_make_the_smartphone_passÃ©_what_comes_next/,r_1nek0o1,,,
r_1nejvhr,reddit,waozen,2025-09-11T20:36:49+00:00,Senator demands to know status of 'duplicate' Social Security database 'immediately',technology,6461,https://www.reddit.com/r/technology/comments/1nejvhr/senator_demands_to_know_status_of_duplicate/,r_1nejvhr,,,
r_1nejqj5,reddit,Majano57,2025-09-11T20:31:29+00:00,Republican Senator Asks Social Security Agency About Whistle-Blowerâ€™s Claims,technology,408,https://www.reddit.com/r/technology/comments/1nejqj5/republican_senator_asks_social_security_agency/,r_1nejqj5,,,
r_1nejji4,reddit,Majano57,2025-09-11T20:23:54+00:00,"Parents, Your Job Has Changed in the A.I. Era",technology,0,https://www.reddit.com/r/technology/comments/1nejji4/parents_your_job_has_changed_in_the_ai_era/,r_1nejji4,,,
r_1nej84z,reddit,Majano57,2025-09-11T20:11:48+00:00,The Question All Colleges Should Ask Themselves About AI,technology,5,https://www.reddit.com/r/technology/comments/1nej84z/the_question_all_colleges_should_ask_themselves/,r_1nej84z,,,
r_1nej5o7,reddit,Majano57,2025-09-11T20:09:11+00:00,Small Businesses Face a New Threat: Pay Up or Be Flooded With Bad Reviews,technology,46,https://www.reddit.com/r/technology/comments/1nej5o7/small_businesses_face_a_new_threat_pay_up_or_be/,r_1nej5o7,,,
r_1nej2it,reddit,upyoars,2025-09-11T20:05:42+00:00,The quest to create gene-edited babies gets a reboot as Silicon Valley venture capitalists fear falling birth rates pose an existential threat to the human race,technology,143,https://www.reddit.com/r/technology/comments/1nej2it/the_quest_to_create_geneedited_babies_gets_a/,r_1nej2it,,,
r_1neizw2,reddit,Majano57,2025-09-11T20:02:52+00:00,"When signing up for Uber, do you waive your right to a trial? Pa.â€™s Supreme Court will decide.",technology,24,https://www.reddit.com/r/technology/comments/1neizw2/when_signing_up_for_uber_do_you_waive_your_right/,r_1neizw2,,,
r_1neilfn,reddit,vriska1,2025-09-11T19:47:15+00:00,Experts scrutinized Ofcom's Online Safety Act governance. They're concerned,technology,7,https://www.reddit.com/r/technology/comments/1neilfn/experts_scrutinized_ofcoms_online_safety_act/,r_1neilfn,,,
r_1nei6ht,reddit,StraightedgexLiberal,2025-09-11T19:31:06+00:00,GOP lawmaker seeks lifetime bans for social media users celebrating Kirk's assassination,technology,16699,https://www.reddit.com/r/technology/comments/1nei6ht/gop_lawmaker_seeks_lifetime_bans_for_social_media/,r_1nei6ht,,,
r_1neh62x,reddit,Wagamaga,2025-09-11T18:52:12+00:00,Right-Wing Activists Are Targeting People for Allegedly Celebrating Charlie Kirk's Death,technology,36926,https://www.reddit.com/r/technology/comments/1neh62x/rightwing_activists_are_targeting_people_for/,r_1neh62x,,,
r_1negqer,reddit,joe4942,2025-09-11T18:35:26+00:00,"Apple Starts Getting Customers Used to the Idea of $2,000 iPhones",technology,856,https://www.reddit.com/r/technology/comments/1negqer/apple_starts_getting_customers_used_to_the_idea/,r_1negqer,,,
r_1neglk2,reddit,esporx,2025-09-11T18:30:20+00:00,Uber Sued by Justice Department Over Disability Discrimination,technology,63,https://www.reddit.com/r/technology/comments/1neglk2/uber_sued_by_justice_department_over_disability/,r_1neglk2,,,
r_1neg56f,reddit,upyoars,2025-09-11T18:13:03+00:00,US considers 'severe restrictions' on Big Pharma licensing affordable Chinese meds,technology,90,https://www.reddit.com/r/technology/comments/1neg56f/us_considers_severe_restrictions_on_big_pharma/,r_1neg56f,,,
r_1neflm5,reddit,BurstYourBubbles,2025-09-11T17:52:19+00:00,Albania appoints AI bot as minister to tackle corruption,technology,12,https://www.reddit.com/r/technology/comments/1neflm5/albania_appoints_ai_bot_as_minister_to_tackle/,r_1neflm5,,,
r_1nefi5o,reddit,Puginator,2025-09-11T17:48:43+00:00,Warner Bros. Discovery stock is up 20% after report Paramount Skydance is preparing a takeover bid,technology,42,https://www.reddit.com/r/technology/comments/1nefi5o/warner_bros_discovery_stock_is_up_20_after_report/,r_1nefi5o,,,
r_1nefh06,reddit,ControlCAD,2025-09-11T17:47:29+00:00,"Lara Croft's French actor pursues legal action against developer Aspyr over alleged AI replications of her work in Tomb Raider 4-6 Remastered | The character's Brazilian actor has also been affected by this, with Aspyr working to remove it",technology,53,https://www.reddit.com/r/technology/comments/1nefh06/lara_crofts_french_actor_pursues_legal_action/,r_1nefh06,,,
r_1nef753,reddit,Conscious-Quarter423,2025-09-11T17:37:16+00:00,Water use figures unveiled for controversial New Mexico data center,technology,221,https://www.reddit.com/r/technology/comments/1nef753/water_use_figures_unveiled_for_controversial_new/,r_1nef753,,,
r_1neezly,reddit,ourlifeintoronto,2025-09-11T17:29:21+00:00,"Snapchat allows drug dealers to operate openly on platform, finds Danish study",technology,67,https://www.reddit.com/r/technology/comments/1neezly/snapchat_allows_drug_dealers_to_operate_openly_on/,r_1neezly,,,
r_1nee0hx,reddit,fchung,2025-09-11T16:52:54+00:00,"On 10th anniversary, LIGO verifies Hawkingâ€™s theorem",technology,12,https://www.reddit.com/r/technology/comments/1nee0hx/on_10th_anniversary_ligo_verifies_hawkings_theorem/,r_1nee0hx,,,
r_1nedsmc,reddit,BurstYourBubbles,2025-09-11T16:44:39+00:00,Bollywood reels as AI reshapes Indian films,technology,0,https://www.reddit.com/r/technology/comments/1nedsmc/bollywood_reels_as_ai_reshapes_indian_films/,r_1nedsmc,,,
r_1nedsij,reddit,rezwenn,2025-09-11T16:44:32+00:00,"Offshore wind has no future in the U.S. under Trump administration, Interior Secretary says",technology,3084,https://www.reddit.com/r/technology/comments/1nedsij/offshore_wind_has_no_future_in_the_us_under_trump/,r_1nedsij,,,
r_1nedc3v,reddit,ourlifeintoronto,2025-09-11T16:27:07+00:00,Chemists Create Next-Gen Rocket Fuel Compound That Packs 150% More Energy,technology,163,https://www.reddit.com/r/technology/comments/1nedc3v/chemists_create_nextgen_rocket_fuel_compound_that/,r_1nedc3v,,,
r_1necqzo,reddit,rkhunter_,2025-09-11T16:04:18+00:00,"After Ukrainian testing, drone-detection radar doubles range with simple software patch",technology,37,https://www.reddit.com/r/technology/comments/1necqzo/after_ukrainian_testing_dronedetection_radar/,r_1necqzo,,,
r_1necilg,reddit,ourlifeintoronto,2025-09-11T15:55:35+00:00,Gravitational waves finally prove Stephen Hawking's black hole theorem,technology,121,https://www.reddit.com/r/technology/comments/1necilg/gravitational_waves_finally_prove_stephen/,r_1necilg,,,
r_1nec43j,reddit,joe4942,2025-09-11T15:39:43+00:00,Alibaba and Baidu Adopt Their Own AI Chips in Major Shift for Chinese Tech,technology,14,https://www.reddit.com/r/technology/comments/1nec43j/alibaba_and_baidu_adopt_their_own_ai_chips_in/,r_1nec43j,,,
r_1ne7oys,reddit,moeka_8962,2025-09-11T12:38:01+00:00,Firefox Finally Introducing Matroska / MKV Playback Support,technology,12,https://www.reddit.com/r/technology/comments/1ne7oys/firefox_finally_introducing_matroska_mkv_playback/,r_1ne7oys,,,
r_1ne7tna,reddit,moeka_8962,2025-09-11T12:43:50+00:00,"Microsoft ends OpenAI exclusivity in Office, adds rival Anthropic &#x2d; Ars Technica",technology,2,https://www.reddit.com/r/technology/comments/1ne7tna/microsoft_ends_openai_exclusivity_in_office_adds/,r_1ne7tna,,,
r_1nebta7,reddit,lurker_bee,2025-09-11T15:27:49+00:00,"AI Use at Large Companies Is in Decline, Census Bureau Says",technology,2262,https://www.reddit.com/r/technology/comments/1nebta7/ai_use_at_large_companies_is_in_decline_census/,r_1nebta7,,,
r_1nebsnu,reddit,Puginator,2025-09-11T15:27:06+00:00,"Alphabet, Meta, OpenAI, xAI and Snap face FTC probe over AI chatbot safety for kids",technology,11,https://www.reddit.com/r/technology/comments/1nebsnu/alphabet_meta_openai_xai_and_snap_face_ftc_probe/,r_1nebsnu,,,
r_1nebktq,reddit,biograf_,2025-09-11T15:18:36+00:00,Epstein â€˜birthday bookâ€™ includes apparent letter from former Microsoft CTO Nathan Myhrvold,technology,4588,https://www.reddit.com/r/technology/comments/1nebktq/epstein_birthday_book_includes_apparent_letter/,r_1nebktq,,,
r_1neb9d6,reddit,ControlCAD,2025-09-11T15:05:38+00:00,"Pay-per-output? AI firms blindsided by beefed up robots.txt instructions. | ""Really Simple Licensing"" makes it easier for creators to get paid for AI scraping.",technology,16,https://www.reddit.com/r/technology/comments/1neb9d6/payperoutput_ai_firms_blindsided_by_beefed_up/,r_1neb9d6,,,
r_1neayhe,reddit,TripleShotPls,2025-09-11T14:53:41+00:00,The New Honda Prelude Doesnâ€™t Have a Transmission. Hereâ€™s Why,technology,0,https://www.reddit.com/r/technology/comments/1neayhe/the_new_honda_prelude_doesnt_have_a_transmission/,r_1neayhe,,,
r_1ne9xx5,reddit,rezwenn,2025-09-11T14:14:55+00:00,NASA rover finds strongest evidence yet of ancient life on Mars,technology,22,https://www.reddit.com/r/technology/comments/1ne9xx5/nasa_rover_finds_strongest_evidence_yet_of/,r_1ne9xx5,,,
r_1ne9cq1,reddit,hunterd189,2025-09-11T13:50:33+00:00,Installing the Google Nest 4 Thermostat is The Biggest Upgrade Iâ€™ve Done to My Apartmentâ€”And the Easiest,technology,0,https://www.reddit.com/r/technology/comments/1ne9cq1/installing_the_google_nest_4_thermostat_is_the/,r_1ne9cq1,,,
r_1ne90uv,reddit,rezwenn,2025-09-11T13:36:32+00:00,Congresswoman Calls on X to Remove Charlie Kirk Assassination Videos: â€˜Take Them Downâ€™,technology,20061,https://www.reddit.com/r/technology/comments/1ne90uv/congresswoman_calls_on_x_to_remove_charlie_kirk/,r_1ne90uv,,,
r_1ne8w42,reddit,Hrmbee,2025-09-11T13:30:53+00:00,"The Software Engineers Paid to Fix Vibe Coded Messes | Linkedin has been joking about â€œvibe coding cleanup specialists,â€ but itâ€™s actually a growing profession",technology,371,https://www.reddit.com/r/technology/comments/1ne8w42/the_software_engineers_paid_to_fix_vibe_coded/,r_1ne8w42,,,
r_1ne8vwy,reddit,rezwenn,2025-09-11T13:30:39+00:00,"Graphic video of Kirk shooting was everywhere online, showing how media gatekeeper role has changed",technology,17879,https://www.reddit.com/r/technology/comments/1ne8vwy/graphic_video_of_kirk_shooting_was_everywhere/,r_1ne8vwy,,,
r_1ne8ec1,reddit,Mammoth-Heat5702,2025-09-11T13:09:35+00:00,Why did Harvard top mathematician Liu Jun leave the US for China?,technology,5,https://www.reddit.com/r/technology/comments/1ne8ec1/why_did_harvard_top_mathematician_liu_jun_leave/,r_1ne8ec1,,,
r_1ne8d8e,reddit,chrisdh79,2025-09-11T13:08:13+00:00,"Anonymity is dead and weâ€™re all content now | We arenâ€™t your friends, and youâ€™ll never be alone again.",technology,993,https://www.reddit.com/r/technology/comments/1ne8d8e/anonymity_is_dead_and_were_all_content_now_we/,r_1ne8d8e,,,
r_1ne88jq,reddit,Wagamaga,2025-09-11T13:02:30+00:00,Video of US missile fired at mystery orb shown at UFO hearing,technology,0,https://www.reddit.com/r/technology/comments/1ne88jq/video_of_us_missile_fired_at_mystery_orb_shown_at/,r_1ne88jq,,,
r_1ne8522,reddit,ourlifeintoronto,2025-09-11T12:58:28+00:00,"How thousands of â€˜overworked, underpaidâ€™ humans train Googleâ€™s AI to seem smart",technology,181,https://www.reddit.com/r/technology/comments/1ne8522/how_thousands_of_overworked_underpaid_humans/,r_1ne8522,,,
r_1ne802b,reddit,indig0sixalpha,2025-09-11T12:51:51+00:00,"Sega Accused Of Using Police To Recover Nintendo Dev Kits It Had ""Negligently Disposed Of""",technology,468,https://www.reddit.com/r/technology/comments/1ne802b/sega_accused_of_using_police_to_recover_nintendo/,r_1ne802b,,,
r_1ne7w9h,reddit,ControlCAD,2025-09-11T12:46:59+00:00,Larry Ellison is $100 billion richer after blowout Oracle earnings report,technology,2329,https://www.reddit.com/r/technology/comments/1ne7w9h/larry_ellison_is_100_billion_richer_after_blowout/,r_1ne7w9h,,,
r_1ne7vvx,reddit,Reddit_INDIA_MOD,2025-09-11T12:46:31+00:00,The iPhone 17 is here: The hidden costs of always chasing the newest technology,technology,0,https://www.reddit.com/r/technology/comments/1ne7vvx/the_iphone_17_is_here_the_hidden_costs_of_always/,r_1ne7vvx,,,
r_1ne7kk5,reddit,Logical_Welder3467,2025-09-11T12:32:19+00:00,"Opendoor taps new CEO and names Keith Rabois chairman, boosting stock 36%",technology,31,https://www.reddit.com/r/technology/comments/1ne7kk5/opendoor_taps_new_ceo_and_names_keith_rabois/,r_1ne7kk5,,,
r_1ne7k2w,reddit,chrisdh79,2025-09-11T12:31:40+00:00,"Misinformation, fear and politics â€“ how a South Dakota county drove away millions in solar energy",technology,2870,https://www.reddit.com/r/technology/comments/1ne7k2w/misinformation_fear_and_politics_how_a_south/,r_1ne7k2w,,,
r_1ne7e08,reddit,AdSpecialist6598,2025-09-11T12:23:51+00:00,Breakthrough 3D printing methods bring artificial skin tissue closer to reality,technology,24,https://www.reddit.com/r/technology/comments/1ne7e08/breakthrough_3d_printing_methods_bring_artificial/,r_1ne7e08,,,
r_1ne7can,reddit,Well_Socialized,2025-09-11T12:21:32+00:00,Revealed: Apple is teaching its AI to adapt to the Trump era,technology,592,https://www.reddit.com/r/technology/comments/1ne7can/revealed_apple_is_teaching_its_ai_to_adapt_to_the/,r_1ne7can,,,
r_1ne6rkk,reddit,Wagamaga,2025-09-11T11:53:51+00:00,"Addictive algorithms should be illegal, says inventor of the world wide web",technology,6760,https://www.reddit.com/r/technology/comments/1ne6rkk/addictive_algorithms_should_be_illegal_says/,r_1ne6rkk,,,
r_1ne6n8b,reddit,Logical_Welder3467,2025-09-11T11:47:38+00:00,VMware to lose 35 percent of workloads in three years â€“ some to its friends at â€˜proper cloudsâ€™,technology,393,https://www.reddit.com/r/technology/comments/1ne6n8b/vmware_to_lose_35_percent_of_workloads_in_three/,r_1ne6n8b,,,
r_1ne6lzu,reddit,Logical_Welder3467,2025-09-11T11:45:56+00:00,Broadcom CEO could get $600m stock payout on AI sales,technology,87,https://www.reddit.com/r/technology/comments/1ne6lzu/broadcom_ceo_could_get_600m_stock_payout_on_ai/,r_1ne6lzu,,,
r_1ne50lv,reddit,MetaKnowing,2025-09-11T10:17:14+00:00,AI Is Coming for YouTube Creators | At least 15 million videos have been snatched by tech companies.,technology,21,https://www.reddit.com/r/technology/comments/1ne50lv/ai_is_coming_for_youtube_creators_at_least_15/,r_1ne50lv,,,
r_1ne4ymh,reddit,chrisdh79,2025-09-11T10:14:07+00:00,"'An embarrassing failure of the US patent system': Videogame IP lawyer says Nintendo's latest patents on PokÃ©mon mechanics 'should not have happened, full stop'",technology,7976,https://www.reddit.com/r/technology/comments/1ne4ymh/an_embarrassing_failure_of_the_us_patent_system/,r_1ne4ymh,,,
r_1ne4rz7,reddit,MetaKnowing,2025-09-11T10:02:36+00:00,"AI systems may feel real, but they don't deserve rights, said Microsoft's AI CEO | His stance contrasts with companies like Anthropic, which has explored ""AI welfare.""",technology,37,https://www.reddit.com/r/technology/comments/1ne4rz7/ai_systems_may_feel_real_but_they_dont_deserve/,r_1ne4rz7,,,
r_1ne4jb4,reddit,Wagamaga,2025-09-11T09:47:29+00:00,NJ makes COVID-19 vaccines widely available amid Trump restrictions,technology,2376,https://www.reddit.com/r/technology/comments/1ne4jb4/nj_makes_covid19_vaccines_widely_available_amid/,r_1ne4jb4,,,
r_1ne4ie6,reddit,tecialist,2025-09-11T09:45:53+00:00,This is the AI chatbot captivating 1 million Korean teens. They script tempting intimacy,technology,32,https://www.reddit.com/r/technology/comments/1ne4ie6/this_is_the_ai_chatbot_captivating_1_million/,r_1ne4ie6,,,
r_1ne4hnj,reddit,katxwoods,2025-09-11T09:44:31+00:00,"A new research project is the first comprehensive effort to categorize all the ways AI can go wrong, and many of those behaviors resemble human psychiatric disorders.",technology,125,https://www.reddit.com/r/technology/comments/1ne4hnj/a_new_research_project_is_the_first_comprehensive/,r_1ne4hnj,,,
r_1ne41vh,reddit,TripleShotPls,2025-09-11T09:15:31+00:00,Volvoâ€™s Boss Says Some Western Brands Wonâ€™t Survive the EV Shift,technology,288,https://www.reddit.com/r/technology/comments/1ne41vh/volvos_boss_says_some_western_brands_wont_survive/,r_1ne41vh,,,
r_1ne40ur,reddit,TripleShotPls,2025-09-11T09:13:36+00:00,"New EV Battery Tech Lasts 600,000 Miles, Charges In 10 Minutes",technology,1133,https://www.reddit.com/r/technology/comments/1ne40ur/new_ev_battery_tech_lasts_600000_miles_charges_in/,r_1ne40ur,,,
r_1ndz8tn,reddit,lurker_bee,2025-09-11T04:11:35+00:00,Move Over Carbon Fiberâ€”Thereâ€™s A New High-Performance Material In Town,technology,78,https://www.reddit.com/r/technology/comments/1ndz8tn/move_over_carbon_fibertheres_a_new/,r_1ndz8tn,,,
r_1ndy4s6,reddit,lurker_bee,2025-09-11T03:11:56+00:00,Rivian CEO: There's No 'Magic' Behind China's Low-Cost EVs,technology,10969,https://www.reddit.com/r/technology/comments/1ndy4s6/rivian_ceo_theres_no_magic_behind_chinas_lowcost/,r_1ndy4s6,,,
r_1nduyaj,reddit,indig0sixalpha,2025-09-11T00:33:07+00:00,Ted Cruzâ€™s new bill would let AI companies set their own rules for up to 10 years. ï»¿The SANDBOX Act would let companies request exemptions from regulation for AI products and services â€” and let the White House override agencies that say no.,technology,5669,https://www.reddit.com/r/technology/comments/1nduyaj/ted_cruzs_new_bill_would_let_ai_companies_set/,r_1nduyaj,,,
r_1ndutmc,reddit,ControlCAD,2025-09-11T00:26:42+00:00,Court rejects Verizon claim that selling location data without consent is legal | Verizon and T-Mobile lost but AT&T beat the FCC. SCOTUS may have to step in.,technology,516,https://www.reddit.com/r/technology/comments/1ndutmc/court_rejects_verizon_claim_that_selling_location/,r_1ndutmc,,,
r_1nduqj2,reddit,Logical_Welder3467,2025-09-11T00:22:29+00:00,AI pricing is currently in a state of â€˜pandemoniumâ€™ says Gartner,technology,62,https://www.reddit.com/r/technology/comments/1nduqj2/ai_pricing_is_currently_in_a_state_of_pandemonium/,r_1nduqj2,,,
r_1ndun1b,reddit,Adept_Ad_2085,2025-09-11T00:17:50+00:00,"Trust in AI: progress, challenges, and future directions",technology,0,https://www.reddit.com/r/technology/comments/1ndun1b/trust_in_ai_progress_challenges_and_future/,r_1ndun1b,,,
r_1ndu8ce,reddit,upyoars,2025-09-10T23:58:15+00:00,Scientists just built a detector that could finally catch dark matter,technology,123,https://www.reddit.com/r/technology/comments/1ndu8ce/scientists_just_built_a_detector_that_could/,r_1ndu8ce,,,
r_1ndu1oz,reddit,Exciting_Teacher6258,2025-09-10T23:49:32+00:00,Melania Trumpâ€™s AI Era Is Upon Us,technology,0,https://www.reddit.com/r/technology/comments/1ndu1oz/melania_trumps_ai_era_is_upon_us/,r_1ndu1oz,,,
r_1ndt8pg,reddit,Logical_Welder3467,2025-09-10T23:12:10+00:00,"Oracle boasts $455B backlog from AI boom, but not all its new friends will live to pay up",technology,29,https://www.reddit.com/r/technology/comments/1ndt8pg/oracle_boasts_455b_backlog_from_ai_boom_but_not/,r_1ndt8pg,,,
r_1ndt7ab,reddit,Logical_Welder3467,2025-09-10T23:10:25+00:00,"Klarnaâ€™s IPO pops, raising $1.4B, with Sequoia as the biggest winner",technology,6,https://www.reddit.com/r/technology/comments/1ndt7ab/klarnas_ipo_pops_raising_14b_with_sequoia_as_the/,r_1ndt7ab,,,
r_1ndt6os,reddit,Logical_Welder3467,2025-09-10T23:09:40+00:00,Southeast Asian Scam Centers Face More Financial Sanctions,technology,15,https://www.reddit.com/r/technology/comments/1ndt6os/southeast_asian_scam_centers_face_more_financial/,r_1ndt6os,,,
r_1ndsy2e,reddit,Valinaut,2025-09-10T22:59:02+00:00,Amazon drivers could be wearing AR glasses with a built-in display next year.,technology,59,https://www.reddit.com/r/technology/comments/1ndsy2e/amazon_drivers_could_be_wearing_ar_glasses_with_a/,r_1ndsy2e,,,
r_1ndsx7p,reddit,lurker_bee,2025-09-10T22:58:04+00:00,"""It's just smoke and mirrors"" â€“ Over 500 cryptography scientists and researchers slam the EU proposal to scan all your WhatsApp chats",technology,16,https://www.reddit.com/r/technology/comments/1ndsx7p/its_just_smoke_and_mirrors_over_500_cryptography/,r_1ndsx7p,,,
r_1ndsvb5,reddit,ThatBlackGuy_,2025-09-10T22:55:45+00:00,"OpenAI, Oracle sign $300 billion computing deal, WSJ reports",technology,294,https://www.reddit.com/r/technology/comments/1ndsvb5/openai_oracle_sign_300_billion_computing_deal_wsj/,r_1ndsvb5,,,
r_1ndsbza,reddit,joe4942,2025-09-10T22:32:03+00:00,Opendoor Technologies Snags Shopify Operating Chief Nejatian as CEO,technology,0,https://www.reddit.com/r/technology/comments/1ndsbza/opendoor_technologies_snags_shopify_operating/,r_1ndsbza,,,
r_1ndsbk7,reddit,ControlCAD,2025-09-10T22:31:33+00:00,Senator blasts Microsoft for making default Windows vulnerable to â€œKerberoastingâ€ | Wyden says default use of RC4 cipher led to last year's breach of health giant Ascension.,technology,157,https://www.reddit.com/r/technology/comments/1ndsbk7/senator_blasts_microsoft_for_making_default/,r_1ndsbk7,,,
r_1ndrhwb,reddit,vriska1,2025-09-10T21:56:18+00:00,Bluesky brings age verification to South Dakota and Wyoming,technology,4,https://www.reddit.com/r/technology/comments/1ndrhwb/bluesky_brings_age_verification_to_south_dakota/,r_1ndrhwb,,,
r_1ndr99w,reddit,Efficient-Ruin-4713,2025-09-10T21:46:10+00:00,"Rock found on Mars could be evidence of ancient life, NASA says",technology,333,https://www.reddit.com/r/technology/comments/1ndr99w/rock_found_on_mars_could_be_evidence_of_ancient/,r_1ndr99w,,,
r_1ndq7ow,reddit,RebelStrategist,2025-09-10T21:02:22+00:00,Cuba hit with fifth blackout in less than a year with 10m people in the dark,technology,614,https://www.reddit.com/r/technology/comments/1ndq7ow/cuba_hit_with_fifth_blackout_in_less_than_a_year/,r_1ndq7ow,,,
r_1ndokgf,reddit,Conscious-Quarter423,2025-09-10T19:56:13+00:00,"Oracle stock booms 35%, on pace for best day since 1992",technology,0,https://www.reddit.com/r/technology/comments/1ndokgf/oracle_stock_booms_35_on_pace_for_best_day_since/,r_1ndokgf,,,
r_1ndofzz,reddit,fchung,2025-09-10T19:51:17+00:00,â€œBottlebrushâ€ particles deliver big chemotherapy payloads directly to cancer cells,technology,39,https://www.reddit.com/r/technology/comments/1ndofzz/bottlebrush_particles_deliver_big_chemotherapy/,r_1ndofzz,,,
r_1ndnky2,reddit,yogthos,2025-09-10T19:17:31+00:00,"CATL's Naxtra sodium-ion battery passes new national safety standards, ready for mass production",technology,53,https://www.reddit.com/r/technology/comments/1ndnky2/catls_naxtra_sodiumion_battery_passes_new/,r_1ndnky2,,,
r_1ndmzjc,reddit,upyoars,2025-09-10T18:55:17+00:00,"Africa could become 'renewable superpower', says UN head Guterres",technology,156,https://www.reddit.com/r/technology/comments/1ndmzjc/africa_could_become_renewable_superpower_says_un/,r_1ndmzjc,,,
r_1ndmkxq,reddit,ubcstaffer123,2025-09-10T18:40:07+00:00,New report shows AI bots are putting kids at riskâ€”what parents can do now,technology,27,https://www.reddit.com/r/technology/comments/1ndmkxq/new_report_shows_ai_bots_are_putting_kids_at/,r_1ndmkxq,,,
r_1ndmbat,reddit,ubcstaffer123,2025-09-10T18:29:55+00:00,Social media is teaching children how to use AI. How can teachers keep up?,technology,0,https://www.reddit.com/r/technology/comments/1ndmbat/social_media_is_teaching_children_how_to_use_ai/,r_1ndmbat,,,
r_1ndlv04,reddit,hety0p,2025-09-10T18:12:52+00:00,Dead Internet Theory Lives: One Out of Three of You Is a Bot,technology,3603,https://www.reddit.com/r/technology/comments/1ndlv04/dead_internet_theory_lives_one_out_of_three_of/,r_1ndlv04,,,
r_1ndl8e5,reddit,Hrmbee,2025-09-10T17:49:55+00:00,Reddit is testing a way to read articles without leaving the app | A suite of new tools is geared toward news publishers and readers,technology,74,https://www.reddit.com/r/technology/comments/1ndl8e5/reddit_is_testing_a_way_to_read_articles_without/,r_1ndl8e5,,,
r_1ndl4ga,reddit,diacewrb,2025-09-10T17:45:57+00:00,"With Cheap Chinese Solar, Developing Countries Leapfrog U.S. on Clean Energy",technology,253,https://www.reddit.com/r/technology/comments/1ndl4ga/with_cheap_chinese_solar_developing_countries/,r_1ndl4ga,,,
r_1ndkl7l,reddit,TripleShotPls,2025-09-10T17:26:27+00:00,Tesla Wants Out of the Car Business,technology,484,https://www.reddit.com/r/technology/comments/1ndkl7l/tesla_wants_out_of_the_car_business/,r_1ndkl7l,,,
r_1ndk7r7,reddit,ControlCAD,2025-09-10T17:12:51+00:00,"AI vs. MAGA: Populists alarmed by Trumpâ€™s embrace of AI, Big Tech: AI â€œthreatens the common manâ€™s liberty,"" says GOP Sen. Josh Hawley.",technology,2154,https://www.reddit.com/r/technology/comments/1ndk7r7/ai_vs_maga_populists_alarmed_by_trumps_embrace_of/,r_1ndk7r7,,,
r_1ndjufh,reddit,Wagamaga,2025-09-10T16:59:30+00:00,The Shocking Far-Right Agenda Behind the Facial Recognition Tech Used by ICE and the FBI,technology,1623,https://www.reddit.com/r/technology/comments/1ndjufh/the_shocking_farright_agenda_behind_the_facial/,r_1ndjufh,,,
r_1ndjq1c,reddit,indig0sixalpha,2025-09-10T16:54:56+00:00,Rand Paul Reveals Venezuela Boat Attack Was a Drone Strike. The senator told The Intercept the attack defied rules of engagement and came from a drone.,technology,25892,https://www.reddit.com/r/technology/comments/1ndjq1c/rand_paul_reveals_venezuela_boat_attack_was_a/,r_1ndjq1c,,,
r_1ndjm2o,reddit,Forward-Answer-4407,2025-09-10T16:50:50+00:00,Teenage Boy in China Suffers Stroke After Hours on Phone with Bent Neck,technology,475,https://www.reddit.com/r/technology/comments/1ndjm2o/teenage_boy_in_china_suffers_stroke_after_hours/,r_1ndjm2o,,,
r_1ndjf1n,reddit,rezwenn,2025-09-10T16:43:43+00:00,Why Appleâ€™s Lackluster New iPhones Will Still Pay Off,technology,0,https://www.reddit.com/r/technology/comments/1ndjf1n/why_apples_lackluster_new_iphones_will_still_pay/,r_1ndjf1n,,,
r_1ndjdsq,reddit,rezwenn,2025-09-10T16:42:27+00:00,"Teslaâ€™s Dangerous Doors: When Teslas lose power, crashes can turn into deadly races against time.",technology,964,https://www.reddit.com/r/technology/comments/1ndjdsq/teslas_dangerous_doors_when_teslas_lose_power/,r_1ndjdsq,,,
r_1ndjc6p,reddit,rezwenn,2025-09-10T16:40:52+00:00,How the AI Boom Is Leaving Consultants Behind,technology,0,https://www.reddit.com/r/technology/comments/1ndjc6p/how_the_ai_boom_is_leaving_consultants_behind/,r_1ndjc6p,,,
r_1ndirq7,reddit,upyoars,2025-09-10T16:20:35+00:00,Turning to the sun: Solar growth in Central Europe exceeds all expectations as it quickly becomes the continentâ€™s battery hub,technology,208,https://www.reddit.com/r/technology/comments/1ndirq7/turning_to_the_sun_solar_growth_in_central_europe/,r_1ndirq7,,,
r_1ndinph,reddit,DonkeyFuel,2025-09-10T16:16:38+00:00,Tesla Cybertruck Won't Get Wireless Charing After All. Here's Why.,technology,0,https://www.reddit.com/r/technology/comments/1ndinph/tesla_cybertruck_wont_get_wireless_charing_after/,r_1ndinph,,,
r_1ndibc2,reddit,DonkeyFuel,2025-09-10T16:04:08+00:00,Inside Bollinger's Bold Pivot: How the EV Maker is Reinventing Itself,technology,4,https://www.reddit.com/r/technology/comments/1ndibc2/inside_bollingers_bold_pivot_how_the_ev_maker_is/,r_1ndibc2,,,
r_1ndi2w5,reddit,Olive_O_,2025-09-10T15:55:49+00:00,Testing of food deliveries by autonomous drones to launch in San Francisco at first-ever facility,technology,10,https://www.reddit.com/r/technology/comments/1ndi2w5/testing_of_food_deliveries_by_autonomous_drones/,r_1ndi2w5,,,
r_1ndhxr1,reddit,rezwenn,2025-09-10T15:50:37+00:00,"Electric school bus bursts into flames, driver and children are okay",technology,38,https://www.reddit.com/r/technology/comments/1ndhxr1/electric_school_bus_bursts_into_flames_driver_and/,r_1ndhxr1,,,
r_1ndhrpc,reddit,Moth_LovesLamp,2025-09-10T15:44:24+00:00,"Konamiâ€™s AI project backfires, Yu-Gi-Oh! videos deleted over unauthorized use of popular actressâ€™ voice",technology,71,https://www.reddit.com/r/technology/comments/1ndhrpc/konamis_ai_project_backfires_yugioh_videos/,r_1ndhrpc,,,
r_1ndhi9j,reddit,aacool,2025-09-10T15:34:53+00:00,Leading through AI disruption: What no CEO talks about,technology,0,https://www.reddit.com/r/technology/comments/1ndhi9j/leading_through_ai_disruption_what_no_ceo_talks/,r_1ndhi9j,,,
r_1ndhhax,reddit,rezwenn,2025-09-10T15:33:54+00:00,"Why Some of Techâ€™s Leading Men Went All-In on Trump, According to Chamath Palihapitiya",technology,0,https://www.reddit.com/r/technology/comments/1ndhhax/why_some_of_techs_leading_men_went_allin_on_trump/,r_1ndhhax,,,
r_1ndher0,reddit,esporx,2025-09-10T15:31:20+00:00,Leaked Ice document shows worker detained in Hyundai raid had valid visa,technology,5450,https://www.reddit.com/r/technology/comments/1ndher0/leaked_ice_document_shows_worker_detained_in/,r_1ndher0,,,
r_1ndgvdl,reddit,TF-Fanfic-Resident,2025-09-10T15:11:29+00:00,How the AP uncovered US big techâ€™s role in Chinaâ€™s digital police state,technology,22,https://www.reddit.com/r/technology/comments/1ndgvdl/how_the_ap_uncovered_us_big_techs_role_in_chinas/,r_1ndgvdl,,,
r_1ndgpal,reddit,Well_Socialized,2025-09-10T15:05:12+00:00,Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health,technology,1,https://www.reddit.com/r/technology/comments/1ndgpal/exploring_user_security_and_privacy_attitudes_and/,r_1ndgpal,,,
r_1ndg9qb,reddit,upyoars,2025-09-10T14:49:04+00:00,"Pfizer, BioNTech showcase new data supporting COVID booster as strain LP.8.1 cases rise in California",technology,38,https://www.reddit.com/r/technology/comments/1ndg9qb/pfizer_biontech_showcase_new_data_supporting/,r_1ndg9qb,,,
r_1ndfoiz,reddit,yogthos,2025-09-10T14:26:11+00:00,China unveils brain-inspired AI that could redefine efficiency,technology,1,https://www.reddit.com/r/technology/comments/1ndfoiz/china_unveils_braininspired_ai_that_could/,r_1ndfoiz,,,
r_1ndfjvo,reddit,DonkeyFuel,2025-09-10T14:21:08+00:00,"â€˜Mythâ€™: Most Drivers Donâ€™t Rely On CarPlay As Much As You Think, BMW Says",technology,2775,https://www.reddit.com/r/technology/comments/1ndfjvo/myth_most_drivers_dont_rely_on_carplay_as_much_as/,r_1ndfjvo,,,
r_1ndfj8v,reddit,DonkeyFuel,2025-09-10T14:20:29+00:00,Arc's $160M tugboat deal shows how electrifying big boats is the right choice,technology,5,https://www.reddit.com/r/technology/comments/1ndfj8v/arcs_160m_tugboat_deal_shows_how_electrifying_big/,r_1ndfj8v,,,
r_1ndficc,reddit,DonkeyFuel,2025-09-10T14:19:32+00:00,BMW Says Europe's Gas Engine Ban 'Can Kill an Industry',technology,420,https://www.reddit.com/r/technology/comments/1ndficc/bmw_says_europes_gas_engine_ban_can_kill_an/,r_1ndficc,,,
r_1ndf9g0,reddit,Easy-Speech7382,2025-09-10T14:10:08+00:00,Exclusive: Official Samsung Galaxy S26 Pro CAD Renders,technology,0,https://www.reddit.com/r/technology/comments/1ndf9g0/exclusive_official_samsung_galaxy_s26_pro_cad/,r_1ndf9g0,,,
r_1ndehfi,reddit,ControlCAD,2025-09-10T13:39:27+00:00,Apple raises iPhone Pro starting price in U.S. for first time since 2017,technology,262,https://www.reddit.com/r/technology/comments/1ndehfi/apple_raises_iphone_pro_starting_price_in_us_for/,r_1ndehfi,,,
r_1ndegwu,reddit,Puginator,2025-09-10T13:38:51+00:00,Amazon's Zoox jumps into U.S. robotaxi race with Las Vegas launch,technology,9,https://www.reddit.com/r/technology/comments/1ndegwu/amazons_zoox_jumps_into_us_robotaxi_race_with_las/,r_1ndegwu,,,
r_1ndef4c,reddit,Logical_Welder3467,2025-09-10T13:36:50+00:00,Sources: AI training startup Mercor eyes $10B+ valuation on $450 million run rate,technology,7,https://www.reddit.com/r/technology/comments/1ndef4c/sources_ai_training_startup_mercor_eyes_10b/,r_1ndef4c,,,
r_1ndecpe,reddit,Logical_Welder3467,2025-09-10T13:33:59+00:00,Nvidia claims software and hardware upgrades allow Blackwell Ultra GB300 to dominate MLPerf benchmarks â€” touts 45% DeepSeek R-1 inference throughput increase over GB200,technology,0,https://www.reddit.com/r/technology/comments/1ndecpe/nvidia_claims_software_and_hardware_upgrades/,r_1ndecpe,,,
r_1nddr81,reddit,Franco1875,2025-09-10T13:08:41+00:00,The web has a new system for making AI companies pay up,technology,35,https://www.reddit.com/r/technology/comments/1nddr81/the_web_has_a_new_system_for_making_ai_companies/,r_1nddr81,,,
r_1nddbx0,reddit,indig0sixalpha,2025-09-10T12:50:58+00:00,Reddit is dropping subscriber counts on subreddits. ï»¿Users will now see seven-day metrics that track active visitors and contributions instead.,technology,7120,https://www.reddit.com/r/technology/comments/1nddbx0/reddit_is_dropping_subscriber_counts_on/,r_1nddbx0,,,
r_1ndcm5v,reddit,chrisdh79,2025-09-10T12:18:27+00:00,Switch modder who represented himself in piracy case ordered to pay Nintendo $2 million | An ill-fated stand against a company seasoned in piracy litigation,technology,2105,https://www.reddit.com/r/technology/comments/1ndcm5v/switch_modder_who_represented_himself_in_piracy/,r_1ndcm5v,,,
r_1ndckn2,reddit,chrisdh79,2025-09-10T12:16:34+00:00,Ted Cruz Wants to Help AI Companies Duck Regulations | Move fast and get permission to break stuff.,technology,804,https://www.reddit.com/r/technology/comments/1ndckn2/ted_cruz_wants_to_help_ai_companies_duck/,r_1ndckn2,,,
r_1ndciw2,reddit,chrisdh79,2025-09-10T12:14:18+00:00,Spotify adds lossless streaming after 8 years of teasing | Subscribers will be able to enjoy 24-bit / 44.1 kHz FLAC as part of their Premium plan.,technology,3174,https://www.reddit.com/r/technology/comments/1ndciw2/spotify_adds_lossless_streaming_after_8_years_of/,r_1ndciw2,,,
r_1ng2h8x,reddit,Andreshere,2025-09-13T16:52:42+00:00,"SplitterMR: a modular library for splitting & parsing documents
Hey guys, I just released **SplitterMR**, a library I built because none of the existing tools quite did what I wanted for slicing up documents cleanly for LLMs / downstream processing.

If you often work with **mixed document types** (PDFs, Word, Excel, Markdown, images, etc.) and **need flexible, reliable splitting/parsing**, this might be useful.

This library supports **multiple input formats**, e.g., text, Markdown, PDF, Word / Excel / PowerPoint, HTML / XML, JSON / YAML, CSV / TSV, and even images.

Files can be read using **MarkItDown** or **Docling**, so this is perfect if you are using those frameworks with your current applications.

Logically, it supports **many different splitting strategies**: not only based on the number of characters but on tokens, schema keys, semantic similarity, and many other techniques. You can even develop your own splitter using the Base object, and it is the same for the Readers!

In addition, **you can process the graphical resources of your documents (e.g., photos) using VLMs** (OpenAI, Gemini, HuggingFace, etc.), so you can extract the text or caption them!

# Whatâ€™s new / whatâ€™s good in the latest release

* Stable Version **1.0.0** is out.
* Supports **more input formats / more robust readers**.
* **Stable API** for the Reader abstractions so you can plug in your own if needed.
* **Better handling of edge cases** (e.g. images, schemaâ€™d JSON / XML) so you donâ€™t lose structure unintentionally.

# Some trade-offs / limitations (so you donâ€™t run into surprises)

* **Heavy dependencies**: because it supports all these formats youâ€™ll pull in a bunch of libs (PDF, Word, image parsing, etc.). If you only care about plain text, many of those wonâ€™t matter, but still.
* **Not a fully â€œLLM prompt managerâ€ or embedding chunker out of the box** â€” splitting + parsing is its job; downstream youâ€™ll still need to decide chunk sizes, context windows, etc.

# Installation and usage

If you want to test:

    uv add splitter-mr

Example usage:

    from splitter_mr.reader import VanillaReader
    from splitter_mr.model.models import AzureOpenAIVisionModel
    
    model = AzureOpenAIVisionModel()
    reader = VanillaReader(model=model)
    output = reader.read(file_path=""data/sample_pdf.pdf"")
    print(output.text)

**Check out the docs for more examples, API details, and instructions on how to write your own Reader for special formats:**  

* ðŸ‘‰ [Github](https://github.com/andreshere00/Splitter_MR)
* ðŸ‘‰ [Documentation server](https://andreshere00.github.io/Splitter_MR/)
* ðŸ‘‰ [PyPi package](https://pypi.org/project/splitter-mr/1.0.1/)
* ðŸ‘‰ [LinkedIn (to contact with me)](https://www.linkedin.com/in/andres-herencia)

If you want to collaborate or you have some suggestions, don't dubt to contact me.

**Thank you so much for reading :)**
",Python,3,https://www.reddit.com/r/Python/comments/1ng2h8x/splittermr_a_modular_library_for_splitting/,r_1ng2h8x,,,
r_1ng10wr,reddit,StarsRonin,2025-09-13T15:54:47+00:00,"The best object notation?
I want your advice regarding the best object notation to use for a python project. If you had the choice to receive data with a specific object notation, what would it be? YAML or JSON? Or another object notation?

YAML looks, to me, to be in agreement with a more pythonic way, because it is simple, faster and easier to understand. On the other hand, JSON has a similar structure to the python dictionary and the native python parser is very much faster than the YAML parser.

Any preferences or experiences?",Python,0,https://www.reddit.com/r/Python/comments/1ng10wr/the_best_object_notation/,r_1ng10wr,,,
r_1nfyq8o,reddit,sciencenerd_1943,2025-09-13T14:21:43+00:00,"MathFlow: an easy-to-use math library for python
Project Site: [https://github.com/cybergeek1943/MathFlow](https://github.com/cybergeek1943/MathFlow)

In the process of doing research for my paper [Combinatorial and Gaussian Foundations of Rational Nth Root Approximations](https://doi.org/10.48550/arXiv.2508.14095) (on arXiv), I created this library to address the pain points I felt when using only SymPy and SciPy separately. I wanted something lightweight, easy to use (exploratory), and something that would support numerical methods more easily. Hence, I created this lightweight wrapper that provides a hybrid symbolic-numerical interface to symbolic and numerical backends. It is backward compatible with Sympy. In short, this enables much faster analysis of symbolic math expressions by providing both numerical and traditional symbolic methods of analysis in the same interface. I have also added additional numerical methods that neither SymPy nor SciPy have (Pade approximations, numerical roots, etc.). The main goal for this project is to provide a tool that requires as little of a learning curve as possible and allows them to just focus on the math they are doing.

# Core features

* **ðŸ”’ Operative Closure**: Mathematical operations return new Expression objects by default
* **âš¡ Mutability Control**: Choose between immutable (default) and mutable expressions for different workflows
* **ðŸ”— Seamless Numerical Integration**: Every symbolic expression has aÂ `.n`Â attribute providing numerical methods without manual lambdification (uses cached lambdified expression when needed)
* **ðŸŽ¨ Enhanced Printing**: Flexible output formatting through theÂ `.print`Â attribute (LaTeX, pretty printing, code generation)
* **ðŸ“¡ Signal System**: Qt-like signals for tracking expression mutations and clones, enabling reactive programming
* **ðŸ”„ Automatic Type Conversions**: Seamlessly and automatically converts between internal Poly and Expr representations based on context
* **ðŸ“¦ Lightweight**: \~0.5 MB itself, \~100 MB including dependencies
* **ðŸ§© Fully backward compatible**: Seamlessly integrate SymPy and MathFlow in the same script. All methods that work on SymPy Expr or Poly objects work on MathFlow objects
* **ðŸ” Exploratory**: Full IDE support, enabling easy tool finding and minimizing the learning curve.

A few examples are shown below. Many more examples can be found in the README of the official GitHub site.

# Quick Start

Install using: `pip install mathflow`

    from mathflow import Expression, Polynomial, Rational
    
    # Create expressions naturally
    f = Expression(""2x^2 + 3x + \frac{1}{2}"")  # latex is automatically parsed
    g = Expression(""sin(x) + cos(x)"")
    
    # Automatic operative closure - operations return new objects of the same type
    h = f + g  # f and g remain unchanged
    hprime = h.diff()  # hprime is still an Expression object
    
    # Numerical evaluation made easy
    result = f(2.5)  # Numerically evaluate at x = 2.5
    
    # Use the .n attribute to access fast numerical methods
    numerical_roots = f.n.all_roots()
    # Call f's n-prefixed methods to use variable precision numerical methods
    precise_roots = f.nsolve_all(prec=50)  # 50 digits of accuracy
    
    # quick and easy printing
    f.print()
    f.print('latex')  # LaTeX output
    f.print('mathematica_code')
    f.print('ccode')  # c code output

# Numerical Computing

MathFlow excels at bridging symbolic and numerical mathematics:

    f = Expression(""x^3 - 2x^2 + x - 1"")
    
    # Root finding
    all_roots = f.n.all_roots(bounds=(-5, 5))
    specific_root = f.nsolve_all(bounds=(-5, 5), prec=50)  # High-precision solve
    
    # Numerical calculus
    derivative_func = f.n.derivative_lambda(df_order=2)  # 2nd derivative numerical function  
    integral_result = f.n.integrate(-1, 1)               # Definite integral  
    
    # Optimization
    minimum = f.n.minimize(bounds=[(-2, 2)])

# Edit:

This project was developed and used primarily for a research project, so a thorough test suite has not yet been developed. The project is still in development, and the current release is an alpha version. I have tried to minimize danger here, however, by designing it as a proxy to the already well-tested SymPy and SciPy libraries.",Python,34,https://www.reddit.com/r/Python/comments/1nfyq8o/mathflow_an_easytouse_math_library_for_python/,r_1nfyq8o,,,
r_1nfvo8y,reddit,elfenpiff,2025-09-13T11:59:15+00:00,"Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library
Hello hello,

I am one of the maintainers of the open-source zero-copy middleware iceoryx2, and weâ€™ve just released iceoryx2 v0.7 which comes with Python language bindings. That means you can now use fast zero-copy communication directly in Python. Here is the full release blog: [https://ekxide.io/blog/iceoryx2-0-7-release/](https://ekxide.io/blog/iceoryx2-0-7-release/)

With iceoryx2 you can communicate between different processes, send data with publish-subscribe, build more complex request-response streams, or orchestrate processes using the event messaging pattern with notifiers and listeners.

Weâ€™ve prepared a set of Python examples here: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python)

On top of that, we invested some time into writing a detailed getting started guide in the iceoryx2 book: [https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html](https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html)

And one more thing: iceoryx2 lets Python talk directly to C, C++ and Rust processes - without any serialization or binding overhead. Check out the cross-language publish-subscribe example to see it in action: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples)

So in short:

* **What My Project Does:** Zero-Copy Inter-Process Communication
* **Target Audience:** Developers building distributed systems, plugin-based applications, or safety-critical and certifiable systems
* **Comparision:** Provides a high-level, service-oriented abstraction over low-level shared memory system calls",Python,5,https://www.reddit.com/r/Python/comments/1nfvo8y/announcing_iceoryx2_v07_fast_and_robust/,r_1nfvo8y,,,
r_1nfupw4,reddit,MrShortCircuitMan,2025-09-13T11:05:08+00:00,"I built QRPorter â€” local Wi-Fi file transfer via QR (PC â†” Mobile)
Hi everyone, I built **QRPorter**, a small open-source utility that moves files between desktop and mobile over your LAN/Wi-Fi using QR codes. No cloud, no mobile app, no accounts â€” just scan & transfer.

# What it does

* **PC â†’ Mobile file transfer:** select a file on your desktop, generate a QR code, scan with your phone and download the file in the phone browser.
* **Mobile â†’ PC file transfer:** scan the QR on the PC, open the link on your phone, upload a file from the phone and itâ€™s saved on the PC.

# Target audience

* Developers, students, and office users who frequently move screenshots, small media or documents between phone â†” PC.
* Privacy-conscious users who want transfers to stay on their LAN/Wi-Fi (no third-party servers).
* Anyone who wants a dead-simple cross-device transfer without installing mobile apps.

# Comparison

* **No extra mobile apps / accounts** â€” works via the phoneâ€™s browser and the desktop app.
* **Local-first** â€” traffic stays on your Wi-Fi/LAN (no cloud).
* **Cross-platform** â€” desktop UI + web interface works with modern mobile browsers (Windows / macOS / Linux / iOS / Android).

# Requirements & tested platforms

* **Python 3.12+** and `pip`.
* Tested on **Windows 11** and **Linux**; macOS should work.
* Key Python deps: `Flask`, `PySide6`, `qrcode`, `Werkzeug`, `Pillow`.

# Installation

You can install from PyPI:

    pip install qrporter

After install, run:

    qrporter

# Troubleshooting

* Make sure **both devices are on the same Wi-Fi/LAN** (guest/isolated networks often block local traffic).
* **Maximum 1 GB file size** limit and commonly used file types allowed.
* **One file at a time.** For multiple files, zip them and transfer the zip.

# License

* MIT License

# GitHub

[https://github.com/manikandancode/qrporter](https://github.com/manikandancode/qrporter)

I beautified and commented the code using AI to improve readability and inline documentation. If you try it out â€” Iâ€™d love feedback, issues, or ideas for improvements. Thanks! ðŸ™",Python,4,https://www.reddit.com/r/Python/comments/1nfupw4/i_built_qrporter_local_wifi_file_transfer_via_qr/,r_1nfupw4,,,
r_1nfphsi,reddit,Fabri10000,2025-09-13T05:40:42+00:00,"Every Python Built-In Function Explained
Hi there, I just wanted to know more about Python and I had this crazy idea about knowing every built-in function from this language. Hope you learn sth new. Any feedback is welcomed. The source has the intention of sharing learning.

[Here's the explanation](https://www.youtube.com/watch?v=frsH10EgZ58)",Python,0,https://www.reddit.com/r/Python/comments/1nfphsi/every_python_builtin_function_explained/,r_1nfphsi,,,
r_1nfiys8,reddit,AutoModerator,2025-09-13T00:00:31+00:00,"Saturday Daily Thread: Resource Request and Sharing! Daily Thread
# Weekly Thread: Resource Request and Sharing ðŸ“š

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! ðŸŒŸ",Python,4,https://www.reddit.com/r/Python/comments/1nfiys8/saturday_daily_thread_resource_request_and/,r_1nfiys8,,,
r_1nff4dw,reddit,Proof_Difficulty_434,2025-09-12T21:15:28+00:00,"Flowfile - An open-source visual ETL tool, now with a Pydantic-based node designer.
Hey r/Python,

I built Flowfile, an open-source tool for creating data pipelines both visually and in code. Here's the latest feature: Custom Node Designer.

# What My Project Does

Flowfile creates bidirectional conversion between visual ETL workflows and Python code. You can build pipelines visually and export to Python, or write Python and visualize it. The Custom Node Designer lets you define new visual nodes using Python classes with Pydantic for settings and Polars for data processing.

# Target Audience

Production-ready tool for data engineers who work with ETL pipelines. Also useful for prototyping and teams that need both visual and code representations of their workflows.

# Comparison

* **Alteryx**: Proprietary, expensive. Flowfile is open-source.
* **Apache NiFi**: Java-based, requires infrastructure. Flowfile is pip-installable Python.
* **Prefect/Dagster**: Orchestration-focused. Flowfile focuses on visual pipeline building.

# Custom Node Example

    import polars as pl
    from flowfile_core.flowfile.node_designer import (
        CustomNodeBase, NodeSettings, Section,
        ColumnSelector, MultiSelect, Types
    )
    
    class TextCleanerSettings(NodeSettings):
        cleaning_options: Section = Section(
            title=""Cleaning Options"",
            text_column=ColumnSelector(label=""Column to Clean"", data_types=Types.String),
            operations=MultiSelect(
                label=""Cleaning Operations"",
                options=[""lowercase"", ""remove_punctuation"", ""trim""],
                default=[""lowercase"", ""trim""]
            )
        )
    
    class TextCleanerNode(CustomNodeBase):
        node_name: str = ""Text Cleaner""
        settings_schema: TextCleanerSettings = TextCleanerSettings()
    
        def process(self, input_df: pl.LazyFrame) -> pl.LazyFrame:
            text_col = self.settings_schema.cleaning_options.text_column.value
            operations = self.settings_schema.cleaning_options.operations.value
            
            expr = pl.col(text_col)
            if ""lowercase"" in operations:
                expr = expr.str.to_lowercase()
            if ""trim"" in operations:
                expr = expr.str.strip_chars()
            
            return input_df.with_columns(expr.alias(f""{text_col}_cleaned""))

Save in `~/.flowfile/user_defined_nodes/` and it appears in the visual editor.

# Why This Matters

You can wrap complex tasksâ€”API connections, custom validations, niche library functionsâ€”into simple drag-and-drop blocks. Build your own high-level tool palette right inside the app. It's all built on Polars for speed and completely open-source.

# Installation

`pip install Flowfile`

# Links

* GitHub: [https://github.com/Edwardvaneechoud/Flowfile/](https://github.com/Edwardvaneechoud/Flowfile/)
* Custom Nodes Documentation: [https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html](https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html)
* Previous discussions: [SideProject post](https://www.reddit.com/r/SideProject/comments/1mp0hor/i_built_a_tool_that_turns_python_data_pipelines/), [FlowFrame post](https://www.reddit.com/r/Python/comments/1kp0er9/flowframe_python_code_that_generates_visual_etl/)",Python,27,https://www.reddit.com/r/Python/comments/1nff4dw/flowfile_an_opensource_visual_etl_tool_now_with_a/,r_1nff4dw,,,
r_1nfe1uq,reddit,kaolay,2025-09-12T20:33:40+00:00,"I Used Python and Bayes to Build a Smart Cybersecurity System
I've been working on an experimental project that combines Python, Bayesian statistics, and psychology to address cybersecurity vulnerabilities - and I'd appreciate your feedback on this approach.

# What My Project Does

The Cybersecurity Psychology Framework (CPF) is an open-source tool that uses Bayesian networks to predict organizational security vulnerabilities by analyzing psychological patterns rather than technical flaws. It identifies pre-cognitive vulnerabilities across 10 categories (authority bias, time pressure, cognitive overload, etc.) and calculates breach probability using Python's pgmpy library.

The system processes aggregated, anonymized data from various sources (email metadata, ticket systems, access logs) to generate risk scores without individual profiling. It outputs a dashboard with vulnerability assessments and convergence risk probabilities.

**Key features:**

* Privacy-preserving aggregation (no individual tracking)
* Bayesian probability modeling for risk convergence
* Real-time organizational vulnerability assessment
* Psychological intervention recommendations

**GitHub:**Â [https://github.com/xbeat/CPF/tree/main/src](https://github.com/xbeat/CPF/tree/main/src)

# Target Audience

This is primarily aÂ **research prototype**Â aimed at:

* Security researchers exploring human factors in cybersecurity
* Data scientists interested in behavioral analytics
* Organizations willing to pilot experimental security approaches
* Python developers interested in Bayesian applications

It's not yet production-ready but serves as a foundation for exploring psychological factors in security environments. The framework is designed for security teams looking to complement their technical controls with human behavior analysis.

# Comparison

Unlike traditional security tools that focus on technical vulnerabilities (firewalls, intrusion detection), CPF addresses the human element that causes 85% of breaches. While existing solutions like security awareness platforms focus on conscious training, CPF targets pre-cognitive processes that occur before conscious decision-making.

**Key differentiators:**

* Focuses on psychological patterns rather than technical signatures
* Uses Bayesian networks instead of rule-based systems
* Privacy-by-design (vs. individual monitoring solutions)
* Predictive rather than reactive approach
* Integrates psychoanalytic theory with data science

Most security tools tell you what happened; CPF attempts to predict what might happen based on psychological states.

# Current Status & Seeking Feedback

This is very much a work in progress. I'm particularly interested in:

* Feedback on the Bayesian network implementation
* Suggestions for additional data sources
* Ideas for privacy-preserving techniques
* Potential collaboration for pilot implementations

The code is experimental but functional, and I'd appreciate any technical or conceptual feedback from this community.

What aspects of this approach seem most promising? What concerns or limitations do you see?",Python,0,https://www.reddit.com/r/Python/comments/1nfe1uq/i_used_python_and_bayes_to_build_a_smart/,r_1nfe1uq,,,
r_1nfdlmq,reddit,dedenorio,2025-09-12T20:15:48+00:00,"Learning machine learning
Is this an appropriate question here? 
I was wondering if anyone could suggest any resources to learn machine learning relatively quickly. By quickly I mean get a general understanding and be able to talk about it. Then I can spend time actually learning it. 
Iâ€™m a beginner in Python. Thanks!",Python,9,https://www.reddit.com/r/Python/comments/1nfdlmq/learning_machine_learning/,r_1nfdlmq,,,
r_1nfdhlu,reddit,initCMD,2025-09-12T20:11:27+00:00,"Thanks r/Python community for reviewing my project Ducky all in one networking tool!
Thanks to this community I received some feedbacks about Ducky that I posted last week on here, I got 42 stars on github as well and some comments for Duckys enhancement. Im thankful for the people who viewed the post and went to see the source code huge thanks to you all.  

**What Ducky Does:**

Ducky is a desktop application that consolidates the essential tools of a network engineer or security enthusiast into a single, easy-to-use interface. Instead of juggling separate applications for terminal connections, network scanning, and diagnostics, Ducky provides a unified workspace to streamline your workflow. Its core features include a tabbed terminal (SSH, Telnet, Serial), an SNMP-powered network topology mapper, a port scanner, and a suite of security utilities like a CVE lookup and hash calculator.

**Target Audience:**

Ducky is built for anyone who works with network hardware and infrastructure. This includes:

* **Network Engineers & Administrators:**Â For daily tasks like configuring switches and routers, troubleshooting connectivity, and documenting network layouts.
* **Cybersecurity Professionals:**Â For reconnaissance tasks like network discovery, port scanning, and vulnerability research.
* **Students & Hobbyists:**Â For those learning networking (e.g., for CompTIA Network+ or CCNA), Ducky provides a free, hands-on tool to explore and interact with real or virtual network devices.
* **IT Support & Help Desk:**Â For frontline technicians who need to quickly run diagnostics like ping and traceroute to resolve user issues.

Github link [https://github.com/thecmdguy/Ducky](https://github.com/thecmdguy/Ducky)",Python,9,https://www.reddit.com/r/Python/comments/1nfdhlu/thanks_rpython_community_for_reviewing_my_project/,r_1nfdhlu,,,
r_1nf79qg,reddit,Ok-Lifeguard-9612,2025-09-12T16:07:21+00:00,"What is 0 to the power of 0? (lim xâ†’0âº of x^x = 1)
I recently came across [this](https://www.youtube.com/watch?v=r0_mi8ngNnM) video from Eddie Woo, about ""**What is 0 to the power of 0?**""

And so I've made this one-line function `def f(x): return x**x`  and tried different inputs.  
I've noticed that you start getting 1 with this value: 0.000000000000000001

Why? Overflow, rounding, special corner case...",Python,0,https://www.reddit.com/r/Python/comments/1nf79qg/what_is_0_to_the_power_of_0_lim_x0_of_xx_1/,r_1nf79qg,,,
r_1nf57hb,reddit,RDE_20,2025-09-12T14:47:00+00:00,"Update: Should I give away my app to my employer for free?
Link to original post - https://www.reddit.com/r/Python/s/UMQsQi8lAX

Hi, since my post gained a lot of attention the other day and I had a lot of messages, questions on the thread etc. I thought I would give an update. 

I didnâ€™t make it clear in my previous post but I developed this app in my own time, but using company resources. 

I spoke to a friend in the HR team and he explained a similar scenario happened a few years ago, someone built an automation tool for outlook, which managed a mailbox receiving 500+ emails a day (dealing/contract notes) and he simply worked on a fund pricing team and only needed to view a few of those emails a day but realised the mailbox was a mess. He took the idea to senior management and presented the cost saving and benefits. Once it was deployed he was offered shares in the company and then a cash bonus once a year of realised savings was achieved. 

Iâ€™ve been advised by my HR friend to approach senior management with my proposal, explain that Iâ€™ve already spoken to my manager and detail the cost savings I can make, ask for a salary increase to provide ongoing support and develop my code further and ask for similar terms to that of the person who did this previously. He has confirmed what Iâ€™ve done doesnâ€™t go against any HR policies or my contract. 

Meeting is booked for next week and Iâ€™ve had 2 messages from senior management saying how excited they are to see my idea :) 

",Python,616,https://www.reddit.com/r/Python/comments/1nf57hb/update_should_i_give_away_my_app_to_my_employer/,r_1nf57hb,,,
r_1nf1lmm,reddit,panspective,2025-09-12T12:15:26+00:00,"Real-world experiences with AI coding agents (Devin, SWE-agent, Aider, Cursor, etc.) â€“ which one is
Iâ€™m trying to get a clearer picture of the current state of **AI agents for software development**. I donâ€™t mean simple code completion assistants, but actual agents that can **manage, create, and modify entire projects almost autonomously**.

Iâ€™ve come across names like **Devin, SWE-agent, Aider, Cursor**, and benchmarks like **SWE-bench** that show impressive results.  
But beyond the marketing and academic papers, Iâ€™d like to hear from the community about **real-world experiences**:

* In your opinion, whatâ€™s the best AI agent youâ€™ve actually used (even based on personal or lesser-known benchmarks)?
* Which model did you run it with?
* In short, as of September 2025, whatâ€™s the best AI-powered coding software you know of that really works?",Python,0,https://www.reddit.com/r/Python/comments/1nf1lmm/realworld_experiences_with_ai_coding_agents_devin/,r_1nf1lmm,,,
r_1nexoe8,reddit,sikerce,2025-09-12T08:27:44+00:00,"I built a from-scratch Python package for classic Numerical Methods (no NumPy/SciPy required!)
Hey everyone,

Over the past few months Iâ€™ve been building a Python package calledÂ `numethods`Â â€” a small but growing collection ofÂ **classic numerical algorithms implemented 100% from scratch**. No NumPy, no SciPy, just plain Python floats and list-of-lists.

The idea is to make algorithms transparent and educational, so you can actuallyÂ *see*Â how LU decomposition, power iteration, or RK4 are implemented under the hood. This is especially useful for students, self-learners, or anyone who wants a deeper feel for how numerical methods work beyond calling library functions.

[https://github.com/denizd1/numethods](https://github.com/denizd1/numethods)

# ðŸ”§ Whatâ€™s included so far

* **Linear system solvers**: LU (with pivoting), Gaussâ€“Jordan, Jacobi, Gaussâ€“Seidel, Cholesky
* **Root-finding**: Bisection, Fixed-Point Iteration, Secant, Newtonâ€™s method
* **Interpolation**: Newton divided differences, Lagrange form
* **Quadrature (integration)**: Trapezoidal rule, Simpsonâ€™s rule, Gaussâ€“Legendre (2- and 3-point)
* **Orthogonalization & least squares**: Gramâ€“Schmidt, Householder QR, LS solver
* **Eigenvalue methods**: Power iteration, Inverse iteration, Rayleigh quotient iteration, QR iteration
* **SVD**Â (via eigen-decomposition of ATAA\^T AATA)
* **ODE solvers**: Euler, Heun, RK2, RK4, Backward Euler, Trapezoidal, Adamsâ€“Bashforth, Adamsâ€“Moulton, Predictorâ€“Corrector, Adaptive RK45

# âœ… Why this might be useful

* Great forÂ **teaching/learning**Â numerical methods step by step.
* Good reference for people writing their own solvers in C/Fortran/Julia.
* Lightweight, no dependencies.
* Consistent object-oriented API (`.solve()`,Â `.integrate()`Â etc).

# ðŸš€ Whatâ€™s next

* PDE solvers (heat, wave, Poisson with finite differences)
* More optimization methods (conjugate gradient, quasi-Newton)
* Spectral methods and advanced quadrature

ðŸ‘‰ If youâ€™re learning numerical analysis, want to peek under the hood, or just like playing with algorithms, Iâ€™d love for you to check it out and give feedback.",Python,108,https://www.reddit.com/r/Python/comments/1nexoe8/i_built_a_fromscratch_python_package_for_classic/,r_1nexoe8,,,
r_1new8g8,reddit,Goldziher,2025-09-12T06:52:02+00:00,"Building with Litestar and AI Agents
In a recent thread in the subreddit - [Would you recommend Litestar or FastAPI for building large scale api in 2025](https://www.reddit.com/r/Python/comments/1mgkwmn/would_you_recommend_litestar_or_fastapi_for/) - I wrote [a comment](https://www.reddit.com/r/Python/comments/1mgkwmn/comment/n6qxwgp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button):

```text
Hi, ex-litestar maintainer here.

I am no longer maintaining a litestar - but I have a large scale system I maintain built with it.

As a litestar user I am personally very pleased. Everything works very smoothly - and there is a top notch discord server to boot.

Litestar is, in my absolutely subjective opinion, a better piece of software.

BUT - there are some problems: documentation needs a refresh. And AI tools do not know it by default. You will need to have some proper CLAUDE.md files etc.
```

Well, life happened, and I forgot.

So two things, first, unabashadly promoting my own tool [ai-rulez](https://github.com/Goldziher/ai-rulez), which I actually use to maintain and generate said CLAUDE.md, subagents and mcp servers (for several different tools - working with teams with different AI tools, I just find it easier to git ignore all the .cursor, .gemini and github copilot instructions, and maintain these centrally). Second, here is the (redacted) versio of the promised CLAUDE.md file:

```markdown
<!-- 
ðŸ¤– GENERATED FILE - DO NOT EDIT DIRECTLY
===========================================

This file was automatically generated by ai-rulez from ai-rulez.yaml.

âš ï¸  IMPORTANT FOR AI ASSISTANTS AND DEVELOPERS:
- DO NOT modify this file directly
- DO NOT add, remove, or change rules in this file
- Changes made here will be OVERWRITTEN on next generation

âœ… TO UPDATE RULES:
1. Edit the source configuration: ai-rulez.yaml
2. Regenerate this file: ai-rulez generate
3. The updated CLAUDE.md will be created automatically

ðŸ“ Generated: 2025-09-11 18:52:14
ðŸ“ Source: ai-rulez.yaml
ðŸŽ¯ Target: CLAUDE.md
ðŸ“Š Content: 25 rules, 5 sections

Learn more: https://github.com/Goldziher/ai-rulez
===========================================
-->

# grantflow

GrantFlow.AI is a comprehensive grant management platform built as a monorepo with Next.js 15/React 19 frontend and Python microservices backend. Features include <REDACTED>.

## API Security

**Priority:** critical

Backend endpoints must use @post/@get decorators with allowed_roles parameter. Firebase Auth JWT claims provide organization_id/role. Never check auth manually - middleware handles it. Use withAuthRedirect() wrapper for all frontend API calls.

## Litestar Authentication Pattern

**Priority:** critical

Litestar-specific auth pattern: Use @get/@post/@patch/@delete decorators with allowed_roles parameter in opt dict. Example: `@get(""/path"", allowed_roles=[UserRoleEnum.OWNER])`. AuthMiddleware reads route_handler.opt[""allowed_roles""] - never check auth manually. Always use allowed_roles in opt dict, NOT as decorator parameter.

## Litestar Dependency Injection

**Priority:** critical

Litestar dependency injection: async_sessionmaker injected automatically via parameter name. Request type is APIRequest. Path params use {param:uuid} syntax. Query params as function args. Never use Depends() - Litestar injects by parameter name/type.

## Litestar Framework Patterns (IMPORTANT: not FastAPI!)

### Key Differences from FastAPI
- **Imports**: `from litestar import get, post, patch, delete` (NOT `from fastapi import FastAPI, APIRouter`)
- **Decorators**: Use `@get`, `@post`, etc. directly on functions (no router.get)
- **Auth**: Pass `allowed_roles` in decorator's opt dict: `@get(""/path"", allowed_roles=[UserRoleEnum.OWNER])`
- **Dependency Injection**: No `Depends()` - Litestar injects by parameter name/type
- **Responses**: Return TypedDict/msgspec models directly, or use `Response[Type]` for custom responses

### Authentication Pattern

from litestar import get, post
from packages.db.src.enums import UserRoleEnum

<> CORRECT - Litestar pattern with opt dict
@get(
    ""/organizations/{organization_id:uuid}/members"",
    allowed_roles=[UserRoleEnum.OWNER, UserRoleEnum.ADMIN],
    operation_id=""ListMembers""
)
async def handle_list_members(
    request: APIRequest,  # Injected automatically
    organization_id: UUID,  # Path param
    session_maker: async_sessionmaker[Any],  # Injected by name
) -> list[MemberResponse]:
    ...

<> WRONG - FastAPI pattern (will not work)
@router.get(""/members"")
async def list_members(
    current_user: User = Depends(get_current_user)
):
    ...

### WebSocket Pattern

from litestar import websocket_stream
from collections.abc import AsyncGenerator

@websocket_stream(
    ""/organizations/{organization_id:uuid}/notifications"",
    opt={""allowed_roles"": [UserRoleEnum.OWNER]},
    type_encoders={UUID: str, SourceIndexingStatusEnum: lambda x: x.value}
)
async def handle_notifications(
    organization_id: UUID,
) -> AsyncGenerator[WebsocketMessage[dict[str, Any]]]:
    while True:
        messages = await get_messages()
        for msg in messages:
            yield msg  # Use yield, not send
        await asyncio.sleep(3)


### Response Patterns

from litestar import Response

<> Direct TypedDict return (most common)
@post(""/organizations"")
async def create_org(data: CreateOrgRequest) -> TableIdResponse:
    return TableIdResponse(id=str(org.id))

<> Custom Response with headers/status
@post(""/files/convert"")
async def convert_file(data: FileData) -> Response[bytes]:
    return Response[bytes](
        content=pdf_bytes,
        media_type=""application/pdf"",
        headers={""Content-Disposition"": f'attachment; filename=""{filename}""'}
    )

### Middleware Access
- AuthMiddleware checks `connection.route_handler.opt.get(""allowed_roles"")`
- Never implement auth checks in route handlers
- Middleware handles all JWT validation and role checking

## Litestar Framework Imports

**Priority:** critical

Litestar imports & decorators: from litestar import get, post, patch, delete, websocket_stream. NOT from fastapi. Route handlers return TypedDict/msgspec models directly. For typed responses use Response[Type]. WebSocket uses @websocket_stream with AsyncGenerator yield pattern.

## Multi-tenant Security

**Priority:** critical

All endpoints must include organization_id in URL path. Use @allowed_roles decorator from services.backend.src.auth. Never check auth manually. Firebase JWT claims must include organization_id.

## SQLAlchemy Async Session Management

**Priority:** critical

Always use async session context managers with explicit transaction boundaries. Pattern: `async with session_maker() as session, session.begin():`. Never reuse sessions across requests. Use `select_active()` from packages.db.src.query_helpers for soft-delete filtering.

## Soft Delete Integrity

**Priority:** critical

Always use select_active() helper from packages.db.src.query_helpers for queries. Never query deleted_at IS NULL directly. Test soft-delete filtering in integration tests for all new endpoints.

## Soft Delete Pattern

**Priority:** critical

All database queries must use select_active() helper from packages.db.src.query_helpers for soft-delete filtering. Never query deleted_at IS NULL directly. Tables with is_deleted/deleted_at fields require this pattern to prevent exposing deleted data.

## Task Commands

**Priority:** critical

Use Taskfile commands exclusively: task lint:all before commits, task test for testing, task db:migrate for migrations. Never run raw commands. Check available tasks with task --list. CI validates via these commands.

## Test Database Isolation

**Priority:** critical

Use real PostgreSQL for all tests via testing.db_test_plugin. Mark integration tests with @pytest.mark.integration, E2E with @pytest.mark.e2e_full. Always set PYTHONPATH=. when running pytest. Use factories from testing.factories for test data generation.

## Testing with Real Infrastructure

**Priority:** critical

Use real PostgreSQL via db_test_plugin for all tests. Never mock SQLAlchemy sessions. Use factories from testing/factories.py. Run 'task test:e2e' for integration tests before merging.

## CI/CD Patterns

**Priority:** high

GitHub Actions in .github/workflows/ trigger on developmentâ†’staging, mainâ†’production. Services deploy via build-service-*.yaml workflows. Always run task lint:all and task test locally before pushing. Docker builds require --build-arg for frontend env vars.

## Development Workflow

### Quick Start

<> Install dependencies and setup
task setup

<> Start all services in dev mode
task dev

<> Or start specific services
task service:backend:dev
task frontend:dev

### Daily Development Tasks

#### Running Tests

<> Run all tests (parallel by default)
task test

<> Python service tests with real PostgreSQL
PYTHONPATH=. uv run pytest services/backend/tests/
PYTHONPATH=. uv run pytest services/indexer/tests/

<> Frontend tests with Vitest
cd frontend && pnpm test

#### Linting & Formatting

<> Run all linters
task lint:all

<> Specific linters
task lint:frontend  # Biome, ESLint, TypeScript
task lint:python    # Ruff, MyPy

#### Database Operations

<> Apply migrations locally
task db:migrate

<> Create new migration
task db:create-migration -- <migration_name>

<> Reset database (WARNING: destroys data)
task db:reset

<> Connect to Cloud SQL staging
task db:proxy:start
task db:migrate:remote

### Git Workflow
- Branch from `development` for features
- `development` â†’ auto-deploys to staging
- `main` â†’ auto-deploys to production
- Commits use conventional format: `fix:`, `feat:`, `chore:`

## Auth Security

**Priority:** high

Never check auth manually in endpoints - middleware handles all auth via JWT claims (organization_id/role). Use UserRoleEnum from packages.db for role checks. Pattern: `@post('/path', allowed_roles=[UserRoleEnum.COLLABORATOR])`. Always wrap frontend API calls with withAuthRedirect().

## Litestar WebSocket Handling

**Priority:** high

Litestar WebSocket pattern: Use @websocket_stream decorator with AsyncGenerator return type. Yield messages in async loop. Set type_encoders for UUID/enum serialization. Access allowed_roles via opt dict. Example: @websocket_stream(""/path"", opt={""allowed_roles"": [...]}).

## Initial Setup

<> Install all dependencies and set up git hooks
task setup

<> Copy environment configuration
cp .env.example .env
<> Update .env with actual values (reach out to team for secrets)

<> Start database and apply migrations
task db:up
task db:migrate

<> Seed the database
task db:seed

## Running Services

<> Start all services in development mode
task dev

## Taskfile Command Execution

**Priority:** high

Always use task commands instead of direct package managers. Core workflow: `task setup dev test lint format build`. Run `task lint:all` after changes, `task test:e2e` for E2E tests with E2E_TESTS=1 env var. Check available commands with `task --list`.

## Test Factories

**Priority:** high

Use testing/factories.py for Python tests and testing/factories.ts for TypeScript tests. Real PostgreSQL instances required for backend tests. Run PYTHONPATH=. uv run pytest for Python, pnpm test for frontend. E2E tests use markers: smoke (<1min), quality_assessment (2-5min), e2e_full (10+min).

## Type Safety

**Priority:** high

Python: Type all args/returns, use TypedDict with NotRequired[type]. TypeScript: Never use 'any', leverage API namespace types, use ?? operator. Run task lint:python and task lint:frontend to validate. msgspec for Python serialization.

## Type Safety and Validation

**Priority:** high

Python: Use msgspec TypedDict with NotRequired[], never Optional. TypeScript: Ban 'any', use type guards from @tool-belt/type-predicates. All API responses must use msgspec models.

## TypeScript Type Safety

**Priority:** high

Never use 'any' type. Use type guards from @tool-belt/type-predicates. Always use nullish coalescing (??) over logical OR (||). Extract magic numbers to constants. Use factories from frontend/testing/factories and editor/testing/factories for test data.

## Async Performance Patterns

**Priority:** medium

Use async with session.begin() for transactions. Batch Pub/Sub messages with ON CONFLICT DO NOTHING for duplicates. Frontend: Use withAuthRedirect() wrapper for all API calls.

## Monorepo Service Boundaries

**Priority:** medium

Services must be independently deployable. Use packages/db for shared models, packages/shared_utils for utilities. <REDACTED>. 

## Microservices Overview

<REDACTED>

### Key Technologies

<REDACTED>

## Service Communication

<REDACTED>

## Test Commands

<> Run all tests (parallel by default)
task test

<> Run specific test suites
PYTHONPATH=. uv run pytest services/backend/tests/
cd frontend && pnpm test

<> E2E tests with markers
E2E_TESTS=1 pytest -m ""smoke""              # <1 min
E2E_TESTS=1 pytest -m ""quality_assessment"" # 2-5 min
E2E_TESTS=1 pytest -m ""e2e_full""          # 10+ min

<> Disable parallel execution for debugging
pytest -n 0

## Test Structure
- **Python**: `*_test.py` files, async pytest with real PostgreSQL
- **TypeScript**: `*.spec.ts(x)` files, Vitest with React Testing Library
- **E2E**: Playwright tests with `data-testid` attributes

## Test Data
- Use factories from `testing/factories.py` (Python)
- Use factories from `frontend/testing/factories.ts` (TypeScript)
- Test scenarios in `testing/test_data/scenarios/` with metadata.yaml configs

## Coverage Requirements
- Target 100% test coverage
- Real PostgreSQL for backend tests (no mocks)
- Mock only external APIs in frontend tests

## Structured Logging

**Priority:** low

Use structlog with key=value pairs: logger.info('Created grant', grant_id=str(id)). Convert UUIDs to strings, datetime to .isoformat(). Never use f-strings in log messages.
```

Important notes: 
   * in larger monorepo what I do (again using ai-rulez) is create layered CLAUDE.md files - e.g., there is a root ai-rulez.yaml file in the repository root, which includes the overall conventions of the codebase, instructions about tooling etc. Then, say under the `services` folder (assuming it includes services of the same type), there is another ai-rulez.yaml file with more specialized instructions for these services, say - all are written in Litestar, so the above conventions etc. Why? Claude Code, for example, reads the CLAUDE.md files in its working context. This is far from perfect, but it does allow creating more focused context.
  * in the above example I removed the code blocks and replaced code block comments from using `#` to using `<>`. Its not the most elegant, but it makes it more readable. ",Python,1,https://www.reddit.com/r/Python/comments/1new8g8/building_with_litestar_and_ai_agents/,r_1new8g8,,,
r_1neuyit,reddit,_unknownProtocol,2025-09-12T05:32:04+00:00,"html2pic: transform basic html&css to image, without a browser (experimental)
Hey everyone,

For the past few months, I've been working on a personal graphics library called [PicTex](https://github.com/francozanardi/pictex). As an experiment, I got curious to see if I could build a lightweight HTML/CSS to image converter on top of it, without the overhead of a full browser engine like Selenium or Playwright.

**Important**: this is a proof-of-concept, and a large portion of the code was generated with AI assistance (primarily Claude) to quickly explore the idea. It's definitely not production-ready and likely has plenty of bugs and unhandled edge cases.

I'm sharing it here to show what I've been exploring, maybe it could be useful for someone.

Here's the link to the repo: [https://github.com/francozanardi/html2pic](https://github.com/francozanardi/html2pic)

---

### What My Project Does

`html2pic` takes a subset of HTML and CSS and renders it into a PNG, JPG, or SVG image, using Python + Skia. It also uses BeautifulSoup4 for HTML parsing, tinycss2 for CSS parsing.

Hereâ€™s a basic example:

```python
from html2pic import Html2Pic

html = '''
<div class=""card"">
  <div class=""avatar""></div>
  <div class=""user-info"">
    <h2>pictex_dev</h2>
    <p>@python_renderer</p>
  </div>
</div>
'''

css = '''
.card {
    font-family: ""Segoe UI"";
    display: flex;
    align-items: center;
    gap: 16px;
    padding: 20px;
    background-color: #1a1b21;
    border-radius: 12px;
    width: 350px;
    box-shadow: 0px 4px 12px rgba(0, 0, 0, 0.4);
}

.avatar {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    background-image: linear-gradient(45deg, #f97794, #623aa2);
}

.user-info {
    display: flex;
    flex-direction: column;
}

h2 {
    margin: 0;
    font-size: 22px;
    font-weight: 600;
    color: #e6edf3;
}

p {
    margin: 0;
    font-size: 16px;
    color: #7d8590;
}
'''

renderer = Html2Pic(html, css)
image = renderer.render()
image.save(""profile_card.png"")
```

And here's the image it generates:

**[Quick Start Result Image](https://i.imgur.com/UKGA0lH.png)**

---

### Target Audience

Right now, this is a **toy project / proof-of-concept**.

It's intended for hobbyists, developers who want to prototype image generation, or for simple, controlled use cases where installing a full browser feels like overkill. For example:
*   Generating simple social media cards with dynamic text.
*   Creating basic components for reports.
*   Quickly visualizing HTML/CSS snippets without opening a browser.

It is **not** meant for production environments or for rendering complex HTML/CSS. It is absolutely not a browser replacement.

---

### Comparison

*   **vs. Selenium / Playwright:** The main difference is the lack of a browser. `html2pic` is much more lightweight and has fewer dependencies. The trade-off is that it only supports a tiny fraction of HTML/CSS.

---

Thanks for checking it out.",Python,21,https://www.reddit.com/r/Python/comments/1neuyit/html2pic_transform_basic_htmlcss_to_image_without/,r_1neuyit,,,
r_1neu7bv,reddit,chinmay06,2025-09-12T04:46:51+00:00,"ðŸ’» [Showcase] MotionSaver: A Python-based Dynamic Video Lockscreen & Screensaver for Windows
**MotionSaver** is a free, open-source application that transforms your Windows desktop into a dynamic, animated space by using videos as a lockscreen and screensaver. Built with **Python** using libraries like **OpenCV** and **Tkinter**, it provides a customizable and hardware-accelerated experience. The core of the project is a video engine that handles multiple formats and ensures smooth playback with minimal CPU usage by leveraging GPU acceleration. It also includes features like a macOS-style password prompt and optional real-time widgets for weather and stocks.

# What My Project Does

MotionSaver lets you set any video as your lockscreen or screensaver on Windows. It's built to be both customizable and performant. The application's video rendering is powered by **OpenCV** with GPU acceleration, which ensures a smooth visual experience without draining your CPU. You can also customize the on-screen clock, set a secure password, and add optional widgets for live data like weather and stock prices.

# Target Audience

This project is primarily a **hobbyist and personal-use application**. It is not a commercial product and should **not be used in production environments** or places requiring high security. The current password mechanism is a basic security layer and can be bypassed. It's designed for Python enthusiasts who enjoy customizing their systems and want a fun, functional way to personalize their PC.

# Comparison

While there are other video wallpaper and screensaver applications for Windows, MotionSaver stands out for a few key reasons:

* **Open-Source and Python-based**: Unlike many commercial alternatives like Wallpaper Engine, MotionSaver is completely free and open-source. This allows developers to inspect, modify, and contribute to the code, which is a core value of the r/Python community.
* **Lightweight and Focused**: While alternatives like **Lively Wallpaper** are very robust and feature-rich, MotionSaver is specifically focused on delivering a high-performance video lockscreen. It uses **OpenCV** for optimized video rendering, ensuring a lean and efficient screensaver without the overhead of a full desktop customization suite.

# Source Code

**GitHub Repository:**[https://github.com/chinmay-sawant/MotionSaver](https://github.com/chinmay-sawant/MotionSaver)",Python,3,https://www.reddit.com/r/Python/comments/1neu7bv/showcase_motionsaver_a_pythonbased_dynamic_video/,r_1neu7bv,,,
r_1ner9mj,reddit,PlanetMercurial,2025-09-12T02:12:23+00:00,"Best way to install python package with all its dependencies on an offline pc. -- Part 2
This is a follow up post to [https://www.reddit.com/r/Python/comments/1keaeft/best\_way\_to\_install\_python\_package\_with\_all\_its/](https://www.reddit.com/r/Python/comments/1keaeft/best_way_to_install_python_package_with_all_its/)  
I followed one of the techniques shown in that post and it worked quite well.  
So in short what i do is  
first do  
`python -m venv .` ( in a  directory)  
then `.\Scripts\activate`  
then do the actual installation of the package with `pip install <packagename>`  
then i do a `pip freeze > requirements.txt`  
and finally i download the wheels using this requirements.txt.  
For that i create a folder called wheel and then I do a `pip download -r requirements.txt`  
then i copy over the wheels folder to the offline pc and create a venv over there and do the install using that wheel folder.

So all this works quite well as long as there as only wheel files in the package.  
Lately I see that there are packages that need some dependencies that need to be built from source so instead of the `whl` file a `tar.gz` file gets downloaded in the wheel folder. And somehow that `tar.gz` doesn't get built on the offline pc due to lack of dependencies or sometimes buildtools or setuptools version mismatch.

Is there a way to get this working?",Python,9,https://www.reddit.com/r/Python/comments/1ner9mj/best_way_to_install_python_package_with_all_its/,r_1ner9mj,,,
r_1neqpor,reddit,LiekkasKono,2025-09-12T01:44:34+00:00,"I Found a Game-Changing Tool for Extracting Hard Subtitles from Videos â€“ Open Source & Super Fast!
I just came across an awesome open-source tool that I had to share: **[RapidVideOCR](https://github.com/SWHL/RapidVideOCR)**.

If youâ€™ve ever struggled with videos that have **hardcoded subtitles** (those burned directly into the video and not in a separate track), this tool might be exactly what youâ€™ve been looking for.

**RapidVideOCR** automatically extracts hardcoded subtitles from video files and generates clean `.srt`, `.ass`, or `.txt` subtitle files â€” perfect for translation, accessibility, or archiving.

### ðŸ” How it works:
1. It uses **VideoSubFinder** (or similar tools) to extract key frames where subtitles appear.
2. Then, **RapidVideOCR** runs OCR (Optical Character Recognition) on those frames using **RapidOCR**, which supports **multiple languages**.
3. Finally, it generates accurate, time-synced subtitle files.

### âœ… Why it stands out:
- **Fast & accurate**: Leverages a powerful OCR engine optimized for speed and precision.
- **Easy to use**: Install via `pip install rapid_videocr` and run in seconds.
- **Batch processing**: Great for handling entire videos or multiple files.
- **Supports many languages**: As long as RapidOCR supports it, so does this tool.
- **Open source & free**: Apache 2.0 licensed, with a clear path for contributions.

Thereâ€™s even a desktop version available if you prefer a GUI: [RapidVideOCRDesktop](https://github.com/SWHL/RapidVideOCRDesktop).

ðŸ‘‰ GitHub: [https://github.com/SWHL/RapidVideOCR](https://github.com/SWHL/RapidVideOCR)

This could be a huge help for content creators, translators, educators, or anyone working with foreign-language videos. The project is still gaining traction, so if you find it useful, consider giving it a â­ on GitHub to support the devs!

Have you tried any tools like this? Iâ€™d love to hear your experiences or alternatives!",Python,0,https://www.reddit.com/r/Python/comments/1neqpor/i_found_a_gamechanging_tool_for_extracting_hard/,r_1neqpor,,,
r_1neosd8,reddit,Upbeat_Marsupial9770,2025-09-12T00:10:35+00:00,"Tips for Sprite Collisions in Platformer
I am using PyGame to make a platformer, and my collisions are pretty buggy. I am pretty new to coding and would appreciate any tips.",Python,4,https://www.reddit.com/r/Python/comments/1neosd8/tips_for_sprite_collisions_in_platformer/,r_1neosd8,,,
r_1neoksd,reddit,AutoModerator,2025-09-12T00:00:44+00:00,"Friday Daily Thread: r/Python Meta and Free-Talk Fridays
# Weekly Thread: Meta Discussions and Free Talk Friday ðŸŽ™ï¸

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! ðŸŒŸ",Python,3,https://www.reddit.com/r/Python/comments/1neoksd/friday_daily_thread_rpython_meta_and_freetalk/,r_1neoksd,,,
r_1nenw34,reddit,Upbeat_Marsupial9770,2025-09-11T23:27:52+00:00,"Why does my program only work in vsc?
    # Created: 7/13/2025
    # Last updated: 8/26/2025
    
    import pygame
    from PIL import Image
    import os
    
    pygame.init()
    
    # Screen setup
    screen = pygame.display.set_mode((800, 600))
    pygame.display.set_caption(""Platformer"")
    clock = pygame.time.Clock()
    
    # Tile size
    TILE_WIDTH, TILE_HEIGHT = 30, 30
    TILEMAP_IMAGE = os.path.join(""Platformer"", ""Sprites"", ""platform.png"")
    PLAYER_SPRITESHEET = os.path.join(""Platformer"", ""Sprites"", ""player_spritesheet.png"")
    
    # Create tile masks for pixel-perfect collisions
    def generate_tilemap(image_path, offset_x=0, offset_y=0):
    Â  Â  img = pygame.image.load(image_path).convert_alpha()
    Â  Â  tiles = []
    Â  Â  masks = []
    
    Â  Â  width, height = img.get_width(), img.get_height()
    Â  Â  for y in range(0, height, TILE_HEIGHT):
    Â  Â  Â  Â  for x in range(0, width, TILE_WIDTH):
    Â  Â  Â  Â  Â  Â  tile_surface = pygame.Surface((TILE_WIDTH, TILE_HEIGHT), pygame.SRCALPHA)
    Â  Â  Â  Â  Â  Â  tile_surface.blit(img, (-x, -y))
    Â  Â  Â  Â  Â  Â  mask = pygame.mask.from_surface(tile_surface)
    Â  Â  Â  Â  Â  Â  if mask.count() > 0:
    Â  Â  Â  Â  Â  Â  Â  Â  rect = pygame.Rect(x + offset_x, y + offset_y, TILE_WIDTH, TILE_HEIGHT)
    Â  Â  Â  Â  Â  Â  Â  Â  tiles.append(rect)
    Â  Â  Â  Â  Â  Â  Â  Â  masks.append((mask, rect.topleft))
    Â  Â  return tiles, masks, img
    
    # Player animation & physics
    class Player(pygame.sprite.Sprite):
    Â  Â  def __init__(self):
    Â  Â  Â  Â  super().__init__()
    Â  Â  Â  Â  self.spritesheet = pygame.image.load(PLAYER_SPRITESHEET).convert_alpha()
    Â  Â  Â  Â  self.frames = []
    Â  Â  Â  Â  self.masks = []
    Â  Â  Â  Â  self.frame_index = 0
    Â  Â  Â  Â  self.animation_timer = 0
    Â  Â  Â  Â  self.load_frames()
    
    Â  Â  Â  Â  self.image = self.frames[self.frame_index]
    Â  Â  Â  Â  self.mask = self.masks[self.frame_index]
    Â  Â  Â  Â  self.rect = self.image.get_rect(topleft=(100, 500))
    
    Â  Â  Â  Â  self.vel_x = 0
    Â  Â  Â  Â  self.vel_y = 0
    Â  Â  Â  Â  self.jump_count = 0
    Â  Â  Â  Â  self.max_jumps = 2
    Â  Â  Â  Â  self.jump_pressed = False
    Â  Â  Â  Â  self.facing_right = True
    Â  Â  Â  Â  self.feet_height = 6 Â  # bottom pixels for floor detection
    Â  Â  Â  Â  self.head_height = 6 Â  # top pixels for ceiling detection
    Â  Â  Â  Â  self.coyote_timer = 0
    Â  Â  Â  Â  self.coyote_time_max = 6 Â # frames allowed after leaving platform
    
    Â  Â  def load_frames(self):
    Â  Â  Â  Â  frame_width = 32
    Â  Â  Â  Â  frame_height = 32
    Â  Â  Â  Â  for i in range(self.spritesheet.get_width() // frame_width):
    Â  Â  Â  Â  Â  Â  frame = self.spritesheet.subsurface((i * frame_width, 0, frame_width, frame_height))
    Â  Â  Â  Â  Â  Â  frame = pygame.transform.scale(frame, (64, 64))
    Â  Â  Â  Â  Â  Â  self.frames.append(frame)
    Â  Â  Â  Â  Â  Â  self.masks.append(pygame.mask.from_surface(frame))
    
    Â  Â  # Create feet mask
    Â  Â  def get_feet_mask(self):
    Â  Â  Â  Â  feet_surface = pygame.Surface((self.rect.width, self.feet_height), pygame.SRCALPHA)
    Â  Â  Â  Â  feet_surface.blit(self.image, (0, -self.rect.height + self.feet_height))
    Â  Â  Â  Â  return pygame.mask.from_surface(feet_surface)
    
    Â  Â  # Create head mask
    Â  Â  def get_head_mask(self):
    Â  Â  Â  Â  head_surface = pygame.Surface((self.rect.width, self.head_height), pygame.SRCALPHA)
    Â  Â  Â  Â  head_surface.blit(self.image, (0, 0))
    Â  Â  Â  Â  return pygame.mask.from_surface(head_surface)
    
    Â  Â  def update(self, tiles, tile_masks):
    Â  Â  Â  Â  keys = pygame.key.get_pressed()
    Â  Â  Â  Â  self.vel_x = 0
    Â  Â  Â  Â  if keys[pygame.K_a] or keys[pygame.K_LEFT]:
    Â  Â  Â  Â  Â  Â  self.vel_x = -5
    Â  Â  Â  Â  Â  Â  self.facing_right = False
    Â  Â  Â  Â  if keys[pygame.K_d] or keys[pygame.K_RIGHT]:
    Â  Â  Â  Â  Â  Â  self.vel_x = 5
    Â  Â  Â  Â  Â  Â  self.facing_right = True
    
    Â  Â  Â  Â  # Animation
    Â  Â  Â  Â  if self.vel_x != 0:
    Â  Â  Â  Â  Â  Â  self.animation_timer += 1
    Â  Â  Â  Â  Â  Â  if self.animation_timer >= 6:
    Â  Â  Â  Â  Â  Â  Â  Â  self.frame_index = (self.frame_index + 1) % len(self.frames)
    Â  Â  Â  Â  Â  Â  Â  Â  self.animation_timer = 0
    Â  Â  Â  Â  else:
    Â  Â  Â  Â  Â  Â  self.frame_index = 0
    
    Â  Â  Â  Â  self.image = self.frames[self.frame_index]
    Â  Â  Â  Â  self.mask = self.masks[self.frame_index]
    Â  Â  Â  Â  if not self.facing_right:
    Â  Â  Â  Â  Â  Â  self.image = pygame.transform.flip(self.image, True, False)
    Â  Â  Â  Â  Â  Â  self.mask = pygame.mask.from_surface(self.image)
    
    Â  Â  Â  Â  # Gravity
    Â  Â  Â  Â  self.vel_y += 0.5
    Â  Â  Â  Â  if self.vel_y > 10:
    Â  Â  Â  Â  Â  Â  self.vel_y = 10
    
    Â  Â  Â  Â  # Jumping (with coyote time)
    Â  Â  Â  Â  self.coyote_timer = max(0, self.coyote_timer - 1)
    Â  Â  Â  Â  jump_key = keys[pygame.K_SPACE] or keys[pygame.K_w] or keys[pygame.K_UP]
    Â  Â  Â  Â  if jump_key and not self.jump_pressed and (self.jump_count < self.max_jumps or self.coyote_timer > 0):
    Â  Â  Â  Â  Â  Â  self.vel_y = -10
    Â  Â  Â  Â  Â  Â  self.jump_count += 1
    Â  Â  Â  Â  Â  Â  self.jump_pressed = True
    Â  Â  Â  Â  Â  Â  self.coyote_timer = 0
    Â  Â  Â  Â  elif not jump_key:
    Â  Â  Â  Â  Â  Â  self.jump_pressed = False
    
    Â  Â  Â  Â  # --- Horizontal movement ---
    Â  Â  Â  Â  if self.vel_x != 0:
    Â  Â  Â  Â  Â  Â  step_x = 1 if self.vel_x > 0 else -1
    Â  Â  Â  Â  Â  Â  for _ in range(abs(self.vel_x)):
    Â  Â  Â  Â  Â  Â  Â  Â  self.rect.x += step_x
    Â  Â  Â  Â  Â  Â  Â  Â  for mask, offset in tile_masks:
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dx = offset[0] - self.rect.x
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dy = offset[1] - self.rect.y
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self.mask.overlap(mask, (dx, dy)):
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.rect.x -= step_x
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
    
    Â  Â  Â  Â  # --- Vertical movement ---
    Â  Â  Â  Â  if self.vel_y != 0:
    Â  Â  Â  Â  Â  Â  step_y = 1 if self.vel_y > 0 else -1
    Â  Â  Â  Â  Â  Â  for _ in range(abs(int(self.vel_y))):
    Â  Â  Â  Â  Â  Â  Â  Â  self.rect.y += step_y
    Â  Â  Â  Â  Â  Â  Â  Â  collided = False
    Â  Â  Â  Â  Â  Â  Â  Â  for mask, offset in tile_masks:
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dx = offset[0] - self.rect.x
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dy = offset[1] - self.rect.y
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self.mask.overlap(mask, (dx, dy)):
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collided = True
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
    Â  Â  Â  Â  Â  Â  Â  Â  if collided:
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.rect.y -= step_y
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if step_y > 0:
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.jump_count = 0
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.coyote_timer = self.coyote_time_max
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.vel_y = 0
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
    
    Â  Â  Â  Â  # --- Feet collision (floor detection) ---
    Â  Â  Â  Â  self.feet_mask = self.get_feet_mask()
    Â  Â  Â  Â  on_floor = False
    Â  Â  Â  Â  for mask, offset in tile_masks:
    Â  Â  Â  Â  Â  Â  dx = offset[0] - self.rect.x
    Â  Â  Â  Â  Â  Â  dy = offset[1] - self.rect.y
    Â  Â  Â  Â  Â  Â  if self.feet_mask.overlap(mask, (dx, dy)):
    Â  Â  Â  Â  Â  Â  Â  Â  on_floor = True
    Â  Â  Â  Â  Â  Â  Â  Â  break
    Â  Â  Â  Â  if on_floor:
    Â  Â  Â  Â  Â  Â  self.jump_count = 0
    Â  Â  Â  Â  Â  Â  self.coyote_timer = self.coyote_time_max
    
    Â  Â  Â  Â  # --- Head collision ---
    Â  Â  Â  Â  self.head_mask = self.get_head_mask()
    Â  Â  Â  Â  for mask, offset in tile_masks:
    Â  Â  Â  Â  Â  Â  dx = offset[0] - self.rect.x
    Â  Â  Â  Â  Â  Â  dy = offset[1] - self.rect.y
    Â  Â  Â  Â  Â  Â  if self.head_mask.overlap(mask, (dx, dy)):
    Â  Â  Â  Â  Â  Â  Â  Â  self.rect.y += 1 Â # push down to prevent sticking
    Â  Â  Â  Â  Â  Â  Â  Â  self.vel_y = 0
    Â  Â  Â  Â  Â  Â  Â  Â  break
    
    Â  Â  Â  Â  # Floor boundary
    Â  Â  Â  Â  if self.rect.bottom >= 600:
    Â  Â  Â  Â  Â  Â  self.rect.bottom = 600
    Â  Â  Â  Â  Â  Â  self.vel_y = 0
    Â  Â  Â  Â  Â  Â  self.jump_count = 0
    Â  Â  Â  Â  Â  Â  self.coyote_timer = self.coyote_time_max
    
    # Platform setup
    platform_offset = (200, 500)
    platform_tiles, platform_masks, platform_img = generate_tilemap(TILEMAP_IMAGE, *platform_offset)
    
    # Spawn player
    player = Player()
    
    # Main loop
    running = True
    while running:
    Â  Â  clock.tick(60)
    Â  Â  for event in pygame.event.get():
    Â  Â  Â  Â  if event.type == pygame.QUIT:
    Â  Â  Â  Â  Â  Â  running = False
    
    Â  Â  player.update(platform_tiles, platform_masks)
    
    Â  Â  screen.fill((135, 206, 235)) Â # sky
    Â  Â  screen.blit(platform_img, platform_offset)
    
    Â  Â  # Optional debug: draw tile rects
    Â  Â  # for tile in platform_tiles:
    Â  Â  # Â  Â  pygame.draw.rect(screen, (0,0,0), tile,1)
    
    Â  Â  screen.blit(player.image, player.rect)
    
    Â  Â  pygame.display.flip()
    
    pygame.quit()
    
    
    

I'm making a platformer game that runs just fine in vsc, but when I try to run it directly, it has an error. Here is the code:  
",Python,0,https://www.reddit.com/r/Python/comments/1nenw34/why_does_my_program_only_work_in_vsc/,r_1nenw34,,,
r_1neno5h,reddit,theReasonablePotato,2025-09-11T23:17:44+00:00,"What is the quickest and easiest way to fix indentation errors?
Context - I've been writing Python for a good number of years and I still find indentation errors annoying. Also I'm using VScode with the Python extension.

How often do you encounter them? How are you dealing with them?  
  
Because in Javascript land (and other languages too), there are some linters that look to be taking care of that.",Python,43,https://www.reddit.com/r/Python/comments/1neno5h/what_is_the_quickest_and_easiest_way_to_fix/,r_1neno5h,,,
r_1nem1ty,reddit,SquarePraline4348,2025-09-11T22:05:50+00:00,"fp-style pattern matching implemented in python
I'm recently working on a functional programming library in python. One thing I've really want in python was a pattern matching that is expression and works well with other fp stuff in python. I went through similar fp libs in python such as `toolz` but didn't yet found a handy pattern matching solution in python. Therefore, I implement this simple pattern matching that works with most of objects (through itemgetter and attrgetter), iterables (just iter through), and literals (just comparison) in python.

- target audience

There's [link](https://github.com/BrandenXia/fp-cate) to the github repo. Note that it's still in very early development and also just a personal toy project, so it's not meant to be used in production at all.

There's some example I wrote using this library. I'd like to get some advice and suggestions about possible features and improvements I make for this functionality :)

```py
from dataclasses import dataclass

from fp_cate import pipe, match, case, matchV, _any, _rest, default


# works with any iterables
a = ""test""
print(
    matchV(a)(
        case(""tes"") >> (lambda x: ""one""),
        case([""a"", _rest]) >> (lambda x, xs: f""list starts with a, rest is {xs}""),
        default >> ""good"",
    )
)
a = [""a"", 1, 2, 3]
pipe(
    a,
    match(
        case([1, 2]) >> (lambda x: ""one""),
        case([""a"", _rest]) >> (lambda x, xs: f""list starts with a, rest is {xs}""),
    ),
    print,
)

# works with dicts
pipe(
    {""test"": 1, ""other"": 2},
    match(
        case({""test"": _any}) >> (lambda x: f""test is {x}""),
        case({""other"": 2}) >> (lambda x: ""other two""),
    ),
    print,
)


@dataclass
class Test:
    a: int
    b: bool


# works with dataclasses as well
pipe(
    Test(1, True),
    match(
        case({""a"": 1}) >> ""this is a good match"",
        case({""b"": False}) >> ""this won't match"",
        default >> ""all other matches failed"",
    ),
    print,
)
```",Python,20,https://www.reddit.com/r/Python/comments/1nem1ty/fpstyle_pattern_matching_implemented_in_python/,r_1nem1ty,,,
r_1neiod8,reddit,SxxVe,2025-09-11T19:50:28+00:00,"Kryypto: New Release
Another release for **Kryypto** is out which  offers new features, bug fixes and more!

# âœ¨ Features

* Lightweight â€“ minimal overhead
* Full Keyboard Support â€“ no need for the mouse, every feature is accessible via hotkeys
* Discord presence
* Live MarkDown Preview
* Session Restore
* Custom Styling
   * `config\configuration.cfg` for editor settings
   * CSS for theme and style customization
* Editing Tools
   * Find text in file
   * Jump to line
   * Adjustable cursor (color & width)
   * Configurable animations (types & duration)
* Git & GitHub Integration
   * View total commits
   * See last commit message & date
   * Track file changes directly inside the editor
* Productivity Features
   * Autocompleter
   * Builtin Terminal
   * Docstring panel (hover to see function/class docstring)
   * Tab-based file switching
   * Bookmarking lines
   * Custom title bar
* Syntax Highlighting for
   * Python
   * CSS
   * JSON
   * Config files
   * Markdown



# Target Audience

* Developers who prefer keyboard-driven workflows (no mouse required)
* Users looking for a lightweight alternative to heavier IDEs
* People who want to customize their editor with CSS and configuration settings
* Anyone experimenting with Python-based editors or open-source text editing tools

# Comparison:

* Lightweight â€“ minimal overhead, focused on speed
* Highly customizable â€“ styling via CSS and config files
* Keyboard-centric â€“ designed to be fully usable without a mouse



Itâ€™s not meant to replace full IDEs (yet), but aims to be a **fast, customizable, Python-powered text editor**.

Please give it a try, comment your feedback, what features to add and support [Kryypto](https://github.com/NaturalCapsule/Kryypto) by giving it a star :).",Python,0,https://www.reddit.com/r/Python/comments/1neiod8/kryypto_new_release/,r_1neiod8,,,
r_1nefnct,reddit,Fun-Improvement424,2025-09-11T17:54:15+00:00,"Early Trial: Using uv for Env Management in Clustered ML Training (Need Advice)
Hi everyone,

Iâ€™ve been tasked with improving the dev efficiency of an ML engineering team at a large tech company. Their daily work is mostly data processing and RL training on 200B+ models. Most jobs finish in 2â€“3 days, but there are also tons of tiny runs just to validate training algorithms. 

tl;dr: The challenge: the research environments are wildly diverse.

Right now the team builds on top of infra-provided Docker images. These images grow huge after being built on top again and again (40â€“80GB, optimization didn't help much, and the images are just the environment), take 40â€“60 minutes to spin up, and nobody wants to risk breaking them by rebuilding from scratch with updated libraries. At the same time, the ML post-training teamâ€”and especially the infra/AI folksâ€”are eager to try the latest frameworks (Megatron, Transformer Engine, Apex, vLLM, SGLang, FlashAttention, etc.). They even want a unified docker image that builds nightly.

Theyâ€™ve tried conda on a shared CephFS, but the experience has been rough:

* Many core libraries mentioned above canâ€™t be installed via conda. They have to go through pip.
* Installation order and env var patching is fragileâ€”C++ build errors everywhere.
* Shared envs get polluted (interns or new hires installing packages directly).
* We donâ€™t have enterprise Anaconda to centrally manage this.

To solve these problems, we recently started experimenting withÂ **uv**Â and noticed some promising signs:

1. **Config-based envs.**Â A singleÂ pyproject.tomlÂ + uvâ€™s config lets us describe CUDA, custom repos, and build dependencies cleanly. We thought only conda could handle this, but it turns out uv meets our needs, and in a cleaner way.
2. **Fast, cache-based installs.**Â The append-only, thread-safe cache means 350+ packages install in under 10 seconds. Docker images shrank from 80GB+ to <8GB. You can make changes to project environment, or ""uv run --with ..."" as you wish, and never worry about polluting a shared environment.
3. **Integration with Ray.**Â Since most RL frameworks already use Ray,Â uvÂ fits nicely: Ray's runtime env agent guarantees that tasks and subtasks can share their envs, no matter which node they are scheduled to, enabling multiple distributed jobs with distinct envs on the same cluster. Scaling these tasks from laptop to a cluster is extremely simple. 
4. **Stability issues.** There were a few times we noticed a bug that when some Ray worker failed to register within time limits, and will be stuck in env preparing even when restarted -- but we quickly learned that doing a ""uv cache prune"" will solve it without clearing the cache. There were also times when nodes went down and re-connected, and Raylet says ""failed to delete environment"", but after a timeout period it will correct itself.

That saidâ€”**this is still an early trial, not a success story.**Â We donâ€™t yet know the long-term stability, cache management pitfalls, or best practices for multi-user clusters.

ðŸ‘‰ Has anyone else triedÂ uvÂ in a cluster or ML training context? Any advice, warnings, or alternative approaches would be greatly appreciated.",Python,4,https://www.reddit.com/r/Python/comments/1nefnct/early_trial_using_uv_for_env_management_in/,r_1nefnct,,,
r_1neet2h,reddit,Impressive-Glass-523,2025-09-11T17:22:31+00:00,"Dynamic Agent-Generated UI via NiceGUI (w/o tooling)
# What My Project Does

I recently created an [agex-ui](https://github.com/ashenfad/agex-ui) repo to demonstrate a new-ish agentic framework in action. There are two demonstration apps, but in both an agent that lives in-process with the NiceGUI process creates the web interface dynamically based on user interactions.

In the ""chat"" demo app shows a traditional looking agent chat interface. But the agent uses NiceGUI components to create all its responses. So can compose NiceGUI components into custom forms as to get structured data from the users. Or it can compose components into small reports, all within its ""response bubble"".

In the ""lorem ipsum"" demo app, the only user input is the url request path. The agent uses the path as a hint for what sort of page it should create and does so to fulfill each ""GET"". So as ask for ""http://127.0.0.1:8080/weather/albany/or"" and you'll see a page of some not-so-accurate weather predictions. Or ""http://127.0.0.1:8080/nba/blazers/roster/2029"" to find out who will be on your favorite basketball team.

The showcase is fundamentally trying to show how the [agex](https://github.com/ashenfad/agex) framework makes it easier to tie into existing Python codebases with less friction from tool abstractions in-between.

* Github for demo apps: [https://github.com/ashenfad/agex-ui](https://github.com/ashenfad/agex-ui)
* A [video of a chat](https://youtu.be/-LaY_QBfkf8?si=08Vh4Z5fMR1uN_Po) with dynamic forms & plots (after analysis)
* A longer-form [blog post](https://ashenfad.github.io/agex/blog/2025/09/11/deep-dive-building-an-agent-driven-ui-with-agex-ui/)

# Target Audience

The \`agex-ui\` project is most certainly a toy / demonstration. The supporting \`agex\` framework is somewhere in between toy and production-ready. Hopefully drifting toward the latter!

# Comparison

For \`agex-ui\`, perhaps the most similar is Microsoft's [Lida](https://microsoft.github.io/lida/)?  I did a bit of reading on DUG vs RUG (Dynamic-Generated UI, Restricted-Generated UI).  Most things I found looked like RUG (because of tooling abstractions).  Probably because production-quality DUG is hard (and agex-ui isn't that either).

As for the \`agex\` framework itself, Huggingface's smol-agents is its closest cousin. The main differences being agex's focus on integration with libraries rather than tools for agent capabilities, and the ability to persist the agent's compute environment.",Python,5,https://www.reddit.com/r/Python/comments/1neet2h/dynamic_agentgenerated_ui_via_nicegui_wo_tooling/,r_1neet2h,,,
r_1ne4z4b,reddit,bleuio,2025-09-11T10:14:56+00:00,"How to Build Your Own Bluetooth Scriptable Sniffer using python for Under $25
AÂ **Bluetooth sniffer**Â is a hardware or software tool that captures and monitors Bluetooth communication between devices. Think of it as a network traffic analyzer, but for Bluetooth instead of Wi-Fi or Ethernet.  
There are high-end Bluetooth sniffers on the market â€” like those fromÂ **Ellisys**Â orÂ **Teledyne LeCroy**Â â€” which are powerful but often costÂ **hundreds or thousands of dollars**.  
You can create your own scriptable BLE sniffer for under $25. the source code is available in this post, you can adjust the code and work further   
[https://www.bleuio.com/blog/how-to-build-your-own-bluetooth-scriptable-sniffer-for-under-30/](https://www.bleuio.com/blog/how-to-build-your-own-bluetooth-scriptable-sniffer-for-under-30/)",Python,16,https://www.reddit.com/r/Python/comments/1ne4z4b/how_to_build_your_own_bluetooth_scriptable/,r_1ne4z4b,,,
r_1ne4t1d,reddit,bbourbonut,2025-09-11T10:04:24+00:00,"detroit: Python implementation of d3js
Hi, I am the maintainer of [detroit](https://github.com/bourbonut/detroit). `detroit` is a Python implementation of the library [d3js](https://d3js.org/). I started this project because I like how flexible data visualization is with `d3js`, and because I'm not a big fan of JavaScript.

You can find the documentation for `detroit` [here](https://detroit.readthedocs.io/en/latest/).

* Target Audience

`detroit` allows you to create **static** data visualizations. I'm currently working on [detroit-live](https://github.com/bourbonut/detroit-live) for those who also want **interactivity**. In addition, `detroit` requires only [lxml](https://lxml.de/) as dependency, which makes it lightweight.

You can find a gallery of examples in the [documentation](https://detroit.readthedocs.io/en/latest/#gallery). Most of examples are directly inspired by [d3js examples on observablehq](https://observablehq.com/@d3/gallery).

* Comparison

The API is almost the same:

    // d3js
    const scale = d3.scaleLinear().domain([0, 10]).range([0, 920]);
    console.log(scale.domain()) // [0, 10]
    
    # detroit
    scale = d3.scale_linear().set_domain([0, 10]).set_range([0, 920])
    print(scale.get_domain()) # [0, 10]

The difference between `d3js`/`detroit` and `matplotlib`/`plotly`/`seaborn` is the approach to data visualization. With `matplotlib`, `plotly`, or `seaborn`, you only need to write a few lines and that's it - you get your visualization. However, if you want to customize some parts, you'll have to add a couple more lines, and it can become really hard to get exactly what you want. In contrast, with `d3js`/`detroit`, you know exactly what you are going to visualize, but it may require writing a few more lines of code.",Python,66,https://www.reddit.com/r/Python/comments/1ne4t1d/detroit_python_implementation_of_d3js/,r_1ne4t1d,,,
r_1ne2g15,reddit,Fuzzy-Translator-414,2025-09-11T07:28:23+00:00,"Python VS Power BI
Why use python (streamlit =(easy but limited), dash=(complex)) for data visualization when there is power bi and tableau ?",Python,0,https://www.reddit.com/r/Python/comments/1ne2g15/python_vs_power_bi/,r_1ne2g15,,,
r_1ndz093,reddit,zskniazi,2025-09-11T03:58:43+00:00,"Python code that can remove ""*-#"" from your word document in the blink of eye.
    from docx import Document
    import re
    
    def remove_chars_from_docx(file_path, chars_to_remove):
        doc = Document(file_path)
    
       
        pattern = f""[{re.escape(chars_to_remove)}]""
        def clean_text(text):
            return re.sub(pattern, """", text)
    
        
        for para in doc.paragraphs:
            if para.text:
                para.text = clean_text(para.text)
    
       
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text:
                        cell.text = clean_text(cell.text)
    
        doc.save(file_path)
    
    
    
    remove_chars_from_docx(""mycode.docx"", ""*-#"")
    print(""Characters removed successfully."")
    ",Python,0,https://www.reddit.com/r/Python/comments/1ndz093/python_code_that_can_remove_from_your_word/,r_1ndz093,,,
r_1ndy9gv,reddit,joshemaggie,2025-09-11T03:18:57+00:00,"From Code to Python: Gentle Guide for Programmers & Learners
This series teaches [Python from code](https://www.bestdesign2hub.com/from-code-to-python-gentle-guide-programmers-learners/) without assuming youâ€™re a total beginner to programming. If youâ€™ve written code in languages like C/C++, Java, JavaScript/TypeScript, Go, or Ruby, youâ€™ll find sideâ€‘byâ€‘side explanations that map familiar concepts to Pythonâ€™s syntax and idioms.",Python,4,https://www.reddit.com/r/Python/comments/1ndy9gv/from_code_to_python_gentle_guide_for_programmers/,r_1ndy9gv,,,
r_1ndua5j,reddit,AutoModerator,2025-09-11T00:00:32+00:00,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!
# Weekly Thread: Professional Use, Jobs, and Education ðŸ¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! ðŸŒŸ",Python,4,https://www.reddit.com/r/Python/comments/1ndua5j/thursday_daily_thread_python_careers_courses_and/,r_1ndua5j,,,
r_1ndsuud,reddit,jpkg1,2025-09-10T22:55:12+00:00,"Streamlit for python apps
iâ€™ve been using streamlit lately and honestly itâ€™s pretty nice, so just wanted to share in case it helps someone.

if youâ€™re into data analysis or working on python projects and want to turn them into something interactive, streamlit is definitely worth checking out. it lets you build web apps super easily â€” like you just write python code and it handles all the front-end stuff for you.

you can add charts, sliders, forms, even upload files, and it all works without needing to learn html or javascript. really useful if you want to share your work with others or just make a personal dashboard or tool.

feels like a good starting point if youâ€™ve been thinking about making web apps but didnâ€™t know where to start.",Python,61,https://www.reddit.com/r/Python/comments/1ndsuud/streamlit_for_python_apps/,r_1ndsuud,,,
r_1nds31l,reddit,Steve91973,2025-09-10T22:21:16+00:00,"[ANNOUNCEMENT] pychub: A new way to ship your Python wheels + deps + extras
Hey fellow deveopers!  
  
I built a packaging tool called [**pychub**](https://github.com/Steve973/pychub) that might fill a weird little gap you didnâ€™t know you had. It came out of me needing a clean way to distribute Python wheels *with* all of their dependencies and optional extras, but *without* having to freeze them into platform-specific binaries like PyInstaller does. And if you want to just install everything into your own current environment?  That's what I wanted, too.

# So what is it?

**pychub** takes your wheel, resolves and downloads its dependencies, and wraps everything into a single executable `.chub` file. That file can then be shipped/copied anywhere, and then run directly like this:

    python yourtool.chub

It installs into the current environment (or a venv, or a conda env, your call), and can even run an entrypoint function or console script *right after* install.

No network calls. No pip. No virtualenv setup. Just `python tool.chub` and go.

# Why I built it:

Most of the Python packaging tools out there either:

* Freeze the whole thing into a binary (PyInstaller, PyOxidizer) â€” which is great, until you hit platform issues or need to debug something. Or you just want to do something different than that.
* Just stop at building a wheel and leave it up to you (or your users) to figure out installation, dependencies, and environment prep.

I wanted something in between: still using the host Python interpreter (so it stays light and portable), but with everything pre-downloaded and reproducible.

# What it can bundle:

* Your main wheel
* Any number of additional wheels
* All their dependencies (downloaded and stored locally)
* Optional include files (configs, docs, whatever)
* Pre-install and post-install scripts (shell, Python, etc.)

And itâ€™s 100% reproducible, so that the archive installs the exact same versions every time, no network access needed.

# Build tool integration:

If you're using **Poetry**, **Hatch**, or **PDM**, Iâ€™ve released plugins for all three:

* Just add the plugin to your `pyproject.toml`
* Specify your build details (main wheel, includes, scripts, etc.)
* Run your normal build command and youâ€™ll get a `.chub` alongside your `.whl`

Itâ€™s one of the easiest ways to ship Python tools that *just work,* whether you're distributing internally, packaging for air-gapped environments, or dropping into Docker builder stages.

Plugins repo:  [https://github.com/Steve973/pychub-build-plugins](https://github.com/Steve973/pychub-build-plugins)

# Why not just use some other bundling/packaging tool?

Well, depending on your needs, maybe you should! I donâ€™t think pychub replaces everything. It just solves a different problem.

If you want sealed apps with bundled runtimes, use PEX or PyOxidizer.  
If you're distributing scripts, zipapp is great.  
But if you want a **wheel-based**, network-free, single-file installer that works on any Python 3.9+ environment, then pychub might be the right tool.

Full comparison table along with everything else:  
ðŸ“˜ [README on GitHub](https://github.com/Steve973/pychub#why-not-just-use-insert-favorite-tool-name-here)

Thatâ€™s it. I built it because I needed it to include plugins for a platform that I am building. If it helps you too, even better.  I will be actively supporting this, and if you would like to take it for a spin and see if you like it, I'd be honored to hear your feedback. If you want a feature added, etc, please let me know.  
Issues, suggestions, and PRs are all welcome.

Thanks for your time and interest!

Steve",Python,13,https://www.reddit.com/r/Python/comments/1nds31l/announcement_pychub_a_new_way_to_ship_your_python/,r_1nds31l,,,
r_1ndpzmo,reddit,johnyeldry,2025-09-10T20:53:14+00:00,"I am going to suggest two ideas for python, what are your thoughts?
a new builtin function used with with that enforces type safety if type hints are present: [https://docs.google.com/document/d/1fBKrDTWUhVFrirD57Rv4i7KENE7kqXojnmq41sGJ9ug/edit?usp=sharing](https://docs.google.com/document/d/1fBKrDTWUhVFrirD57Rv4i7KENE7kqXojnmq41sGJ9ug/edit?usp=sharing)

  
a new system for defining custom operators: [https://docs.google.com/document/d/1oi5MBuZGh3JAxtCjyamiyyg76T6ficaSf6FZ\_d7RWCo/edit?usp=sharing](https://docs.google.com/document/d/1oi5MBuZGh3JAxtCjyamiyyg76T6ficaSf6FZ_d7RWCo/edit?usp=sharing)",Python,0,https://www.reddit.com/r/Python/comments/1ndpzmo/i_am_going_to_suggest_two_ideas_for_python_what/,r_1ndpzmo,,,
r_1ndo680,reddit,cyberOG01,2025-09-10T19:40:31+00:00,"""I wanted to learn Scripting In python"" any one want to join !!
Hi, writers if you are also looking to start programing in python for cyber security, lets do it together.   
my domain is cyber security and now day scripting and automation is highly required, so lets sync up and decide how we should plan and start.  
",Python,0,https://www.reddit.com/r/Python/comments/1ndo680/i_wanted_to_learn_scripting_in_python_any_one/,r_1ndo680,,,
r_1ndnusy,reddit,AlSweigart,2025-09-10T19:27:58+00:00,"A Complete List of Python Tkinter Colors, Valid and Tested
I needed a complete list of valid color names for Python's Tkinter package as part of my [ButtonPad](https://pypi.org/project/ButtonPad/) GUI framework development. The lists I found on the internet were either incomplete, buried under ads, and often just plain wrong. Here's a list of all 760 color names (valid and personally tested) for Python Tkinter.

https://inventwithpython.com/blog/complete-list-tkinter-colors-valid-and-tested.html",Python,31,https://www.reddit.com/r/Python/comments/1ndnusy/a_complete_list_of_python_tkinter_colors_valid/,r_1ndnusy,,,
r_1ndns22,reddit,ThatCreepyMf,2025-09-10T19:25:02+00:00,"tips for a 15 y/o starting ML
so i got into coding last year and was learning react js and generally front end stuff but seeing how fast AI is progressing, with AGI soon, iâ€™ve deciding to dedicate my time to python, machine learning and in some time deep learning. I am 15 years old and really good at math for my age. iâ€™ve already learned the basic and some more advanced python concepts. What should i push to learn? any general tips and advice?",Python,0,https://www.reddit.com/r/Python/comments/1ndns22/tips_for_a_15_yo_starting_ml/,r_1ndns22,,,
r_1ndm9zl,reddit,styrofoamshotgun,2025-09-10T18:28:31+00:00,"Update: Python-based MTG Commander Deck Builder â€” Now With Combos, Bracket Enforcement, and Include/
Hi r/Python, I wanted to share another update on my Python-based project: a **Magic: The Gathering Commander deck builder**. My first post here was when I had a mostly command-line tool; then I moved to a basic web interface. Since then Iâ€™ve added quite a few new features, cleaned up the backend, and expanded both the web and CLI sides.

# What My Project Does

* Pick a commander and up to three themes (e.g., Aristocrats, +1/+1, Kindred, Aggro).
* The builder generates a complete 100-card list with stage-by-stage reasoning.
* Handles multi-copy strategies (Petitioners, Dragonâ€™s Approach, Shadowborn Apostle) with packages that keep the deck at 100 and adjust land counts automatically.
* Lets you lock favorite cards, reroll just creatures/spells/lands, or swap cards for alternatives.
* Supports â€œowned-onlyâ€ and â€œprefer ownedâ€ builds by uploading TXT/CSV lists of your collection.
* Exports to TXT (Moxfield/Archidekt), CSV with tags/Owned info, or a simple printout.

# Target Audience

* **Magic: The Gathering players** who like to theorycraft and spin up decks quickly.
* People who want to give a few high-level instructions (commander, themes, composition) and get a playable decklist back.
* Developers or hobbyists interested in Python projects that mix data handling, web UI, and CLI tooling.

# Comparison

I built this because I wasnâ€™t finding much in the way of Python-based, â€œhands-offâ€ deck builders. Tools like EDHRec, Moxfield, and Archidekt are great, but they generally need a lot of manual input. My approach is closer to: â€œgive me a commander and some themes, generate a deck, and let me iterate fast.â€ It also lets me compare multiple builds for the same commander or themes to see how choices shift.

# Whatâ€™s New

* **Combos & Synergies:** detects curated two-card combos, surfaces them in the web UI with badges, and honors color identity.
* **Bracket Compliance:** validates decks against configurable bracket rules (like tutors/extra turns); includes inline enforcement and optional auto-fixing.
* **Include/Exclude Lists:** add must-have or must-exclude cards via text/file input; supports fuzzy matching, EDH color checks, and JSON import/export.
* **Web UI Polish:** improved New Deck modal, integrated multi-copy suggestions, cleaner alternatives panel, and mobile-friendly layouts.
* **CLI Parity:** theme selection by name, deck composition flags (`--land-count`, `--wipe-count`, etc.), and full include/exclude support with detailed console summaries.
* **Performance & Stability:** exclude filtering benchmarked under 50ms on 20k+ cards; Docker image seeds defaults automatically; fixes for land counts, exports mismatches, and mobile scaling quirks.

# Tech Stack

* **Backend:** Python 3.x with structured logging, modular orchestration, and test suite for validation and backward compatibility.
* **Web:** Flask + Jinja templates, partial caching, validation endpoints, and Playwright end-to-end tests.
* **CLI:** argparse interface with type indicators, grouped help, and full parity with web features.
* **Deployment:** Docker with multi-arch builds (x86/ARM), sample docker-compose configs.

# Try it

* Live demo: [deck-builder.wiz-ops.com](https://deck-builder.wiz-ops.com/) (setup may take a minute).
* Docker Hub (easiest): [mwisnowski/mtg-python-deckbuilder](https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder)
* Source & releases: [GitHub repo](https://github.com/mwisnowski/mtg_python_deckbuilder)

# Roadmap

* Budget mode with price caps and recommended pickup lists.
* Smarter land base profiles tuned by curve and pip breakdown.
* Random build modes (â€œsurprise me,â€ random by theme, or full random).

This is my first real â€œfrom-scratchâ€ software project, so if you have thoughts on the Python side â€” code structure, testing, deployment â€” Iâ€™d love to hear them.

Do you want me to keep this **balanced between MTG features and technical notes**, or make it **more developer-focused** (leaning heavier on Python design decisions, logging, testing, etc.) since itâ€™s for r/Python?",Python,5,https://www.reddit.com/r/Python/comments/1ndm9zl/update_pythonbased_mtg_commander_deck_builder_now/,r_1ndm9zl,,,
r_1ndk80g,reddit,reidhoch,2025-09-10T17:13:07+00:00,"Scaling asyncio on Free-Threaded Python
https://labs.quansight.org/blog/scaling-asyncio-on-free-threaded-python

From the author: ""In this blog post, we will explore the changes I made in the upcoming Python 3.14 release to enable asyncio to scale on the free-threaded build of CPython.""",Python,20,https://www.reddit.com/r/Python/comments/1ndk80g/scaling_asyncio_on_freethreaded_python/,r_1ndk80g,,,
r_1ndj5vz,reddit,EricHermosis,2025-09-10T16:34:32+00:00,"I decoupled FastAPI dependency injection system in pure python, no dependencies.
**What My Project Does**

When building FastAPI endpoints, I found the dependency injection system such a pleasure to use that I wanted it everywhere, not just in my endpoints. I explored a few libraries that promised similar functionality, but each had drawbacks, some required Pydantic, others bundled in features beyond dependency injection, and many were riddled with bugs.

That's way I created [PyDepends](https://github.com/entropy-flux/PyDepends), a lightweight dependency injection system that I now use in my own projects and would like to share with you.

**Target Audience**  
This is mainly aimed at:

* FastAPI developers who want to use dependency injection in the service layer.

* Domain-Driven Design practitioners who want to decouple their services from infrastructure.

* Python developers who arenâ€™t building API endpoints but would still like to use dependency injection in their projects.
Itâ€™s not production-grade yet, but itâ€™s stable enough for everyday use and easy to extend.

**Comparison**  

Compared to other similar packages, it does just that, inject dependencies, is not bloated with other functionalities. 

* FastDepends: It also cannot be used with non-serializable classes, and I wanted to inject machine learning models into services. On top of that, it does unpredictable things beyond dependency injection.

Repo: [https://github.com/entropy-flux/PyDepends](https://github.com/entropy-flux/PyDepends)

Hope you find it useful!

EDIT: Sorry to Lancetnik12 I think he did a great job with fastdepends and faststream, I was a to rude with his job, the reality is fastdepends just have other use cases, I don't really like to compare my job with other but it is a requirement to publish here. ",Python,127,https://www.reddit.com/r/Python/comments/1ndj5vz/i_decoupled_fastapi_dependency_injection_system/,r_1ndj5vz,,,
r_1ndhycj,reddit,Goldziher,2025-09-10T15:51:12+00:00,"AI-Rulez v2.0: Universal AI Assistant Configuration Management
I'm happy to showcase AI-Rulez v2, which is a major next step in the development of this tool. 

**The Problem:** If you're using multiple AI coding assistants (Claude Code, Cursor, Windsurf, GitHub Copilot), you've probably noticed the configuration fragmentation. Each tool demands its own format - `CLAUDE.md`, `.cursorrules`, `.windsurfrules`, `.github/copilot-instructions.md`. Keeping coding standards consistent across all these tools is frustrating and error-prone.

**The Solution:** AI-Rulez lets you write your project configuration once and automatically generates native files for every AI tool - current and future ones. It's like having a build system for AI context.

## Why This Matters for Development Teams

Teams using AI assistants face common challenges:
- **Multiple tools, multiple configs**: Your team uses Claude Code for reviews, Cursor for development, Copilot for completions
- **Framework-specific standards**: Type safety, testing patterns, dependency management (uv, poetry, npm, etc.)  
- **Monorepo complexity**: Multiple services and packages all need different AI contexts
- **Team consistency**: Junior devs get different AI guidance than seniors

AI-Rulez solves this with a single `ai-rulez.yaml` that understands your project's conventions.

## Key Features

### AI-Powered Project Analysis
The `init` command is where AI-Rulez shines. Instead of manually writing configurations, let AI analyze your codebase:

```bash
# AI analyzes your codebase and generates tailored config
uvx ai-rulez init ""My Project"" --preset popular --use-agent claude --yes
```

This automatically:
- Detects your tech stack (Python/Node/Go, testing frameworks, linters)
- Identifies project patterns and conventions
- Generates appropriate coding standards and practices
- Creates specialized agents for different tasks (code review, testing, docs)
- **Automatically adds all generated AI files to .gitignore** - no more committing `.cursorrules` or `CLAUDE.md` by accident

### Universal Output Generation
One YAML config generates files for every tool:

```yaml
# ai-rulez.yaml
metadata:
  name: ""Python API Service""

presets:
  - ""popular""  # Auto-configures Claude, Cursor, Windsurf, Copilot

rules:
  - name: ""Python Type Safety""
    priority: critical
    content: |
      - Python 3.11+ with complete type annotations
      - Use | for unions: str | None not Optional[str]
      - mypy strict mode required
      - Type all function signatures and returns

  - name: ""Testing Standards""
    priority: high
    content: |
      - pytest with async support and fixtures
      - 100% coverage for new code
      - Use factory_boy for test data
      - Integration tests with real PostgreSQL

agents:
  - name: ""python-reviewer""
    description: ""Python code review specialist""
    system_prompt: ""Focus on type safety, performance, and Pythonic patterns""
```

Run `uvx ai-rulez generate` and get:
- `CLAUDE.md` for Claude Code
- `.cursorrules` for Cursor
- `.windsurfrules` for Windsurf  
- `.github/copilot-instructions.md` for GitHub Copilot
- Custom formats for any future AI tool

### Advanced Features

**MCP Server Integration**: Direct integration with Claude Code and other MCP-compatible tools:
```bash
# Start built-in MCP server with 19 configuration management tools
uvx ai-rulez mcp
```

**Comprehensive CLI**: Manage configs without editing YAML:
```bash
# Add Python-specific rules on the fly
uvx ai-rulez add rule ""FastAPI Standards"" --priority high --content ""Use Pydantic v2 models with Field validation""

# Create specialized agents
uvx ai-rulez add agent ""pytest-expert"" --description ""Testing specialist for Python projects""
```

**Team Collaboration**: 
- Remote config includes: `includes: [""https://github.com/myorg/python-standards.yaml""]`
- Local overrides: Personal customization via `.local.yaml` files
- Monorepo support: `--recursive` flag handles complex Python projects

### Enterprise Features

**Security & Compliance**:
- SSRF protection for remote config includes
- Schema validation prevents configuration errors
- Audit trails for configuration changes

**Performance**:
- Written in Go - instant startup even for large Python monorepos
- Concurrent generation for multiple output files
- Smart caching for remote configurations

## Target Audience

- **Python developers** using multiple AI coding assistants
- **Python teams** needing consistent AI behavior across projects  
- **DevOps engineers** managing AI configurations in CI/CD pipelines
- **Open source maintainers** wanting AI-ready Python project documentation
- **Enterprise teams** requiring centralized AI assistant management

## Comparison to Alternatives

### vs Manual Configuration Management
**Manual approach**: Maintain separate `.cursorrules`, `CLAUDE.md`, `.windsurfrules` files
- Problem: Configuration drift, inconsistent standards, manual syncing
- **AI-Rulez solution**: Single source generates all formats automatically

### vs Basic Tools (airules, template-ai)
**Basic tools**: Simple file copying or template systems
- **AI-Rulez advantages**: 
  - AI-powered codebase analysis and config generation
  - MCP protocol integration for live configuration management
  - Full CRUD CLI for configuration management
  - Enterprise security features and team collaboration

### vs Tool-Specific Solutions
**Tool-specific**: Each AI assistant has its own configuration system
- **AI-Rulez advantages**:
  - Future-proof: works with new AI tools without reconfiguration
  - Repository-level management for complex Python projects
  - Consistent behavior across your entire AI toolchain

## Installation & Usage

```bash
# Install via pip
pip install ai-rulez

# Or run without installing
uvx ai-rulez init ""My Python Project"" --preset popular --yes

# Generate configuration files
ai-rulez generate

# Add to your pre-commit hooks
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/Goldziher/ai-rulez
    rev: v2.1.3
    hooks:
      - id: ai-rulez-validate
      - id: ai-rulez-generate
```

## Real-World Example

Here's how a Django + React monorepo benefits from AI-Rulez:

```yaml
# ai-rulez.yaml
extends: ""https://github.com/myorg/python-base.yaml""

sections:
  - name: ""Architecture""
    content: |
      - Django REST API backend with PostgreSQL
      - React TypeScript frontend
      - Celery for async tasks
      - Docker containerization

agents:
  - name: ""django-expert""
    system_prompt: ""Django specialist focusing on DRF, ORM optimization, and security""
  
  - name: ""frontend-reviewer""  
    system_prompt: ""React/TypeScript expert for component architecture and testing""

mcp_servers:
  - name: ""database-tools""
    command: ""uvx""
    args: [""mcp-server-postgres""]
    env:
      DATABASE_URL: ""postgresql://localhost/myproject""
```

This generates tailored configurations for each AI tool, ensuring consistent guidance whether you're working on Django models or React components.

## Documentation & Resources

- **Full Documentation**: [https://goldziher.github.io/ai-rulez/](https://goldziher.github.io/ai-rulez/)
- **GitHub Repository**: [https://github.com/Goldziher/ai-rulez](https://github.com/Goldziher/ai-rulez)
- **Quick Start Guide**: [https://goldziher.github.io/ai-rulez/quick-start/](https://goldziher.github.io/ai-rulez/quick-start/)

---

AI-Rulez has evolved significantly since v1.0, adding AI-powered initialization, comprehensive MCP integration, and enterprise-grade features. It's being used by teams managing large Python codebases who need consistent AI assistant behavior across their entire development workflow.

I've personally seen this solve major headaches in production Python projects where different team members were getting inconsistent AI guidance. The `init` command with AI analysis is particularly powerful for getting started quickly.

**If this sounds useful for your Python projects, please check out the [GitHub repository](https://github.com/Goldziher/ai-rulez) and consider giving it a star - it helps with visibility and keeps development motivation high!**

Would love to hear about your use cases and any feedback from the Python community.",Python,0,https://www.reddit.com/r/Python/comments/1ndhycj/airulez_v20_universal_ai_assistant_configuration/,r_1ndhycj,,,
r_1ndgpwv,reddit,osm_22,2025-09-10T15:05:50+00:00,"Curious about moving from Mechanical Engineering to Data Science
 Hey everyone,

Iâ€™m wrapping up my final year in **Mechanical Engineering**, and lately Iâ€™ve been fascinated by how data is shaping decisions in engineering, manufacturing, and beyond. The more I read about **data analysis, machine learning, and predictive modeling**, the more I feel drawn to explore this path.

My background is heavy on problem-solving, math, and physics, and Iâ€™ve done some basic coding in Python and MATLAB for academic projects. Iâ€™m now experimenting with SQL and data visualization tools, and Iâ€™m considering building small projects that combine engineering concepts with data insights.

Iâ€™d love to hear from people whoâ€™ve made a similar shift:

* What was the most valuable skill or habit you developed early on?
* Did you start in a data-related role within your original industry, or switch fields entirely?
* Any project ideas that helped you stand out when you were starting out?

Thanks in advance for sharing your experiences!",Python,8,https://www.reddit.com/r/Python/comments/1ndgpwv/curious_about_moving_from_mechanical_engineering/,r_1ndgpwv,,,
r_1nd72p7,reddit,dooditydoot,2025-09-10T06:46:41+00:00,"Wondering how many of you have successfully developed and monetized an API
Hey everyone! Iâ€™m interested and curious to know from your experiences in developing and monetizing APIs.

What niche did you choose?
What are your distribution channels?
Your top challenges?

TIA!",Python,0,https://www.reddit.com/r/Python/comments/1nd72p7/wondering_how_many_of_you_have_successfully/,r_1nd72p7,,,
r_1nd1go9,reddit,hammyhami,2025-09-10T01:41:36+00:00,"I created a pretty-printed dir function to make debugging complex classes easier
**What My Project Does**

You can check it out on github:Â [https://pypi.org/project/pretty-dir/](https://pypi.org/project/pretty-dir/)

This library generates a better **dir** output for debugging. For a quick example, check out the [with dir](https://github.com/douglassimonsen/ppdir/raw/main/example_images/before.png) and [with ppdir](https://github.com/douglassimonsen/ppdir/raw/main/example_images/after.png) outputs using a simple pydantic model.

  
**Target Audience**

This is mainly aimed at developers who are debugging code that uses any libraries that have large, complex, deeply nested classes. Libraries such as pydantic, dataclasses, and openpyxl.

**Comparison**

It exists in a similar niche as icecream and rich.inspect where it's meant to improve the debugging experience. Unlike similar libraries, this only shows the structure, not the values themselves. This is valuable in pydantic environments, where instances can be too verbose to be meaningful when printed to the console.

**Details**

The library uses the output of the **dir(obj)** function as a baseline, but improves the output in a number of ways:

* Visually groups the methods and attributes by the classes they were defined on. Therefore, if you're subclassing the [pydantic.BaseModel](https://docs.pydantic.dev/latest/api/base_model/) class, it separates the generic basemodel methods from the subclass' specific methods.
* Pulls the first line of the docstrings for the class, all methods, and all class attributes.
* Can enable showing the function signature for all class methods
* By default, hides private and and dunder methods from the outputs
* Prints the source code location of all parent classes
* Uses [colorama](https://pypi.org/project/colorama/) to color the different sections of the output

I've set it to automatically import (see **Auto-loading in PDB (Breakpoint)** on PyPI) when I use breakpoint() and it's been a nice quality of life improvement!

This is my first project I expect other people to use, so let me know if I can improve anything!",Python,36,https://www.reddit.com/r/Python/comments/1nd1go9/i_created_a_prettyprinted_dir_function_to_make/,r_1nd1go9,,,
r_1ncy8av,reddit,wbcm,2025-09-09T23:13:04+00:00,"Most Performant Python Compilers/Transpilers in 2025
Today I find myself in the unfortunate position to create a program that must compile arbitrary python code :(  For the use case I am facing now performance is everything, and luckily the target OS for the executable file will only be linux. The compiled codes will be standalone local computational tools without any frills (no guis, no i|o or r|w operations, no system access, and no backend or configuration needs to pull in). Python code is >=3.8 and can pull in external libraries (eg: numpy). However, the codes may be multithreaded/multiprocessed and any static type-like behavior is not guaranteed.

Historically I have used tools like pyinstaller, py2exe, py2app, which work robustly, but create stand alone executable files that are often pretty slow. I have been looking at a host of transpilers instead, eg: [https://github.com/dbohdan/compilers-targeting-c?tab=readme-ov-file](https://github.com/dbohdan/compilers-targeting-c?tab=readme-ov-file), and am somewhat overwhelmed by the amount of choices therein. Going through stackoverflow naturally recovered a lot of great recommendations that were go-to's 10-20 years ago, but do not have much promise for recent python versions. Currently I am considering:  
wax [https://github.com/LingDong-/wax](https://github.com/LingDong-/wax) ,  
11l-lang [https://11l-lang.org/transpiler/](https://11l-lang.org/transpiler/),  
nuitka [https://nuitka.net/](https://nuitka.net/),  
prometeo  [https://github.com/zanellia/prometeo](https://github.com/zanellia/prometeo),  
pytran [https://pythran.readthedocs.io/en/latest/](https://pythran.readthedocs.io/en/latest/),  
rpython [https://rpython.readthedocs.io/en/latest/](https://rpython.readthedocs.io/en/latest/),  
or py14  https://github.com/lukasmartinelli/py14.  
However, this is a lot to consider without rigorously testing all of them out. Does anyone on this sub have experience in modern Transpilers or other techniques for compiling numerical python codes for linux? If so, can you share any tools, techniques, or general guidance? Thank you!

Edit for clarification:  
This will be placed in a user facing application wherein users can upload their tools to be autonomously deployed in a on demand/dynamic runtime basis. Since we cannot know all the codes that users are uploading, a lot of the traditional and well defined methods are not possible. We are including C, C++, Rust, Fortran, Go, and Cobol compilers to support these languages, but seeking a similar solution for python.",Python,36,https://www.reddit.com/r/Python/comments/1ncy8av/most_performant_python_compilerstranspilers_in/,r_1ncy8av,,,
r_1ncxl3i,reddit,szymoffk,2025-09-09T22:46:09+00:00,"Method overloading: in ~30 lines of code. Simple enough?
Getting into the deeper parts of Python and thought of this simple Metaclass that allows method overloading.

    from typing import get_type_hints
    
    class OverloadingDict(dict):
    Â  Â  def __setitem__(self, key, value):
    Â  Â  Â  Â  if callable(value) and key in self:
    Â  Â  Â  Â  Â  Â  old_func = super().__getitem__(key)
    Â  Â  Â  Â  Â  Â  if not isinstance(old_func, Overloader):
    Â  Â  Â  Â  Â  Â  Â  Â  Overloader(old_func)
    Â  Â  Â  Â  Â  Â  value = Overloader(value)
    
    Â  Â  Â  Â  super().__setitem__(key, value)
    
    class AllowOverload(type):
    Â  Â  def __prepare__(*args):
    Â  Â  Â  Â  return OverloadingDict()
    
    class Overloader:
    Â  Â  registry = {}
    
    Â  Â  def __new__(cls, func):
    Â  Â  Â  Â  hint = get_type_hints(func)
    
    Â  Â  Â  Â  # Hack to get first (and only) hint...
    Â  Â  Â  Â  for hint in get_type_hints(func).values():
    Â  Â  Â  Â  Â  Â  break
    Â  Â  Â  Â  
    Â  Â  Â  Â  cls.registry[hint] = func
    Â  Â  Â  Â  return super().__new__(cls)
    Â  Â  
    Â  Â  def __call__(self, arg):
    Â  Â  Â  Â  arg_type = type(arg)
    Â  Â  Â  Â  func = self.registry[arg_type]
    Â  Â  Â  Â  return func(self, arg)
    Â  Â  Â  Â  
    
    class Dog(metaclass=AllowOverload):
    Â  Â  def bark(self, n: int):
    Â  Â  Â  Â  print(""Bark! "" * n)
    
    Â  Â  def bark(self, at: str):
    Â  Â  Â  Â  print(""Barking at "" + at)
    
    doge = Dog()
    
    doge.bark(2)
    doge.bark(""cat"")

    Output:
    Bark! Bark!
    Barking at cat

It obviously is only a proof of concept.  
I didn't have the patience for many args/kwargs matching. Overloader could also be quasi-sentinel (one instance per class) and work for many classes. But you get the idea.

I think fully working overloading metaclass could be done in 100-200 lines of code.  
Do you think method overloading metaclass should be added to stdlib?",Python,0,https://www.reddit.com/r/Python/comments/1ncxl3i/method_overloading_in_30_lines_of_code_simple/,r_1ncxl3i,,,
r_1ncnz58,reddit,Goal-based76,2025-09-09T16:42:12+00:00,"Need ideas for hackathon project, Real-time collaborative coding SaaS
Our team picked â€œReal-Time Collaborative Coding SaaSâ€ as the problem statement for an upcoming hackathon. Basically, itâ€™s like Google Docs but for coding,  multiple devs working on the same project with live debugging and version control.

I know there are already tools like VS Code Live Share and more, but since this is the given challenge, we are looking for innovative ideas to make it stand out.

Any feature suggestions, unique use cases, or crazy ideas are welcome. Thanks!",Python,0,https://www.reddit.com/r/Python/comments/1ncnz58/need_ideas_for_hackathon_project_realtime/,r_1ncnz58,,,
r_1ncn5fq,reddit,BrightSheepherder323,2025-09-09T16:11:00+00:00,"imgbatch â€“ A Python tool for batch-processing images from the command line
**What My Project Does**

[https://github.com/booo2233/imgbatch](https://github.com/booo2233/imgbatch)

 is a simple Python tool that lets you batch-process images (resize, compress, or convert formats) directly from the command line. Instead of opening heavy software, you can point it at a folder and quickly process all your images in one go.

**Target Audience**  
This is mainly aimed at:

* Developers who need quick image preprocessing for projects
* Photographers or designers who want to resize/compress many images at once
* Anyone who prefers lightweight CLI tools instead of GUIs

Itâ€™s not production-grade yet, but itâ€™s stable enough for everyday use and easy to extend.

**Comparison**  
Compared to tools like ImageMagick or Pillow scripts:

* imgbatch is **simpler** (minimal commands, no need to learn a big toolset)
* Itâ€™s **focused only on batch tasks** (not a general-purpose graphics library)
* Written in Python, so easy to tweak or add custom functions if you know a little code

ðŸ‘‰ Repo: [https://github.com/booo2233/imgbatch](https://github.com/booo2233/imgbatch)

Would love feedback, and if you find it useful, a â­ would be amazing!  
thank you guys",Python,7,https://www.reddit.com/r/Python/comments/1ncn5fq/imgbatch_a_python_tool_for_batchprocessing_images/,r_1ncn5fq,,,
r_1ncmlwv,reddit,RDE_20,2025-09-09T15:50:58+00:00,"Should I give away my app to my employer for free?
I work for a fintech company in the UK (in operations to be specific) however my daily role doesnâ€™t require any coding knowledge. I have built up some python knowledge over the past few years and have developed an app that far outperforms the workflow tool my company currently uses. I have given hints to my manager that I have some coding knowledge and given them snippets of the tool Iâ€™ve created, sheâ€™s pretty much given me free reign to stop any of my usual tasks and focus on this full time. My partner used to work for the same company in the finance department so I know they paid over Â£200k for 3 people to develop the current workflow tool (these developers had no operations experience so built something unfit for purpose). Iâ€™ve estimated if I can get my app functional it would save the company Â£20k per month (due to all the manual work we usually have to do vs what I can automate). My manager has already said this puts me in a good position for a decent bonus next year (it wouldnâ€™t be anymore than Â£10k) so Iâ€™m a little stuck on what to do and if Iâ€™m sounding greedy. 

Has anyone ever been in a similar position? 

EDIT TITLE: I know itâ€™s not â€˜for freeâ€™ as of course Iâ€™m paid to do my job. But I would be handing over hours of work that I havenâ€™t been paid for. ",Python,419,https://www.reddit.com/r/Python/comments/1ncmlwv/should_i_give_away_my_app_to_my_employer_for_free/,r_1ncmlwv,,,
r_1nckydw,reddit,yousefabuz,2025-09-09T14:48:10+00:00,"Cythonize Python Code
# Context

This is my first time messing with **Cython** (or really anything related to optimizing Python code).  
I usually just stick with yielding and avoiding keeping much in memory, so bear with me.

# Context

Iâ€™m building a Python project thatâ€™s kind of like `zipgrep` / `ugrep`.  
It streams through archive(s) file contents (nothing kept in memory) and searches for whatever pattern is passed in.

# Benchmarks

(Results vary depending on the pattern, hence the wide gap)

* âœ… **\~15â€“30x faster** than `zipgrep` (expected)
* âŒ **\~2â€“8x slower** than `ugrep` (also expected, since itâ€™s C++ and much faster)

I tried:

* `cythonize` from [`Cython.Build`](http://Cython.Build) with setuptools
* Nuitka

But the performance was basically identical in both cases. I didnâ€™t see any difference at all.  
Maybe I compiled Cython/Nuitka incorrectly, even though they both built successfully?

# Question

Is it actually worth:

* Manually writing `.c` files
* Switching the right parts over to `cdef`

Or is this just one of those cases where Pythonâ€™s overhead will always keep it behind something like `ugrep`?

Gitub Repo: [pyzipgrep](https://github.com/yousefabuz17/pyzipgrep)",Python,24,https://www.reddit.com/r/Python/comments/1nckydw/cythonize_python_code/,r_1nckydw,,,
r_1nchgtb,reddit,Dry_Structure8990,2025-09-09T12:25:04+00:00,"Absolute Cinema (or.. programming language in this case)
Had to knowledge python (thanks filters) In class, quickly got bored of it.

Get home, try to make calculator with it.

this is fucking sick.",Python,0,https://www.reddit.com/r/Python/comments/1nchgtb/absolute_cinema_or_programming_language_in_this/,r_1nchgtb,,,
r_1ncgwas,reddit,hunvreus,2025-09-09T11:58:36+00:00,"[Project] /dev/push - An open source Vercel for Python apps
**What My Project Does**

[/dev/push](https://github.com/hunvreus/devpush) is an open source deployment platform that lets you deploy Python apps with a UX similar to Vercel/Render. It handles git-based deployments, environment variables, real-time logs, custom domains...

**Target Audience**

Python developers who want an easier way to self-host and deploy apps. Itâ€™s ready for use (I run it for my own apps) but still in beta. Bug reports and feedback is welcome.

**Comparison**

Unlike Vercel or Render, /dev/push is fully open source and self-hosted. You can install and run it on your own Debian/Ubuntu server with a single command, without relying on a third-party platform. Compared to Coolify or CapRover, itâ€™s lighter and more focused on delivering a polished UX.

**How to get started**

You can install it on a any Debian/Ubuntu server with a single command:

    curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/install.sh | sudo bash

More info on installation steps: [https://devpu.sh/docs/installation/#quickstart](https://devpu.sh/docs/installation/#quickstart)

**Links**

* GitHub: [https://github.com/hunvreus/devpush](https://github.com/hunvreus/devpush)
* Docs: [https://devpu.sh/docs](https://devpu.sh/docs)
* Website: [https://devpu.sh](https://devpu.sh)",Python,6,https://www.reddit.com/r/Python/comments/1ncgwas/project_devpush_an_open_source_vercel_for_python/,r_1ncgwas,,,
r_1ncdmvb,reddit,onestardao,2025-09-09T08:48:01+00:00,"cosine=0.91 but answer is wrong. a tiny python MRE for â€œsemantic â‰  embeddingâ€ and before/after fix
What My Project Does

WFGY Problem Map 1.0 is a reasoning-layer â€œsemantic firewallâ€ for python AI pipelines. it defines 16 reproducible failure modes and gives exact fixes without changing infra. for r/Python this post focuses on No.5 semantic â‰  embedding and No.8 retrieval traceability. the point is to show a minimal numpy repro where cosine looks high but the answer is wrong, then apply the before/after firewall idea to make it stick.

---

Target Audience

python folks who ship RAG or search in production. users of faiss, chroma, qdrant, pgvector, or a homegrown numpy knn. if you have logs where neighbors look close but citations point to the wrong section, this is for you.

---

Comparison

most stacks fix errors after generation by adding rerankers or regex. the same failure returns later. the WFGY approach checks the semantic field before generation. if the state is unstable, loop or reset. only a stable state can emit output.

acceptance targets: Î”S(question, context) â‰¤ 0.45, coverage â‰¥ 0.70, Î» convergent. once these hold, that class of bug stays fixed.

---

Minimal Repro (numpy only)

```

import numpy as np
np.random.seed(0)
dim = 8

# clean anchors for two topics

A = np.array([1,0,0,0,0,0,0,0.], dtype=np.float32)
B = np.array([0,1,0,0,0,0,0,0.], dtype=np.float32)

# chunks: B cluster is tight, A is sloppy, which fools raw inner product

chunks = np.stack([
    A + 0.20*np.random.randn(dim),
    A + 0.22*np.random.randn(dim),
    B + 0.05*np.random.randn(dim),
    B + 0.05*np.random.randn(dim),
]).astype(np.float32)

def ip_search(q, X, k=2):
    scores = X @ q
    idx = np.argsort(-scores)[:k]
    return idx, scores[idx]

def l2norm(X):
    n = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
    return X / n

q = (A + 0.10*np.random.randn(dim)).astype(np.float32)  # should match topic A

# BEFORE: raw inner product, no normalization

top_raw, s_raw = ip_search(q, chunks, k=2)
print(""BEFORE idx:"", top_raw, ""scores:"", np.round(s_raw, 4))

# AFTER: enforce cosine by normalizing both sides

top_cos, s_cos = ip_search(q/np.linalg.norm(q), l2norm(chunks), k=2)
print(""AFTER idx:"", top_cos, ""scores:"", np.round(s_cos, 4))

```

---

on many runs the raw version ranks the tight B cluster above A even though the query is A. enforcing a cosine contract flips it back.

---

Before vs After Fix (what to ship)

1. enforce L2 normalization for both stored vectors and queries when you mean cosine.

2. add a chunk id contract that keeps page or section fields. avoid tiny fragments, normalize casing and width.

3. apply an acceptance gate before you generate. if Î”S or coverage fail, re-retrieve or reset instead of emitting.

full map here, includes No.5 and No.8 details and the traceability checklist

WFGY Problem Map 1.0 â†’ 

https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md

License
MIT. no SDK. text instructions only.

What feedback Iâ€™m looking for

short csvs or snippets where cosine looks high but the answer is wrong. 10â€“30 rows are enough. i will run the same contract and post before/after. if you enforce normalization at ingestion or at query time, which one worked better for you",Python,0,https://www.reddit.com/r/Python/comments/1ncdmvb/cosine091_but_answer_is_wrong_a_tiny_python_mre/,r_1ncdmvb,,,
r_1ncbh1t,reddit,Funny-Ad-5060,2025-09-09T06:23:21+00:00,"I built a Django job scraper that saves listings directly into Google Sheets
Hey everyone

I was spending way too much time manually checking job boards, copying jobs into spreadsheets, and still missing good opportunities. So I built a small Django project to automate the whole process.

Hereâ€™s what it does:

* âœ… Scrapes job listings from TimesJobs using **BeautifulSoup + Requests**
* âœ… Saves them in a **Django SQLite database**
* âœ… Pushes jobs into **Google Sheets** via API
* âœ… Avoids duplicates and formats data cleanly
* âœ… Runs automatically every few hours with Pythonâ€™s `schedule` library

**Source code (GitHub):** [jobscraper](https://github.com/coderdigi01/jobscraper)  
**Full step-by-step tutorial (with code snippets):** [Blog Post]()

This was a fun project that taught me a lot about:

* Rate limiting (got blocked early on for too many requests)
* Handling inconsistent HTML in job listings
* Google Sheets API quotas and batching updates",Python,0,https://www.reddit.com/r/Python/comments/1ncbh1t/i_built_a_django_job_scraper_that_saves_listings/,r_1ncbh1t,,,
r_1ncaaqa,reddit,000wall,2025-09-09T05:10:51+00:00,"trying to find old rtmidi module
I am trying to get MIDI input working in a very old Python 2.7 game, which is based on pygame 1.9.6.  
This game requires ""rtmidi"", but I've been unable to find exactly which rtmidi it needs.

These are the API calls used by the game;

    import rtmidi
    .RtMidiOut()
    .RtMidiIn()
    .getPortCount()
    .openPort()
    .getMessage()

which rules out `rtmidi-python` and `python-rtmidi` as those use `.MidiOut`/`.MidiIn` instead of `.RtMidiOut`/`.RtMidiIn`.

I also tried every version of `rtmidi` which uses the API expected by this game, but the game crashes on startup with the error `TypeError: object of type 'NoneType' has no len()`.",Python,3,https://www.reddit.com/r/Python/comments/1ncaaqa/trying_to_find_old_rtmidi_module/,r_1ncaaqa,,,
r_1nc7tpm,reddit,TankBorn,2025-09-09T02:58:08+00:00,"What is the best framework for working with data from remote devices and applying it to the web?
I need to get data from IoT devices and work with them, being able to manipulate them on the web and in databases.

I was thinking about Django Rest - Frameworkâ€¦.",Python,5,https://www.reddit.com/r/Python/comments/1nc7tpm/what_is_the_best_framework_for_working_with_data/,r_1nc7tpm,,,
r_1nc7r45,reddit,AlSweigart,2025-09-09T02:54:32+00:00,"Python Type System and Tooling Survey 2025
This survey was developed with support from the Pyrefly team at Meta, the PyCharm team at JetBrains, and the typing community on discourse.python.org. No typing experience needed -- your perspective as a Python dev matters most. Take a couple minutes to help improve Python typing for all:

https://docs.google.com/forms/d/e/1FAIpQLSeOFkLutxMLqsU6GPe60OJFYVN699vqjXPtuvUoxbz108eDWQ/viewform?fbzx=-4095906651778441520",Python,82,https://www.reddit.com/r/Python/comments/1nc7r45/python_type_system_and_tooling_survey_2025/,r_1nc7r45,,,
r_1nc41wf,reddit,AutoModerator,2025-09-09T00:00:29+00:00,"Tuesday Daily Thread: Advanced questions
# Weekly Wednesday Thread: Advanced Questions ðŸ

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! ðŸŒŸ",Python,4,https://www.reddit.com/r/Python/comments/1nc41wf/tuesday_daily_thread_advanced_questions/,r_1nc41wf,,,
r_1nc3glf,reddit,RichardHapb,2025-09-08T23:34:36+00:00,"Just LSPDock v0.1.3 (before named LSProxy) released, multi-lsp handling feature
I have news: I implemented the feature in the proxy for handling multiple LSP in the same path/project using an `--exec` argument. The details are in the README.

LSPDock allows you to connect to an LSP running inside a Docker container directly from the IDE and automatically handles the differences in paths.

Note: I renamed the project because a conflict with another project.

The link of the repo:

[https://github.com/richardhapb/lspdock](https://github.com/richardhapb/lspdock)",Python,1,https://www.reddit.com/r/Python/comments/1nc3glf/just_lspdock_v013_before_named_lsproxy_released/,r_1nc3glf,,,
r_1nc0etx,reddit,Imaginary-Medium7360,2025-09-08T21:27:02+00:00,"Baba is you, learning games
Anyone played it? I heard itâ€™s based on the logic of python. ðŸ 
Was thinking of downloading to keep me thinking about the topic while I am in the process of learning

https://youtu.be/z3_yA4HTJfs?si=OR6gXX6xCTiarFbM

Doesnâ€™t apply to anything in my current job field but I am learning it to eventually make a lateral job move until the opportunity presents itself

Itâ€™s available on mobile so thinking of getting it",Python,7,https://www.reddit.com/r/Python/comments/1nc0etx/baba_is_you_learning_games/,r_1nc0etx,,,
r_1nbx7l6,reddit,DeWildAsh,2025-09-08T19:24:22+00:00,"cython for coding a game engine?
So I have plans to write a game engine, I wanna incorporate python as the main scripting language, and write the backend in C (maybe eventually c++) could I write the whole engine in cython getting the power of c but writing it in python or just stick to writing the backend in C?    ",Python,13,https://www.reddit.com/r/Python/comments/1nbx7l6/cython_for_coding_a_game_engine/,r_1nbx7l6,,,
r_1nbugq8,reddit,Mysterious_Crow_7827,2025-09-08T17:42:55+00:00,"Error en Visual Studio Code: Terminal lenta y problema con la base de datos al usar Flask y GitHub.
Hola a todos,

Necesito su ayuda con un problema que estoy teniendo con mi proyecto de Python/Flask en Visual Studio Code. He intentado varias cosas, pero no he logrado resolverlo.

Antecedentes del problema

Anteriormente, utilizaba GitHub Desktop para gestionar mis repositorios. De repente, me empezÃ³ a dar un error que decÃ­a que no podÃ­a encontrar el repositorio local, a pesar de que los archivos seguÃ­an en mi computadora.

Mi soluciÃ³n temporal fue clonar de nuevo el repositorio, y eso funcionÃ³ para GitHub Desktop. Sin embargo, ahora tengo un problema en Visual Studio Code que no sÃ© cÃ³mo solucionar.

El problema actual

Terminal excesivamente lenta: Cuando uso la terminal de Visual Studio Code para ejecutar comandos como flask db init o flask run, el proceso se vuelve muy lento. Aunque eventualmente me muestra que el proceso fue exitoso, el tiempo de espera es anormal.

No se visualiza la base de datos: A pesar de que la terminal indica que el comando flask db init se ejecutÃ³ correctamente, no puedo ver la base de datos (generalmente un archivo .db) en el explorador de archivos de Visual Studio Code. Es como si el archivo no se estuviera creando o se estuviera creando en un lugar incorrecto, aunque no me lanza ningÃºn error.

Lo que he revisado

RevisÃ© que mi entorno virtual (venv) estÃ© activado correctamente.

ConfirmÃ© que los archivos del proyecto, como app.py y config.py, estÃ¡n bien configurados para la base de datos.

VerifiquÃ© que el archivo del repositorio estÃ¡ en el mismo lugar de siempre en mi computadora.

Mis preguntas

Â¿PodrÃ­a este problema estar relacionado con la forma en que GitHub Desktop maneja los repositorios?

Â¿Hay alguna configuraciÃ³n especÃ­fica en Visual Studio Code que deba revisar?

Â¿CÃ³mo puedo solucionar la lentitud de la terminal y asegurar que la base de datos se cree y se muestre en mi explorador de archivos?

Agradezco de antemano cualquier sugerencia o ayuda que puedan darme. ",Python,0,https://www.reddit.com/r/Python/comments/1nbugq8/error_en_visual_studio_code_terminal_lenta_y/,r_1nbugq8,,,
r_1nbtpoo,reddit,suraj_chandola,2025-09-08T17:15:45+00:00,"Which 1 language to master for Al & Web in 2025?""
If you had to choose only one programming language to master for Al and web development in 2025, which one would it be and why?",Python,0,https://www.reddit.com/r/Python/comments/1nbtpoo/which_1_language_to_master_for_al_web_in_2025/,r_1nbtpoo,,,
r_1nbnqh7,reddit,DataScience123888,2025-09-08T13:27:40+00:00,"Questions for interview on OOPs concept.
I have python interview scheduled this week.

OOPs concept will be asked in depth, What questions can be asked or expected from OOPs concept in python given that there will be in depth grilling on OOPs.

Need this job badly already in huge debt.",Python,0,https://www.reddit.com/r/Python/comments/1nbnqh7/questions_for_interview_on_oops_concept/,r_1nbnqh7,,,
r_1nbmdj7,reddit,ILDaviz,2025-09-08T12:27:56+00:00,"Aicontextator - A CLI tool to safely bundle your project's code for LLMs


Hi,

I'm David. I built Aicontextator to scratch my own itch. I was spending way too much time manually gathering and pasting code files into LLM web UIs. It was tedious, and I was constantly worried about accidentally pasting an API key or another secret.

Aicontextator is a simple CLI tool built with Python that automates this entire process. You run it in your project directory, and it bundles all the relevant files into a single, clean string ready for your prompt.

The GitHub repo is here: [https://github.com/ILDaviz/aicontextator](https://github.com/ILDaviz/aicontextator)

I'd love to get your feedback and suggestions!

**What My Project Does**

Aicontextator is a command-line utility designed to make it easier and safer to provide code context to Large Language Models. Its main features are:

* **Context Bundling:** It recursively finds all files in your project, respects your `.gitignore` rules, and concatenates them into a single string for easy copy-pasting.
* **Security First:** It uses the `detect-secrets` engine to scan every file *before* adding it to the context. If it finds a potential secret (like an API key or password), it warns you and excludes that line, preventing accidental leaks.
* **User-Friendly Features:** It includes an interactive mode to visually select which files to include, a token counter to stay within the LLM's context limit, and the ability to automatically split the output into multiple chunks if the context is too large.

**Target Audience**

This tool is for any developer who regularly uses LLMs (like ChatGPT, Claude, Gemini, etc.) for coding assistance, debugging, or documentation. It's particularly useful for those working on projects with a non-trivial number of files (e.g., web developers, data scientists, backend engineers) where manually providing context is impractical. It's designed as a practical utility to be integrated into a daily development workflow, not just a toy project.

**Comparison with Alternatives**

* **vs. Manual Copy-Pasting:** This is the most common method, but it's slow, error-prone (it's easy to miss a file), and risky (you might accidentally paste a file like `.env`). Aicontextator automates this, making it fast, comprehensive, and safe.
* **vs. IDE Extensions (e.g., GitHub Copilot Chat, Cursor):** These tools are powerful but tie you to a specific editor and often a specific LLM ecosystem. Aicontextator is **editor-agnostic** and **LLM-agnostic**. It generates a simple string that you can use in any web UI or API you prefer, giving you complete flexibility.
* **vs. Other Context-Aware CLI Tools:** Many alternative tools try to be full-fledged chat clients in your terminal. Aicontextator has a much simpler scope: it does one thing and does it well. It focuses solely on **preparing the context**, acting as a powerful pre-processor for any LLM interaction, without forcing you into a specific chat interface.

Cheers!",Python,0,https://www.reddit.com/r/Python/comments/1nbmdj7/aicontextator_a_cli_tool_to_safely_bundle_your/,r_1nbmdj7,,,
r_1nblyt6,reddit,Ok-Raspberry-5333,2025-09-08T12:08:58+00:00,"Webscraping twitter or any
So I was trying to learn webscraping. I was following a github repo project based learning. The methods were outdated so the libraries were. It was snscrape. I found the twitter's own mining api but after one try it was not working . It had rate limit. I searched for few and found playwright and selenium . I only want to learn how to get the data and convert it into datasets. Later I will continue doing analysis on them for learning purpose. Can anyone suggest me something that should follow ?",Python,25,https://www.reddit.com/r/Python/comments/1nblyt6/webscraping_twitter_or_any/,r_1nblyt6,,,
r_1nbkych,reddit,PastPicture,2025-09-08T11:17:22+00:00,"Stop building UI frameworks in Python
7 years back when I started coding, I used Tkinter. Then PyQt. 

I spent some good 2 weeks debating if I should learn Kivy or Java for building an Android app.

Then we've got modern ones: FastUI by Pydantic, NiceGUI (amazing project, it's the closest bet).

Python is great for a lot of things. Just stop abusing it by building (or trying to) UI with it. 

Even if you ship something you'll wake up in mid of night thinking of all the weird scenarios, convincing yourself to go back to sleep since you'll find a workaround like last time. 

Why I am saying this: Because I've tried it all. I've tried every possible way to avoid JavaScript and keep building UIs with Python.

I've contributed to some really popular UI libraries in Python, tried inventing one back in Tkinter days. 

I finally caved in and I now build UI with JavaScript, and I'm happier person now. I feel more human.",Python,872,https://www.reddit.com/r/Python/comments/1nbkych/stop_building_ui_frameworks_in_python/,r_1nbkych,,,
r_1nbkguo,reddit,piequals-3,2025-09-08T10:50:58+00:00,"I built a programming language interpreted in Python!
Hey!

I'd like to share a project I've been working on: A functional programming language that I built entirely in Python.

I'm primarily a Python developer, but I wanted to understand functional programming concepts better. Instead of just reading about them, I decided to build my own FP language from scratch. It started as a tiny DSL (domain specific language) for a specific problem (which it turned out to be terrible for!), but I enjoyed the core ideas enough to expand it into a full functional language.

## What My Project Does

NumFu is a pure functional programming language interpreted in Python featuring:
- **Arbitrary precision arithmetic** using `mpmath` - no floating point issues
- **Automatic partial application** and function composition 
- **Built-in testing syntax** with readable assertions
- **Tail call optimization** for efficient recursion
- **Clean syntax** with only four types (Number, Boolean, List, String)

Here's a taste of the syntax:

```numfu
// Functions automatically partially apply
>>> {a, b, c -> a + b + c}(_, 5)
{a, c -> a+5+c}  // Even prints as readable syntax!

// Composition and pipes
let add1 = {x -> x + 1},
    double = {x -> x * 2}
in 5 |> (add1 >> double) // 12

// Built-in testing
let square = {x -> x * x} in
square(7) ---> $ == 49  // âœ“ passes
```

## Target Audience

This is **not** a production language - it's 2-5x slower than Python due to double interpretation. It's more of a learning tool for:
- Teaching functional programming concepts without complex syntax
- Sketching mathematical algorithms where precision matters more than speed
- Understanding how interpreters work

## Comparison

NumFu has much simpler syntax than traditional functional languages like Haskell or ML and no complex type system - just four basic types. It's less powerful but much more approachable. I designed it to make FP concepts accessible without getting bogged down in advanced language features. Think of it as functional programming with training wheels.

## Implementation Details

The implementation is about 3,500 lines of Python using:
- *Lark* for parsing
- *Tree-walking interpreter* - straightforward recursive evaluation  
- *mpmath* for arbitrary precision arithmetic


## Try It Out

```bash
pip install numfu-lang
numfu repl
```

## Links

I actually enjoy web design, so NumFu has a (probably overly fancy) landing page + documentation site. ðŸ˜…

- GitHub: https://github.com/rphle/numfu  
- Website: https://rphle.github.io/numfu/
- Documentation: https://rphle.github.io/numfu/docs
- PyPI: https://pypi.org/project/numfu-lang/

I built this as a learning exercise and it's been fun to work on. Happy to answer questions about design choices or implementation details! I also really appreciate issues and pull requests!
",Python,80,https://www.reddit.com/r/Python/comments/1nbkguo/i_built_a_programming_language_interpreted_in/,r_1nbkguo,,,
r_1nbh74t,reddit,Silver_Equivalent_58,2025-09-08T07:21:48+00:00,"what are some concepts i need to know to build a mini ""FASTAPI""
ive been wanting to implement a super minimalist version of fastapi, but the codebase is a bti overwhelming. what are some concepts i need to understand and how to approach building this?

  
thanks",Python,0,https://www.reddit.com/r/Python/comments/1nbh74t/what_are_some_concepts_i_need_to_know_to_build_a/,r_1nbh74t,,,
r_1nb8x34,reddit,AutoModerator,2025-09-08T00:00:30+00:00,"Monday Daily Thread: Project ideas!
# Weekly Thread: Project Ideas ðŸ’¡

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project ideaâ€”be it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! ðŸŒŸ",Python,8,https://www.reddit.com/r/Python/comments/1nb8x34/monday_daily_thread_project_ideas/,r_1nb8x34,,,
r_1nb8bdn,reddit,_unknownProtocol,2025-09-07T23:32:44+00:00,"My Python library to create images from simple layouts
Hey r/Python,

I'm working on an open-source library for creating images from code. The idea is to build visuals by describing them as simple layouts, instead of calculating `(x, y)` coordinates for everything.

For example, I used it to generate this fake Reddit post card:

[Resulting Image](https://i.imgur.com/JUFXMzK.png)

This whole image was created with the Python code below. It handles all the layout, font fallbacks, text wrapping, and rendering for you.

```python
from pictex import *

# --- 1. Define the small components ---
upvote_icon = Image(""upvote.png"")
downvote_icon = Image(""downvote.png"")
comment_icon = Image(""comment.png"").resize(0.7)
python_icon = Image(""python_logo.png"").size(25, 25).border_radius('50%')

flair = Text(""Showcase"").font_size(12).padding(2, 6).background_color(""#0079D3"").color(""white"").border_radius(10)

# --- 2. Build the layout by composing components ---
vote_section = Column(
    upvote_icon,
    Text(""51"").font_size(40).font_weight(700),
    downvote_icon
).horizontal_align('center').gap(5)

post_header = Row(
    python_icon,
    Text(""r/Python â€¢ Posted by u/_unknownProtocol"").font_size(14),
    flair
).gap(8).vertical_align('center')

post_title = Text(
    ""My Python library to create images from simple layouts""
).font_size(22).font_weight(700).line_height(1.2)

post_footer = Row(
    comment_icon,
    Text(""12 Comments"").font_size(14).font_weight(700),
).gap(8).vertical_align('center')

# --- 3. Assemble the final card ---
main_card = Row(
    vote_section.padding(0, 15, 0, 0),
    Column(post_header, post_title, post_footer).gap(10)
).padding(20).background_color(""white"").border_radius(10).size(width=600).box_shadows(
    Shadow(offset=(5, 5), blur_radius=10, color=""#00000033"")
)

# --- 4. Render on a canvas ---
canvas = Canvas().background_color(LinearGradient([""#F0F2F5"", ""#DAE0E6""])).padding(40)
image = canvas.render(main_card)
image.save(""reddit_card.png"")
```

---

### What My Project Does

It's a layout engine that renders to an image. You build your image by nesting components (`Row`, `Column`, `Text`, `Image`), and the library figures out all the sizing and positioning for you, using a model inspired by CSS Flexbox. You can style any element with padding, borders, backgrounds, and shadows. It also handles fonts and emojis, automatically finding fallbacks if a character isn't supported.

### Target Audience

It's for any Python dev who wants to create images from code, especially when the content is dynamic. For example:
*   Automating social media posts or quote images.
*   Generating Open Graph images for a website on the fly.
*   Creating parts of an infographic or a report.

The project is currently in Beta. It's pretty solid for most common use cases, but you might still find some rough edges.

### Comparison

*   **vs. Pillow/OpenCV:** Think of Pillow/OpenCV as a digital canvas where you have to specify the exact `(x, y)` coordinates for everything you draw. This library is more of a layout manager: you describe *how* elements should be arranged, and it does the math for you.
*   **vs. HTML/CSS-to-Image libraries:** They're powerful, but they usually require a full web browser engine (like Chrome) to work, which can be a heavy dependency. This library uses Skia directly and is a standard `pip install`.

---

I'm still working on it, and any feedback or suggestions are very welcome.

You can find more examples in the repository. Thanks for taking a look!

*   **GitHub Repo:** [https://github.com/francozanardi/pictex](https://github.com/francozanardi/pictex)
*   **PyPI Page:** [https://pypi.org/project/pictex/](https://pypi.org/project/pictex/)",Python,4,https://www.reddit.com/r/Python/comments/1nb8bdn/my_python_library_to_create_images_from_simple/,r_1nb8bdn,,,
r_1nb5rhw,reddit,easy_peazy,2025-09-07T21:42:32+00:00,"lilpipe: a tiny, typed pipeline engine (not a DAG)
At work, I develop data analysis pipelines in Python for the lab teams. Oftentimes, the pipelines are a little too lightweight to justify a full DAG.Â [lilpipe](https://github.com/andrewruba/lilpipe)Â is my attempt at the minimum feature set to run those pipelines without extra/unnecessary infrastructure.

# What My Project Does

* Runs sequential, in-process pipelines (not a DAG/orchestrator).
* Shares a typed, Pydantic PipelineContext across steps (assignment-time validation if you want it).
* Skips work via fingerprint caching (fingerprint\_keys).
* Gives simple control signals: ctx.abort\_pass() (retry current pass) and ctx.abort\_pipeline() (stop).
* Lets you compose steps: Step(""name"", children=\[...\]).

# Target Audience

* Data scientists / lab scientists who use notebooks or small scripts and want a shared context across steps.
* Anyone maintaining â€œglueâ€ scripts that could use caching and simple retry/abort semantics.
* Bio-analytical analysis: load plate â†’ calibrate â†’ QC â†’ report (ie. this project's origin story).
* Data engineers with one-box batch jobs (CSV â†’ clean â†’ export) who donâ€™t want a scheduler and metadata DB (a bit of a stretch, I know).

# Comparison

* Airflow/Dagster/Prefect: Full DAG/orchestrators with schedulers, UIs, state, lineage, retries, SLAs/backfills. lilpipe is intentionally not that. Itâ€™s for linear, in-process pipelines where that stack is overkill.
* scikit-learn Pipeline: ML-specific fit/transform/predict on estimators. lilpipe is general purpose steps with a Pydantic context.
* Other lightweight pipeline libraries: don't have the exact features that I use on a day-to-day basis. lilpipe does have those features haha.

Thanks, hoping to get feedback. I know there are many variations of this but it may fit a certain data analysis niche.

[lilpipe](https://github.com/andrewruba/lilpipe)",Python,51,https://www.reddit.com/r/Python/comments/1nb5rhw/lilpipe_a_tiny_typed_pipeline_engine_not_a_dag/,r_1nb5rhw,,,
r_1nb05ab,reddit,swiss_shepherd,2025-09-07T18:02:44+00:00,"Prompt components - a better library for managing LLM prompts
I started an Agentic AI company that has recently winded down, and we're happy to open source this library for managing prompts for LLMs!


### What My Project Does

Create components (blocks of text) that can be composed and shared across different prompts. This library enables isolated testing of each component, with support for standard python string formatting and jinja2.

The library came about because we were pulling our hair out trying to re-use different prompts across our codebase.

### Target Audience

This library is for you if you:

\- have written templates for LLMs and want proper type hint support

\- want a clean way to share blocks of text between prompts

### Comparison

Standard template engines lack clear ways to organize shared text between different prompts.

This library utilizes dataclasses to write prompts.

### Dataclasses for composable components

    @dataclass_component
    class InstructionsXml:
        _template = ""<instructions> {text} </instructions>""
        text: str
    
    @dataclass_component
    class Prompt(StringTemplate):
        _template = """"""
        ## AI Role
        {ai_role}
    
        ## Instructions
        {instructions}
        """"""
    
        ai_role: str
        instructions: Instructions
    
    prompt = Prompt(
        ai_role=""You are an expert coder."",
        instructions=Instructions(
           text=""Write python code to satisfy the user's query.""
        )
    )
    print(prompt.render()) # Renders the prompt as a string

The \`InstructionsXml\` component can be used in other prompts and also is easily swapped out! More powerful constructs are possible using dataclass features + jinja2.

Library here:Â [https://github.com/jamesaud/prompt-components](https://github.com/jamesaud/prompt-components)",Python,0,https://www.reddit.com/r/Python/comments/1nb05ab/prompt_components_a_better_library_for_managing/,r_1nb05ab,,,
r_1nazumb,reddit,Bob_Dieter,2025-09-07T17:51:49+00:00,"Class type parameters that actually do something
I was bored, so I made type parameters for python classes that are accessible within your class and contribute to behaviour . Check them out:

[https://github.com/arikheinss/ParametricTypes.py](https://github.com/arikheinss/ParametricTypes.py)

    T = TypeVar(""T"")
    
    class wrapper[T](metaclass = ParametricClass):
        ""silly wrapper class with a type restriction""
    
        def __init__(self, x: T):
            self.set(x)
    
        def set(self, v: T):
            if not isinstance(v, T):
                raise TypeError(f""wrapper of type ({T}) got value of type {type(v)}"")
            self.data = v
    
        def get(self) -> T:
            return self.data
    # =============================================
        
    w_int = wrapper[int](2)
    
    w_int.set(4)
    print(w_int.get()) # 4
    
    print(isinstance(wrapper[int], type)) # True
    
    w_int.set(""hello"") # error!! Wrong type!
    w_2 = wrapper(None) # error!! Missing type parameters!!

edit: after some discussion in the comments, I want to highlight that one central component of this mechanism is that we get different types from applying the type parameters, i.e.:

```
isinstance(w_int, wrapper) # True
isinstance(w_int, wrapper[int]) # True
isinstance(w_int, wrapper[float]) # False
type(wrapper[str]("""")) == type(wrapper[int](2)) # False
```

For the Bot, so it does not autoban me again:

* **What My Project Does** Is explained above
* **Target Audience** Toyproject - Anyone who cares
* **Comparison** The Python GenericAlias exists, but does not really integrate with the rest of the type system.",Python,55,https://www.reddit.com/r/Python/comments/1nazumb/class_type_parameters_that_actually_do_something/,r_1nazumb,,,
r_1nax88a,reddit,3DMakeorg,2025-09-07T16:10:08+00:00,"ML Data Pipeline pain points
Researching ML data pipeline pain points. For production ML builders: what's your biggest training data prep frustration?

ðŸ” Data quality?
â±ï¸ Labeling bottlenecks? 
ðŸ’° Annotation costs?
âš–ï¸ Bias issues?

Share your real experiences!",Python,0,https://www.reddit.com/r/Python/comments/1nax88a/ml_data_pipeline_pain_points/,r_1nax88a,,,
r_1navllc,reddit,SnooBooks7077,2025-09-07T15:06:54+00:00,"Does any body have problems with the openai agents library?
    from
     agents 
    import
     Agent, Runner, trace
    from
     agents.mcp 
    import
     MCPServerStdio

for these two lines It took over 2 mins to complete and in the end I got this error: 

    ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    Cell In[1], line 1
    ----> 1 from agents import Agent, Runner, trace
          2 from agents.mcp import MCPServerStdio
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\__init__.py:22
         19 from __future__ import print_function
         21 from . import algorithms
    ---> 22 from . import scripts
         23 from . import tools
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\__init__.py:21
         18 from __future__ import division
         19 from __future__ import print_function
    ---> 21 from . import train
         22 from . import utility
         23 from . import visualize
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\train.py:33
         30 import tensorflow as tf
         32 from agents import tools
    ---> 33 from agents.scripts import configs
         34 from agents.scripts import utility
         37 def _create_environment(config):
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\configs.py:26
         23 import tensorflow as tf
         25 from agents import algorithms
    ---> 26 from agents.scripts import networks
         29 def default():
         30   """"""Default configuration for PPO.""""""
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\networks.py:30
         26 import tensorflow as tf
         28 import agents
    ---> 30 tfd = tf.contrib.distributions
         33 # TensorFlow's default implementation of the KL divergence between two
         34 # tf.contrib.distributions.MultivariateNormalDiag instances sometimes results
         35 # in NaN values in the gradients (not in the forward pass). Until the default
         36 # implementation is fixed, we use our own KL implementation.
         37 class CustomKLDiagNormal(tfd.MultivariateNormalDiag):
    
    AttributeError: module 'tensorflow' has no attribute 'contrib'

All of the libraries were installed right before running the code.   
Had it also happened to you?

",Python,0,https://www.reddit.com/r/Python/comments/1navllc/does_any_body_have_problems_with_the_openai/,r_1navllc,,,
r_1naohtd,reddit,TieTraditional5532,2025-09-07T09:05:23+00:00,"7 Free Python PDF Libraries You Should Know in 2025
# Why PDFs Are Still a Headache

You receive a PDF from a client, and it looks harmless. Until you try to copy the data. Suddenly, the text is broken into random lines, the tables look like modern art, and youâ€™re thinking:Â *â€œThis canâ€™t be happening in 2025.â€*

Clients donâ€™t want excuses. They want clean Excel sheets or structured databases. And you? Youâ€™re left staring at a PDF that seems harder to crack than the Da Vinci Code.

Luckily, the Python community has createdÂ **free Python PDF libraries**Â that can do everything: extract text, capture tables, process images, and even apply OCR for scanned files.

A client once sent me a 200-page scanned contract. They expected all the financial tables in Excel by the next morning. Manual work? Impossible. So I pulled out my toolbox of Python PDF librariesâ€¦ and by sunrise, the Excel sheet was sitting in their inbox. (Coffee was my only witness.)

# 1. pypdf

SeeÂ [repositoryÂ ](https://github.com/py-pdf/pypdf)on GitHub

**What itâ€™s good for:**Â splitting, merging, rotating pages, extracting text and metadata.

* Tip: Great for automation workflows where you donâ€™t need perfect formatting, just raw text or document restructuring.

**Client story:**Â A law firm I worked with had to merge thousands of PDF contracts into one document before archiving them. WithÂ `pypdf`, the process went from hours to minutes

    from pypdf import PdfReader, PdfWriter
    
    reader = PdfReader(""contract.pdf"")
    writer = PdfWriter()
    for page in reader.pages:
        writer.add_page(page)
    
    with open(""merged.pdf"", ""wb"") as f:
        writer.write(f)

# 2. pdfplumber

SeeÂ [repositoryÂ ](https://github.com/jsvine/pdfplumber)on GitHub

**Why people love it:**Â It extracts textÂ **with structure**Â â€” paragraphs, bounding boxes, tables.

* Pro tip: UseÂ `extract_table()`Â when you want quick CSV-like results.
* Use case: A marketing team used pdfplumber to extract pricing tables from competitor brochures â€” something copy-paste would never get right.

&#8203;

    import pdfplumber
    with pdfplumber.open(""brochure.pdf"") as pdf:
        first_page = pdf.pages[0]
        print(first_page.extract_table())

# 3. PDFMiner.six

[See repository on GitHub](https://github.com/pdfminer/pdfminer.six)

**What makes it unique:**Â Access to low-level layout details â€” fonts, positions, character mapping.

* **Example scenario:**Â An academic researcher needed to preserve footnote references and exact formatting when analyzing historical documents.Â `PDFMiner.six`Â was the only library that kept the structure intact.

&#8203;

    from pdfminer.high_level import extract_text
    print(extract_text(""research_paper.pdf""))

# 4. PyMuPDF (fitz)

[See repository on GitHub](https://github.com/pymupdf/PyMuPDF)

**Why it stands out:**Â Lightning-fast and versatile. It handles text, images, annotations, and gives you precise coordinates.

* Tip: UseÂ `""blocks""`Â mode to extract content by sections (paragraphs, images, tables).
* Client scenario: A publishing company needed to extract all embedded images from e-books for reuse. With PyMuPDF, they built a pipeline that pulled images in seconds.

&#8203;

    import fitz
    doc = fitz.open(""ebook.pdf"")
    page = doc[0]
    print(page.get_text(""blocks""))

# 5. Camelot

[See repository on GitHub](https://github.com/camelot-dev/camelot)

**What itâ€™s built for:**Â ExtractingÂ **tables**Â with surgical precision.

* Modes:Â `lattice`Â (PDFs with visible lines) andÂ `stream`Â (no visible grid).
* Real use: An accounting team automated expense reports, saving dozens of hours each quarter.

&#8203;

    import camelot
    tables = camelot.read_pdf(""expenses.pdf"", flavor=""lattice"")
    tables[0].to_csv(""expenses.csv"")

# 6. tabula-py

[See repository on GitHub](https://github.com/chezou/tabula-py)

**Why itâ€™s popular:**Â A Python wrapper aroundÂ **Tabula (Java)**Â that sends tables straight into pandas DataFrames.

* **Tip for analysts:**Â If your workflow is already in pandas,Â `tabula-py`Â is the fastest way to integrate PDF data.
* **Example:**Â A data team at a logistics company parsed invoices and immediately used pandas for KPI dashboards.

&#8203;

    import tabula
    df_list = tabula.read_pdf(""invoices.pdf"", pages=""all"")
    print(df_list[0].head())

# 7. OCR with pytesseract + pdf2image

[Tesseract OCR | pdf2image](https://github.com/tesseract-ocr/tesseract)

**When you need it:**Â For scanned PDFs with no embedded text.

* Pro tip: Always preprocess images (resize, grayscale, sharpen) before sending them to Tesseract.
* Real scenario: A medical clinic digitized old patient records. OCR turned piles of scans into searchable text databases.

&#8203;

    from pdf2image import convert_from_path
    import pytesseract
    
    pages = convert_from_path(""scanned.pdf"", dpi=300)
    text = ""\n"".join(pytesseract.image_to_string(p) for p in pages)
    print(text)

# Bonus: Docling (AI-Powered)

[See repository on GitHub](https://github.com/DS4SD/docling)

**Why itâ€™s trending:**Â Over 10k â­ in weeks. It uses AI to handle complex layouts, formulas, diagrams, and integrates with modern frameworks like LangChain.

* Example: Researchers use it to process scientific PDFs with math equations, something classic libraries often fail at.

# Final Thoughts

Extracting data from PDFs no longer has to feel like breaking into a vault. With theseÂ **free Python PDF libraries**, you can choose the right tool depending on whether you need raw text, structured tables, or OCR for scanned documents.",Python,0,https://www.reddit.com/r/Python/comments/1naohtd/7_free_python_pdf_libraries_you_should_know_in/,r_1naohtd,,,
r_1nalqfh,reddit,Monster-07,2025-09-07T06:11:50+00:00,"Need advice with low-level disk wiping (HPA/DCO, device detection)
iâ€™m currently working on a project that wipes data from storage devices including hidden sectors like **HPA (Host Protected Area)** and **DCO (Device Configuration Overlay)**.

Yes, I know tools already exist for data erasure, but most donâ€™t properly handle these hidden areas. My goal is to build something that:

* Communicates at a **low level** with the disk to securely wipe even HPA/DCO.
* **Detects disk type** automatically (HDD, SATA, NVMe, etc.).
* Supports multiple sanitization methods (e.g., **NIST SP 800-88, DoD 5220.22-M**, etc.).

Iâ€™m stuck on the part about **low-level communication with the disk for wiping**. Has anyone here worked on this or can guide me toward resources/approaches?",Python,0,https://www.reddit.com/r/Python/comments/1nalqfh/need_advice_with_lowlevel_disk_wiping_hpadco/,r_1nalqfh,,,
r_1nakbd6,reddit,nicholashairs,2025-09-07T04:48:52+00:00,"Python-JSON-Logger v4.0.0.rc1 Released
Hi All, maintainer of [python-json-logger](https://github.com/nhairs/python-json-logger) here with a new (pre) release for you.

It can be installed using `python-json-logger==4.0.0.rc1`

# What's new?

This release has a few quality of life improvements that also happen to be breaking changes. The [full change log is here](https://nhairs.github.io/python-json-logger/4.0.0/changelog/) but to give an overview:

**Support for** `ext://` **when using** `dictConfig` **/** `fileConfig`

This allows you to reference Python objects in your config for example:

    version: 1
    disable_existing_loggers: False
    formatters:
      default:
        ""()"": pythonjsonlogger.json.JsonFormatter
        format: ""%(asctime)s %(levelname)s %(name)s %(module)s %(funcName)s %(lineno)s %(message)s""
        json_default: ext://logging_config.my_json_default
        rename_fields:
          ""asctime"": ""timestamp""
          ""levelname"": ""status""
        static_fields:
          ""service"": ext://logging_config.PROJECT_NAME
          ""env"": ext://logging_config.ENVIRONMENT
          ""version"": ext://logging_config.PROJECT_VERSION
          ""app_log"": ""true""
    handlers:
      default:
        formatter: default
        class: logging.StreamHandler
        stream: ext://sys.stderr
      access:
        formatter: default
        class: logging.StreamHandler
        stream: ext://sys.stdout
    loggers:
      uvicorn.error:
        level: INFO
        handlers:
          - default
        propagate: no
      uvicorn.access:
        level: INFO
        handlers:
          - access
        propagate: no

**Support for easier to use formats**

We now support a comma `style="",""` style which lets use a comma seperate string to specific fields.

    formatter = JsonFormatter(""message,asctime,exc_info"", style="","")

We also using any sequence of strings (e.g. lists or tuples).

    formatter = JsonFormatter([""message"", ""asctime"", ""exc_info""])

# What is Python JSON Logger

If you've not heard of this package, Python JSON Logger enables you produce JSON logs when using Python'sÂ `logging`Â package.

JSON logs are machine readable allowing for much easier parsing and ingestion into log aggregation tools.

For example here is the (formatted) log output of one of my programs:

    {
      ""trace_id"": ""af922f04redacted"",
      ""request_id"": ""cb1499redacted"",
      ""parent_request_id"": null,
      ""message"": ""Successfully imported redacted"",
      ""levelname"": ""INFO"",
      ""name"": ""redacted"",
      ""pathname"": ""/code/src/product_data/consumers/games.py"",
      ""lineno"": 41,
      ""timestamp"": ""2025-09-06T08:00:48.485770+00:00""
    }

# Why post to Reddit?

Although Python JSON Logger [is in the top 300 downloaded packaged from PyPI](https://hugovk.github.io/top-pypi-packages/) (in the last month it's been downloaded more times that UV! ... just), there's not many people watching the repository [after it changed hands](https://www.reddit.com/r/Python/comments/1hcm2rr/pythonjsonlogger_has_changed_hands/) at the end of 2024.

This seemed the most appropriate way to share the word in order to minimise disruptions once it is released.",Python,61,https://www.reddit.com/r/Python/comments/1nakbd6/pythonjsonlogger_v400rc1_released/,r_1nakbd6,,,
r_1nagdcd,reddit,dareenmahboi,2025-09-07T01:19:34+00:00,"TempoCut â€” Broadcast-style audio/video time compression in Python
Hi all â€” I just released \*\*TempoCut\*\*, a Python project that recreates broadcast-style time compression (like the systems TV networks used to squeeze shows into fixed time slots).



\### What My Project Does

\- Compresses video runtimes while keeping audio/video/subtitles in sync

\- Audio â€œskippyâ€ compression with crossfade blending (stereo + 5.1)

\- DTW-based video retiming at 59.94p with micro-smear blending

\- Exports Premiere Pro markers for editors

\- Automatic subtitle retiming using warp maps

\- Includes a one-click batch workflow for Windows



Repo: https://github.com/AfvFan99/TempoCut



\### Target Audience

TempoCut is for:

\- Hobbyists and pros curious about how broadcast time-tailoring works

\- Editors who want to experiment with time compression outside of proprietary hardware

\- Researchers or students interested in DSP / dynamic time warping in Python



This is not intended for mission-critical production broadcasting, but itâ€™s close to what real networks used.



\### Comparison

\- Professional solutions (like Prime Image Time Tailor) are \*\*expensive, closed-source, and hardware-based\*\*.  

\- TempoCut is \*\*free, open-source, and Python-based\*\* â€” accessible to anyone.  

\- While simple FFmpeg speed changes distort pitch or cause sync drift, TempoCut mimics broadcast-style micro-skips with far fewer artifacts.  



Would love feedback â€” especially on DSP choices, performance, and making it more portable for Linux/Mac users. ðŸš€

",Python,4,https://www.reddit.com/r/Python/comments/1nagdcd/tempocut_broadcaststyle_audiovideo_time/,r_1nagdcd,,,
r_1nag19u,reddit,poopatroopa3,2025-09-07T01:02:47+00:00,"ensures: simple Design by Contract
* **What My Project Does**

There are a few other packages for this, but I decided to make one that is simple, readable, accepts arbitrary functions, and uses the Result type from functional programming. You can find more details in the readme: [https://github.com/brunodantas/ensures](https://github.com/brunodantas/ensures)

>ensures is a simple Python package that implements the idea of Design by Contract described in the Pragmatic Paranoia chapter of The Pragmatic Programmer. That's the chapter where they say you should trust nobody, not even yourself.

* **Target Audience**Â (e.g., Is it meant for production, just a toy project, etc.)

Anyone interested in ~~paranoia~~ decorating functions with precondition functions etc and use a Functional data structure in the process.

I plan to add pytest tests to make this more production-ready. Any feedback is welcome.

* **Comparison**Â (A brief comparison explaining how it differs from existing alternatives.)

None of the alternatives I found seem to implement arbitrary functions plus the Result type, while being simple and readable.

But some of the alternatives are icontract, contracts, deal. Each with varying levels of the above.",Python,27,https://www.reddit.com/r/Python/comments/1nag19u/ensures_simple_design_by_contract/,r_1nag19u,,,
r_1naeqh3,reddit,AutoModerator,2025-09-07T00:00:31+00:00,"Sunday Daily Thread: What's everyone working on this week?
# Weekly Thread: What's Everyone Working On This Week? ðŸ› ï¸

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! ðŸŒŸ",Python,2,https://www.reddit.com/r/Python/comments/1naeqh3/sunday_daily_thread_whats_everyone_working_on/,r_1naeqh3,,,
r_1naeauz,reddit,caudor,2025-09-06T23:40:02+00:00,"Another free Python 3 Tkinter Book
If you are interested, you can click the top link on my landing page and download my eBook, ""Tkinter in Python 3, De-mystified"" for free:Â [https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)

I recently gave away a Beginner's Python Book and that went really well

So I hope this 150 page pdf will be useful for someone interested in Tkinter in Python. Since it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well.  The link will download all 3 as a zip file.  No donations will be requested. Only info needed is an email address to get the download link.",Python,3,https://www.reddit.com/r/Python/comments/1naeauz/another_free_python_3_tkinter_book/,r_1naeauz,,,
r_1na9zkr,reddit,thought_terror,2025-09-06T20:31:17+00:00,"I used Python and pdfplumber to build an agentic system for analyzing arXiv papers
Hey guys, I wanted to share a project I've been working on,Â arxiv-agent. It's an open-source tool built entirely in Python

Live Demo (Hugging Face Spaces): [https://huggingface.co/spaces/midnightoatmeal/arxiv-agent](https://huggingface.co/spaces/midnightoatmeal/arxiv-agent)

Code (GitHub):Â [https://github.com/midnightoatmeal/arxiv-agent](https://github.com/midnightoatmeal/arxiv-agent)

**What My Project Does**

arxiv-agent is an agentic AI system that ingests an academic paper directly from an arXiv ID and then stages a structured, cited debate about its claims. It uses three distinct AI personas: an Optimist, a Skeptic, and an Ethicist, to analyze the paper's strengths, weaknesses, and broader implications. The pipeline is built usingÂ `requests`Â to fetch the paper andÂ `pdfplumber`Â to parse the text, which is then orchestrated through an LLM to generate the debate.

**Target Audience**

Right now, it's primarily aÂ portfolio project and a proof-of-concept. It's designed forÂ researchers, students, and ML engineersÂ who want a quick, multi-faceted overview of a new paper beyond a simple summary. While it's a ""toy project"" in its current form, the underlying agentic framework could be adapted for more production-oriented use cases like internal research analysis or due diligence.

**Comparison**

Most existing tools for paper analysis focus onÂ single-perspective summarizationÂ (like TLDR generation) orÂ keyword extraction. The main difference withÂ arxiv-agentÂ is itsÂ multi-perspective, dialectical approach. Instead of just telling youÂ *what*Â the paper says, it modelsÂ *how to think about*Â the paper by staging a debate. This helps uncover potential biases, risks, and innovative ideas that a standard summary might miss. It also focuses on grounding its claims in the source text to reduce hallucination.

Would love any feedback! thank you checking it out!",Python,0,https://www.reddit.com/r/Python/comments/1na9zkr/i_used_python_and_pdfplumber_to_build_an_agentic/,r_1na9zkr,,,
r_1na9od6,reddit,benbenbang,2025-09-06T20:18:17+00:00,"Built a free VS Code extension for Python dependencies - no more PyPI tab switching
Tired of switching to PyPI tabs to check package versions?

Just released **Tombo** \- brings PyPI directly into VS Code:

**What it does (complements your existing workflow):**

* uv/poetry handle installation â†’ Tombo handles version selection
* Hover `requests` â†’ see ALL versions + Python compatibility
* Type `numpy>=` â†’ intelligent version suggestions for your project
* Perfect for big projects (10+ deps) - no more version hunting
* Then let uv/poetry create the lock files

**Demo in 10 seconds:**

1. Open any Python project
2. Type `django>=`
3. Get instant version suggestions
4. Hover packages for release info

**Installation:** VS Code â†’ Search ""Tombo"" â†’ Install

**Free & open source** \- no tracking, no accounts, just works.

â­ **Star the project** if you find it useful: [https://github.com/benbenbang/tombo](https://github.com/benbenbang/tombo)

VS Code Marketplace: [https://marketplace.visualstudio.com/items?itemName=benbenbang.tombo](https://marketplace.visualstudio.com/items?itemName=benbenbang.tombo)

Documentation: [https://benbenbang.github.io/tombo/](https://benbenbang.github.io/tombo/)

Anyone else tired of manual PyPI lookups? ðŸ¤¦â€â™‚ï¸",Python,40,https://www.reddit.com/r/Python/comments/1na9od6/built_a_free_vs_code_extension_for_python/,r_1na9od6,,,
r_1na6xqg,reddit,fran_m99,2025-09-06T18:28:20+00:00,"Simple Keyboard Count Tracker
**What My Project Does:**  
This simple Python script tracks your keyboard in the background and logs every key you press. You can track your total keystrokes, see which keys you hit the most, and all that with a fancy keyboard display with a color gradient.

Whether youâ€™re curious about your productivity, want to visualize your keyboard usage, or just enjoy quirky data experiments

**Target Audience:**  
People interested in knowing more about their productivity, or just data enthusiasts like me :)

**Comparison:**  
I Couldn't find a similar lightweight tool that works in the background and is easy to use, so I decided to build my own using Python.

**Repo Link:**  
[https://github.com/Franm99/keyboard-tracker](https://github.com/Franm99/keyboard-tracker)

Would love feedback, suggestions, or improvements from the community!",Python,3,https://www.reddit.com/r/Python/comments/1na6xqg/simple_keyboard_count_tracker/,r_1na6xqg,,,
r_1na61l2,reddit,initCMD,2025-09-06T17:53:19+00:00,"Ducky, my open-source networking & security toolkit for Network Engineers, Sysadmins, and Pentester
Hey everyone, For a long time, I've been frustrated with having to switch between a dozen different apps for my networking tasks PuTTY for SSH, a separate port scanner, a subnet calculator, etc.

To solve this, I builtÂ **Ducky**, a free and open-source, all-in-one toolkit that combines these essential tools into one clean, tabbed interface.

**What it does:**

* **Multi-Protocol Tabbed Terminal:**Â Full support for SSH, Telnet, and Serial (COM) connections.
* **Network Discovery:**Â An ARP scanner to find live hosts on your local network and a visual Topology Mapper.
* **Essential Tools:**Â It also includes a Port Scanner, CVE Vulnerability Lookup, Hash Cracker, and other handy utilities.

**Target Audience:**  
I built this for anyone who works with networks or systems, including:

* **Network Engineers & Sysadmins:**Â For managing routers, switches, and servers without juggling multiple windows.
* **Cybersecurity Professionals & Students:**Â A great all-in-one tool for pentesting, vulnerability checks (CVE), and learning.
* **Homelabbers & Tech Enthusiasts:**Â The perfect command center for managing your home lab setup.
* **Fellow Python Developers:**Â To see a practical desktop application built withÂ **PySide6**.

**How you can help:**  
The project is 100% open-source, and I'm actively looking for contributors and feedback!

* **Report bugs or issues:**Â Find something that doesn't work right? Please open an issue on GitHub.
* **Suggest enhancements:**Â Have an idea for a new tool or an improvement? Let's discuss it!
* **Contribute code:**Â Pull Requests are always welcome.
* **GitHub Repo (Source Code & Issues):**Â [https://github.com/thecmdguy/Ducky](https://github.com/thecmdguy/Ducky)
* **Project Homepage:**Â [https://ducky.ge/](https://ducky.ge/)

Thanks for taking a look!",Python,59,https://www.reddit.com/r/Python/comments/1na61l2/ducky_my_opensource_networking_security_toolkit/,r_1na61l2,,,
r_1na5fk2,reddit,AgitatedFunction3721,2025-09-06T17:29:34+00:00,"From Stress to Success: Load Testing Python Apps â€“ Open Source Example
**What My Project Does:**  
This project demonstrates **load testing Python applications** and **visualizing performance metrics**. It uses a sample Python app, Locust for stress testing, Prometheus for metrics collection, and Grafana for dashboards. Itâ€™s designed to give a hands-on example of how to simulate load and understand app performance.

**Target Audience:**  
Developers and Python enthusiasts who want to learn or experiment with load testing and performance visualization. Itâ€™s meant as a **learning tool and reference**, not a production-ready system.

**Comparison:**  
Unlike generic tutorials or scattered examples online, this repo bundles everything togetherâ€”app, load scripts, Prometheus, and Grafana dashboardsâ€”so you can **see the full workflow from stress testing to visualization in one place**.

**Repo Link:**  
[https://github.com/Alleny244/locust-grafana-prometheus](https://github.com/Alleny244/locust-grafana-prometheus)

Would love feedback, suggestions, or improvements from the community!",Python,14,https://www.reddit.com/r/Python/comments/1na5fk2/from_stress_to_success_load_testing_python_apps/,r_1na5fk2,,,
r_1na5fiq,reddit,RRTheGuy,2025-09-06T17:29:31+00:00,"A tool to create a database of all the items of a directory
# What my project does

My project creates a database of all the items and sub-items of a directory, including the name, size, the number of items and much more.

And we can use it to quickly extract the files/items that takes the most of place, or also have the most of items, and also have a timeline of all items sorted by creation date or modification date.

# Target Audience

For anyone who want to determine the files that takes the most of place in a folder, or have the most items (useful for OneDrive problems)

For anyone who want to manipulate files metadata on their own.

For anyone who want to have a timeline of all their files, items and sub-items.

I made this project for myself, and I hope it will help others.

# Comparison

As said before, to be honest, I didn't really compare to others tools because I think sometimes comparison can kill confidence or joy and that we should mind our own business with our ideas.

I don't even know if there's already existing tools specialized for that, maybe there is.

And I'm pretty sure my project is unique because I did it myself, with my own inspiration and my own experience.

So if anyone know or find a tool that looks like mine or with the same purpose, feel free to share, it would be a big coincidence.

# Conclusion

Here's the project source code:Â [https://github.com/RadoTheProgrammer/files-db](https://github.com/RadoTheProgrammer/files-db)

I did the best that I could so I hope it worth it. Feel free to share what you think about it.

Edit: It seems like people didn't like so I made this repository private and I'll see what I can do about it",Python,0,https://www.reddit.com/r/Python/comments/1na5fiq/a_tool_to_create_a_database_of_all_the_items_of_a/,r_1na5fiq,,,
r_1na21zu,reddit,Important-Sound2614,2025-09-06T15:14:15+00:00,"JollyRadio - A web based radio
**What My Project Does** 

JollyRadio is a web based, simple radio where you can find lots of live streams. It's designed to be easy to navigate and have less extra fluff. 

**Target Audience** 

JollyRadio is for people who want to listen to radio! It has basic filtering to filter out bad stuff, but you may still need to know what to do and not do. 

**Comparison** 

Compared to other web based radios, JollyRadio is designed to be local-focused and more minimalistic. There are three sections, exploring, local stations and searching for stations. It is better if you want a easy, minimal interface.

**Technical Explanation**

JollyRadio is written in Python (Flask) with HTML (Bootstrap). I'm new to programming, so please don't expect a perfect product. It uses the RadioBrowser API to find the radio stations.

**Links**

GitHub Link: [https://github.com/SeafoodStudios/JollyRadio](https://github.com/SeafoodStudios/JollyRadio)

Radio Link: [https://tryjollyradio.seafoodstudios.com/](https://tryjollyradio.seafoodstudios.com/)",Python,14,https://www.reddit.com/r/Python/comments/1na21zu/jollyradio_a_web_based_radio/,r_1na21zu,,,
r_1n9urtc,reddit,StreetTeacher2,2025-09-06T08:58:25+00:00,"Automating Power Supply Measurements with PyVisa & Pytest
**Target Audience:**

* R&D Development & Test Enginners
* Electrical Engineering Students
* Python Automation Experts

**What My Project Does:**

I created a small python library: [pypm-test](https://github.com/ammarkh95/pypm-test) which could be used for automating measurements with the pictured instruments.

You could also use it as reference to automate similar functions with your available instruments. The library is Python based and makes use of [PyVisa ](https://pyvisa.readthedocs.io/en/latest/)library for communction with electronic eqipment supporting [SCPI ](https://www.ivifoundation.org/About-IVI/scpi.html)standard.

The library also includes some [pytest-fixtures](https://docs.pytest.org/en/stable/explanation/fixtures.html) which makes it nice to use in automated testing environment.

Below I share summary of the hardware used and developed python library as well as some example results for an automated DC-DC converter measurements. You can find all the details in my [blog post](https://ak-experiments.blogspot.com/2025/09/automating-power-supply-measurements.html)

**Hardware:**

I had access to the following instruments:

[Keysight U3606B](https://www.keysight.com/us/en/support/U3606B/multimeter-dc-power-supply.html): Combination of a 5.5 digit digital multimeter and 30-W power supply in a single unit  
[Keysight U2723A:](https://www.keysight.com/us/en/products/source-measure-units-smu/u2722a-u2723a-usb-modular-source-measure-units-smu.html) Modular source measure unit (SMU) Four-quadrant operation (Â± 120 mA/Â± 20 V)

**Software:**

The developd library contain wrapper classes that implement the control and measurement functions of the above instruments.

The exposed functions by the SCPI interface are normally documented in the programming manuals of the equipment published online. So it was just a matter of going through the manuals to get the required [SCPI](https://www.ivifoundation.org/About-IVI/scpi.html) commands / queries for a given instrument function and then sending it over to the instrument using [PyVisa](https://pyvisa.readthedocs.io/en/latest/) write and query functions.

**Example:**

A classical example application with a power supply and source measure unit is to evaluate the efficiency of DC-DC conversion for a given system. It is also a nice candiate ""parameteric study"" for automation to see how does the output power compares to the input power (i.e. effeciency) at different inputs voltges / sink currents. You can view the code behind similar test directly from my repo [here](https://github.com/ammarkh95/pypm-test/blob/f5434110e7dffd4adeff23f09d9ca10877fc1dbb/testing/example_tests/test_dc_dc_converter.py#L84)",Python,8,https://www.reddit.com/r/Python/comments/1n9urtc/automating_power_supply_measurements_with_pyvisa/,r_1n9urtc,,,
r_1n9qlkv,reddit,Ill-Pirate4249,2025-09-06T04:41:33+00:00,"What are some non-AI tools/extensions which have really boosted your work life or made life easier?
It can be an extension or a CLI tool or something else, My work mainly involves in developing managing mid sized python applications deployed over aws. I mostly work through cursor and agents have been decently useful but these days all the development on programming tools seems to be about AI integration. Is there something that people here have been using that's come out in last few years and has made serious impact in how you do things? Can be open source or not, anything goes it just shouldn't be something AI or a framework.",Python,46,https://www.reddit.com/r/Python/comments/1n9qlkv/what_are_some_nonai_toolsextensions_which_have/,r_1n9qlkv,,,
r_1n9q2p1,reddit,Beginning_Task_5515,2025-09-06T04:11:59+00:00,"Python IDLE's practical upgrade: file tree, tabbed editing, console view using only stdlib+tkinter.
I was tinkering with IDLE and wondered: what if it had just a few modern quality-of-life improvements, but implemented entirely with Pythonâ€™s standard library (so no extra dependencies, just `tkinter`)?

Specifically:

* File tree view (browse/open files inside the IDE itself)
* Tabbed editing (each opened file gets its own tab)
* Console view embedded alongside tabs
* Still dead-simple, light, and portable

The idea isnâ€™t to compete with full IDEs like PyCharm or VS Code, but to provide a *corporate-safe*, zero-install, batteries-included IDE that works even on fenced machines where you canâ€™t pull in external editors or packages.

Think of it as â€œIDLE-plusâ€ â€” familiar, lightweight, but with just enough features to make small/medium coding tasks more pleasant.

Iâ€™m curious:

* Would people here find this genuinely useful?
* Do fenced corporate environments still rely on IDLE as the only safe option?
* Is it worth polishing into a small open-source project (maybe even proposing as an official IDLE enhancement)?

What do you think â€” niche toy, or something that could actually see adoption?",Python,5,https://www.reddit.com/r/Python/comments/1n9q2p1/python_idles_practical_upgrade_file_tree_tabbed/,r_1n9q2p1,,,
r_1n9ov57,reddit,Educational-Comb4728,2025-09-06T03:07:42+00:00,"Simple Python expression that does complex things?
First time I saw `a[::-1]` to invert the list `a`, I was blown away. 

`a, b = b, a` which swaps two variables (without temp variables in between) is also quite elegant. 

  
What's your favorite example?",Python,281,https://www.reddit.com/r/Python/comments/1n9ov57/simple_python_expression_that_does_complex_things/,r_1n9ov57,,,
r_1n9l3dr,reddit,AutoModerator,2025-09-06T00:00:30+00:00,"Saturday Daily Thread: Resource Request and Sharing! Daily Thread
# Weekly Thread: Resource Request and Sharing ðŸ“š

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! ðŸŒŸ",Python,1,https://www.reddit.com/r/Python/comments/1n9l3dr/saturday_daily_thread_resource_request_and/,r_1n9l3dr,,,
r_1n9kste,reddit,BeamMeUpBiscotti,2025-09-05T23:46:17+00:00,"Python Type System and Tooling Survey 2025 (From Meta & JetBrains)
As mentioned in the title, this survey was developed by Meta & Jetbrains w/ community input to collect opinions around Python's type system and type-related tooling.

> The goal of this survey is to gain insights into the tools and practices you use (if any!), the challenges you face, and how you stay updated on new features. Your responses will help the Python typing community identify common blockers, improve resources, and enhance the overall experience of using Python's type system. Even if you have never actively used type hints in your code, your thoughts are still valuable and we want to hear from you.

Take the survey [here](https://docs.google.com/forms/d/e/1FAIpQLSeOFkLutxMLqsU6GPe60OJFYVN699vqjXPtuvUoxbz108eDWQ/viewform).

Original LinkedIn posts (so you know it's legit):

[Meta Open Source](https://www.linkedin.com/posts/meta-open-source_python-type-system-and-tooling-survey-2025-activity-7369400929546092548-A0hh?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB9aSUsBqmxSbrhoW2URuDnxCgS5eVD1AS0)

[Python Software Foundation](https://www.linkedin.com/posts/thepsf_python-type-system-and-tooling-survey-2025-activity-7368968760252059648-ICjo?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAB9aSUsBqmxSbrhoW2URuDnxCgS5eVD1AS0)

",Python,17,https://www.reddit.com/r/Python/comments/1n9kste/python_type_system_and_tooling_survey_2025_from/,r_1n9kste,,,
r_1n9i3v4,reddit,Lumfort,2025-09-05T21:49:32+00:00,"ÐŸÐ¾Ð¼Ð¾Ð³Ð¸Ñ‚Ðµ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð· ÑƒÑ‡ÐµÐ±Ð½Ð¸ÐºÐ°.
Â«ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ , Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð²Ð²Ð¾Ð´Ð¸Ñ‚ Ñ†ÐµÐ»Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾, Ð° Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð² ÑÑ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ñ†Ð¸Ñ„Ñ€ 0,1,2,3,4,5,6,7,8,9.Â»
Ð£Ñ‡ÐµÐ±Ð½Ð¸Ðº Ð’Ð°ÑÐ¸Ð»ÑŒÐµÐ² Ð.Ð. ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° ÐŸÐ°Ð¹Ñ‚Ð¾Ð½ Ð² Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ñ… Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. ",Python,0,https://www.reddit.com/r/Python/comments/1n9i3v4/Ð¿Ð¾Ð¼Ð¾Ð³Ð¸Ñ‚Ðµ_Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ_Ð·Ð°Ð´Ð°Ð½Ð¸Ðµ_Ð¸Ð·_ÑƒÑ‡ÐµÐ±Ð½Ð¸ÐºÐ°/,r_1n9i3v4,,,
r_1n9fb2a,reddit,Flimsy_Bison_4215,2025-09-05T19:57:24+00:00,"Giving up on coding for the third time.
Context: I am 23 and I have tried to learn coding thrice, once in school, then undergrad, and last year. 

Python each time.

I make some progress, but soon I lose all interest. Not because of difficulty, but it just doesnâ€™t capture my attention. 

I know coding is gonna be more or less essential soon and I have been trying to get into it because it plays well with my field (i.e. Finance - and yes I have tried an interdisciplinary approach) 

But I just donâ€™t enjoy it. Any tips on how to make it more interesting as a learning process? ",Python,0,https://www.reddit.com/r/Python/comments/1n9fb2a/giving_up_on_coding_for_the_third_time/,r_1n9fb2a,,,
r_1n9ew7e,reddit,doktorfuturee,2025-09-05T19:40:50+00:00,"am i slow at coding ? should i afraid ?
I started coding like 3 days ago specifically to python. First i looked to  a youtube  video about basics and then started to exercises in a site called genepy. It was easy at first but now i am at the mid level and spent 2.5 hours to code 'from\_roman\_numeral' function. I wanted to ask you is that slow for that code because after i finished the code looked so small to me. am i slow or it is normal?",Python,0,https://www.reddit.com/r/Python/comments/1n9ew7e/am_i_slow_at_coding_should_i_afraid/,r_1n9ew7e,,,
r_1n9d8oj,reddit,caudor,2025-09-05T18:35:37+00:00,"I thought I'd give away my Python eBook (pdf) for free.
If you are interested, you can click the top link on my landing page and download my eBook, ""Programming Basics in Python 3"" for free: [https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)

I hope this 99 page pdf will be useful for someone interested in Python.  No donations will be requested.  Only info needed is an email address to get the download link.",Python,5,https://www.reddit.com/r/Python/comments/1n9d8oj/i_thought_id_give_away_my_python_ebook_pdf_for/,r_1n9d8oj,,,
r_1n98zq3,reddit,Wonderful-Reserve728,2025-09-05T15:53:54+00:00,"Winion: a Linux-like command interpreter for Windows with built-in package manager (Coming September
Salut tout le monde,

Je suis en train de dÃ©velopperÂ **Winion**, un nouvel interprÃ©teur de ligne de commande pour Windows qui se comporte comme un terminal Linux. Il est livrÃ© avec :

* Un gestionnaire de paquets intÃ©grÃ© pour une installation facile des outils
* Des commandes et des flux de travail de style Linux (`apt`, etc.)
* Prise en charge des scripts et de l'automatisation similaire aux shells Linux

Il est conÃ§u pour les utilisateurs avancÃ©s de Windows qui veulent une expÃ©rience de terminal de type Linux sans quitter Windows.

**Date de sortie :**Â Septembre 2025 Je recherche des retours et des testeurs prÃ©coces pour l'amÃ©liorer avant le lancement.

Des captures d'Ã©cran et des GIF de son fonctionnement sont disponibles dans le dÃ©pÃ´t.

GitHub :Â [https://github.com/JuanForge/Winion](https://github.com/JuanForge/Winion)

J'adorerais savoir ce que vous en pensez !

[https://youtu.be/dEWdlBmZ1\_o](https://youtu.be/dEWdlBmZ1_o)",Python,0,https://www.reddit.com/r/Python/comments/1n98zq3/winion_a_linuxlike_command_interpreter_for/,r_1n98zq3,,,
r_1n97j3w,reddit,Square-Speaker2033,2025-09-05T14:57:12+00:00,"Free GPU options for training LLaMA 7B?
Hi,

Iâ€™m looking for concrete experiences on a mix of hardware resources and model training logic.

Goal: train or adapt a LLaMA 7B model (no QLoRA quantization, full precision) for a very specific use case. The purpose is not creative chatting but to build a model that can understand natural language instructions and reliably map them to predefined system actions. For example:

if I say â€œshut down the PCâ€ â†’ it should map directly to the correct command without inventing anything,

if I say â€œcreate a file called new folderâ€ â†’ it should trigger the correct action,

it should only pick from a database of known actions and nothing else.


Constraints / challenges:

I need a free or very low-cost environment with enough GPU power (Colab, community servers, credits, etc.) to actually handle a 7B model in full precision.

If full 7B without quantization is unrealistic, what are the most practical alternatives (smaller models, different architectures) while keeping the text â†’ action reliability?

How to add conversation memory so the model can keep track of context across multiple commands?

Iâ€™m especially interested in ready-to-use setups that people have already tested (not just theoretical advice).


In short: has anyone successfully trained or used a model in this setup (natural language â†’ action database, no hallucinations) with free or accessible resources? Which tools/environments would you recommend?

Thanks in advance for any insights.
",Python,0,https://www.reddit.com/r/Python/comments/1n97j3w/free_gpu_options_for_training_llama_7b/,r_1n97j3w,,,
r_1n96z9e,reddit,bgdnandrew,2025-09-05T14:35:53+00:00,"Python equivalent for Mark comments (Swift)
Is there such thing? Paired with XCode's jump bar, I qucikly grew to love the Mark-type comments and how they help you to quickly naviagte and understand someone else's code.",Python,4,https://www.reddit.com/r/Python/comments/1n96z9e/python_equivalent_for_mark_comments_swift/,r_1n96z9e,,,
r_1n95jwd,reddit,sebst,2025-09-05T13:39:25+00:00,"AWS for Python devs - made simple
**What is Stelvio?**  
Stelvio is a Python framework for managing and deploying AWS infrastructure. Instead of writing YAML, JSON, or HCL, you define your infrastructure in **pure Python**. The framework provides **smart defaults** for networking, IAM, and security so you can focus on your application logic rather than boilerplate setup.

With the `stlv` CLI, you can go from zero to a working AWS environment in seconds, without heavy configuration.

**What My Project Does**  
Stelvio lets Python developers:

* Spin up AWS resources (e.g. compute, storage, networking) using Python code.
* Deploy isolated environments (personal or team-based) with a single command.
* Skip most of the manual setup thanks to opinionated defaults for IAM roles, VPCs, and security groups.

The goal is to make cloud deployments **approachable to Python developers who arenâ€™t infrastructure experts**.

**Target Audience**

* **Python developers** who want to deploy applications to AWS without learning all of Terraform or CloudFormation.
* **Small teams and projects** that need quick, reproducible environments.
* Itâ€™s designed for **real-world usage**, not just as a toy project, but itâ€™s still early-stage and evolving rapidly.

**Comparison to Alternatives**

* Compared to **Terraform**: Stelvio is Python-native, so you donâ€™t need to learn HCL or use external templating.
* Compared to **AWS CDK**: Stelvio emphasizes **zero setup** and **smart defaults**. CDK is very flexible but requires more boilerplate and AWS-specific expertise.
* Compared to **Pulumi**: Stelvio is lighter-weight and focuses narrowly on AWS, aiming to reduce complexity rather than cover all clouds.

**Links**

* GitHub: [https://github.com/michal-stlv/stelvio](https://github.com/michal-stlv/stelvio?utm_source=chatgpt.com)
* Website: [https://stelvio.dev](https://stelvio.dev?utm_source=chatgpt.com)",Python,13,https://www.reddit.com/r/Python/comments/1n95jwd/aws_for_python_devs_made_simple/,r_1n95jwd,,,
r_1n95gzi,reddit,Pitiful-Ad8345,2025-09-05T13:35:59+00:00,"[Showcase] Modernized Gower Distance Package - 20% Faster, GPU Support, sklearn Integration
**What My Project Does**
    
[Gower Express](https://github.com/momonga-ml/gower-express) is a modernized Python implementation of Gower distance calculation for mixed-type data (categorical + numerical). It computes pairwise distances between records containing both categorical and numerical features without requiring preprocessing or encoding.
    

**Target Audience**
    
It's for data scientists and ML engineers working with uses for customer segmentation, mixed clinical data, recommendation with tabular data, and clustering tasks.
    
This replaces the unmaintained `gower` package (last updated 2022) with modern Python standards.
    
**Comparison**
    
Unlike the original `gower` package (unmaintained since 2022), this implementation offers 20% better performance via Numba JIT, GPU acceleration through CuPy (3-5x speedup), and native scikit-learn integration. Compared to UMAP/t-SNE embeddings, Gower provides deterministic results without hyperparameter tuning while maintaining full interpretability of distance calculations.

    
**Installation & Usage**
    
```python
pip install gower_exp[gpu,sklearn]
```
    
```python
import gower_exp as gower
from sklearn.cluster import AgglomerativeClustering
    
# Mixed data (categorical + numerical)
distances = gower.gower_matrix(customer_data)
clusters = AgglomerativeClustering(metric='precomputed').fit(distances)
    
# GPU acceleration for large datasets
distances = gower.gower_matrix(big_data, use_gpu=True)
    
# Find top-N similar items (memory-efficient)
similar = gower.gower_topn(target_item, catalog, n=10)
```
    
**Performance**

| Dataset Size | CPU Time | GPU Time | Memory Usage |
|--------------|----------|----------|--------------|
| 1K records   | 0.08s    | 0.05s    | 12MB         |
| 10K records  | 2.1s     | 0.8s     | 180MB        |
| 100K records | 45s      | 12s      | 1.2GB        |
| 1M records   | 18min    | 3.8min   | 8GB          |

Source: https://github.com/momonga-ml/gower-express
    
I built it with Claude Code assistance over a weekend. Happy to answer questions about the implementation or discuss when classical methods outperform modern embeddings!",Python,5,https://www.reddit.com/r/Python/comments/1n95gzi/showcase_modernized_gower_distance_package_20/,r_1n95gzi,,,
r_1n9267v,reddit,tinoomihael,2025-09-05T11:01:47+00:00,"I built a visual component library for instrumentation
Hello everyone,

as Python is growing more and more in industrial field, I decided to create visual component library for instrumentation.

**What My Project Does:**  
A Python library with **40+ visual and non-visual components** for building industrial and lab GUIs. Includes analog instruments, sliders, switches, buttons, graphs, and oscilloscope & logic analyzer widgets (PyVISA-compatible). Components are **highly customizable** and designed with a **retro industrial look**.

**Target Audience:**  
Engineers, scientists, and hobbyists building technical or industrial GUIs. Suitable for both **prototypes and production-ready applications**.

**Comparison / How Itâ€™s Different:**  
Unlike general GUI frameworks, this library is **instrumentation-focused** with ready-made industrial-style meters, gauges, and analyzer componentsâ€”saving development time and providing a consistent professional look.

**Demo:** [Imgur](https://imgur.com/a/0j89hPf?utm_source=chatgpt.com) (Not all components are being shown, just a small sneek-peak)  
**GitHub Repo:** [Thales](https://github.com/tino-posedi/Thales?utm_source=chatgpt.com) (private, still in progress)

**Feedback Questions:**

* Are there components youâ€™d find particularly useful for industrial or lab GUIs?
* Is the retro industrial style appealing, or would you prefer alternative themes?
* Any suggestions for improving customization, usability, or performance?",Python,63,https://www.reddit.com/r/Python/comments/1n9267v/i_built_a_visual_component_library_for/,r_1n9267v,,,
r_1n91acl,reddit,Thinker_Assignment,2025-09-05T10:11:57+00:00,"Showcase: I co-created dlt, an open-source Python library that lets you build data pipelines in minu
As a 10y+ data engineering professional, I got tired of the boilerplate and complexity required to load data from messy APIs and files into structured destinations. So, with a team, I built `dlt` to make data loading ridiculously simple for anyone who knows Python.

**Features:**

* âž¡ï¸ **Load anything with Schema Evolution:** Easily pull data from any API, database, or file (JSON, CSV, etc.) and load it into destinations like DuckDB, BigQuery, Snowflake, and more, handling types and nested data flawlessly.
* âž¡ï¸ **No more schema headaches:** `dlt` automatically creates and maintains your database tables. If your source data changes, the schema adapts on its own.
* âž¡ï¸ **Just write Python:** No YAML, no complex configurations. If you can write a Python function, you can build a production-ready data pipeline.
* âž¡ï¸ **Scales with you:** Start with a simple script and scale up to handle millions of records without changing your code. It's built for both quick experiments and robust production workflows.
* âž¡ï¸ **Incremental loading solved:** Easily keep your destination in sync with your source by loading only new data, without the complex state management.
* âž¡ï¸ **Easily extendible:** `dlt` is built to be modular. You can add new sources, customize data transformations, and deploy anywhere.

**Link to repo:**[https://github.com/dlt-hub/dlt](https://github.com/dlt-hub/dlt)

Let us know what you think! We're always looking for feedback and contributors.

# What My Project Does

`dlt` is an open-source Python library that simplifies the creation of robust and scalable data pipelines. It automates the most painful parts of Extract, Transform, Load (ETL) processes, particularly schema inference and evolution. Users can write simple Python scripts to extract data from various sources, and `dlt` handles the complex work of normalizing that data and loading it efficiently into a structured destination, ensuring the target schema always matches the source data.

# Target Audience

The tool is for **data scientists, analysts, and Python developers** who need to move data for analysis, machine learning, or operational dashboards but don't want to become full-time data engineers. It's perfect for anyone who wants to build production-ready, maintainable data pipelines without the steep learning curve of heavyweight orchestration tools like Airflow or writing extensive custom code. Itâ€™s suitable for everything from personal projects to enterprise-level deployments.

# Comparison (how it differs from existing alternatives)

Unlike complex frameworks such as **Airflow** or **Dagster**, which are primarily orchestrators that require significant setup, `dlt` is a lightweight library focused purely on the ""load"" part of the data pipeline. Compared to writing **custom Python scripts** using libraries like `SQLAlchemy` and `pandas`, `dlt` abstracts away tedious tasks like schema management, data normalization, and incremental loading logic. This allows developers to create declarative and resilient pipelines with far less code, reducing development time and maintenance overhead.",Python,73,https://www.reddit.com/r/Python/comments/1n91acl/showcase_i_cocreated_dlt_an_opensource_python/,r_1n91acl,,,
r_1n90ss5,reddit,mattdocumatt,2025-09-05T09:41:42+00:00,"Sphinx Docs Translation: tutorial and template
Localizing documentation, manuals, or help is a challenging task. But itâ€™s also an area whereÂ Sphinx documentation generatorÂ really shines.  I wrote [tutorial how to localize Sphinx docs](https://documatt.com/blog/25/sphinx-translation-tutorial/) and [sample repository](https://github.com/liborjelinek/sphinx-doc-i18n-example) to showcase a full localization workflow on a minimal yet realistic Sphinx documentation example. If youâ€™re maintaining docs in multiple languages, this might help you get started.",Python,3,https://www.reddit.com/r/Python/comments/1n90ss5/sphinx_docs_translation_tutorial_and_template/,r_1n90ss5,,,
r_1n8ryo5,reddit,Druber13,2025-09-05T01:16:46+00:00,"Highly relevant moderation rant
Iâ€™ve tried several times to ask questions or get advice here and things have been flagged, reported, and removed. Itâ€™s never been why isnâ€™t my hello world working or other super basic things. 

I think this really needs to be adjusted as most online searches are useless now days. The amount of AI garbage you get when looking stuff up is out of hand. Stack overflow is about useless for anything Iâ€™ve looked at recently. 

Leaving folk looking for somewhere like this to find real people that can actually help or offer useful opinions. In fact typing this is telling me itâ€™s probably going to be flaggedâ€¦. It feels like this is defending the purpose of this subreddit and any community that can be built. ",Python,0,https://www.reddit.com/r/Python/comments/1n8ryo5/highly_relevant_moderation_rant/,r_1n8ryo5,,,
r_1n8roey,reddit,Batkid_760,2025-09-05T01:03:05+00:00,"What Server to use for YOLOv11.
Hello,

I am looking for a compute server systems that uses YOLOv11 on high resolution Hikvision IP cameras. Rough 20-25 cameras will be installed for object detection and will need a high GPU and CPU. What do you guys recommend? ",Python,0,https://www.reddit.com/r/Python/comments/1n8roey/what_server_to_use_for_yolov11/,r_1n8roey,,,
r_1n8qcam,reddit,AutoModerator,2025-09-05T00:00:47+00:00,"Friday Daily Thread: r/Python Meta and Free-Talk Fridays
# Weekly Thread: Meta Discussions and Free Talk Friday ðŸŽ™ï¸

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! ðŸŒŸ",Python,2,https://www.reddit.com/r/Python/comments/1n8qcam/friday_daily_thread_rpython_meta_and_freetalk/,r_1n8qcam,,,
r_1n8lp06,reddit,Trustycoat,2025-09-04T20:45:49+00:00,"Notes for Python
I have completed the course for Python and now want to have a complete notes. I made my notes but itâ€™s lost now.
A complete structured note would be very helpful.
Hereâ€™s the course that I did:-
100 days of python by code with harry
https://youtube.com/playlist?list=PLu0W_9lII9agwh1XjRt242xIpHhPT2llg&si=8P7E4j1RuSqOiVRI",Python,0,https://www.reddit.com/r/Python/comments/1n8lp06/notes_for_python/,r_1n8lp06,,,
r_1n8jasr,reddit,Avienir,2025-09-04T19:13:21+00:00,"I'm building local, open-source, fast minimal, and extendible python RAG library and CLI tool
  
I got tired of overengineered and bloated AI libraries and needed something to prototype local RAG apps quickly so I decided to make my own library,  
Features:  
âž¡ï¸ Get to prototyping local RAG applications in seconds: uvx rocketrag prepare & uv rocketrag ask is all you need  
âž¡ï¸ CLI first interface, you can even visualize embeddings in your terminal  
âž¡ï¸ Native llama.cpp bindings - no Ollama bullshit  
âž¡ï¸ Ready to use minimalistic web app with chat, vectors visualization and browsing documentsâž¡ï¸ Minimal footprint: milvus-lite, llama.cpp, kreuzberg, simple html web app  
âž¡ï¸ Tiny but powerful - use any chucking method from chonkie, any LLM with .gguf provided and any embedding model from sentence-transformers  
âž¡ï¸ Easily extendible - implement your own document loaders, chunkers and BDs, contributions welcome!  
Link to repo: [https://github.com/TheLion-ai/RocketRAG](https://github.com/TheLion-ai/RocketRAG)  
Let me know what you think. If anybody wants to collaborate and contribute DM me or just open a PR!  
  
**What My Project Does**  
RocketRAG is a high-performance Retrieval-Augmented Generation (RAG) library that loads documents (PDF/TXT/MDâ€¦), performs semantic chunking, indexes embeddings into a fast vector DB, then serves answers via a local LLM. It provides both a CLI and a FastAPI-based web server with OpenAI-compatible `/ask` and streaming endpoints, and is built to prioritize speed, a minimal code footprint, and easy extensibility

**Target Audience**  
Developers and researchers who want a fast, modular RAG stack for local or self-hosted inference (GGUF / llama-cpp-python), and teams who value low-latency document processing and a plug-and-play architecture. Itâ€™s suitable both for experimentation and for production-ready local/offline deployments where performance and customizability matter. 

**Comparison (how it differs from existing alternatives)**  
Unlike heavier, opinionated frameworks, RocketRAG focuses on performance-first building blocks: ultra-fast document loaders (Kreuzberg), semantic chunking (Chonkie/model2vec), Sentence-Transformers embeddings, Milvus Lite for sub-millisecond search, and llama-cpp-python for GGUF inference â€” all in a pluggable architecture with a small footprint. The goal is lower latency and easier swapping of components compared to larger ecosystems, while still offering a nice CLI ",Python,16,https://www.reddit.com/r/Python/comments/1n8jasr/im_building_local_opensource_fast_minimal_and/,r_1n8jasr,,,
r_1n8gzmy,reddit,_Drkshdw_,2025-09-04T17:45:48+00:00,"What is an application?
If I write a hello world print statement in a Python file and that's it, is that considered an application? 

My friend is arguing with me about what an application and a micro service is. I keep saying that micro services are just small applications, and  that even a hello world print in a Python statement is considered an application, but he's saying no. 

Who's right?",Python,0,https://www.reddit.com/r/Python/comments/1n8gzmy/what_is_an_application/,r_1n8gzmy,,,
r_1n8f68b,reddit,PINKINKPEN100,2025-09-04T16:37:21+00:00,"Has anyone here tried using MCP to give Python LLM agents live web access?
Iâ€™ve been experimenting with Model Context Protocol (MCP) in my Python workflows, and it honestly feels like giving an agent a pair of eyes. Normally, the moment you ask an LLM for live data, it either hallucinates, gives outdated info, or makes you copy-paste results manually. With MCP, I was able to fetch URLs in real time, handle JS-heavy pages, and pass structured HTML or Markdown back into Python without babysitting scrapers.

I tried it with the [Crawlbase MCP Server](https://github.com/crawlbase/crawlbase-mcp) since it already works with tools like Claude Desktop and Cursor, and so far itâ€™s been surprisingly smooth. Much less time fighting with proxies and CAPTCHAs, and more time actually building. Thereâ€™s also a [guide](https://crawlbase.com/blog/introducing-crawlbase-mcp-feed-real-time-web-data-to-the-llms/) for Crawlbase MCP Server if you want to try setting it up yourself, but Iâ€™m mostly curious to hear how others are using MCP in their Python projects.

Anyone else been playing with this? What kind of workflows or hacks have you tried?",Python,0,https://www.reddit.com/r/Python/comments/1n8f68b/has_anyone_here_tried_using_mcp_to_give_python/,r_1n8f68b,,,
r_1n8f0xu,reddit,aherontas,2025-09-04T16:31:51+00:00,"PyCon 2025 Workshop: Agentic Apps with Pydantic-AI
**Hey all!**

I recently gave a workshop talk at **PyCon Greece 2025** about building production-ready agent systems.  
To check it out, I put together a demo repo (slides coming soon on my blog: [petrostechchronicles.com](https://www.petrostechchronicles.com/?utm_source=chatgpt.com)):

Repo: [github.com/Aherontas/Pycon\_Greece\_2025\_Presentation\_Agents](https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents?utm_source=chatgpt.com)

**The idea**: show how multiple AI agents can collaborate using **FastAPI + Pydantic-AI**, with protocols like **MCP (Model Context Protocol)** and **A2A (Agent-to-Agent)** for safe communication and orchestration.

**Features:**

* Multiple agents running in containers
* MCP servers (Brave search, GitHub, filesystem, etc.) as tools
* A2A communication between services
* Minimal UI for experimentation (e.g., repo analysis)

**Why I built this**:  
Most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application.  
My goal was to help people experiment with these patterns and move closer to real-world use cases.

Itâ€™s not production-grade, but Iâ€™d love **feedback, criticism, or war stories** from anyone whoâ€™s tried building multi-agent systems.

**Big question for discussion:**  
Do you think agent-to-agent protocols like MCP/A2A will stick?  
Or will the future be mostly single powerful LLMs with plugin stacks?",Python,19,https://www.reddit.com/r/Python/comments/1n8f0xu/pycon_2025_workshop_agentic_apps_with_pydanticai/,r_1n8f0xu,,,
r_1n8d6pi,reddit,finallyanonymous,2025-09-04T15:23:33+00:00,"Production-Grade Python Logging Made Easier with Loguru
While Python's standard logging module is powerful, navigating its system of handlers, formatters, and filters can often feel like more work than it should be.

[I wrote a guide](https://www.dash0.com/guides/python-logging-with-loguru) on how to achieve the same (and better) results with a fraction of the complexity using Loguru. Itâ€™s approachable, can intercept logs from the standard library, and exposes its other great features in a much cleaner API.

Looking forward to hearing what you think!",Python,148,https://www.reddit.com/r/Python/comments/1n8d6pi/productiongrade_python_logging_made_easier_with/,r_1n8d6pi,,,
r_1n8c4ou,reddit,zeya07,2025-09-04T14:44:14+00:00,"FileSweep, a fast duplicate & clutter file cleaner
Hey everyone! I built FileSweep, a utility to help keep duplicates and clutter under control. I have the bad habit of downloading files and then *copying* them someplace else, instead of moving and deleting them. My downloads folder is currently 23 gigabytes, with 4 year old files and quadruple copies. Checking 3200 files manually is a monumental task, and I would never start doing it. That is why I build FileSweep. It is designed to allow fine-grained control over what gets deleted, with a focus on file duplicates.

Get the source code at [https://github.com/ramsteak/FileSweep](https://github.com/ramsteak/FileSweep)

# What My Project Does

FileSweep is a set-and-forget utility that:

* is easily configurable for your own system,
* detects duplicates across multiple folders, with per-directory priorities and policies,
* moves files to recycle bin / trash with send2trash,
* is very fast (with cache enabled, scans the above-described download directory in 1.2 seconds) with only the necessary disk reads,
* is cross-platform,
* can select files based on name, extension, regex, size and age,
* supports different policies (from keep to always delete),
* has dry-run mode for safe testing, guaranteeing that no file is deleted,
* can be set up as a cron / task scheduler task, and work in the background.

# How it works

* You set up a filesweep.yaml config describing which folders to scan, their priorities, and what to do with duplicates or matches (an example config with the explanation for every field is available in the repo)
* FileSweep builds a cache of file metadata and hashes, so future runs are much faster
* Respect rules for filetype, size, age, ...

# Target Audience

Any serial downloader of files that wants to keep their hard drive in check

# Comparison

dupeGuru is another duplicate-manager software. It uses Qt5 as GUI, so it can be more intuitive to beginners, and the user manually parses through duplicates. FileSweep is an automated CLI tool, can be configured and run without the need of a display and with minimal user intervention.

FileSweep is freely available (MIT License) from the [github repo](https://github.com/ramsteak/FileSweep)

Tested with Python 3.12+",Python,3,https://www.reddit.com/r/Python/comments/1n8c4ou/filesweep_a_fast_duplicate_clutter_file_cleaner/,r_1n8c4ou,,,
r_1n8c0yq,reddit,Tricky_Channel2918,2025-09-04T14:40:17+00:00,"Looking for a tutor
Dallas grad student needs tutor. Prefers in person but open to online. Who do you recommend as best? Any to avoid completely? ",Python,0,https://www.reddit.com/r/Python/comments/1n8c0yq/looking_for_a_tutor/,r_1n8c0yq,,,
r_1n8b41e,reddit,LordPeter_s,2025-09-04T14:06:15+00:00,"I built a Python library to simplify complex SQLAlchemy queries with a clean architecture.
HeyÂ r/Python,

Like many of you, I've spent countless hours writing boilerplate code for web APIs that use SQLAlchemy. Handling dynamic query parameters for filtering on nested relationships, sorting, full-text search, and pagination always felt repetitive and prone to errors.

To solve this, I createdÂ **fastapi-query-builder**.

Don't let the name fool you! While it was born from a FastAPI project, it's fundamentally a powerful, structured way to handle SQLAlchemy queries that can be adapted to any Python framework (Flask, Django Ninja, etc.).

The most unique part is its installation, inspired byÂ shadcn/ui. Instead of being just another black-box package, you runÂ query-builder init, and it copies the entire source code into your project. This gives youÂ **full ownership**Â to customize, extend, or fix anything you need.

**GitHub Repo:**Â [**https://github.com/Pedroffda/fastapi-query-builder**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2FPedroffda%2Ffastapi-query-builder)

# How it Works: A Clean Architecture

The library encourages a clean, three-layer architecture to separate concerns:

1. **BaseService**: The data access layer. It talks to the database using SQLAlchemy and the coreÂ QueryBuilder. It only deals with SQLAlchemy models.
2. **BaseMapper**: The presentation layer. It's responsible for transforming SQLAlchemy models into Pydantic schemas, intelligently handling relationship loading and field selection (select\_fields).
3. **BaseUseCase**: The business logic layer. It coordinates the service and the mapper. Your API endpoint talks to this layer, keeping your routes incredibly clean.

# A Quick, Realistic Example

Hereâ€™s a one-time setup for aÂ PostÂ model that has a relationship with aÂ UserÂ model.

    # --- In your project, after running 'query-builder init' ---
    
    # Import from your local, customizable copy
    from query_builder import BaseService, BaseMapper, BaseUseCase, get_dynamic_relations_map
    from your_models import User, Post
    from your_schemas import UserView, PostView
    
    # 1. Define Mappers (SQLAlchemy Model -> Pydantic Schema)
    user_mapper = BaseMapper(model_class=User, view_class=UserView, ...)
    post_mapper = BaseMapper(
        model_class=Post,
        view_class=PostView,
        relationship_map={
            'user': {'mapper': user_mapper.map_to_view, ...}
        }
    )
    
    # 2. Define the Service (Handles all the DB logic)
    post_service = BaseService(
        model_class=Post,
        relationship_map=get_dynamic_relations_map(Post),
        searchable_fields=[""title"", ""content"", ""user.name""] # <-- Search across relationships!
    )
    
    # 3. Define the UseCase (Connects Service & Mapper)
    post_use_case = BaseUseCase(
        service=post_service,
        map_to_view=post_mapper.map_to_view,
        map_list_to_view=post_mapper.map_list_to_view
    )

After this setup, your API endpoint becomes trivial. Here's a FastAPI example, but you can adapt the principle to any framework:

    from query_builder import QueryBuilder
    
    query_builder = QueryBuilder()
    
    u/router.get(""/posts"")
    async def get_posts(query_params: QueryParams = Depends(), ...):
        filter_params = query_builder.parse_filters(query_params)
        
        # The UseCase handles everything!
        return await post_use_case.get_all(
            db=db,
            filter_params=filter_params,
            ... # all other params like search, sort_by, etc.
        )

This setup unlocks powerful, clean, and complex queries directly from your URL, like:

* **Find posts with ""Python"" in the title, by authors named ""Pedro"":** .../posts?filter\[title\]\[ilike\]=%Python%&filter\[user.name\]\[ilike\]=%Pedro%
* **Sort posts by user's name, then by post ID descending:** .../posts?sort\_by=user.name,-id
* **Select specific fields from both the post and the related user:** .../posts?select\_fields=id,title,user.id,user.name

# I'd love your feedback!

This is my first open-source library, and Iâ€™m keen to hear from experienced Python developers.

* What are your thoughts on the three-layer (Service,Â Mapper,Â UseCase) architecture?
* Is theÂ shadcn/uiÂ ""vendoring"" approach (copying the code into your project) appealing?
* What crucial features do you think are missing?
* Any obvious pitfalls or suggestions for improvement in the code?

It's on TestPyPI now, and I'm hoping to make a full release after getting some community feedback.

**TestPyPI Link:**Â [https://test.pypi.org/project/fastapi-query-builder/](https://www.google.com/url?sa=E&q=https%3A%2F%2Ftest.pypi.org%2Fproject%2Ffastapi-query-builder%2F)

Thanks for taking the time to look at my project",Python,5,https://www.reddit.com/r/Python/comments/1n8b41e/i_built_a_python_library_to_simplify_complex/,r_1n8b41e,,,
r_1n87g91,reddit,HommeMusical,2025-09-04T11:23:00+00:00,"Rant: use that second expression in `assert`!
The `assert` statement is wildly useful for developing and maintaining software. I sprinkle `assert`s liberally in my code at the beginning to make sure what I think is true, is actually true, and this practice catches a vast number of idiotic errors; and I keep at least some of them in production.

But often I am in a position where someone else's assert triggers, and I see in a log something like `assert foo.bar().baz() != 0` has triggered, and I have no information at all.

Use that second expression in `assert`! 

It can be anything you like, even some calculation, and it doesn't get called unless the assertion fails, so it costs nothing if it never fires. When someone has to find out why your assertion triggered, it will make everyone's life easier if the assertion explains what's going on.

I often use

    assert some_condition(), locals()

which prints every local variable if the assertion fails. (`locals()` might be impossibly huge though, if it contains some massive variable, you don't want to generate some terabyte log, so be a little careful...)

And remember that `assert` is a statement, not an expression. That is why this `assert` will never trigger:

    assert (
       condition,
       ""Long Message""
    )

because it asserts that the expression `(condition, ""Message"")` is truthy, which it always is, because it is a two-element tuple.

Luckily I read an article about this long before I actually did it. I see it every year or two in someone's production code still.

Instead, use 

    assert condition, (
        ""Long Message""
    )",Python,251,https://www.reddit.com/r/Python/comments/1n87g91/rant_use_that_second_expression_in_assert/,r_1n87g91,,,
r_1n86hnz,reddit,haddock420,2025-09-04T10:29:31+00:00,"I made a script that identifies graded Pokemon cards with OCR
Hi everyone,

I run a [Pokemon deal finder](https://www.jimmysdealfinder.com) site that finds deals on Pokemon cards on eBay by comparing listing prices to historical card values.

I used to have graded cards on there, but I had to remove them from the site because too many people would lie in the title about what grade it is. For example, they might put ""PSA 10"" when it's only a PSA 9 or they might put ""Easily a PSA 10"" or ""Potential PSA 10"" when the card was ungraded. There were enough cards like this that I had to remove graded cards from the site because there were too many misleading graded listings.

I decided to try to use OCR on the card images to identify the grade rather than trusting what the user says in the title. I managed to write a surprisingly accurate script for identifying the grade of PSA 9 and PSA 10 cards.

It uses the cv2 and easyocr libraries, and it searches for sections that look purely black and white in the image (likely to be text), then it scans that section for the words ""MINT"" (grade 9) or ""GEM MT"" (grade 10) to determine the grade of the card.

It works surprisingly well, and the best thing is there are no false positives.

Now I've got graded cards back on my site, and they all seem to be identified correctly.

**What My Project Does**

Takes an image of a Pokemon card, and determiners whether it's a grade 9 or 10 or ungraded.

**Target Audience**

This is mainly for myself as a tool to add graded cards back to my site. Though it could be useful for anyone who needs to identify a graded card from an image.

**Comparison**

When I was first writing this, I did search on Google to see if anyone had done OCR recognition on graded Pokemon cards, but I didn't really find anything. I think this is unique in that regard.

You can run it with get_grade_ocr() on either a filename or a URL.

Github: https://github.com/sgriffin53/pokemon_ocr",Python,27,https://www.reddit.com/r/Python/comments/1n86hnz/i_made_a_script_that_identifies_graded_pokemon/,r_1n86hnz,,,
r_1n85395,reddit,Top_Decision_6132,2025-09-04T09:04:14+00:00,"flattening elements from a  nested list
    I am trying to write a program using list comprehension to flat the list like [[1,2,3],[4,5],6,7,[8,9],10] - a nested list having subslists and integer type elements. ",Python,0,https://www.reddit.com/r/Python/comments/1n85395/flattening_elements_from_a_nested_list/,r_1n85395,,,
r_1n85285,reddit,Consistent-Hat-6032,2025-09-04T09:02:18+00:00,"Typewriter sound program
I love the sound of a typewriter. I like the mechanical sound but I don't like typing on mechanical keyboards. How would one go about writing a program that imitates the typewriter sound as I'm typing?",Python,9,https://www.reddit.com/r/Python/comments/1n85285/typewriter_sound_program/,r_1n85285,,,
r_1n84tr5,reddit,empi91,2025-09-04T08:46:45+00:00,"Newsletters/people to follow and read
Looking for recommendations who to follow on LinkedIn to get some quality Python content? Because my current Ln bubble focus mostly on AI, which started to be bit boring, I'm looking for some actual Python devs/architects/etc posting quality stuff (around mid level preferably).   
Also, if there are any newsletters (free) worth signing I'd love a recommendation as well. ",Python,1,https://www.reddit.com/r/Python/comments/1n84tr5/newsletterspeople_to_follow_and_read/,r_1n84tr5,,,
r_1n84top,reddit,klaasvanschelven,2025-09-04T08:46:38+00:00,"Showcase: ecma426: Source Maps in Pure Python
### What My Project Does

**ecma426** is a pure-Python implementation of [ECMA-426: Source Maps](https://tc39.es/source-map/). It decodes and encodes sourcemaps, including index maps with `sections`, and aims to stay close to the specification.

### Target Audience

Anyone working with JavaScript toolchains from Python. For example, build systems, bundlers, error trackers, or debugging tools that need to parse or emit sourcemaps. Itâ€™s intended for production use, not just experimentation.

### Comparison

Most Python sourcemap libraries are either unmaintained or only handle decoding. **ecma426** covers both directions (decode and encode) and supports `sections` as defined in the spec, while staying dependency-free.

### Usage

```python
import ecma426, json

smap = ecma426.loads(json.load(open(""app.min.js.map"")))

# strict lookup (exact match only, raises KeyError if absent)
m = smap[(10, 42)]

# nearest-left lookup (devtools convention)
m = smap.lookup_left(10, 42)

# map back into the original text
line = smap.raw[""sourcesContent""][0].splitlines()[m.original_line]
print(line)
print("" "" * m.original_column + ""^ here"")
```

### Source

[https://github.com/bugsink/ecma426](https://github.com/bugsink/ecma426)",Python,7,https://www.reddit.com/r/Python/comments/1n84top/showcase_ecma426_source_maps_in_pure_python/,r_1n84top,,,
r_1n84q1m,reddit,Significant_Fill_452,2025-09-04T08:40:07+00:00,"# How to train a AI in windows (easy)
To train a AI in windows use a python library called automated-neural-adapter-ANA
This library allows the user to lora train there AI using a Gui below are the steps to finetune your AI:
## Installation
*1: Installation*
install the library using
python
pip install automated-neural-adapter-ANA
**2: Usage **
run python python -m ana  in your command prompt (it might take a while)
*3: What it dose*
The base model id is the hugging face id of the model you want to training in this case we are training tinyllama1.1b you can chose any model by going to https://huggingface.co/models eg if you want to train TheBloke/Llama-2-7B-fp16 replace TinyLlama/TinyLlama-1.1B-Chat-v1.0 with TheBloke/Llama-2-7B-fp16
*4: Output*
output directory is the path where your model is stored
*5: Disk offload*
offloads the model to a path if it cant fit inside your vram and ram (this will slow down the process significantly)
*6: Local dataset*
is the path in the local dataset path you can select the data in which you want to train your model also if you click on hugging face hub you can use a hugging face dataset
*7: Training Parameters*
In this section you can adjust how your AI will be trained:
â€¢	Epochs â†’ how many times the model goes through your dataset.
â€¢	Batch size â†’ how many samples are trained at once (higher = faster but needs more VRAM).
â€¢	Learning rate â†’ how fast the model adapts (default is usually fine for beginners).
Tip: If youâ€™re just testing, set epochs = 1 and a small dataset to save time.
*8: Start Training*
Once everything is set, click Start Training.
â€¢	A log window will open showing progress (loss going down = your model is learning).
â€¢	Depending on your GPU/CPU and dataset size, this can take minutes to days. (If you donâ€™t have a gpu it will take a lottt of time, and if you have one but it dosent detect it install cuda and pytorch for that specific cuda version)
Congratulation you have successfully lora finetuned your AI
to talk to your AI you must convert it to a gguf format there are many tutorials online for that",Python,0,https://www.reddit.com/r/Python/comments/1n84q1m/how_to_train_a_ai_in_windows_easy/,r_1n84q1m,,,
r_1n84hjt,reddit,MelcoreHat,2025-09-04T08:24:02+00:00,"PyconFR at Lyon (France)
The French-Speaking Python Association (AFPy) is organizing PyConFR 2025 from Thursday, October 30 to Sunday, November 2. For this 16th edition, weâ€™ll be hosted by the RenÃ© Cassin Campus in Lyon!

PyConFR is a free, four-day event centered around the Python programming language. It includes two days of collaborative development (sprints), followed by two days of talks and workshops.

The call for proposals is now closed, and weâ€™ll be publishing the schedule soon at https://www.pycon.fr/2025/en/schedule.html. There will be an English-language track.

While attendance is free, registration is required for all participants.

As every year, we offer support to people who are usually underrepresented at conferences â€” help with finding a topic, writing a proposal, preparing slides, and rehearsing. Feel free to contact us at [diversite@afpy.org]()",Python,24,https://www.reddit.com/r/Python/comments/1n84hjt/pyconfr_at_lyon_france/,r_1n84hjt,,,
r_1n802wo,reddit,Redstonedust653,2025-09-04T03:55:58+00:00,"I made a chat program
# What my project does

It's a simple socket-based python messaging ""app"" that works on linux. I don't know if it works on windows, so comment if it does

# Target audience

I dunno, if you want a template for a chat program you can expand on this? I just made it to mess with socket

# Comparison

I mean, there are a lot of online tutorials for stuff like this, but i dunno, this one has a *bit* more than *most* of the tutorials.

Anyways, [here's a link](https://github.com/Redstonedust653/pychat) to the github repository.

enjoy!

  
NOTE:

Don't read the comments! look at the repository. if you have issues with some part of it, LEAVE AN ISSUE ON THE REPOSITORY! ALL COMMENTS WILL BECOME OUTDATED EVERY TIME I PATCH IT.

SEVERAL OF THE ISSUES IN COMMENTS HAVE BEEN FIXED.

BUT PLEASE DON'T COMMENT ISSUES.",Python,2,https://www.reddit.com/r/Python/comments/1n802wo/i_made_a_chat_program/,r_1n802wo,,,
r_1n800hy,reddit,Turbulent-Start-4840,2025-09-04T03:52:16+00:00,"Airfoil Optimizer.
Hey yall!  
So recently, for a personal plane project of mine, I developed FoilNet,Â [https://github.com/AvnehSBhatia/FoilNet](https://github.com/AvnehSBhatia/FoilNet)

It's an airfoil optimizer, as the title suggests. However, I am not too certain about these results that I'm getting from the optimizer.

If anyone knows a good bit about Airfoils and think they can validate my results, please feel free to do so!

Any comments or criticism is appreciated.

Thanks!",Python,2,https://www.reddit.com/r/Python/comments/1n800hy/airfoil_optimizer/,r_1n800hy,,,
r_1n7v62y,reddit,AutoModerator,2025-09-04T00:00:34+00:00,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!
# Weekly Thread: Professional Use, Jobs, and Education ðŸ¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! ðŸŒŸ",Python,6,https://www.reddit.com/r/Python/comments/1n7v62y/thursday_daily_thread_python_careers_courses_and/,r_1n7v62y,,,
r_1n7sr1x,reddit,Ordinary_Run_2513,2025-09-03T22:16:26+00:00,"Why does ProcessPoolExecutor mark some tasks as ""running"" even though all workers are busy?
Iâ€™m using Pythonâ€™s `ProcessPoolExecutor` to run a bunch of tasks. Something I noticed is that some tasks are marked as *running* even though all the workers are already working on other tasks.

From my understanding, a task should only switch from *pending* to *running* once a worker actually starts executing it. But in my case, it seems like the executor marks extra tasks as running before theyâ€™re really picked up.

Is this normal behavior of `ProcessPoolExecutor`? Or am I missing something about how it manages its internal task queue?",Python,12,https://www.reddit.com/r/Python/comments/1n7sr1x/why_does_processpoolexecutor_mark_some_tasks_as/,r_1n7sr1x,,,
r_1n7r4xb,reddit,OllieOps,2025-09-03T21:11:38+00:00,"Niche Python tools, libraries and features - whats your favourite?
I know we see this get asked every other week, but it always makes for a good discussion.

I only just found out about `pathlib` \- makes working with files so much cleaner.

Whats a python tool or library you wish youd known about earlier?",Python,131,https://www.reddit.com/r/Python/comments/1n7r4xb/niche_python_tools_libraries_and_features_whats/,r_1n7r4xb,,,
r_1n7qxeb,reddit,SchizmOne,2025-09-03T21:03:21+00:00,"About the spheres of the Python and career paths
Hey, guys. I wanted to ask Python Developers here in case any of you had similar doubts about their career paths.  
  
So, I'm a Python Test Automation Engineer with about 6 years of experience, and Iâ€™ve recently started to seriously think about **how I can grow** as a specialist in the industry and **what I actually want to do**. After a bit of introspection, I picked the possible paths:

1. **SDET** â€“ keep digging deeper into QA Automation. Thereâ€™s still a lot to learn, like load testing, etc.
2. **DevOps** â€“ build on what Iâ€™ve already done as part of QA Automation, such as preparing CI/CD pipelines, scripting, support, etc.
3. **Developer** â€“ move straight into the pure development sphere.

Right now, Iâ€™m really leaning toward option 3, because (and I think many of you will understand this feeling) I genuinely enjoy solving problems, creating solutions, building something piece by piece, and then seeing how it works, how cool it looks, and. Something you can actually use. Those little â€œahhh, thatâ€™s how it worksâ€ moments, you know.

But thereâ€™s one thing thatâ€™s a bit upsetting to me: the modern spheres of Python. Specifically, how much of it is tied to AI Development, Data Science, Machine Learning, etc. It feels like half of the Python market is focused on these things.

Of course I donâ€™t hate AI, itâ€™s just a technology after all. As specialists, we still need to use it in our work. So maybe this is just my prejudice, and itâ€™s time for me to accept that this is simply how things are. Still, if I had the choice, Iâ€™d prefer not to work in that space. But if I will ignore it, I feel like Iâ€™d be cutting myself off from about half of the possible opportunities as a Python Developer.

What do you think about the current market and your options as Python Developers? Maybe Iâ€™m missing something obvious, or maybe my understanding of the market isnâ€™t close to reality.",Python,0,https://www.reddit.com/r/Python/comments/1n7qxeb/about_the_spheres_of_the_python_and_career_paths/,r_1n7qxeb,,,
r_1n7qs6r,reddit,Priler96,2025-09-03T20:57:56+00:00,"Python for impatient people - Basics in 10 minutes
Hey everyone,

I just uploaded a short and beginner-friendly **Python tutorial** on YouTube where I explain the core concepts in only 10 minutes. Perfect if you're just starting out or need a quick refresher.

ðŸ‘‰ [Watch it here on YouTube](https://www.youtube.com/watch?v=uBhe1Rvp4PI)

I kept it simple, practical, and straight to the point - no fluff, just code and examples.  
Would love your feedback on whether you'd like to see more quick lessons like this!

Thanks!",Python,6,https://www.reddit.com/r/Python/comments/1n7qs6r/python_for_impatient_people_basics_in_10_minutes/,r_1n7qs6r,,,
r_1n7pe37,reddit,jcfitzpatrick12,2025-09-03T20:05:24+00:00,"Removing a dependency - Major, Minor or Patch bump?
I've been collaborating on an [issue](https://github.com/jcfitzpatrick12/spectre/issues/167) for [*Spectre*](https://github.com/jcfitzpatrick12/spectre), a Python program for recording radio spectrograms with software-defined radios. The motivation for the issue was to remove [Scipy](https://scipy.org/) as dependency from a Python package used by the program called [spectre-core](https://github.com/jcfitzpatrick12/spectre-core).

The [PR](https://github.com/jcfitzpatrick12/spectre-core/pull/52) introduced no changes from the perspective of the public API of the package. It just reimplemented the same functionality for our particular use case. However, we removed Scipy as a dependency since it was no longer required. Under [semantic versioning](https://semver.org/), would this constitute a major, minor or patch bump?

I considered making this a major bump, since any consumer of the package relying on Scipy being a transitive dependency would see a breaking change. But since the Scipy functionality wasn't exposed publically, I didn't think this argument was strong enough and so opted for a minor bump. What would you do?",Python,26,https://www.reddit.com/r/Python/comments/1n7pe37/removing_a_dependency_major_minor_or_patch_bump/,r_1n7pe37,,,
r_1n7neyq,reddit,Blasman13,2025-09-03T18:51:08+00:00,"Streamledge - Launch YouTube and Twitch Videos in a Minimal Browser Window
Source: [https://github.com/Blasman/Streamledge](https://github.com/Blasman/Streamledge)

Streamledge is a command-line tool for playing YouTube and [Twitch.tv](http://Twitch.tv) videos.

**What My Project Does**

Streamledge works by loading a lightweight (\~30MB RAM) local flask web server in the background when first ran. This allows Streamledge to be ran with command line arguments that utilize the server to embed and play videos in a **minimal** Chromium-based web browser `--app` window.

**Target Audience**

Streamledge may be of use to anyone who watches YouTube and/or Twitch and/or works from the command prompt / terminal. It can also be useful if you are a minimalist or have multiple monitors and want the freedom to move videos around. It can be combined with the web browser extension to be used on the YouTube and Twitch websites to launch links in the Streamledge embedded player.

**Comparison**

Streamledge is not yet another YouTube downloader. It's different because the videos play immediately in a locally embedded player.",Python,7,https://www.reddit.com/r/Python/comments/1n7neyq/streamledge_launch_youtube_and_twitch_videos_in_a/,r_1n7neyq,,,
r_1n7ibkk,reddit,papersashimi,2025-09-03T15:44:11+00:00,"DINOv3-CLIP Adapter
Created a tiny adapter that connects DINOv3's image encoder to CLIP's text space.

Essentially, DINOv3 has better vision than CLIP, but no text capabilities. This lets you use dinov3 for images and CLIP for text prompts. This is still v1 so the next stages will be mentioned down below. 

**Target Audience:**

ML engineers who want zero-shot image search without training massive models

Works for zero shot image search/labeling. Way smaller than full CLIP. Performance is definitely lower because it wasnt trained on image-text pairs.

**Next steps**: May do image-text pair training. Definitely adding a segmentation or OD head. Better calibration and prompt templates

Code and more info can be found here: [https://github.com/duriantaco/dinov3clip](https://github.com/duriantaco/dinov3clip)

If you'll like to colab or whatever do ping me here or drop me an email. ",Python,7,https://www.reddit.com/r/Python/comments/1n7ibkk/dinov3clip_adapter/,r_1n7ibkk,,,
r_1n7e1oa,reddit,zubanls,2025-09-03T12:55:41+00:00,"Zuban is now Open Source
Zuban, the successor of Jedi is now Open Source: [https://github.com/zubanls/zuban](https://github.com/zubanls/zuban)

Zuban is a high-performance Python Language Server and type checker implemented in Rust, by the author of Jedi. Zuban is 20â€“200Ã— faster than Mypy, while using roughly half the memory and CPU compared to Ty and Pyrefly. It offers both a PyRight-like mode and a Mypy-compatible mode, which behaves just like Mypy; supporting the same config files, command-line flags, and error messages.

Most important LSP features are supported. Features include diagnostics, completions, goto, references, rename, hover and document highlights.

Zuban passes over 95% of Mypyâ€™s relevant test suite and offers comprehensive support for Python's [type system](https://htmlpreview.github.io/?https://github.com/python/typing/blob/main/conformance/results/results.html).",Python,214,https://www.reddit.com/r/Python/comments/1n7e1oa/zuban_is_now_open_source/,r_1n7e1oa,,,
r_1n7cwjq,reddit,Apart-Television4396,2025-09-03T12:02:53+00:00,"PySurf is now Quantum!
Hello, everyone! I made a decision to abandon the PySurf project, and start a new web browser from scratch, called Quantum. Quantum is made in Electron JS, which allows more customisation of both the UI, and the functionality itself. Unfortunately, I'll not be able to post updates on this subreddit, because Electron JS is not Python, but you'll be able to find Quantum on r/browsers, r/SideProject, and more. Quantum is still in early stages of development, so please contribute on GitHub, if you can.

Check out Quantum here: [https://github.com/VG-dev1/Quantum](https://github.com/VG-dev1/Quantum)

Or, check out the legacy PySurf here: [https://github.com/VG-dev1/PySurf](https://github.com/VG-dev1/PySurf)

",Python,0,https://www.reddit.com/r/Python/comments/1n7cwjq/pysurf_is_now_quantum/,r_1n7cwjq,,,
r_1n77jvv,reddit,Thick-Mushroom6151,2025-09-03T06:37:20+00:00,"I built a Python library for working with LLMs â€” would love your feedback

# akgpt

I built a Python library for working with LLMs â€” looking for feedback ðŸ™Œ  

## ðŸ“¦ Installation
```bash
pip install akgpt

ðŸš€ Example usage

from akgpt.main import AKGPT

client = AKGPT()

prompt = ""Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚?""
result = client.query(prompt)

if result:
    print(""ÐžÑ‚Ð²ÐµÑ‚ API:"", result)


âœ¨ Features

Simple client interface (AKGPT.query)

Configurable generation parameters (temperature, top_p, penalties, etc.)

Supports both text and JSON outputs

Works with multiple providers (OpenAI, Mistral, Pollinations)

Python 3.8+


ðŸ’¡ Feedback wanted

Iâ€™d really appreciate your feedback:

How do you feel about the API design?

Which features would be most useful for you (async client, FastAPI integration, more model providers)?


ðŸ‘‰ Project on PyPI: akgpt

Thanks for checking it out ðŸ™


",Python,0,https://www.reddit.com/r/Python/comments/1n77jvv/i_built_a_python_library_for_working_with_llms/,r_1n77jvv,,,
r_1n75r76,reddit,Grand-Parsley-636,2025-09-03T04:49:03+00:00,"contribution of python to the world is underratedâ€¦
found this on youtube scrolling,Â [https://youtu.be/DRU-0tHOayc](https://youtu.be/DRU-0tHOayc)

found it good at explaining how we got hereâ€¦from first neuronâ€™s birth to chatGPT, then the thought just struck me, none of it would have been possible without pythonâ€¦much of the world, still not aware about the contribution. Python has done so much in making lives of humans better in every possible wayâ€¦",Python,9,https://www.reddit.com/r/Python/comments/1n75r76/contribution_of_python_to_the_world_is_underrated/,r_1n75r76,,,
r_1n717ga,reddit,data_diva_0902,2025-09-03T01:04:10+00:00,"Working with MCP and tired of boilerplate? You might like what weâ€™re launching
 Saw the MCP Toolkit thread here â€” super cool stuff. Weâ€™ve been running into the same friction: too much boilerplate, unclear abstractions, and devs spending more time wiring than building.

Weâ€™ve been working on a solution that streamlines agentic workflows â€” combining trusted control, orchestration, and reasoning through MCP without the usual overhead.

We're doing a live walkthrough of what weâ€™re launching â€” how teams are using it to build faster, integrate smoother, and avoid rebuilding the wheel every time they want an agent to do something non-trivial.

If youâ€™re working with MCP or just want to see how the tooling is evolving, check it out: [https://www.thoughtspot.com/spotlight-series-boundaryless?utm\_source=livestream&utm\_medium=webinar&utm\_term=post1&utm\_content=reddit&utm\_campaign=wb\_productspotlight\_boundaryless25](https://www.thoughtspot.com/spotlight-series-boundaryless?utm_source=livestream&utm_medium=webinar&utm_term=post1&utm_content=reddit&utm_campaign=wb_productspotlight_boundaryless25)",Python,0,https://www.reddit.com/r/Python/comments/1n717ga/working_with_mcp_and_tired_of_boilerplate_you/,r_1n717ga,,,
r_1n6xw8z,reddit,WildAppearance2153,2025-09-02T22:37:59+00:00,"Meet THOAD, High Order Derivatives for PyTorch Graphs
Iâ€™m excited to share **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a research project at Universidad Pontificia de Comillas (ICAI), and we are considering publishing an academic article in the future that reviews the mathematical details and the implementation design.

At its core, thoad takes a one output to many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1â†’N problem can be rewritten as 1â†’1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoadâ€™s graph aware formulation enables an optimization based on **unifying independent dimensions** (especially batch). This delivers **asymptotically better scaling** with respect to batch size. Additionally, we compute derivatives **vectorially** rather than component by component, which is what makes a pure PyTorch implementation practical without resorting to custom C++ or CUDA.

The package is **easy to maintain**, because it is written entirely in Python and uses **PyTorch** as its only dependency. The implementation stays at a high level and leans on PyTorchâ€™s vectorized operations, which means no custom C++ or CUDA bindings, no build systems to manage, and fewer platform specific issues.

The package can be installed from **GitHub** or **PyPI**:

* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)
* PyPI: [https://pypi.org/project/thoad/](https://pypi.org/project/thoad/)

In our benchmarks, **thoad outperforms** `torch.autograd` **for Hessian calculations even on CPU**. See the notebook that reproduces the comparison: https://github.com/mntsx/thoad/blob/master/examples/benchmarks/benchmark\_vs\_torch\_autograd.ipynb.

The user experience has been one of our main concerns during development. **thoad** is designed to align closely with PyTorchâ€™s interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorchâ€™s own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.

# USING THE PACKAGE

**thoad** exposes two primary interfaces for computing high-order derivatives:

1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).
2. `thoad.Controller`: a class-based interface that wraps the output tensorâ€™s subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.

**thoad.backward**

The `thoad.backward` function computes high-order partial derivatives of a given output tensor and stores them in each leaf tensorâ€™s `.hgrad` attribute.

**Arguments**:

* `tensor`: A PyTorch tensor from which to start the backward pass. This tensor must require gradients and be part of a differentiable graph.
* `order`: A positive integer specifying the maximum order of derivatives to compute.
* `gradient`: A tensor with the same shape as `tensor` to seed the vector-Jacobian product (i.e., custom upstream gradient). If omitted, the default is used.
* `crossings`: A boolean flag (default=`False`). If set to `True`, mixed partial derivatives (i.e., derivatives that involve more than one distinct leaf tensor) will be computed.
* `groups`: An iterable of disjoint groups of leaf tensors. When `crossings=False`, only those mixed partials whose participating leaf tensors all lie within a single group will be calculated. If `crossings=True` and `groups` is provided, a *ValueError* will be raised (they are mutually exclusive).
* `keep_batch`: A boolean flag (default=`False`) that controls how output dimensions are organized in the computed gradients.
   * **When** `keep_batch=False`: The derivative preserves one first flattened ""primal"" axis, followed by each original partial shape, sorted in differentiation order. Concretelly:
      * A single ""primal"" axis that contains every element of the graph output tensor (flattened into one dimension).
      * A group of axes per derivative order, each matching the shape of the respective differentially targeted tensor.
   * For an N-th order derivative of a leaf tensor with `input_numel` elements and an output with `output_numel` elements, the gradient shape is:
      * **Axis 1:** indexes all `output_numel` outputs
      * **Axes 2â€¦(sum(Nj)+1):** each indexes all `input_numel` inputs
   * **When** `keep_batch=True`: The derivative shape follows the same ordering as in the previous case, but includes a series of ""independent dimensions"" immediately after the ""primal"" axis:
      * **Axis 1** flattens all elements of the output tensor (size = `output_numel`).
      * **Axes 2...(k+i+1)** correspond to dimensions shared by multiple input tensors and treated independently throughout the graph. These are dimensions that are only operated on element-wise (e.g. batch dimensions).
      * **Axes (k+i+1)...(k+i+sum(Nj)+1)** each flatten all `input_numel` elements of the leaf tensor, one axis per derivative order.
* `keep_schwarz`: A boolean flag (default=`False`). If `True`, symmetric (Schwarz) permutations are retained explicitly instead of being canonicalized/reducedâ€”useful for debugging or inspecting non-reduced layouts.

**Returns**:

* An instance of `thoad.Controller` wrapping the same tensor and graph

Executing the automatic differentiation via `thoad.backprop` looks like this.

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Call thoad backward
    order = 2
    thoad.backward(tensor=Z, order=order)
    
    #### Checks
    ## check derivative shapes
    for o in range(1, 1 + order):
       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))
       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))
    ## check first derivatives (jacobians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)
    J = torch.autograd.functional.jacobian(fn, (X, Y))
    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)
    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)
    ## check second derivatives (hessians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()
    H = torch.autograd.functional.hessian(fn, (X, Y))
    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)
    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)

**Instantiation**

Use the constructor to create a controller for any tensor requiring gradients:

    controller = thoad.Controller(tensor=GO)  ## takes graph output tensor

* `tensor`: A PyTorch `Tensor` with `requires_grad=True` and a non-`None` `grad_fn`.

**Properties**

* `.tensor â†’ Tensor` The output tensor underlying this controller. **Setter**: Replaces the tensor (after validation), rebuilds the internal computation graph, and invalidates any previously computed gradients.
* `.compatible â†’ bool` Indicates whether every backward function in the tensorâ€™s subgraph has a supported high-order implementation. If `False`, some derivatives may fall back or be unavailable.
* `.index â†’ Dict[Type[torch.autograd.Function], Type[ExtendedAutogradFunction]]` A mapping from base PyTorch `autograd.Function` classes to thoadâ€™s `ExtendedAutogradFunction` implementations. **Setter**: Validates and injects your custom high-order extensions.

**Core Methods**

**.backward(order, gradient=None, crossings=False, groups=None, keep\_batch=False, keep\_schwarz=False) â†’ None**

Performs the high-order backward pass up to the specified derivative `order`, storing all computed partials in each leaf tensorâ€™s `.hgrad` attribute.

* `order` (`int > 0`): maximum derivative order.
* `gradient` (`Optional[Tensor]`): custom upstream gradient with the same shape as `controller.tensor`.
* `crossings` (`bool`, default `False`): If `True`, mixed partial derivatives across different leaf tensors will be computed.
* `groups` (`Optional[Iterable[Iterable[Tensor]]]`, default `None`): When `crossings=False`, restricts mixed partials to those whose leaf tensors all lie within a single group. If `crossings=True` and `groups` is provided, a *ValueError* is raised.
* `keep_batch` (`bool`, default `False`): controls whether independent output axes are kept separate (batched) or merged (flattened) in stored/retrieved gradients.
* `keep_schwarz` (`bool`, default `False`): if `True`, retains symmetric permutations explicitly (no Schwarz reduction).

**.display\_graph() â†’ None**

Prints a tree representation of the tensorâ€™s backward subgraph. Supported nodes are shown normally; unsupported ones are annotated with `(not supported)`.

**.register\_backward\_hook(variables: Sequence\[Tensor\], hook: Callable) â†’ None**

Registers a user-provided `hook` to run during the backward pass whenever gradients for any of the specified leaf `variables` are computed.

* `variables` (`Sequence[Tensor]`): Leaf tensors to monitor.
* `hook` (`Callable[[Tuple[Tensor, Tuple[Shape, ...], Tuple[Indep, ...]], dict[AutogradFunction, set[Tensor]]], Tuple[Tensor, Tuple[Shape, ...], Tuple[Indep, ...]]]`): Receives the current `(Tensor, shapes, indeps)` plus contextual info, and must return the modified triple.

**.require\_grad\_(variables: Sequence\[Tensor\]) â†’ None**

Marks the given leaf `variables` so that all intermediate partials involving them are retained, even if not required for the final requested gradients. Useful for inspecting or re-using higher-order intermediates.

**.fetch\_hgrad(variables: Sequence\[Tensor\], keep\_batch: bool = False, keep\_schwarz: bool = False) â†’ Tuple\[Tensor, Tuple\[Tuple\[Shape, ...\], Tuple\[Indep, ...\], VPerm\]\]**

Retrieves the precomputed high-order partial corresponding to the ordered sequence of leaf `variables`.

* `variables` (`Sequence[Tensor]`): the leaf tensors whose mixed partial you want.
* `keep_batch` (`bool`, default `False`): if `True`, each independent output axis remains a separate batch dimension in the returned tensor; if `False`, independent axes are distributed/merged into derivative dimensions.
* `keep_schwarz` (`bool`, default `False`): if `True`, returns derivatives retaining symmetric permutations explicitly.

Returns a pair:

1. **Gradient tensor**: the computed partial derivatives, shaped according to output and input dimensions (respecting `keep_batch`/`keep_schwarz`).
2. **Metadata tuple**
   * **Shapes** (`Tuple[Shape, ...]`): the original shape of each leaf tensor.
   * **Indeps** (`Tuple[Indep, ...]`): for each variable, indicates which output axes remained independent (batch) vs. which were merged into derivative axes.
   * **VPerm** (`Tuple[int, ...]`): a permutation that maps the internal derivative layout to the requested `variables` order.

Use the combination of independent-dimension info and shapes to reshape or interpret the returned gradient tensor in your workflow.

    import torch
    import thoad
    from torch.nn import functional as F
            
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
            
    #### Instantiate thoad controller and call backward
    order = 2
    controller = thoad.Controller(tensor=Z)
    controller.backward(order=order, crossings=True)
            
    #### Fetch Partial Derivatives
    ## fetch X and Y 2nd order derivatives
    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))
    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))
    assert torch.allclose(partial_XX, X.hgrad[1])
    assert torch.allclose(partial_YY, Y.hgrad[1])
    ## fetch cross derivatives
    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))
    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))

>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)

If you give it a try, I would love feedback on the API.",Python,26,https://www.reddit.com/r/Python/comments/1n6xw8z/meet_thoad_high_order_derivatives_for_pytorch/,r_1n6xw8z,,,
r_1n6xqjn,reddit,cassiel663,2025-09-02T22:31:22+00:00,"algÃºien tiene proyectos de programaciÃ³n inconclusos que pueda compartir?
hola comunidad estoy aprendiendo programaciÃ³n y quisiera practicar con proyectos reales que hayan quedado inconclusos. la idea es :
âœ“revisar el codigo
âœ“intentar completarlo o mejorarlo
âœ“aprender de la experiencia de otros
Si algÃºien tiene algun proyecto pequeÃ±o o grande en python me gustaria que me compartiera",Python,0,https://www.reddit.com/r/Python/comments/1n6xqjn/algÃºien_tiene_proyectos_de_programaciÃ³n/,r_1n6xqjn,,,
r_1n6v3tl,reddit,Xgf_01,2025-09-02T20:46:35+00:00,"PyLine Update - terminal based text editor (Linux, WSL, MacOS) (New Feats)
Hello, this is a hobby project I coded entirely in Python 3 , created longer time ago. But came back to it this spring. Now updated with new functionality and better code structure currently at v0.9.7.

Source at -Â [PyLine GitHub repo](https://github.com/Peter-L-SVK/PyLine)  (you can see screenshots in readme)

# What My Project Does:

It is CLI text editor with:  
\- function like wc - cw - counts chars, words and lines  
\- open / create / truncate file  
\- exec mode that is like file browser and work with directories  
\- scroll-able text-buffer, currently set to 52 lines  
\- supports all clipboards for GUI: X11,Wayland, win32yank for WSL and pbpaste for MacOS  
\- multiple lines selection copy/paste/overwrite and delete  
\- edit history implemented via LIFO - Last In First Out (limit set to 120)  
\- highlighting of .py syntax (temporary tho, will find the better way)  
\- comes with proper install script

# New features:

\- Support of args <filename>, -i/--info and -h/--help  
\- Modular hooks system with priority, runtime enable/disable, cross-language support (Python, Perl, Bash, Ruby, Lua, Node.js, PHP)  
\- Hook manager UI (list, enable/disable, reload hooks, show info)  
\- BufferManager, NavigationManager, SelectionManager, PasteBuffer, UndoManager all refactored for composition and extensibility (micro-kernel like architecture)  
\- Hook-enabled file loading/saving, multi-language event handlers  
\- Enhanced config and state management (per-user config dir)  
\- Improved argument parsing and info screens

It also comes with prepackaged hooks like smart tab indent.

The editor is using built-in to the terminal foreground/background but I plan to implement themes and config.ini alongside search / replace feature.

# Target Audience:

Basically anyone with Linux, WSL or other Unix-like OS. Nothing complicated to use.

(I know it's not too much.. I don't have any degree in CS or IT engineering or so, just passion)",Python,39,https://www.reddit.com/r/Python/comments/1n6v3tl/pyline_update_terminal_based_text_editor_linux/,r_1n6v3tl,,,
r_1n6sa5a,reddit,Neat-Instance-6537,2025-09-02T18:59:57+00:00,"Feedback Wanted: GUI App to Convert Python Scripts to .exe Files
Hey everyone ðŸ‘‹

Iâ€™m working on a desktop app that helps users convert Python scripts into standalone .exe files using a simple graphical interface. The goal is to make the process more intuitive for folks who arenâ€™t comfortable with command-line tools like PyInstaller or cx\_Freeze. I'm familiar with other similar tools out there (e.g., auto-py-to-exe) but my goal is to create a more modern looking intuitive UI with more features.

Hereâ€™s what it currently does:

* Upload Python files
* Basic configuration options (e.g. console vs windowed, icon selection)
* One-click build process using PyInstaller under the hood
* Error logging and build status updates in the GUI

Iâ€™d love your feedback on:

* ðŸ§  Features youâ€™d want in a tool like this
* ðŸ§© Pain points youâ€™ve had converting scripts to executables
* ðŸŽ¨ UI/UX suggestions to make it more beginner-friendly
* ðŸ› ï¸ Any tools or workflows you currently use that I should consider integrating

If youâ€™re open to testing a beta version soon, let me know and Iâ€™ll reach out when itâ€™s ready!

Thanks in advance ðŸ™  
",Python,0,https://www.reddit.com/r/Python/comments/1n6sa5a/feedback_wanted_gui_app_to_convert_python_scripts/,r_1n6sa5a,,,
r_1n6o0x8,reddit,X_wrld_1,2025-09-02T16:21:55+00:00,"Vacancy for a python tutor
I'm opening an online coding institution and looking for someone to fill in the role of teaching Python. 

If interested comment down below or dm me",Python,0,https://www.reddit.com/r/Python/comments/1n6o0x8/vacancy_for_a_python_tutor/,r_1n6o0x8,,,
r_1n6jfon,reddit,s_basu,2025-09-02T13:26:45+00:00,"I built a CLI tool for database migration
# What My Project Does

Wandern is a CLI tool similar to alembic or django migrations to manage and apply SQL migrations, currently supporting sqlite and postgresql.  
It  keeps track of the sequence of migrations applied and allows specifying additional migration metadata such as author name, tags to filter migrations. You can generate empty migrations and write the SQL yourself, or use the prompting feature (requires additional dependency and LLM API key) to let the agent generate the migration. The agent support is added using pydantic-ai, and can generate revisions based on previous migration file contexts.

It is very lightweight, only supporting sqlite out-of-box, needing to install additional dependency for postgresql or agents.

# Target Audience

I primarily intended to built this to use myself, partly because I wanted to get away from the bulky setup that comes with alembic or sqlalchemy for smaller projects. So this is for anyone who prefers to write their own SQL statements, and those who want to have versioned migration without the added overhead of the sqlalchemy ecosystem, and with a nicer TUI and support for AI agents, 

# Comparison

Wandern is meant to be a minimal and configurable CLI alternative to existing tools like Alembic or Django migrations for smaller or more barebone projects. I thought adding agents would be a cool addition as well so there's that. 

You can find it on Github here: [https://github.com/s-bose/wandern](https://github.com/s-bose/wandern)  
Or download from Pypi: [https://pypi.org/project/wandern/](https://pypi.org/project/wandern/)

",Python,2,https://www.reddit.com/r/Python/comments/1n6jfon/i_built_a_cli_tool_for_database_migration/,r_1n6jfon,,,
r_1n6fm1w,reddit,Due_Care_7629,2025-09-02T10:13:47+00:00,"I built a Python bot that automatically finds remote jobs and sends them to Telegram.
# Built a Python bot to automate remote job hunting - sharing the code

How many job sites do you check daily? (I was at 12 before building this)Â   
  
What My Project Does

A Python script that scrapes remote job boards and sends filtered results to Telegram:

* Monitors RemoteOK, WeWorkRemotely, GitHub Jobs, etc.
* Filters by custom keywords
* Telegram notifications for new matches
* Saves data locally for debugging

# Target Audience

Personal automation tool for individual job seekers. Production-ready but meant for personal use only - not commercial application.

# Comparison

vs Manual checking:Â Eliminates repetitive browsing  
vs Job alerts:Â More customizable, covers niche remote job boards  
vs Paid services:Â Open source, no restrictions

# Technical Implementation

Built with Python requests + BeautifulSoup, configurable via environment variables. Includes error handling and rate limiting.

Code:Â [https://github.com/AzizB283/job-hunter](https://github.com/AzizB283/job-hunter)

Anyone else built job automation tools?Â Curious what approaches others have taken.",Python,0,https://www.reddit.com/r/Python/comments/1n6fm1w/i_built_a_python_bot_that_automatically_finds/,r_1n6fm1w,,,
r_1n6fgah,reddit,FickleAd3708,2025-09-02T10:03:56+00:00,"Python OOP is clever
Python also feels like the only real OOP cuz you can actually modify almost anything in a class BUT at the same time you can totally ignore any OOP and write pure functions, while still utilizing OOP cuz the function is an object and can have attributes, lol ðŸ˜‚ this is clever",Python,0,https://www.reddit.com/r/Python/comments/1n6fgah/python_oop_is_clever/,r_1n6fgah,,,
r_1n6aos0,reddit,Kooky_Fee_4423,2025-09-02T04:58:43+00:00,"[ANN] tblkit â€” Swiss-army CLI for tabular data (CSV/TSV)
A small, fast command-line tool for the table chores between raw files and a notebookâ€”clean/rename, robust column selects, filter/unique, exact & fuzzy joins, numeric/date-aware sort, group/aggregate, pivot/melt, pretty view. Plays nicely with pipes.

Designed for data scientists preparing analysis-ready tables quickly.

    pip install git+https://github.com/nbatada/tblkit

Repo & README: [https://github.com/nbatada/tblkit](https://github.com/nbatada/tblkit)

Available commands are

    tblkit --commands
    tblkit
    â”œâ”€â”€ col                         (Column operations)
    â”‚   â”œâ”€â”€ add                     (Add a new column)
    â”‚   â”œâ”€â”€ clean                   (Normalize string values in selected columns.)
    â”‚   â”œâ”€â”€ drop                    (Drop columns by name/glob/position/regex)
    â”‚   â”œâ”€â”€ extract                 (Extract regex groups into new columns.)
    â”‚   â”œâ”€â”€ join                    (Join values from multiple columns into a new column.)
    â”‚   â”œâ”€â”€ move                    (Reorder columns by moving a selection.)
    â”‚   â”œâ”€â”€ rename                  (Rename column(s) via map string)
    â”‚   â”œâ”€â”€ replace                 (Value replacement in selected columns.)
    â”‚   â”œâ”€â”€ split                   (Split a column by pattern into multiple columns)
    â”‚   â”œâ”€â”€ strip                   (Trim/squeeze whitespace; optional substring/fixed-count strip.)
    â”‚   â””â”€â”€ subset                  (Select a subset of columns by name/glob/position/regex)
    â”œâ”€â”€ header                      (Header operations)
    â”‚   â”œâ”€â”€ add                     (Add a generated header to a headerless file.)
    â”‚   â”œâ”€â”€ add-prefix              (Add a fixed prefix to columns.)
    â”‚   â”œâ”€â”€ add-suffix              (Add a fixed suffix to columns.)
    â”‚   â”œâ”€â”€ clean                   (Normalize all column names (deprecated; use: tbl clean))
    â”‚   â”œâ”€â”€ prefix-num              (Prefix headers with 1_, 2_, ... (or custom fmt).)
    â”‚   â”œâ”€â”€ rename                  (Rename headers via map string or file)
    â”‚   â””â”€â”€ view                    (View header column names)
    â”œâ”€â”€ row                         (Row operations)
    â”‚   â”œâ”€â”€ add                     (Add a row with specified values.)
    â”‚   â”œâ”€â”€ drop                    (Drop rows by 1-based index.)
    â”‚   â”œâ”€â”€ grep                    (Filter rows by a list of words or phrases.)
    â”‚   â”œâ”€â”€ head                    (Select first N rows)
    â”‚   â”œâ”€â”€ sample                  (Randomly sample rows)
    â”‚   â”œâ”€â”€ shuffle                 (Randomly shuffle all rows.)
    â”‚   â”œâ”€â”€ subset                  (Select a subset of rows using a query expression)
    â”‚   â”œâ”€â”€ tail                    (Select last N rows)
    â”‚   â””â”€â”€ unique                  (Filter unique or duplicate rows)
    â”œâ”€â”€ sort                        (Sort rows or columns)
    â”‚   â”œâ”€â”€ cols                    (Sort columns by their names)
    â”‚   â””â”€â”€ rows                    (Sort rows by column values)
    â”œâ”€â”€ tbl                         (Whole-table operations)
    â”‚   â”œâ”€â”€ aggregate               (Group and aggregate numeric columns.)
    â”‚   â”œâ”€â”€ clean                   (Clean headers and string values throughout the table.)
    â”‚   â”œâ”€â”€ collapse                (Group rows and collapse column values into delimited strings.)
    â”‚   â”œâ”€â”€ concat                  (Concatenate tables vertically.)
    â”‚   â”œâ”€â”€ frequency               (Show top N values per column.)
    â”‚   â”œâ”€â”€ join                    (Relational join between two tables.)
    â”‚   â”œâ”€â”€ melt                    (Melt table to long format.)
    â”‚   â”œâ”€â”€ pivot                   (Pivot wider.)
    â”‚   â”œâ”€â”€ sort                    (Sort rows by column values (alias for 'sort rows').)
    â”‚   â””â”€â”€ transpose               (Transpose the table.)
    â””â”€â”€ view                        (Pretty-print a table (ASCII, non-folding).)

**Why shell scripters may want it**

* Handles CSV edge cases (quotes, commas, encodings) better than ad-hoc sed/awk/join.
* Column- and type-aware operations reduce brittle regex and indexing hacks.
* One focused tool instead of long chains; easier to read, test, and reuse in scripts or Makefiles.

**Why notebook/one-off Python users may want it**

* Faster first mile: prepare tidy inputs before opening a notebook.
* Less boilerplate than short pandas scripts; declarative commands you can paste into CI.
* Consistent results across machines; easy to share as a single CLI pipeline.

Feedback, bug reports, and contributions are very welcome.",Python,5,https://www.reddit.com/r/Python/comments/1n6aos0/ann_tblkit_swissarmy_cli_for_tabular_data_csvtsv/,r_1n6aos0,,,
r_1n69tas,reddit,ZtaDev,2025-09-02T04:08:34+00:00,"I created a playground to my python UI framework DARS
I'm excited to share the new Dars Playground! I have been working on this project for a long time now and I am expanding its ecosystem as much as I can. Now I have just launched a playground so that everyone can try Dars on the web without installing anything, just reading a little documentation and using bases from other frameworks. The next step will be to implement a VDom (virtual dom) option to the framework itself and a signals (hooks) system, all of this optional for those who want to use the virtual dom and those who do not, so use the export or hot reload that is already integrated.

The playground allows you to experiment with Dars UI code and preview the results instantly in your browser. It's a great way to learn, prototype, and see how Dars turns your Python code into static HTML/CSS/JS.

Key Features:

	â€¢ Write Dars Python code directly in the editor.
	â€¢ Instant preview with a single click (or Ctrl + Enter).
	â€¢ Ideal for experimenting and building UI quickly.

Give it a try and tell me what you think!

Link to Playground: https://dars-playground.vercel.app
Dars GitHub repository: https://github.com/ZtaMDev/Dars-Framework

#Python #UI #WebDevelopment #DarsFramework",Python,3,https://www.reddit.com/r/Python/comments/1n69tas/i_created_a_playground_to_my_python_ui_framework/,r_1n69tas,,,
r_1n65ef0,reddit,Top-Hawk8095,2025-09-02T00:29:29+00:00,"PrÃ©dire un match virtuel FIFA sur un bookmakers comme 1xbet
Comment collecter les donnÃ©es des matchs virtuels FIFA sur un bookmakers comme 1xbet ? J'en ai besoin vraiment, aidez moi.",Python,0,https://www.reddit.com/r/Python/comments/1n65ef0/prÃ©dire_un_match_virtuel_fifa_sur_un_bookmakers/,r_1n65ef0,,,
r_1n658es,reddit,frankieepurr,2025-09-02T00:21:40+00:00,"Is it a good idea to teach students Python but using an old version?
EDIT: Talking about IDLE here

Sorry if this is the wrong sub.

When i went to high school (UK) in 2018, we had 3.4.2 (which at the time wasn't even the latest 3.4.x). In 2020 they upgraded to 3.7, but just days later downgraded back to 3.4.2. I asked IT manager why and they said its because of older students working on long projects. But doubt that was the reason because fast forward to 2023 the school still had 3.4.2 which was end of life.

Moved to a college that same year that had 3.12, but this summer 2025, after computer upgrades to windows 11, we are now on 3.10 for some reason. I start a new year in college today so I'll be sure to ask the teacher.

Are there any drawbacks to teaching using an old version? It will just be the basics and a project or 2",Python,85,https://www.reddit.com/r/Python/comments/1n658es/is_it_a_good_idea_to_teach_students_python_but/,r_1n658es,,,
r_1n64s7q,reddit,AutoModerator,2025-09-02T00:00:30+00:00,"Tuesday Daily Thread: Advanced questions
# Weekly Wednesday Thread: Advanced Questions ðŸ

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! ðŸŒŸ",Python,5,https://www.reddit.com/r/Python/comments/1n64s7q/tuesday_daily_thread_advanced_questions/,r_1n64s7q,,,
r_1n64bla,reddit,msarabi,2025-09-01T23:38:36+00:00,"I built a simple, open-source Windows wallpaper changer because the built-in one kept failing.
# What My Project Does

This is a simple, lightweight desktop application for Windows that automatically changes your desktop wallpaper from a folder of images. You can choose a folder, set a custom time interval (in seconds, minutes, or hours), and have your pictures shuffle randomly. It can be minimized to the system tray. The application is built using `customtkinter` for the GUI and `pystray` for the system tray functionality.

# Target Audience

I write it for personal use and for anyone who wants a simple and minimalist way to manage their desktop wallpapers. It is a ""toy project"" in the sense that it started as a solution to a personal frustration, but it is meant to be a tool for everyday use.

# Comparison

I wrote this because the built-in Windows slideshow feature randomly stops working, which is incredibly frustrating and annoying, and they have been too lazy to fix it. Other third-party programs I looked at were often too cluttered with features I didn't need and/or were also resource-hungry. This application is meant to be a clean, minimal alternative that focuses on its single task.

You can find it here: [Wallpaper Changer](https://github.com/m-sarabi/wallpaper_changer/releases/tag/v1.0.0)",Python,31,https://www.reddit.com/r/Python/comments/1n64bla/i_built_a_simple_opensource_windows_wallpaper/,r_1n64bla,,,
r_1n5yppn,reddit,Royal-Bug-5025,2025-09-01T19:46:23+00:00,"Job application requirement
Hello. So I am trying to apply for an internship-level job in a large company. The position is financial risk management. I know that this post may seem completely irrelevant to the sub, but one of the requirements is ""Experience in python or R statistics"". Now I know basics in statistics and can use SPSS semi-proficiently, as in I have completed a course on it. I understand that this may be useless info, but I know Excel well as well.  

If anyone could tell me just how much experience would be expected for the latter in an entry-level position primarily focused on financial aspects and related risk management, mixed with statistical elements, that would be very appreciated. I don't have much time until the application due date runs out (around 2 weeks), but I am willing to learn and show desire that I can very much develop my knowledge in said area. 

If there is any possibility of making this happen, what tips are there to learn either of the mentioned programs in the aforementioned limited time space and what aspects would be the most resourceful to learn. 

Thanks a lot!",Python,0,https://www.reddit.com/r/Python/comments/1n5yppn/job_application_requirement/,r_1n5yppn,,,
r_1n5ux0w,reddit,unknown,2025-09-01T17:25:57+00:00,"Python + OCR: Automatically analyze Dota 2 player stats ðŸ‘€
# What My Project Does

This Python script uses OCR to read Dota 2 friend IDs from your screen, fetches match data from the OpenDota API, and calculates winrates and most played heroes to detect potential smurfs.  
It provides a simple GUI that shows overall winrate and the most played hero of the selected player.



# Target Audience

Python enthusiasts, Dota 2 players, or anyone interested in game data analysis and automation.  
This is mainly an educational and experimental project, not intended for cheating or modifying the game.



# Comparison

Unlike other Dota 2 analytics tools, this script uses OCR to automatically read friend IDs from the screen, eliminating the need to manually input player IDs.  
It combines GUI feedback, Python automation, and API integration in a single lightweight tool.

  
[GitHub Repository](https://github.com/N3uvin/opendota2-vision)

***Iâ€™m open to feedback, feature suggestions, or any ideas to improve the script!***",Python,34,https://www.reddit.com/r/Python/comments/1n5ux0w/python_ocr_automatically_analyze_dota_2_player/,r_1n5ux0w,,,
r_1n5sf57,reddit,2TB_NVME,2025-09-01T15:53:53+00:00,"Looking for a study buddy in Angela Yu""s 100 Days of Python, day 32
[Help](https://www.reddit.com/r/Python/?f=flair_name%3A%22Help%22)

Hi, I am a teenager and I am currently attending the 100 Days of Code course on Udemy and currently, I""ve been slacking off a little and falling behind schedule, because of this I am looking for a study partner that can hold me accountable and learn with me. So if you are a teenager like me and are on day 27-35 of the course, then we can start studying together!

DISCORD:arasaccount",Python,0,https://www.reddit.com/r/Python/comments/1n5sf57/looking_for_a_study_buddy_in_angela_yus_100_days/,r_1n5sf57,,,
r_1n5q8n0,reddit,onyx-zero-software,2025-09-01T14:30:17+00:00,"Introducing DLType, an ultra-fast runtime type and shape checking library for deep learning tensors!
# What My Project Does

DL (Deep-learning) Typing, a runtime shape and type checker for your pytorch tensors or numpy arrays! No more guessing what the shape or data type of your tensors are for your functions. Document tensor shapes using familiar syntax and take the guesswork out of tensor manipulations.

```python
@dltyped()
def transform_tensors(
    points: Annotated[np.ndarray, FloatTensor[""N 3""]]
    transform: Annotated[torch.Tensor, IntTensor[""3 3""]]
) -> Annotated[torch.Tensor, FloatTensor[""N 3""]]:
    return torch.from_numpy(points) @ transform
```

# Target Audience 

Machine learning engineers primarily, but anyone who uses numpy may find this useful too! 

# Comparison

- Jaxtyping-inspired syntax for expressions, literals, and anonymous axes
- Supports any version of pytorch and numpy (Python >=3.10)
- First class Pydantic model support, shape and dtype validation directly in model definitions
- Dataclass, named tuple, function, and method checking 
- Lightweight and fast, benchmarked to be on-par with manual shape checking and (at least last time we tested it) was as-fast or faster than the current de-facto solution of Jaxtyping + beartype, in some cases by an order of magnitude.
- Custom tensor types, define your own tensor type and override the check method with whatever custom logic you need

GitHub Page: https://github.com/stackav-oss/dltype

```
pip install dltype
```

Check it out and let me know what you think! ",Python,20,https://www.reddit.com/r/Python/comments/1n5q8n0/introducing_dltype_an_ultrafast_runtime_type_and/,r_1n5q8n0,,,
r_1n5jjnl,reddit,LostAmbassador6872,2025-09-01T08:49:32+00:00,"[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs
I previously shared the openâ€‘source library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.

**Live Demo:**Â [**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)

**Github :** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)

Would love to hear feedbacks!

Original Post :Â [https://www.reddit.com/r/Python/comments/1mh914m/open\_source\_tool\_for\_structured\_data\_extraction/](https://www.reddit.com/r/Python/comments/1mh914m/open_source_tool_for_structured_data_extraction/)",Python,24,https://www.reddit.com/r/Python/comments/1n5jjnl/update_docstrange_structured_data_extraction_from/,r_1n5jjnl,,,
r_1n5idvx,reddit,No_Pomegranate7508,2025-09-01T07:34:17+00:00,"Omni-LPR: A multi-interface server for automatic license plate recognition in Python
**What My Project Does**

Hi everyone,

I've made an open-source server in Python (called Omni-LPR) that exposes automatic license plate recognition (or ALPR) as a toolbox for LLMs and AI agents. It can also be used as a standalone microservice.

Here are some of its features:

* Installable as a Python package: `pip install omni-lpr`.
* Self-hostable for 100% local and private inference.
* Exposes tools via a native MCP endpoint for agents and a standard REST API.
* Includes examples for direct integration with tools like LM Studio.
* Hardware-accelerated backends for CPU, OpenVINO, and CUDA for faster performance.

Project's GitHub repo: [https://github.com/habedi/omni-lpr](https://github.com/habedi/omni-lpr)",Python,8,https://www.reddit.com/r/Python/comments/1n5idvx/omnilpr_a_multiinterface_server_for_automatic/,r_1n5idvx,,,
r_1n59zyk,reddit,AutoModerator,2025-09-01T00:00:32+00:00,"Monday Daily Thread: Project ideas!
# Weekly Thread: Project Ideas ðŸ’¡

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project ideaâ€”be it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! ðŸŒŸ",Python,7,https://www.reddit.com/r/Python/comments/1n59zyk/monday_daily_thread_project_ideas/,r_1n59zyk,,,
r_1n562vq,reddit,Rollgus,2025-08-31T21:04:57+00:00,"My first kinda complicated code (started like a month ago)
WHAT MY PROJECT DOES
I have made a card game where you are against a bot, and is trying to be the first to have only one Card left. 

TARGET AUDIENCE
This is just a project I made for fun, but I hope some people who are new to Python, or is interested in small text based games Will like this.

COMPARISON
I haven't seen any project like this, and I at least hope there aren't any. I feel this is a unique fun card game.

GitHub link:
https://github.com/Simonkamon11/One-Card.git",Python,27,https://www.reddit.com/r/Python/comments/1n562vq/my_first_kinda_complicated_code_started_like_a/,r_1n562vq,,,
r_1n54kbx,reddit,RPSpayments,2025-08-31T20:03:48+00:00,"Django vs FastAPI for SaaS with heavy transactions + AI integrations?
Iâ€™m building a SaaS that processes lots of transactions, handles AI-driven communications, and integrates with multiple external APIs.

Would you start with Django for quick ramp up or FastAPI for long-term flexibility? Is Django feasible for my use case? While FastAPI seems to be better due to async, my lack of experience with prod grade DB management makes Django seem good too, due to things such as automated migrations and the in built ORM. Current setup is FastAPI + SQLAlchemy and Alembic.

1. Anyone successfully combine them, Django for the monolith, FastAPI for specific endpoints?",Python,47,https://www.reddit.com/r/Python/comments/1n54kbx/django_vs_fastapi_for_saas_with_heavy/,r_1n54kbx,,,
r_1n5229t,reddit,big_like_a_pickle,2025-08-31T18:24:10+00:00,"How is Python 4 ever going to reach critical mass once everyone is using AI to write code?
I know that there is a lot of skepticism around using LLM tools to generate code. There is a tremendous amount of hype. However, I'd have to argue that at this point, it's inevitable that it's here to stay and is almost certain to continually improve.

Historically, AI usage maps to the same progression up the abstraction layers that we've seen for 80 years. Binary / machine code --> assembler --> C --> Python. It's a continual march to moving the coder further and further away from the machine.

Let's pretend that Python 4 is released. It contains a lot of great new features. However a LLM won't be able to utilize them because Python 4 wasn't part of its training corpora. But by this point, the software development industry has already shifted heavily to agent based development workflows. Many, many developers balk at this trend (much like they did from assembler to COBOL/FORTRAN) but the business economics make this shift inevitable. 

The problem is then, if everyone is using LLMs to write code, Python 4 will never be adopted because LLMs can't write it. And it is now economically undeniable to hand code anything in sufficient volume to result in enough training data for new languages. I'm wondering who in the computer science world is thinking about this problem? Is it hypothetical or is this going to be a real problem in a few years?",Python,0,https://www.reddit.com/r/Python/comments/1n5229t/how_is_python_4_ever_going_to_reach_critical_mass/,r_1n5229t,,,
r_1n4v2zm,reddit,PretendLead3104,2025-08-31T13:43:57+00:00,"gen-dual: Python library for high-order partial derivatives with dual numbers
**What My Project Does:**  
gen-dual is a Python library for vectorized computation of arbitrary-order partial derivatives of multivariable functions. It supports complex numbers and many functions like LambertW, Gamma, InverseErf, and Abs. Derivatives are computed all at once using a dual-number-like method, useful for analyzing Taylor series, function behavior, or any derivative-related computations.

**Target Audience:**  
This library is meant for anyone interested in exploring high-precision, multi-variable differentiation in Python, including researchers, students, or hobbyists.

**Comparison:**  
Unlike standard automatic differentiation libraries, gen-dual supports arbitrary-order derivatives, full vectorization, complex numbers, and rich function support, making it more flexible than most existing alternatives.

**GitHub Link:**  
https://github.com/LukaLavs/Generalized-Dual
",Python,17,https://www.reddit.com/r/Python/comments/1n4v2zm/gendual_python_library_for_highorder_partial/,r_1n4v2zm,,,
r_1n4ri23,reddit,WonderfulAccident836,2025-08-31T10:41:14+00:00,"MIDI Scripter - a framework for scripting MIDI, OSC, keyboard and mouse input and output
# What My Project Does

Receives, modifies, and sends MIDI, OSC, keyboard, and mouse I/O with minimal boilerplate and a configurable GUI for controls and logging.

# Target Audience

* Musicians who need custom and complex MIDI setups that may also use OSC, keyboard, and mouse I/O or control Ableton Live.
* Developers of MIDI I/O-centric apps.

# Comparison

MIDI Scripter is a hub framework for python-rtmidi, python-osc, and pynput that unifies them with a common minimalistic documented API and uses PySide6 for an optional GUI. It doesn't do more than these libraries, but it minimizes boilerplate and allows to focus on the I/O handling part.

As a Python framework, MIDI Scripter is more versatile than GUI MIDI modification apps. C-based apps may have less latency and jitter, but MIDI Scripter remains within the margins of what is noticeable in a real-time performance.

# Example

An octave transposer with GUI controls:

    from midiscripter import *  
      
    midi_keyboard = MidiIn('MIDI Keyboard')  # GUI will provide you the port names  
    proxy_output = MidiOut('To DAW', virtual=True)  # virtual proxy port for output  
      
    # GUI widget in a single line  
    octave_selector = GuiButtonSelectorH(('-2', '-1', '0', '+1', '+2'), select='0')  
      
    @midi_keyboard.subscribe  # decorated function will receive port's messages 
    def transpose(msg: MidiMsg) -> None:  
        if msg.type == MidiType.NOTE_ON or msg.type == MidiType.NOTE_OFF:  # filter       
    	msg.data1 += 12 * int(octave_selector.selected_item_text)  # modify
    	proxy_output.send(msg)  # route  
    	    
    if __name__ == '__main__':  
        start_gui()  # opens helpful customizable GUI

# Links 

* [GitHub](https://github.com/Maboroshy/midi-scripter) 
* [Documentation](https://maboroshy.github.io/midi-scripter/)",Python,1,https://www.reddit.com/r/Python/comments/1n4ri23/midi_scripter_a_framework_for_scripting_midi_osc/,r_1n4ri23,,,
r_1n4rahf,reddit,YoussefBenhammouda,2025-08-31T10:27:49+00:00,"Just built: pydantic-gsheets to bring Google Sheets and Pydantic together
Hey everyone,  
I have developed a small experimental package calledÂ **pydantic-gsheets**.

# What My Project Does

[pydantic-gsheets](https://github.com/Youssefbenhammouda/pydantic-gsheets) is a small experimental package that lets you read and write Google Sheets data in Python using nothing but Pydantic models. Define a BaseModel, and you can validate, parse, and sync data with Sheets without extra boilerplate.

# Target Audience

Itâ€™s meant for quick prototypes, small projects, or teams that love using Google Sheets but want type safety when bringing that data into Python. At this stage itâ€™s still **experimental**, so not yet recommended for production â€” but great for tinkering, demos, or internal tools.

# Comparison

There are other ways to connect Python to Google Sheets (e.g., gspread, pygsheets), but they typically give you raw dicts or lists that you then have to validate manually. The difference here is that pydantic-gsheets plugs directly into **Pydantic BaseModels**, so your schema, validation, and type coercion happen automatically. You donâ€™t have to write glue code.

# Links

Links if you want to peek:  
\* Blog: \[Exploring pydantic-gsheets\](https://youssef.benhammouda.ma/blog/pydantic-gsheets)

\* Docs: \[pydantic-gsheets documentation\](https://youssefbenhammouda.github.io/pydantic-gsheets/)

\* GitHub: \[pydantic-gsheets repo\](https://github.com/Youssefbenhammouda/pydantic-gsheets)

Would love to hear thoughts or ideas if you try it out ðŸ™‚

PS: If you find it useful and want to use it, please know itâ€™s stillÂ **experimental**. That also means collaborators areÂ **very welcome,**Â whether itâ€™s testing, bug reports, or PRs.",Python,38,https://www.reddit.com/r/Python/comments/1n4rahf/just_built_pydanticgsheets_to_bring_google_sheets/,r_1n4rahf,,,
r_1n4qygz,reddit,cwt114,2025-08-31T10:06:58+00:00,"Introducing NeoSQLite
**Showcase: NeoSQLite â€“ Use SQLite with a PyMongo-like API**

I'm excited to introduce **NeoSQLite** (https://github.com/cwt/neosqlite), a lightweight Python library that brings a PyMongo-compatible interface to SQLite. This means you can interact with SQLite using familiar MongoDB-style syntaxâ€”inserting, querying, and indexing JSON-like documentsâ€”while still benefiting from SQLiteâ€™s simplicity, reliability, and zero configuration.

### What My Project Does

NeoSQLite allows you to:
- Use MongoDB-style operations like `insert_one`, `find`, `update_one`, and `delete_many` with SQLite.
- Perform full-text search across multiple languages using the `$text` operator, powered by an ICU-based tokenizer (via my [fts5-icu-tokenizer](https://github.com/cwt/fts5-icu-tokenizer)).
- Automatically compress query results using [quez](https://github.com/cwt/quez), reducing memory usage by 50â€“80% for large result sets.
- Work with embedded documents and nested queries, all backed by SQLiteâ€™s ACID-compliant storage.

Itâ€™s designed for developers who love MongoDBâ€™s ease of use but want a lightweight, file-based alternative without external dependencies.

### Target Audience

NeoSQLite is ideal for:
- Developers building small to medium-sized applications (e.g., CLI tools, desktop apps, IoT devices) where deploying a full MongoDB instance is overkill.
- Projects that need a schema-flexible, document-style database but must remain portable and dependency-free.
- Prototyping or educational use, where a MongoDB-like interface speeds up development without requiring server setup.
- Environments with limited resources, thanks to its memory-efficient result compression.

Itâ€™s not intended to replace MongoDB in high-concurrency, large-scale production systems, but itâ€™s production-ready for lightweight, embedded use cases.

### Comparison with Existing Alternatives

Unlike other SQLite-to-document-store wrappers, NeoSQLite stands out by:
- Offering **deep API compatibility with PyMongo**, minimizing the learning curve for developers already familiar with MongoDB.
- Supporting **true multilingual full-text search** via ICU (not just ASCII or basic Unicode), which most SQLite FTS solutions lack.
- Reducing memory footprint significantly through built-in result compressionâ€”something not offered by standard SQLite ORMs like SQLAlchemy or dataset.
- Being **zero-configuration and serverless**, unlike MongoDB (which requires a running service) or libraries like TinyDB (which lack indexing, full-text search, or performance optimizations).

In short, if youâ€™ve ever wished you could use MongoDBâ€™s API with SQLiteâ€™s simplicity, NeoSQLite is for you.

---

Feedback and contributions are welcome. Check it out at: https://github.com/cwt/neosqlite

---

20250903: Iâ€™ve made a lot of updates since my last post. Performance has improved thanks to the use of temp table. Please check it out and give it a try!",Python,26,https://www.reddit.com/r/Python/comments/1n4qygz/introducing_neosqlite/,r_1n4qygz,,,
r_1n4pitk,reddit,Raytracer,2025-08-31T08:33:40+00:00,"IntentGraph â€“ Open-source Python library for repo dependency graphs & clustering
Hello everybody,

I started this project out of a pain point I kept hitting: when working with larger repos, itâ€™s easy to lose track of how files connect. And when trying to use automation tools (AI or otherwise), the problem gets worse: once you go past a few files, context just disappears, or the token count explodes every time the tool has to look through the whole codebase.

Thatâ€™s what led me to build **IntentGraph**: a Python library to map dependencies and structure repos in a way thatâ€™s useful for developers *and* for programmatic agents.

**What My Project Does**

IntentGraph is a Python library for analyzing large codebases. It:

* Maps dependencies between files and modules

* Clusters code (analysis, refactoring, navigation)

* Produces structured outputs at 3 levels (minimal â†’ full detail)

* Designed to be **programmatically queryable**: useful for developers and AI agents that need structured repo context

**Target Audience**

* Developers who want to explore or refactor large Python repos

* Tool builders needing a structured representation of a codebase

* Researchers interested in program analysis and code graphing

* AI/automation workflows that require repo-wide context

**Comparison**

Unlike linting/static analysis tools, IntentGraph focuses on structural understanding of the codebase. This structured output makes it lightweight enough for automated tools and AI agents to consume directly.

**Links:**

GitHub: [https://github.com/Raytracer76/IntentGraph](https://github.com/Raytracer76/IntentGraph)

PyPI: [https://pypi.org/project/intentgraph/](https://pypi.org/project/intentgraph/)

**Open Source & Call for Contributions**

IntentGraph is fully open source. I encourage forks, experiments, and extensions â€” for example, expanding it into other languages (Java, Rust, C#, etc.).
I likely wonâ€™t drive this much further myself, but Iâ€™d love to see where the community takes it.

**Looking for feedback:**

* Whatâ€™s missing for practical use in Python projects?

* Ideas for integrations (e.g., VS Code)?

* Languages youâ€™d want supported next?",Python,19,https://www.reddit.com/r/Python/comments/1n4pitk/intentgraph_opensource_python_library_for_repo/,r_1n4pitk,,,
r_1n4nhbk,reddit,esSdoem,2025-08-31T06:23:00+00:00,"My python mini project
I have made an app that is great for studing python and begginer friendly as well, I would like to introduce you to `lisq` a single file, lightweight and portable python note-taking app. It would not only serve you as notes but also allow you to add your own functions, advanced searching through out the notes, edit, encrypt and much more (please read README for more information!).

Official github repository:
https://github.com/funnut/Lisq.git

Share & leave a star ðŸŒŸ",Python,10,https://www.reddit.com/r/Python/comments/1n4nhbk/my_python_mini_project/,r_1n4nhbk,,,
r_1n4mipq,reddit,status-code-200,2025-08-31T05:25:05+00:00,"SecBrowser: A simple visual interface for SEC Filings
**What my project does**

Provides a visual interface for the functions in my package [datamule](https://github.com/john-friedman/datamule-python) using flask. You can do stuff such as:

* View XBRL
* View Company Fundamentals
* View extracted text
* View documents (html, pdf) converted to dictionary form ([doc2dict](https://github.com/john-friedman/doc2dict))
* Apply NLP such as basic entity recognition on text and on the dictionary form (NLP is in an early stage)

**Target Audience**

* Me to debug stuff.
* Maybe you if you like SEC data or enjoy looking at document parsing visualizations?

**Why I made it**

I needed a visual interface to hel-p me debug doc2dict and datamule's early nlp features.

**Comparison**

This is kind of a niche thing. I decided to release it on pypi in case someone found it useful.

**Installation**

pip install datamule

**Links**

* [GitHub](https://github.com/john-friedman/secbrowser)
* [Medium](https://medium.com/@jgfriedman99/secbrowser-edb36db3230f) \- I think the medium link might get this removed, but adding it because it is 99% photos of what my package does and why you might find it cool.",Python,5,https://www.reddit.com/r/Python/comments/1n4mipq/secbrowser_a_simple_visual_interface_for_sec/,r_1n4mipq,,,
r_1n4ilwx,reddit,teslah3,2025-08-31T01:50:49+00:00,"PySimpleGUI Hobbyist License Canceled
So I used PySimpleGUI for a single project and received the 30 day free trial assuming Id be able to get the hobbyist version once it was over. Is it crazy to anyone else that it cost $99 to just save a few lines of code considering I can create the same, if not a more customizable GUI using C/C++. My project which wasnt too crazy (firetv remote using adb protocol) is now garbage because I will not pay for the dumb licensing fee, but hey maybe a single person should pay the same amount a billion dollar company pays right???\`",Python,99,https://www.reddit.com/r/Python/comments/1n4ilwx/pysimplegui_hobbyist_license_canceled/,r_1n4ilwx,,,
r_1n4hc9e,reddit,No_Blackberry_617,2025-08-31T00:46:58+00:00,"Python type system
(Just sharing something)  
  
As someone who has taken advantage of TypeScript's type safety for most of its career, using Python without type safety feels a bit awkward. I put together a page explaining how to take advantage of Python's type system and how to extend it on your editor.

[https://crocus-ceres-509.notion.site/How-Python-type-system-works-and-how-to-extend-it-on-your-editor-21e3826aa7ed808b93e2f4d18493c6ea](https://crocus-ceres-509.notion.site/How-Python-type-system-works-and-how-to-extend-it-on-your-editor-21e3826aa7ed808b93e2f4d18493c6ea)",Python,13,https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/,r_1n4hc9e,,,
r_1n4gdyj,reddit,AutoModerator,2025-08-31T00:00:19+00:00,"Sunday Daily Thread: What's everyone working on this week?
# Weekly Thread: What's Everyone Working On This Week? ðŸ› ï¸

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! ðŸŒŸ",Python,7,https://www.reddit.com/r/Python/comments/1n4gdyj/sunday_daily_thread_whats_everyone_working_on/,r_1n4gdyj,,,
r_1n40rht,reddit,tigert1998,2025-08-30T12:46:54+00:00,"I built my own torch in the last two weeks!
**What my project does:**

In the last two weeks, I have been working on building my own toy project: a deep learning training framework. It is named ""mytorch"". It was written from scratch except that I use cublaslt for high performance matmul operations. Now it can do most of the pytorch stuff:

\- cuda support for forward/backward operators in CNN MNIST training and evaluations, such as, BN, Conv, Linear, many elementwise ops, many reduce ops, many essential ops;

\- SGD optimizer;

\- Load/save state dict for module/optimizer

\- Dataset/DataLoader

\- Autograd system: topsort for backward.

**Target Audience:**

It is a toy project for education.

**Comparison with other products:**

In terms of results, when training MNIST for 3 epochs in my 4060 laptop, PyTorch takes 33 seconds while ""mytorch"" takes 41 seconds which is just 25% slower. PyTorch is a highly optimized framework for production. But my project is for fun and for learning more about cuda programming/autograd system.

Please leave a star on my git repo or leave a comment below if you are interested. Thanks so much!  
[s://github.com/tigert1998/mytorch/tree/main](https://github.com/tigert1998/mytorch/tree/main)",Python,60,https://www.reddit.com/r/Python/comments/1n40rht/i_built_my_own_torch_in_the_last_two_weeks/,r_1n40rht,,,
r_1n3un64,reddit,CODE-with-SHEEL,2025-08-30T06:32:58+00:00,"Let's Learn Together<3
So ive been willing to do frontend development since a week and now ive made all the important things sum up like lectures, documents, project ideas, etc.

Lets grow together, see im new to this and will take all the positive feedbacks from you guys. Anyone up to work and lean together? should i make a discord channel? ",Python,0,https://www.reddit.com/r/Python/comments/1n3un64/lets_learn_together3/,r_1n3un64,,,
r_1n3ne68,reddit,styrofoamshotgun,2025-08-30T00:05:10+00:00,"Python-Based Magic: The Gathering Commander Deck Builder
HiÂ r/Python, I've been working off-and-on (mostly off) on a Python-based deck builder for a Magic: the Gathering Commander format. Last week I had a mostly working command line driven version I shared over on those related subs, but this week I've got a fleshed out build, this time with a fully-featured web UI.

This is my first actual software dev release and I'm proud to put it out there.

# What my Project Does

* Pick your commander and up to three themes (e.g., Aristocrats, +1/+1, Kindred, Aggro).
* It proposes a clean 100â€‘card list that fits those themes, with clear stageâ€‘byâ€‘stage reasons.
* Multiâ€‘copy strategies? If your pick supports Persistent Petitioners, Dragonâ€™s Approach, or Shadowborn Apostle, it offers a package. You choose how many, it keeps you at 100, and you can include Thrumming Stone when it makes sense.
* Web: multiâ€‘copy packages are now offered right after commander selection, so there are no surprises later.
* Web: the package is applied first, and land building happens afterâ€”counts and targets autoâ€‘adjust so the deck stays clean at 100.
* Web polish: the UI shows when targets were adjusted and if anything was clamped. Small fixes for names with apostrophes.

# Target Audience

* Magic: The Gathering fans
* People like me, who like to theorycraft, who like to throw together decks online they may not ever actually use
* People who just want to give a base set of instructions and have something throw a deck together for them

# Comparison

Honestly I'm not sure if there is one or at least that I've seen? Obviously EDHRec and Moxfield/Archidekt can help with the deck building, but you generally need to do input every step of the way.

I originally started working on this last November because I wanted a way to throw a bunch of decks together without needing to do it all manually. At the time I wasn't really seeing anything Python-based or otherwise that does it in a more hands-off way.

This way also let's me throw together a handful of the decks with the same commander, themes, and ideologies, then compare them for differences or see what's different.

# Web UI at a glance

* Mobile support not quite working (landscape get squished), recommended to load from a computer or in portrait mode
* ""New Deckâ€ modal: search commander, pick up to 3 themes (AND/OR), choose bracket (not fully implemented), an optional deck name, and the ideal counts for a variety of card types you'll want in every deck (lands, card draw, wipes, etc...).
* Multi-copy packages: suggests Petitioners/Approach/Apostles when relevant; you pick counts (Thrumming Stone optional). Applied first with auto target tweaks and a 100-card clamp.
* Fast iteration: lock favorites, Replace any pick with alternatives (Owned-only filter), and Rerun Stage to re-roll just creatures/spells/lands (respects locks).
* Use your collection: upload TXT/CSV owned lists; build owned-only or prefer owned. Short owned-only builds get a recommendations file.
* Visual clarity: Mana Curve, Color Pips, and Sources with hover-to-highlight and cross-highlighting; includes colorless â€˜Câ€™.
* Exports: TXT for Moxfield/Archidekt, CSV with tags (and Owned column), plus a simple printout.
* Nice-to-use touches: optional virtualized lists for speed, lazy-loaded images, reduced-motion friendly, theme selector, and helpful keyboard shortcuts.

# Tune and iterate

* Lock cards you love so reruns keep them.
* Swap any pick for an alternative; filter to owned cards if you want.
* Compare versions sideâ€‘byâ€‘side to see what changed.

# Use your collection

* Drop TXT/CSV lists of your owned cards.
* Build using only owned cards, or simply prefer owned while still picking the best fits.
* If an ownedâ€‘only build runs short, it exports a â€œrecommended pickupsâ€ list so you can finish it out.

# Atâ€‘aâ€‘glance clarity

* Mana curve and color sources summaries with hoverâ€‘toâ€‘highlight matching cards.
* CSV export marks which cards you own.

# Exports

* TXT ready for Moxfield/Archidekt
* CSV with tags and details
* Simple printable list

# Try it

* Live example available here:Â [https://deck-builder.wiz-ops.com/](https://deck-builder.wiz-ops.com/)Â (do note if you run the setup/tag it will take a few minutes)
* Docker Hub (easiest, opens the Web UI):Â [https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder](https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder)
* Use the dockerhub-docker-compose.yml file to do it all for you.
* Windows EXE or run from source: see the latest release below.

# Links

* Latest release (notes + downloads):Â [https://github.com/mwisnowski/mtg\_python\_deckbuilder/releases/latest](https://github.com/mwisnowski/mtg_python_deckbuilder/releases/latest)
* Source:Â [https://github.com/mwisnowski/mtg\_python\_deckbuilder](https://github.com/mwisnowski/mtg_python_deckbuilder)

# Roadmap

* Proper bracket implementation: tighter, consistent power targets across all stages.
* Random modes: â€œsurprise meâ€ overall, random by theme, and oneâ€‘click random complete builds.
* Budget mode: soft/hard caps with price tiers and a pickups list that fits a budget.
* Mustâ€‘include / mustâ€‘exclude lists: lock in pet cards or avoid specific pieces.
* Smarter land bases: basicsâ€‘heavy vs. fixingâ€‘heavy profiles guided by curve and color pips.
* Expanded multiâ€‘copy helpers (where legal) with clearer guidance when theyâ€™re viable.

Missing a theme for your favorite commander or found a bug? Issues/PRs welcome.",Python,30,https://www.reddit.com/r/Python/comments/1n3ne68/pythonbased_magic_the_gathering_commander_deck/,r_1n3ne68,,,
r_1n3nauu,reddit,AutoModerator,2025-08-30T00:00:54+00:00,"Saturday Daily Thread: Resource Request and Sharing! Daily Thread
# Weekly Thread: Resource Request and Sharing ðŸ“š

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! ðŸŒŸ",Python,2,https://www.reddit.com/r/Python/comments/1n3nauu/saturday_daily_thread_resource_request_and/,r_1n3nauu,,,
r_1n3i9r3,reddit,freshly_brewed_ai,2025-08-29T20:26:06+00:00,"Feedback on my daily python newsletter
Wanted to share the free (no subscription or paywall) newsletter I have created where I send bite sized Python snippets daily for absolute beginners. My personal journey led to the creation of this as I was not being consistent when I started with Python, and felt many busy professionals might be in a similar situation. 
Happy to remove this if it's not the right group, but any feedback (even like this newsletter is not needed) is helpful. 
https://pandas-daily.kit.com/",Python,6,https://www.reddit.com/r/Python/comments/1n3i9r3/feedback_on_my_daily_python_newsletter/,r_1n3i9r3,,,
r_1n3gmbr,reddit,Embarrassed_Twist232,2025-08-29T19:20:52+00:00,"PyData Seattle Tickets Labor Day Sale: 25% Off This Weekend Only!
Hey r/Python

PyData is a program of NumFOCUS, a nonprofit fiscal sponsor to open source projects like pandas, NumPy and many more. We're excited to bring a conference to Seattle and are running a **Labor Day Flash Sale** this weekend!

ðŸŽŸ **Get 25% off your conference ticket**. The sale ends Monday at midnight PT!

ðŸ‘‰ [Grab your discounted ticket here](https://ti.to/pydata/pydata-seattle-2025/discount/SUPERSEATTLE25)

**PyData Seattle** will be November 7â€“9 at Bellevue College for three days of talks, tutorials, and networking with Python data enthusiasts from around the world.

Donâ€™t miss out â€” more information at [https://pydata.org/seattle2025/](https://pydata.org/seattle2025/) ",Python,4,https://www.reddit.com/r/Python/comments/1n3gmbr/pydata_seattle_tickets_labor_day_sale_25_off_this/,r_1n3gmbr,,,
r_1n39ov5,reddit,n1k0h1k0,2025-08-29T14:57:17+00:00,"What are your tips to find the newest libraries/tools?
The question is more for your intended use case, but it still stands for improvements I might not even know that I wanted.

I've tried looking through my favorite libraries for documentation updates, listening to podcasts and watching Youtube videos, etc.",Python,47,https://www.reddit.com/r/Python/comments/1n39ov5/what_are_your_tips_to_find_the_newest/,r_1n39ov5,,,
r_1n38w1w,reddit,Greedy_Point7755,2025-08-29T14:25:59+00:00,"Python e-commerce store
I am currently building an e-commerce store using AWS services and Django framework. Anyone have advice on how make the website look better as my skills in front end development lacks creativity. Any advice is appreciated. ",Python,0,https://www.reddit.com/r/Python/comments/1n38w1w/python_ecommerce_store/,r_1n38w1w,,,
r_1n38glw,reddit,Secretor_Aliode,2025-08-29T14:09:13+00:00,"Handwritten image to text.
Hi, there. Is there's existing JavaScript library ocr, for image but handwritten turn into text?.

Except: Tesseract.js I test it to my hand written not accurate.

My choice is Pytesseract but I doubt that the set up is time consuming or when deployment I need to pay expensive.

I know image to pdf like pdf-lib, but still can't guarantee about ocr handwritten accuracy.

Thank you.

Thank you for your suggestions ðŸ˜ƒ.",Python,0,https://www.reddit.com/r/Python/comments/1n38glw/handwritten_image_to_text/,r_1n38glw,,,
r_1n38cz5,reddit,unknown,2025-08-29T14:05:03+00:00,"I need feedback for my first personal python project
# ReArgs - My First Python Project

I just started -2 or 3 months- my backend development journey using a platform, and after some courses the platform required me to build my own project to get out of tutorial hell and build something by myself.

To be honest, I already knew JavaScript and TypeScript and have an amateur frontend past -like 3 years- but wanted to switch to backend due to my dissatisfaction with frontend development. So, this was not exactly a first project for me.

Building this application took 2 weeks -counting weekends and breaks- and I believe I gave it a fair amount of effort and thought.

Before starting building the application I spent a day to decide what to build. I wanted to build something personal and might actually use in the future.

# What My Project Does

I like writing articles, posts, writings but often I fall into repeating myself and text I write turns into a mess. I don't want to limit my pen or stop myself with that thought because I see writing as a process that shouldn't be stopped when there are things to write, it's personal for me. I do keep journals.

Technically my application takes a txt file -path passed as an argument-, copies it, finds the similarities on the text using Sentence Transformers and internally saves the clusters, create an output text and a cli output from the clusters.

# Comparison

Then I thought, what if I built an app that showed me the semantic similarities in my article. Then I said to myself why don't I use ChatGPT for that? Then I said well, I don't want this program to fix the article, or give me advice or the things I don't want to see like ChatGPT does. I wanted a simple program that showed me the similarities and actually after a day of thinking of what to build, this was the most doable and realistic one.

# Target Audience

So, I built the app for my personal use, got myself 5000xp, and a GitHub repository, although the course description said this is application will probably not something you show in your portfolio, I still shared it on LinkedIn.

But if you are interested in writing stuff -and actually can use this application on any text- and like to see the semantic similarities in your text, this is the app for you. I even used it on this reddit post too. 

All kind of feedback is welcome, I built tests, and did not face any bugs during production phase, but you never know what might happen.

GitHub Repository Link: [GitHub Repo](https://github.com/mehmetcagriekici/reargs)

README from the GitHub Repository:

>\# ReArgs

>

>\*\*ReArgs (not â€œregardsâ€)\*\* is a command-line Python application that analyzes \`.txt\` files for semantic repetitions and similarities.

>It does \*\*not\*\* rewrite your text for youâ€”it simply helps you \*\*visualize and organize\*\* your writing by highlighting repetitions and grouping similar content.

>

>\---

>

>\##  Motivation

>

>I enjoy writing posts and articles (often on Reddit), but I noticed a recurring problem:

>my drafts quickly turned into a mess because of poor planning and constant repetition.

>

>Reading an entire article multiple times to catch repetitions was frustrating, so I built \*\*ReArgs\*\* to automatically surface these similarities.

>It helps me:

>

>\- Write \*\*cleaner articles\*\* by avoiding unintentional repetition.

>\- \*\*Understand other articles\*\* better by grouping sentences and paragraphs with similar meaning.

>

>\---

>

>\##  How to Use

>

>Clone the repo and install dependencies:

>

>Run the provided shell script with a \`.txt\` file as an argument:

>

>\`\`\`bash

>./run.sh path/to/article.txt

>\`\`\`

>

>\### Notes

>

>\- The application only accepts \*\*one \`.txt\` file at a time\*\*.

>\- Your original file is never modified.

>\- Results are displayed in the console and also written to the \`output/\` folder.

>\- The \`transforms/\` folder is used internallyâ€”do not manually modify its contents.

>

>\---

>

>\## How It Works

>

>

>2. It splits the article into \*\*paragraphs\*\* and \*\*sentences\*\*.

>3. Using \[Sentence Transformers\](https://github.com/UKPLab/sentence-transformers), it:

>

>\- Finds semantic similarities within each paragraph.

>\- Then checks similarities \*\*across the entire article\*\*.

>

>\### Similarity Clusters

>

>\- \*\*Hard clusters (â‰¥ 0.8 similarity):\*\* treated as duplicates.

>\- \*\*Soft clusters (0.6â€“0.8 similarity):\*\* treated as sentences with close meaning.

>

>Finally:

>

>\- A \*\*similarity graph\*\* and grouped results are printed to the console.

>\- A summary report is written to the \`output/\` folder.

>

>The purpose is to highlight repetitions, not to automatically generate polished text.

>

>\---

>

>\##  Disclaimer

>

>ReArgs is a \*\*writing assistant\*\*, not an article generator.

>It is designed to \*\*help you improve your own writing\*\* by making patterns more visible.",Python,2,https://www.reddit.com/r/Python/comments/1n38cz5/i_need_feedback_for_my_first_personal_python/,r_1n38cz5,,,
r_1n37c65,reddit,amosmj,2025-08-29T13:23:09+00:00,"Abstracting a script for general use
I'm going through an exercise right now of taking a script that I wrote linearly and ran manually and trying to convert it into something more general and abstract and it's pretty rough. I'm sure there are things I could have done from the the start to make this process easier. I'm looking for tips or frameworks on the conversation but also tips and frameworks that my betters would have used from the start.

For example:  
I wrote a script that is pointed at a folder and it scans for github repos. Once it finds the repos it scans for certain types of files (sql for the most part). It then scans each file for keywords to document table reads and writes.

From the beginning I broke it out similar to the sentences above, each as a function. But, now I'm trying to convert it so someone else can import it just call a piece of it, e.g. you want to manually scan just one file, you can import this and run just that function. I'm in the phase of trying to track down any variables that need to be passed as a parameter when I call it in the abstract vs run it in main.

Basically any tips on turning what was meant as a script into a reusable package. ",Python,7,https://www.reddit.com/r/Python/comments/1n37c65/abstracting_a_script_for_general_use/,r_1n37c65,,,
r_1n36mcu,reddit,InternationalBoat727,2025-08-29T12:52:43+00:00,"Phicode Runtime Engine (Open-Source)
Hey all,

I've been working on **Phicode**, a Python runtime engine designed to be reliable, stable, performant, and secure while maintaining your existing workflow.

\## What My Project Does

Phicode is a Python runtime engine that runs your existing Python code with automatic optimizations. It provides robust caching (source, bytecode, spec, imports) with integrity checks, optional security modules with sandboxing and threat detection, and automatically switches between PyPy & CPython based on workload analysis. It includes a built-in benchmarking suite that outputs CSV/JSON/Mermaid diagrams, a RESTful API, and optional custom syntax support (.Ï† or .phi files) that's fully configurable and mixable with standard .py files.

\## Target Audience

This is for Python developers who want performance optimization (& customization) without changing their existing codebase. Whether you're running data processing pipelines, web applications, or computational workloads, Phicode automatically manages your runtime environment. The engine runs standard Python out of the box with negligible overhead, making it suitable for both development and production environments.

\## Comparison

Unlike standard Python interpreters that require manual switching between CPython and PyPy, or tools like pyenv that only manage Python versions, Phicode provides automatic interpreter switching based on workload characteristics. While PyPy offers performance gains and CPython provides compatibility, Phicode intelligently chooses between them. It combines the benefits of both with comprehensive caching, security features, and performance monitoring that typically require separate tools. The Engine acts like a middleman between ur codebase and the interpreters.

**Current features:**

* Robust caching with integrity checks
* Optional security modules (sandboxing + threat detection)
* Auto-switch between PyPy & CPython based on workload
* Custom syntax support (configurable)
* Built-in benchmarking suite with CSV/JSON/Mermaid output
* RESTful API

**In development:**

* Daemon support (process management)
* Intelligent interpreter switching based on project's Python version

The syntax extension is completely optional. You can adopt it gradually or not at all. It allows for domain specific keywords, you yourself can define via a config.json

The VS Code extension allows running your scripts from the editor, or right-click to convert Python files if desired.

    pip install phicode
    phicode my_script

**Requirements:** Python 3.8+ | **License:** Phicode-License | **Platforms:** Windows, Linux

I'm curious how you experience the engine for yourself! More information is covered in the GitHub README.

Open to contributions & feedback!

**GitHub:** [https://github.com/Varietyz/phicode-runtime](https://github.com/Varietyz/phicode-runtime)  
**PyPI:** [https://pypi.org/project/phicode/](https://pypi.org/project/phicode/)  
**VS Code Extension:** [https://marketplace.visualstudio.com/items?itemName=Banes-Lab.phicode](https://marketplace.visualstudio.com/items?itemName=Banes-Lab.phicode)",Python,2,https://www.reddit.com/r/Python/comments/1n36mcu/phicode_runtime_engine_opensource/,r_1n36mcu,,,
r_1n32lxf,reddit,Ninteendo19d0,2025-08-29T09:20:23+00:00,"Can I get some feedback on the documentation of jsonyx?
`jsonyx` is the second library I've written and the first one with proper documentation. I've tried to make it as detailed as possible, but I've no idea whether everything is clear. What do you think?

- pypi: https://pypi.org/project/jsonyx
- docs: https://jsonyx.readthedocs.io/en/latest/index.html",Python,3,https://www.reddit.com/r/Python/comments/1n32lxf/can_i_get_some_feedback_on_the_documentation_of/,r_1n32lxf,,,
r_1n324wb,reddit,NullPointerMood_1,2025-08-29T08:49:52+00:00,"Python feels easyâ€¦ until it doesnâ€™t. What was your first real struggle?

When I started Python, I thought it was the easiest language everâ€¦ until virtual environments and package management hit me like a truck.

What was your first â€˜Oh no, this isnâ€™t as easy as I thoughtâ€™ moment with Python?",Python,799,https://www.reddit.com/r/Python/comments/1n324wb/python_feels_easy_until_it_doesnt_what_was_your/,r_1n324wb,,,
r_1n2y5ch,reddit,Jealous_Driver_1716,2025-08-29T04:40:10+00:00,"AIpowered desktop app for content summarization and chat (PDF/YouTube/audio processing with PySide6)
**What My Project Does:**
Learnwell is an AI-powered desktop application that processes various content formats (PDFs, YouTube videos, audio files, images with OCR) and generates intelligent summaries using Google's Gemini API. It features real-time chat functionality with processed content, automatic content categorization (lectures, conversations, news, gaming streams), and conversation history management.

**Target Audience:**
Students, researchers, content creators, and professionals who need to quickly process and summarize large amounts of content from different sources. Particularly useful for anyone dealing with mixed media content who wants a unified tool rather than switching between multiple specialized applications.

**Comparison:**
Unlike web-based tools like Otter.ai (audio-only) or ChatPDF (PDF-only), Learnwell runs locally with your own API key, processes multiple formats in a single application, and maintains conversation context across sessions. It combines the functionality of several specialized tools into a unified desktop experience while keeping your data local.

**Technical Implementation:**
- PySide6 (Qt) for cross-platform GUI
- Google Gemini API for AI processing
- OpenAI Whisper for speech-to-text
- Multiprocessing architecture to prevent UI freezing during long operations
- Custom streaming response manager for optimal performance
- Dynamic dependency installation system
- Smart text chunking for large documents

The app processes content locally and only sends extracted text to the Gemini API. Users provide their own API keys (free tier available).

**GitHub:** https://github.com/1shishh/learnwell

Built over a weekend as a learning tool. Looking for feedback on the multiprocessing implementation and UI responsiveness optimizations.",Python,0,https://www.reddit.com/r/Python/comments/1n2y5ch/aipowered_desktop_app_for_content_summarization/,r_1n2y5ch,,,
r_1n2wr1n,reddit,Similar_Bad_3120,2025-08-29T03:25:56+00:00,"ðŸš€ I built a Regex & Grok Tester tool (UPYNG) â€“ Feedback welcome!
Hey folks,

I wanted to share something Iâ€™ve been working on recently â€“ a web tool called UPYNG that lets you test both Regex and Grok patterns in real time.

ðŸ‘‰ Why I built it?
At my company, most of the widely used regex/grok testing websites are blocked. That made day-to-day troubleshooting and log parsing pretty frustrating. So, I decided to build my own tool for personal use â€“ and then thought, why not share it with others who might face the same issue?

ðŸ‘‰ What it does:
	â€¢	Test Regex patterns with instant results
	â€¢	Test Grok patterns (like you would in Logstash or Beats)
	â€¢	History panel so you can revisit past tests
	â€¢	Comes with sample patterns + guides for quick reference
	â€¢	Responsive design (works well on desktop & mobile)
	â€¢	Non-intrusive space for ads (so it stays free)

ðŸ‘‰ Why use it?
	â€¢	No login required
	â€¢	Runs directly in your browser
	â€¢	Lightweight, modern UI

Iâ€™m calling it UPYNG and my goal is to make it a simple, reliable companion for developers, DevOps engineers, and anyone wrangling with logs.

âœ¨ Iâ€™d really love if you all could check it out, give it a spin, and share your feedback. Whether itâ€™s bug reports, feature ideas, or UI suggestions â€“ Iâ€™m all ears!

Hereâ€™s the link: https://upyng.com

Thanks in advance, and I hope this makes debugging just a little less painful for some of you ðŸ™Œ",Python,0,https://www.reddit.com/r/Python/comments/1n2wr1n/i_built_a_regex_grok_tester_tool_upyng_feedback/,r_1n2wr1n,,,
r_1n2uol6,reddit,ThatTurtleGM,2025-08-29T01:45:58+00:00,"D&D twitch bot update 1!
So I posted about this about a week ago and included a little video link ( I think for the python groups I just made a short post, I forgot tbh), but tldr, I made a D&D themed twitch bot for twitch chatters to use while I stream. I worked on it a little since my last post, so here is the official update! 

I was wondering what other features I should go about adding, and any ideas I might want to look into. 

Here is what works: 

1.) You can pick any of the 12 D&D classes (Artificer soon)   
2.) Each class has its own channel point redemption ability that does something special  
3.) Bosses attack players who miss them, take damage in real time, and respawn after awhile.   
4.) Partake on adventures, earn EXP to level up.   
5.) You can change classes at a whim, and even between streams it memorizes your levels and current EXP for each of your classes.   
6.) (Items are MADE, but not working at the moment)   
7.) Each class and item has a value for how much they deal base damage, resist boss damage, and influence other numbers. (Some to come later)   
8.) Visuals/ sounds for each ability, bosses dying, critical hits, critical failures, and more.   
9.) Gold, earn cold hard coins for doing quests and killing bosses. 

Here is what's coming at some point:   
1.) Artificer  
2.) Boss special abilities and CC abilities, like stuns, deflections, and even temp. chatter bans.  
3.) New bosses and more quests  
4.) Working items and a shop system to spend the gold you earn.  
5.) A way to reward and punish players like the traditional Game master I am lol   
6.) A vote system for quests, and a possible skip system for quests we don't like

SO THATS THE QUESTION??? 

What should I add next? I am really interested in the ideas you may have, but I will say I'm super duper new to coding, so please go easy on me here. 

I'm coding through python, feel free to pm me! ",Python,2,https://www.reddit.com/r/Python/comments/1n2uol6/dd_twitch_bot_update_1/,r_1n2uol6,,,
r_1n2sexh,reddit,AutoModerator,2025-08-29T00:00:55+00:00,"Friday Daily Thread: r/Python Meta and Free-Talk Fridays
# Weekly Thread: Meta Discussions and Free Talk Friday ðŸŽ™ï¸

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! ðŸŒŸ",Python,3,https://www.reddit.com/r/Python/comments/1n2sexh/friday_daily_thread_rpython_meta_and_freetalk/,r_1n2sexh,,,
r_1n2khld,reddit,Repulsive-Leading932,2025-08-28T18:39:55+00:00,"AI devlopement And learning to make one
How to build an AI? What will i need to learn (in Python)? Is learning frontend or backend also part of this? Any resources you can share ",Python,0,https://www.reddit.com/r/Python/comments/1n2khld/ai_devlopement_and_learning_to_make_one/,r_1n2khld,,,
r_1n2h87o,reddit,francoisnt,2025-08-28T16:37:49+00:00,"A declarative fake data generator for sqlalchemy ORM
# SeedLayer: Declarative Fake Data for SQLAlchemy ORM

## What My Project Does
SeedLayer is a Python library that simplifies generating realistic fake data for SQLAlchemy ORM models. It allows you to define seeding behavior directly in model definitions using a declarative approach, respecting primary key (PK), foreign key (FK), and unique constraints. By leveraging the `Faker` library, it generates data for testing, development, and demo environments, automatically handling model and inter-column dependencies. The example below shows a schema with related tables (`Category`, `Product`, `Customer`, `Order`, `OrderItem`) to demonstrate FK relationships, a link table, and inter-column dependencies.

**Example**:
```python
from sqlalchemy import create_engine, Integer, String, Text, ForeignKey
from sqlalchemy.orm import DeclarativeBase, Session
from seedlayer import SeedLayer, SeededColumn, Seed, ColumnReference

class Base(DeclarativeBase):
    pass

class Category(Base):
    __tablename__ = ""categories""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""word"")

class Product(Base):
    __tablename__ = ""products""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""word"")
    description = SeededColumn(
        Text,
        seed=Seed(
            faker_provider=""sentence"",
            faker_kwargs={""nb_words"": ColumnReference(""name"", transform=lambda x: len(x.split()) + 5)}
        )
    )
    category_id = SeededColumn(Integer, ForeignKey(""categories.id""))

class Customer(Base):
    __tablename__ = ""customers""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""name"", unique=True)

class Order(Base):
    __tablename__ = ""orders""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    customer_id = SeededColumn(Integer, ForeignKey(""customers.id""))

class OrderItem(Base):
    __tablename__ = ""order_items""
    order_id = SeededColumn(Integer, ForeignKey(""orders.id""), primary_key=True)
    product_id = SeededColumn(Integer, ForeignKey(""products.id""), primary_key=True)

engine = create_engine(""sqlite:///:memory:"")
Base.metadata.create_all(engine)
seed_plan = {
    Category: 5,
    Product: 10,
    Customer: 8,
    Order: 15,
    OrderItem: 20
}
with Session(engine) as session:
    seeder = SeedLayer(session, seed_plan)
    seeder.seed()  # Seeds related tables with realistic data
```

This example creates a schema where:
- `Category` and `Customer` have simple attributes with fake data.
- `Product` has an FK to `Category` and a `description` that depends on `name` via `ColumnReference`.
- `Order` has an FK to `Customer`.
- `OrderItem` is a link table connecting `Order` and `Product`.

Check out the [GitHub repository](https://github.com/francoisnt/seedlayer) for more details and installation instructions.

## Target Audience
SeedLayer is designed for Python developers using SQLAlchemy ORM, particularly those working on:
- **Testing**: Generate realistic test data for unit tests, integration tests, or CI/CD pipelines.
- **Development**: Populate local databases for prototyping or debugging.
- **Demos**: Create demo data for showcasing applications (e.g., Flask, FastAPI, or Django apps using SQLAlchemy).
- **Learning**: Help beginners explore SQLAlchemy by quickly seeding models with data.

Itâ€™s suitable for both production-grade testing setups and educational projects, especially for developers familiar with SQLAlchemy who want a streamlined way to generate fake data without manual scripting.

## Comparison
Unlike existing alternatives, SeedLayer emphasizes a **declarative** approach integrated with SQLAlchemyâ€™s ORM:
- **Manual Faker Usage**: Using `Faker` directly requires writing custom scripts to generate and insert data, manually handling constraints like FKs and uniqueness. SeedLayer automates this, respecting model relationships and constraints out of the box.
- **factory_boy**: A popular library for creating test fixtures, `factory_boy` is great for Python ORMs but requires defining separate factory classes. SeedLayer embeds seeding logic in model definitions, reducing boilerplate and aligning closely with SQLAlchemyâ€™s declarative style.
- **SQLAlchemy-Fixtures**: This library focuses on predefined data fixtures, which can be rigid. SeedLayer generates dynamic, randomized data with Faker, offering more flexibility for varied test scenarios.
- **Alembic Seeding**: Alembicâ€™s seeding capabilities are limited and not designed for fake data generation. SeedLayer provides a robust, Faker-powered solution tailored for SQLAlchemy ORM.

SeedLayer stands out for its seamless integration with SQLAlchemy models, automatic dependency resolution, and support for complex scenarios like link tables and inter-column dependencies, making it a lightweight yet powerful tool for testing and development.

---

Iâ€™d love feedback from the Python community! Have you faced challenges generating test data for SQLAlchemy? Try SeedLayer and let me know your thoughts: [GitHub link](https://github.com/francoisnt/seedlayer).",Python,15,https://www.reddit.com/r/Python/comments/1n2h87o/a_declarative_fake_data_generator_for_sqlalchemy/,r_1n2h87o,,,
r_1n2gypa,reddit,tsvikas,2025-08-28T16:27:48+00:00,"I Built a tool that auto-syncs pre-commit hook versions with `uv.lock`
**TL;DR:** Auto-sync your pre-commit hook versions with `uv.lock`

    # Add this to .pre-commit-config.yaml
    - repo: https://github.com/tsvikas/sync-with-uv
      rev: v0.3.0
      hooks:
        - id: sync-with-uv

**Benefits:**

* Consistent tool versions everywhere (local/pre-commit/CI)
* Zero maintenance
* Keeps pre-commit's isolation and caching benefits
* Works with [pre-commit.ci](http://pre-commit.ci)

# The Problem

[PEP 735](https://peps.python.org/pep-0735/) recommends putting dev tools in `pyproject.toml` under `[dependency-groups]`. But if you also use these tools as pre-commit hooks, you get version drift:

* `uv update` bumps `black` to `25.1.0` in your lockfile
* Pre-commit still runs `black==24.2.0`
* Result: inconsistent results between local tool and pre-commit.

# What My Project Does

This tool reads your `uv.lock` and automatically updates `.pre-commit-config.yaml` to match.

Works as a pre-commit (see above) or as a one-time run: `uvx sync-with-uv`

# Target Audience

developers using `uv` and `pre-commit`

# ComparisonÂ 

âŒ Using manual updates?

* Cumbersome
* Easy to forget

âŒ Using  local hooks?

    - repo: local
      hooks:
        - id: black
          entry: uv run black

* Breaks [pre-commit.ci](http://pre-commit.ci)
* Loses pre-commit's environment isolation and tool caching

âŒ Removing the tools from `pyproject.toml`?

* Annoying to repeatedly type `pre-commit run black`
* Can't pass different CLI flags (`ruff --select E501 --fix`)
* Some IDE integration breaks (when it requires the tool in your environment)
* Some CI integrations break (like the black action auto-detect of the installed version)

Similar tools:

* [`sync_with_poetry`](https://github.com/floatingpurr/sync_with_poetry) \- Poetry version
* [`sync-pre-commit-lock`](https://github.com/GabDug/sync-pre-commit-lock) \- PDM/Poetry plugin

# Try it out: [https://github.com/tsvikas/sync-with-uv](https://github.com/tsvikas/sync-with-uv)

â­ **Star if it helps!** Issues and PRs welcome. â­",Python,101,https://www.reddit.com/r/Python/comments/1n2gypa/i_built_a_tool_that_autosyncs_precommit_hook/,r_1n2gypa,,,
r_1n2ekkm,reddit,Optimal_Act_6987,2025-08-28T14:58:51+00:00,"Lightweight Statistical Forecasting (Own Model Design)
Hi everyone! Iâ€™ve released a new Python library called randomstatsmodels that bundles error metrics (MAE, RMSE, MAPE, SMAPE) with autoâ€¯tuned forecasting models like AutoNEO, AutoFourier, AutoKNN, AutoPolymath and AutoThetaAR. The library makes it easy to benchmark and build univariate forecasts; each model automatically selects hyperparameters for you.

The package is available on PyPI: https://pypi.org/project/randomstatsmodels/ (install via `pip install randomstatsmodels`).

Iâ€™d love any feedback, questions or contributions!

The GitHub for the code is: https://github.com/jacobwright32/randomstatsmodels",Python,8,https://www.reddit.com/r/Python/comments/1n2ekkm/lightweight_statistical_forecasting_own_model/,r_1n2ekkm,,,
r_1n28x84,reddit,DefenderXD,2025-08-28T10:48:26+00:00,"what's the best way to organize your code app.py
Hi everyone,

Iâ€™m working on a Flask app, and right now everything is in one file â€” `app.py`.  
That one file has over **3000 lines** of code. It has:

* All my routes
* Database setup
* Forms
* Helper functions
* Everything else

The app is **not fully finished yet**. Iâ€™m still adding the main features.

Iâ€™m starting to feel like the file is too big and hard to manage. But Iâ€™m not sure how to organize it


**Any advice or examples would really help!**  
Thanks a lot!",Python,43,https://www.reddit.com/r/Python/comments/1n28x84/whats_the_best_way_to_organize_your_code_apppy/,r_1n28x84,,,
r_1n26zm9,reddit,marcogorelli,2025-08-28T08:49:19+00:00,"pd.col: Expressions are coming to pandas
[https://labs.quansight.org/blog/pandas\_expressions](https://labs.quansight.org/blog/pandas_expressions)

In pandas 3.0, the following syntax will be valid:

    import numpy as np
    import pandas as pd
    
    df = pd.DataFrame({'city': ['Sapporo', 'Kampala'], 'temp_c': [6.7, 25.]})
    df.assign(
        city_upper = pd.col('city').str.upper(),
        log_temp_c = np.log(pd.col('temp_c')),
    )

This post explains why it was introduced, and what it does",Python,191,https://www.reddit.com/r/Python/comments/1n26zm9/pdcol_expressions_are_coming_to_pandas/,r_1n26zm9,,,
r_1n25l46,reddit,Trinity_software,2025-08-28T07:16:01+00:00,"Student mental health analysis using python and SQL
https://youtu.be/1evMpzJxnJ8?si=NIWsAEPDfg414Op9

Hi, this is part 1 of performing (univariate)data analysis in students mental health dataset, using python and SQL",Python,0,https://www.reddit.com/r/Python/comments/1n25l46/student_mental_health_analysis_using_python_and/,r_1n25l46,,,
r_1n24hu3,reddit,DarkRevolutionary320,2025-08-28T06:06:43+00:00,"Need someone to guide me on my Audio to text script
I have been trying to make script with converts my .mp4 file to text, which enables audio diarization and timestamp. Tried whisperx, pyanote, kaldi and more. My output isnâ€™t able to recognize speaker and diarize it. Need some guidance. ",Python,9,https://www.reddit.com/r/Python/comments/1n24hu3/need_someone_to_guide_me_on_my_audio_to_text/,r_1n24hu3,,,
r_1n22kbc,reddit,Traditional-Let-856,2025-08-28T04:13:19+00:00,"We created an open-source Agentic AI framework and gathering feedback
[https://github.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai)

ðŸš€ Weâ€™ve have been working on our open-source Agentic AI framework (FloAI) for a while now. This started as something to make the use of langchain easier, so eventually it became complicated. Now we have re-vamped it to make it more lightweight, simple, and customizable â€” and weâ€™ve officially removed all LangChain dependencies!

Why the move away from LangChain?  
We decided to move away from langchain because of the dependency hell it was creating and so much blotted code, which we never want to use. Even implementing new architectures became difficult with langchain

By removing LangChain, weâ€™ve:  
âœ¨ Simplified agent creation & execution flows  
âœ¨ Improved extensibility & customizability  
âœ¨ Reduced overhead for cleaner, production-ready builds

We have also created a visual editor for Agentic Flow creation. The visual editor is still work in progress but you can find the first version in our repo

Feel free to have a look and maybe give it spin. Would be a great encouragement if you can give us a star â­  
[https://github.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai)

",Python,0,https://www.reddit.com/r/Python/comments/1n22kbc/we_created_an_opensource_agentic_ai_framework_and/,r_1n22kbc,,,
r_1n21i61,reddit,Wooden_Ambassador346,2025-08-28T03:17:48+00:00,"built a clash of clans bot after a day and a half of learnin python
[https://github.com/mimslarry0007-cpu/clash-of-clans-bot/commit/545228e1eb1a5e207dcc7bcf356ddf3d58bdf949](https://github.com/mimslarry0007-cpu/clash-of-clans-bot/commit/545228e1eb1a5e207dcc7bcf356ddf3d58bdf949)

its pretty bad cause it needs the specific cords an allat. i played with image recognition and got it to work but it was bad at its job and got confused all the time.

what my project does: it automatically upgrades mines, pumps, storage and the townhall. it also attacks after all that finishes.

  
Target audience: its just a thing im using to learn scripting and automation.

  
comparison: idk its prolly pretty bad lmao",Python,2,https://www.reddit.com/r/Python/comments/1n21i61/built_a_clash_of_clans_bot_after_a_day_and_a_half/,r_1n21i61,,,
r_1n1xjmq,reddit,Economy-Purchase-339,2025-08-28T00:09:21+00:00,"[Looking for a Collaborator] Python Programmer to finish betting bot on Telegram
Hey everyone!

I'm working on a personal Python project: a bot called **Neuroxyn** that runs on **Telegram**.
The bot detects **live value bets** (like Over goals, corners, etc.) using APIs and filters that I designed myself, and then sends the alerts directly to the Telegram channel.

The problem is, I left it halfway because I lack more advanced Python knowledge and time to polish it.
That's why I'm looking for someone who wants to **join as a collaborator** to improve the project.

What I need:
- Optimize the bot's filters and algorithms.
- Improve integration with sports APIs.
- Add extra functions (e.g., user management, statistics, logs).
- Scalability so it works more professionally.

What I offer:
- A **real and functional** project (it already detects and sends live bets).
- Participate as part of the **core team**, not as an outsider.
- Potential for income in the future if the bot is monetized or offered as a premium service.

I'm looking for people who are passionate about **Python, bots, data scraping/sports APIs** and who want to work on something innovative.
If you're interested, send me a message or leave your Telegram/Discord username.

Let's build something great together!",Python,0,https://www.reddit.com/r/Python/comments/1n1xjmq/looking_for_a_collaborator_python_programmer_to/,r_1n1xjmq,,,
r_1n1xczy,reddit,AutoModerator,2025-08-28T00:00:43+00:00,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!
# Weekly Thread: Professional Use, Jobs, and Education ðŸ¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! ðŸŒŸ",Python,13,https://www.reddit.com/r/Python/comments/1n1xczy/thursday_daily_thread_python_careers_courses_and/,r_1n1xczy,,,
r_1n1t36y,reddit,bemna94,2025-08-27T21:04:05+00:00,"Is it normal for a package to overwrite/add files of another already installed package?
Hello all, I ran into something really strange and wanted check with the community.

I was running PySpark **3.5.5** and everything worked fine. Then I upgraded MLflow from a **2.x** to 3.x (with the databricks extra). Suddenly, PySpark started behaving weirdly (i. e. showing errors that should on be part of spark 4)

After isolating things in a clean environment, and analysing the impact of each dependency upon install, I discovered that **databricks-connect** (transitive dependency of mlflow) is actually modifying PySparkâ€™s installed files directly in site-packages upon install. Not patching at runtime, not wrapping APIs; but literally overwriting PySparkâ€™s code in place.

My assumption was that if you need custom behavior youâ€™d monkey patch or provide an extension layer, not directly rewrite another packageâ€™s files.

Maybe this is probably better suited in r/mlflow r/apachespark or r/databricks, but my question is purely about Python package/dependency management. Is this considered normal practice anywhere, and I'm wrong to be surprised?

EDIT:

Here's how I checked this, let me know if my logic is right:  
i'm on python 3.10

* I created a fresh virtual env
* I installed pyspark==3.5.5
   * site-packages only has pyspark and its dependency (besides the default tools), and it's consistent with what I see here [https://github.com/apache/spark/tree/v3.5.5/python/pyspark/](https://github.com/apache/spark/tree/v3.5.5/python/pyspark/)
   * `pip show pyspark` shows I have 3.5.5
   * 3.5.5 is also the version I see on site-packages/pyspark/version.py
   * when I run a function import such as `from pyspark.sql.functions import lit`, it's working as expected.
* I installed databricks-conenct 16
   * I checked site-packages/pyspark, and it's nothing like [v3.5.5](https://github.com/apache/spark/tree/v3.5.5/python/pyspark/), namely, some spark 4 additions such as functions.builtin. I even ran a script to check differences between the folder before and after the install of databricks-connect and I see ""ADDED: 85 files, CHANGED: 623 files""
   * `pip show pyspark` still shows I have 3.5.5
   * on site-packages/pyspark/version.py I see 3.5.2, which is strange, and the package looks nothing like [3.5.2](https://github.com/apache/spark/tree/v3.5.2/python/pyspark)
   * running the same import gives an error
      * \`ImportError: cannot import name '\_with\_origin' from 'pyspark.errors.utils'\`",Python,69,https://www.reddit.com/r/Python/comments/1n1t36y/is_it_normal_for_a_package_to_overwriteadd_files/,r_1n1t36y,,,
r_1n1mkj5,reddit,fabriqus,2025-08-27T16:57:45+00:00,"Strategic approach for mechanical engineering student
What modules or projects should I be looking at as a mechanical engineering student? I'm aware of all the big data science stuff but what else? Specifically, materials science, metallurgy, and dynamics, as well as FEA.",Python,0,https://www.reddit.com/r/Python/comments/1n1mkj5/strategic_approach_for_mechanical_engineering/,r_1n1mkj5,,,
r_1n1lwch,reddit,NotAron13,2025-08-27T16:33:12+00:00,"need support. LITTLE BRO TOOK PICTURES.
so my little brother who is about the young teenager age has taken some photos of stuff in my room I don't want him to have. Nothing illegal but still could get me In trouble with my parents. I've just gotten into python the last few days and really enjoy it. do you guys know how I can get access to his Samsung s22 ultra passcode (with numbers I think 6 digits) to delete the pictures. Does somebody have a SAFE script or tutorial? At least something that could bring me [further.like](http://further.like) I said no bad or illegal intent just wanna delete the pictures!

thanks to any answer in advance

",Python,0,https://www.reddit.com/r/Python/comments/1n1lwch/need_support_little_bro_took_pictures/,r_1n1lwch,,,
r_1n1k42a,reddit,byaruhaf,2025-08-27T15:26:11+00:00,"Python: The Documentary premieres on YouTube in a few hours
Who else is setting a reminder?

[Python: The Documentary | An origin story](https://youtu.be/GfH4QL4VqJ0?si=aJWHZN7AlFL77Gnn)  
",Python,110,https://www.reddit.com/r/Python/comments/1n1k42a/python_the_documentary_premieres_on_youtube_in_a/,r_1n1k42a,,,
r_1n1hkls,reddit,andrewpfl,2025-08-27T13:49:04+00:00,"I bundled my common Python utilities into a library (alx-common) â€“ feedback welcome
Over the years I found developers rewriting the same helper functions across multiple projects â€” things like:

* Sending text + HTML emails easily
* Normalizing strings and filenames
* Simple database utilities (SQLite, MariaDB, PostgreSQL, with parameter support)
* Config handling + paths setup

So I wrapped them up into a reusable package called [**alx-common**](https://pypi.org/project/alx-common/)

I use it daily for automation, SRE, and DevOps work, and figured it might save others the â€œcopy-paste from old projectsâ€ routine.

Itâ€™s under GPLv3, so free to use and adapt. Docs + examples are in the repo, and Iâ€™m adding more over time.

Would love any feedback:

* Anything that feels missing from a â€œcommon utilsâ€ package?
* Is the API style clean enough, or too opinionated?
* Anyone else packaging up their â€œutility functionsâ€ into something similar?

Appreciate any thoughts, and happy to answer questions.



",Python,28,https://www.reddit.com/r/Python/comments/1n1hkls/i_bundled_my_common_python_utilities_into_a/,r_1n1hkls,,,
r_1n1dsqh,reddit,decodingchris,2025-08-27T10:54:45+00:00,"I built prompttest - a testing framework for LLMs. It's like pytest, but for prompts.
**What My Project Does**

**prompttest** is a command-line tool that brings automated testing to your LLM prompts. Instead of manually checking whether prompt changes break behavior, you can write tests in simple YAML files and run them directly from your terminal.

It works by running your prompt with different inputs, then using another LLM to evaluate whether the output meets the criteria you define in plain English.

Hereâ€™s a quick look at how it works:

1. Create a `.txt` file for your prompt with placeholders like `{variable}`.
2. Write a corresponding `.yml` test file where you define test cases, provide inputs for the placeholders, and specify the success criteria.
3. Run `prompttest` in your terminal to execute all your tests.
4. Get a summary in the console and detailed Markdown reports for each test run.

You can see a demo of it in the projectâ€™s README on GitHub.

**Target Audience**

This tool is for developers and teams building applications with LLMs who want to bring more rigor to their prompt engineering process. If you find it difficult to track how prompt modifications affect your outputs, **prompttest** helps catch regressions and ensures consistent quality.

Itâ€™s designed to fit naturally into a CI/CD pipeline, just like you would use `pytest` for code.

**Comparison**

The main difference between **prompttest** and other prompt-engineering tools is its focus on **automated, code-free testing from the command line**.

While many tools provide a GUI for prompt experimentation, **prompttest** is developer-firstâ€”built to integrate into your existing workflow. The philosophy is to treat prompts as a critical part of your codebase, worthy of their own automated tests.

Another key advantage is the use of **YAML for test definitions**, which keeps tests readable and easy to manage, even for non-coders. Since it uses **OpenRouter**, you can also test against a wide variety of LLMs with just a single API key.

ðŸ’¡ Iâ€™d love to hear your feedback and answer any questions!   
ðŸ”— GitHub Repo: [https://github.com/decodingchris/prompttest](https://github.com/decodingchris/prompttest)",Python,0,https://www.reddit.com/r/Python/comments/1n1dsqh/i_built_prompttest_a_testing_framework_for_llms/,r_1n1dsqh,,,
r_1n16al4,reddit,Relative_Spinach7950,2025-08-27T03:19:25+00:00,"Python package for NCAA Baseball & MLB Draft stats
**What My Project Does:**

**ncaa\_bbStats**Â is an open-source Python package for retrieving, parsing, and analyzing Division I, II, and III college baseball team statistics (2002â€“2025), player statistics (2021-2025), and MLB Draft data (1965-2025).

**Target Audience:**

Researchers, analysts, or general fans looking to see how teams perform from 2002-2025 and players from 2021-2025. 

**Comparison**:

It was hard finding any resources for college baseball, but of the ones I did find I couldn't find direct statistical retrieve functions for research purposes. Especially that of players and team statistics. I hope this project is able to fulfill that.

**Main Text:**

Hey everyone,

I built a Python package calledÂ **ncaa\_bbStats**Â that lets you pull and analyze NCAA Division I, II, and III baseball stats (2002â€“2025), player stats (2021â€“2025), and MLB Draft data (1965â€“2025).

Some things you can do with it:

* Get team stats like BA, ERA, OBP, SLG, FPCT
* Compute Pythagorean expectation & compare to actual records
* Build player leaderboards (HR leaders, K/9 leaders, etc.)
* Retrieve MLB Draft picks for any NCAA team since 1965

Docs:Â [https://collegebaseballstatspackage.readthedocs.io/](https://collegebaseballstatspackage.readthedocs.io/)  
PyPI:Â [https://pypi.org/project/ncaa-bbStats/](https://pypi.org/project/ncaa-bbStats/)  
GitHub:Â [https://github.com/CodeMateo15/CollegeBaseballStatsPackage](https://github.com/CodeMateo15/CollegeBaseballStatsPackage)

Itâ€™s still under development, so Iâ€™d love feedback, collaborators, or even just a GitHub â­ if you think itâ€™s cool.

If youâ€™re into college baseball, MLB draft history, or sports analytics with Python, check it out and let me know what you think!

NOTE: new profile cause I have public info on the github I don't want to link to my actual account lol

",Python,9,https://www.reddit.com/r/Python/comments/1n16al4/python_package_for_ncaa_baseball_mlb_draft_stats/,r_1n16al4,,,
r_1n10c30,reddit,Material_Pool_986,2025-08-26T22:44:21+00:00,"jupytercad-mcp: Control JupyterCAD using LLMs/natural language.
**What My Project Does**: An MCP server for JupyterCAD that allows you to control it using LLMs/natural language.

**Target Audience:** Anyone interested in CAD + generative AI.

**Comparison**: I couldn't find any other MCP servers for JupyterCAD(?)

Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7

Repo: https://github.com/asmith26/jupytercad-mcp",Python,2,https://www.reddit.com/r/Python/comments/1n10c30/jupytercadmcp_control_jupytercad_using/,r_1n10c30,,,
r_1n0yv7u,reddit,03cranec,2025-08-26T21:44:03+00:00,"Python DX for data & analytics infrastructure
Hey everyone - Iâ€™ve been thinking a lot about Python developer experience for data infrastructure, and why it matters almost as much performance. Weâ€™re not just building data warehouses for BI dashboards and data science anymore. OLAP and real-time analytics are powering massively scaled software development efforts. But the DX is still pretty outdated relative to modern software devâ€”things like schemas in YAML configs, manual SQL workflows, and brittle migrations.

Iâ€™d like to propose eight core principles to bring analytics developer tooling in line with modern software engineering: **git-native workflows, local-first environments, schemas as python code, modularity, openâ€‘source tooling, AI/copilotâ€‘friendliness, and transparent CI/CD + migrations.**

Weâ€™ve started implementing these ideas in[ MooseStack](https://github.com/514-labs/moosestack) (open source, MIT licensed):

* **Migrations** â†’ before deploying, your code is diffed against the live schema and a migration plan is generated. If drift has crept in, it fails fast instead of corrupting data.
* **Local development** â†’ your entire data infra stack materialized locally with one command. Branch off main, and all production models are instantly available to dev against.
* **Type safety** â†’ rename a column in your code, and every SQL fragment, stream, pipeline, or API depending on it gets flagged immediately in your IDE.

Iâ€™d love to spark a genuine discussion here, especially with those of you who have worked with analytical systems like Snowflake, Databricks, BigQuery, ClickHouse, etc and tried building production workloads in Python:

* Is developing in a local environment that mirrors production important for these workloads?
* How do you currently move from dev â†’ prod in OLAP or analytical systems? Do you use staging environments?Â 
* Where do your workflows stallâ€”migrations, environment mismatches, config?
* Which of the eight principles seem most lacking in your toolbox today?

For anyone interested, I helped write a blog post on this topic, and you can read it here: [*https://clickhouse.com/blog/eight-principles-of-great-developer-experience-for-data-infrastructure*](https://clickhouse.com/blog/eight-principles-of-great-developer-experience-for-data-infrastructure)",Python,15,https://www.reddit.com/r/Python/comments/1n0yv7u/python_dx_for_data_analytics_infrastructure/,r_1n0yv7u,,,
r_1n0yj9r,reddit,Fragrant_Steak_5,2025-08-26T21:30:44+00:00,"New weekly series: Realistic bug-fixing exercises for beginners
Hi everyone ðŸ‘‹

Iâ€™ve been working as a software engineer for about 10 years, and I wanted to start a small initiative to give programming practice a fresh twist. Instead of the usual abstract exercises, Iâ€™m creatingÂ *realistic bug-fixing scenarios*Â inspired by problems you might face in actual projects.

Every week Iâ€™ll be sharing a new â€œbug to fixâ€ in the form of a Colab notebook (for now), so people can practice, learn, and reinforce concepts while thinking like engineers solving real-world issues.

This very first one is designed for beginners who are just starting out ðŸ‘¶, but the idea is to build a series with different levels:Â *intern, junior, and semi-senior*. That way, people can grow step by step and tackle challenges that fit their journey.

For now, all exercises will be in Python ðŸ, but I believe they could be just as valuable as a starting point for people who later want to work with other technologies too.

Please send me a private message and I will share the challenge with you.

Iâ€™d love to hear your feedback ðŸ™â€”does this approach feel useful, fun, or motivating to you? Any suggestions to improve it are more than welcome!

Thanks a lot for taking a look ðŸ’™",Python,0,https://www.reddit.com/r/Python/comments/1n0yj9r/new_weekly_series_realistic_bugfixing_exercises/,r_1n0yj9r,,,
r_1n0wemp,reddit,TheRallyMaster,2025-08-26T20:08:56+00:00,"I Just released Sagebox - a procedural GUI library for Python (Initial Beta)
**What My Project Does:**

Sagebox is a comprehensive GUI providing GUI-based controls and graphics, that can be used in a simple procedural manner. 

**Target Audience:**

Anyone, really.   Hobbyists, research, professional.  I have used in the industry quite a lot, but also use it for quick prototyping and just playing around with graphics.  The github page has examples of many different ypes.



**Comparison**:

Sagebox is meant to provide easily-used and access controls that are also scalable into more complex controls as-you-go, which is the main emphasis -- easily-used but scalable as a procedural GUI with a lot of control, widgets, and graphics functions.    
  
One of the main differences, besides being procedural (which some GUIs are, too) is having controls and graphics as specialized areas that can work independently or together, to create personalized control-based windows, as well quick developer-based controls that are easily created and automatically placed. 

  
It's also purposely designed to work with all other GUIs and libraries, so you can use it, for example, to provide controls while using Matlplot lib (see examples on the github page), and it can work along side PySimple Gui or Pygame, since every GUI has it's strengths that people like. 

  
**Here is the main text:** 

  [http://github.com/Sagebox/Pybox](http://github.com/Sagebox/Pybox) (Overview, pip install, screenshots, getting-started example code, and working example projects).

# Sagebox Procedural GUI Toolset Initial Beta
I'm pleased to announce the initial public beta release of Sagebox, a comprehensive, procedurally-based GUI library for Python. This project started a few years ago as a professional tool for my own work, and after being used and proven in industry, I'm excited to finally share it with the developer community as a free GUI toolset. 
> **A quick note on this release**:
As a first release, your feedback and discussion would be great regarding your experiences, any kinks in the process, bugs, etc.  For more details on the current status and roadmap, please see the [About This Beta Release](#about-this-beta-release) section at the end of this post.

# A Comprehensive, Procedural GUI 
Sagebox is a set of GUI tools designed for creative development and rapid prototyping, allowing you to build powerful, graphics-based programs without forms or boilerplate code.

It was designed from scratch for creating everything from full desktop applications and console-mode programs with controls, to just having fun with graphics.
Sagebox has been used for a few years in industry at places like Pioneer, Pentair and ASML, where it was called ***""that magic program.""*** 

## **Some of the key design principles behind Sagebox**

#### No Boilerplate

- Sagebox starts itself up when you use any function, so there is no need to initialize it or set up an environment. You can call up a slider in a console program, for example, with just a few lines of code.

#### Acts as a simple Library
- Built as a self-contained GUI kernel, Sagebox functions as a set of library calls. You can add or remove calls as you want and use all standard types (e.g. numpy arrays, lists, tuples) of choice, without changing your code to suit Sagebox.

#### Scalability
- Sagebox is designed for any level of complexity, from simple console tools to full desktop applications. Controls can be created and used with as little as two lines of code, and the library scales to more powerful graphics and controls as needed (see examples).
- *Self-contained platform- and language-agnostic GUI kernel.* The Sagebox GUI kernel is completely self-contained, allowing it to manage the entire OS GUI environment so your program does not have to, generally creating controls and graphics in fire-and-forget fashion. This also allows the GUI kernel to work on any platform (e.g. Windows, Linux, macOS, Android) as well as remain language-agnostic to work on any language on its own idiomatic terms.

#### Compatible with Other Libraries
- Sagebox is designed to be compatible with other GUI and general libraries like PySimpleGUI, PyGame, *Matplotlib*, etc. . For example, the Python GitHub page has examples of using Sagebox GUI controls with *Matplotlib*.

## GitHub Pages, Installation, Examples and Screenshots
For simple (and full program) code examples, installation instructions, and roadmap details, click on the GitHub page: 

- Python - [http://github.com/Sagebox/Pybox](http://github.com/Sagebox/Pybox) (called Pybox for Python version. C++ and Rust are also supported.)

## Video Examples (YouTube)

You can also view some examples on the YouTube page: 
- [https://www.youtube.com/@projectsagebox](https://www.youtube.com/@projectsagebox) 
**note**: the current videos are Rust examples, but they 
work and look exactly the same in all languages.
 Other C++ and Python videos are currently offline and will be put back online shortly.

# About This Beta Release

This is the first release of Sagebox, which has been used in private industry for a few years. It works with Windows, with Linux support coming in just a few months. 
 
All screenshots and video examples were created with the current version of Sagebox.  It is used already as a robust and comprehensive working beta, and a lot of work has been put in to make it useful for everyone, from hobbyists, professionals, research & education, to just having fun with programming. 

I'm excited about what can be added to it in future versions and the current roadmap: 

- **Break-In Period (2-3 weeks).**  This initial beta period is just 2-3 weeks long to get first impressions, any bugs, kinks, to generally make sure it works for everyone.
- **Next Beta Release (4-6 weeks)**. The next release is scheduled for 4-6 weeks from now with:
  - **Added functionality.**  There is a lot of functionality in Sagebox that has not yet been added to the interface.  This is being completed now, and expect even more interesting things. 
  - **Documentation.**  More documentation will be added.  Right now, the functions have full documentation for the editor, and documentation is always something there can be more of.
- **Windows and Linux.**  The Windows version was released before the linux version on purpose, to help get feedback and usage experiences as the Linux version is being completed.  This was done purposely to get community feedback to help with preferred community directions in the Linux version, particularly with look-and-feel and what things people would prefer prioritized over others (e.g. GPU functions vs. added widgets and other features) -- as well as interoperability with other preferred libraries. 
- **Future Development.** Sagebox is a free GUI toolset. As Sagebox continues to evolve, your feedback and suggestions are appreciated. To follow the project's roadmap and learn more about its future as a community-focused library, please see the GitHub Page.

I look forward to answering any questions you have, feedback and suggestions. 

",Python,36,https://www.reddit.com/r/Python/comments/1n0wemp/i_just_released_sagebox_a_procedural_gui_library/,r_1n0wemp,,,
r_1n0ufg0,reddit,Wrong-Cat-5014,2025-08-26T18:53:24+00:00,"PyWire-eel, a lightweight Python library like eel
Came across a small project called PyWire-eel on GitHub and thought it was interesting.

Itâ€™s similar to Eel (which recently got archived), but the idea is to provide a lightweight way to connect Python functions with a frontend built in HTML/CSS/JS. Basically you can call Python from JavaScript and the other way around without pulling in something heavy like Electron.

Repo link: https://github.com/Fadi002/PyWire-eel

Curious if anyone here has tried this kind of approach recently. Would you consider it useful, or would you just stick with PyWebView / Qt / Electron?
",Python,5,https://www.reddit.com/r/Python/comments/1n0ufg0/pywireeel_a_lightweight_python_library_like_eel/,r_1n0ufg0,,,
r_1n0tlht,reddit,watchmoviestime,2025-08-26T18:21:13+00:00,"Apple Notes MCP Server â€“ Connect your Apple Notes with LLMs.
# What My Project Does

I built **Apple Notes MCP Server**, a tool that integrates Apple Notes with the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol). It provides a bridge between your notes and MCP-compatible clients (like Claude Desktop, [Continue.dev](http://Continue.dev), or Perplexity).

With this, you can fully **automate Apple Notes from Python** â€” from managing notes to organizing folders â€” all via a clean MCP interface.

# Features

* Full **CRUD support** for both notes and folders (create, read, update/rename, delete, move)
* **Search & structure tools** to query notes and view folder hierarchies
* Supports **rich HTML content** (headers, lists, tables, links, emoji ðŸš€ðŸ“)
* Works seamlessly with **multiple MCP clients** (Claude Desktop, [Continue.dev](http://Continue.dev), Perplexity, etc.)

# Quick Start

1. **Install uv** (if not already installed)

&#8203;

    curl -LsSf https://astral.sh/uv/install.sh | sh

1. **Add MCP configuration** to your client (e.g., [Continue.dev](http://Continue.dev), Claude Desktop):

&#8203;

    {
      ""mcpServers"": {
        ""apple-notes"": {
          ""command"": ""uvx"",
          ""args"": [""mcp-apple-notes@latest""]
        }
      }
    }

Thatâ€™s it â€” your MCP client will install and run the package automatically.

# Links

ðŸ“¦ PyPI: [https://pypi.org/project/mcp-apple-notes/](https://pypi.org/project/mcp-apple-notes/)

ðŸ’» Source Code: [https://github.com/henilcalagiya/mcp-apple-notes](https://github.com/henilcalagiya/mcp-apple-notes)

# Target Audience

* **Developers** who want to automate or script Apple Notes workflows.
* **AI/LLM users** whoâ€™d like to use their personal notes as context in AI tools.
* **macOS power users** who want better control of Apple Notes through automation.This project is in **beta** but stable enough for experimentation and light productivity use.

# Comparison

* Unlike general Apple Notes automation scripts, this project uses **MCP (Model Context Protocol)**, which means it plugs directly into multiple AI/LLM clients.
* It provides **full CRUD for both notes and folders** (many existing scripts only handle basic read/write).
* It supports **rich HTML formatting, search, and folder hierarchies** â€” making it more feature-complete than simple AppleScript snippets.
* Built to be **modular and extendable** for future MCP integrations.

Would love to hear your thoughts, feedback, or use-cases you see for this.",Python,0,https://www.reddit.com/r/Python/comments/1n0tlht/apple_notes_mcp_server_connect_your_apple_notes/,r_1n0tlht,,,
r_1n0tgja,reddit,Beginning_Task_5515,2025-08-26T18:15:59+00:00,"Would a ""venv"" wrapper around multiprocessing be useful? (hardware-aware pools, NUMA, GPU, etc.)
Hey folks,

Iâ€™ve been tinkering with an idea to extend Pythonâ€™s built-in `multiprocessing` by adding a concept I call **compute\_venvs** (like virtual environments, but for compute). The idea is to let you define resource-scoped pools that know about CPU cores, NUMA nodes, GPUs, I/O limits, and even niceness/cgroups, and then route tasks accordingly.

`from compute_venv import VEnv, VPool`

`cpu0 = VEnv(name=""cpu0_fast"", cpu_cores=[0,1,2,3], numa_node=0, nice=5)`

`gpu0 = VEnv(name=""gpu0"", gpu=""cuda:0"")`

`with VPool([cpu0, gpu0]) as pool:`

`pool.submit(cpu_heavy_fn, data, hint=""cpu0_fast"")`

`pool.submit(gpu_heavy_fn, data, hint=""gpu0"")`

The module would:

* Add **affinity and isolation** (set process affinity, NUMA binding, GPU selection, nice priority).
* Provide an **auto-tuning scheduler** that benchmarks chunk sizes/queue depth and routes tasks to the best venv.
* Remain **stdlib-compatible**: you can swap in/out `multiprocessing` pools with almost no code change.
* Target single-machine jobs: preprocessing, simulation, ML data prep, video/audio encoding, etc.

Itâ€™s meant as a **lightweight alternative to Ray/Dask** for cases where you donâ€™t need distributed orchestration, just better hardware-aware tasking on one box.

**Questions for you all:**

1. Would this be useful in your workflows, or is it too niche?
2. Do you think sticking close to `multiprocessing` API is the right approach, or should it be more opinionated?
3. Any obvious â€œgotchasâ€ I should be aware of (esp. cross-platform)?
4. Benchmarks I should definitely include to prove value?

Thanks! Iâ€™d love to hear your perspectives before I get dirty with this.",Python,0,https://www.reddit.com/r/Python/comments/1n0tgja/would_a_venv_wrapper_around_multiprocessing_be/,r_1n0tgja,,,
r_1n0t07s,reddit,EngineerRemy,2025-08-26T17:59:14+00:00,"GenEC v1.0.0 - A Python data extraction and comparison tool
Hi, just this weekend I finalized the 1.0.0 version of my Tool, GenEC, and now I want the world to know ahah. I've already been using it for myself quite a lot of my own work, as well as subtly pushing my coworkers to start using it. I am confident many other people should be able to find a use for my tool as well, so if you're interested in using it, I am always happy to answer questions and provide support.

Repository: [https://github.com/RemyKroese/GenEC](https://github.com/RemyKroese/GenEC)

# What My Project Does

*GenEC (Generic Extraction & Comparison) is a Python-based tool for extracting structured data from files or folders. It offers a flexible, one-size-fits-all extraction framework that you can tailor precisely using configuration parameters.*

It is a tool that lets you extract and count occurrences of data using your own configurations. It can also compare this extracted data against reference files to spot differences. Your configurations can get saved as presets, so you can easily reuse them or automate the whole process by calling GenEC from other tools.

Once you have several presets, you can do batch analysis using a ""preset-list"" file, which is basically a collection of presets to run together. This scales you from analyzing single files to processing entire folders.

To summarize, there are 3 workflows for this tool:

* **Basic:** for experimentation of configurations as well as getting acquainted with the tool
* **Preset:** for single command data extraction (and comparison) using a preset
* **Preset-list:** Enable batch processing by processing data in folders using a group of presets, all with only 1 command

Being a CLI tool, GenEC displays results in [neat tables](https://imgur.com/a/PnQxLGY) right in your terminal. But you can also export everything to CSV, JSON, YAML, or TXT files for further analysis. Which has the following benefits

* **Human readable** output tables in CLI and TXT
* **Machine-readable** output in CSV, JSON and YAML (for the AI enjoyers out there, YAML is likely the best input format for it :P)

I have written extensive documentation on the tool within the repository, but to just link it here separately:

* [README.md](https://github.com/RemyKroese/GenEC/blob/main/README.md)
* [Documentation overview](https://github.com/RemyKroese/GenEC/blob/main/docs/overview.md)

# Target Audience

I like to believe my tool will be applicable for anyone who has the technical knowledge on how to use CLI tooling. The more, you work with data, the more you benefit from this of course:

* Data engineers / analysts / scientists
* Programmers
* QA/Test engineers
* Functions in a data reporting capacity: For example, my Scrum Master has been using it in order to provide data reporting to stakeholders, since we lack internal tooling for all the data we have.

# Comparison

It competes with almost any data analysis tooling, which are:

* Enterprise tooling
* CLI tools / open source (diff / grep, etc.)

I believe GenEC fulfills a nice middle-ground niche, as it creates structured output, allows for reusability and automation and has dynamic configuration parameters, whilst being a lightweight tool.",Python,11,https://www.reddit.com/r/Python/comments/1n0t07s/genec_v100_a_python_data_extraction_and/,r_1n0t07s,,,
r_1n0r5qd,reddit,QuasiEvil,2025-08-26T16:51:37+00:00,"Why no dunder methods for list append/extend?
I was just recently working on some code where I wanted controlled access to a list attribute (i.e., ensure every element is > 0 say). I naively started writing a descriptor but didn't get very far before realizing that neither `__set__()` nor` __setitem__()` (nor any other dunder method) would do the trick. This seems odd, as having controlled access to a list attribute via getters and setters would be useful, and consistent with other object types.

One could subclass list and override the append/extend methods with the desired behaviour, but I don't really understand why the descriptor pattern couldn't be applied to a list in the usual manner?",Python,0,https://www.reddit.com/r/Python/comments/1n0r5qd/why_no_dunder_methods_for_list_appendextend/,r_1n0r5qd,,,
r_1n0qtav,reddit,szymonmaszke,2025-08-26T16:39:01+00:00,"lintkit - framework to create linters/checks for Python code, JSON, YAML or TOML
Hey all,

## What my project does

Created a framework which allows you to create new linters/checkers/rules for `Python`, `YAML`, `JSON` or `TOML` (loose plans to extend the list if there's interest).

__Repository__: https://github.com/open-nudge/lintkit

## Key features

- Multiple formats supported (as mentioned)
- Supports well-known `noqa`/ignore comments (not only inline, but also per-file or even range-wise)
- Python-wise small (less than `1000` LOC, see `/src`), provides [tutorials](https://open-nudge.github.io/lintkit/latest/tutorials/) and [API reference](https://open-nudge.github.io/lintkit/latest/reference/lintkit/) to make your life easier
- Flexible - work directly with Python's [`ast`](https://docs.python.org/3/library/ast.html#ast.AST), make rules even across multiple files, settings to adjust the linter to your preference

## Example linter

Below is a linter which verifies no `function` or `class` names contain word `util` (or variations of it):

```python
import lintkit

# Set the name of the linter
lintkit.settings.name = ""NOUTILS""

class _NoUtils(lintkit.check.Regex, lintkit.loader.Python, lintkit.rule.Node):
    def regex(self):
        # Regex to match util(s) variations in function/class name
        return r""_?[Uu]til(s|ities)?""

    def values(self):
        # Yield class or function names from a Python file
        data = self.getitem(""nodes_map"")
        for node in data[self.ast_class()]:
            yield lintkit.Value.from_python(node.name, node)

    def message(self, _):
        return f""{self.ast_class()} name contains util(s) word""

# Concrete rules and their codes
# Disabling linter using noqas supported out of the box!
class ClassNoUtils(_NoUtils, code=0):  # noqa: NOUTILS0
    # ast type we want to focus on in this rule
    def ast_class(self):
        return ast.ClassDef

class FunctionNoUtils(_NoUtils, code=1):  # noqa: NOUTILS0
    def ast_class(self):
        return ast.FunctionDef

lintkit.run(""linter.py"", ""file1.py"", ""file2.py"")

# Example output
#/path/file1.py:23:17 NOUTILS0: ClassDef name contains util(s) word
#/path/file2.py:73:21 NOUTILS1: FunctionDef name contains util(s) word
```

## Target audience

People who would like to create their own linter/automated checks for their code. Mostly Python, but not only (could be used to lint GitHub Actions or `k8s` manifests).

## Comparison

- [`ruff`](https://github.com/astral-sh/ruff) - provides rules out of the box, way faster and production ready, but AFAICT has no interface to add easily your own custom rules via Python, less flexible
- [`flake8`](https://flake8.pycqa.org/en/latest/plugin-development/) - provides plugins, but with less flexibility and that's not the main goal of the project AFAIK

## Other info

- Python template which created all of the boilerplate during initialization (except code in `/src`, `/tests` and docs): https://github.com/open-nudge/opentemplate
- GitHub repo: https://github.com/open-nudge/lintkit

Welcoming feedback/requests either here or on GitHub, you can also follow on [__LinkedIn__](https://www.linkedin.com/company/opennudge),
[__Twitter/X__](https://x.com/opennudge) or [GitHub organization](https://github.com/open-nudge) to have direct info about new tooling, thanks!


",Python,16,https://www.reddit.com/r/Python/comments/1n0qtav/lintkit_framework_to_create_linterschecks_for/,r_1n0qtav,,,
r_1n0qjqj,reddit,zskniazi,2025-08-26T16:29:00+00:00,"How I Make My Life Easier Using Python and AI (And How You Can Too)

I used to spend hours on boring tasks.
Copying data. Renaming files. Writing emails. Searching the same stuff again and again.
It was exhausting.

Then Python happened. And laterâ€¦ AI.
Life changed.


---

The Wake-Up Moment

One night, around 2 a.m., I was stuck.
I had this huge Excel file â€” thousands of rows.
I needed to clean it, find patterns, and prepare a report.
Normally, it would take me two days. Minimum.

But I thought, â€œWhat if I justâ€¦ automate it?â€
I opened Python. Wrote a few lines using pandas.
Boom. Five minutes later, the job was done.

I swear, it felt like cheating.


---

Then Came AI

Python was great. But AI? Whole different game.

Imagine this:
I have Python pulling data from multiple sources.
AI reads it. Summarizes it. Writes me a neat report.
Suddenly, Iâ€™m the guy who finishes two days of work before lunch.

Example?
I built a tiny script:

Python scrapes product prices from websites.

AI analyzes trends.

AI then writes a full market report â€” in plain English.


Guess how long it takes?
Fifteen minutes.


---

The Magic Combo

Python + AI isnâ€™t about coding for the sake of coding.
Itâ€™s about building shortcuts.
Little tools that save you hours.

Some things Iâ€™ve automated:

Auto-generating emails based on data

Daily expense tracking with instant summaries

Bulk image renaming + resizing

Writing blog drafts using AI, then refining them myself

Creating personalized study plans for my kid


Each one saves me time. Mental energy. Sanity.


---

You Donâ€™t Need to Be a Genius

Iâ€™m not some 10x Silicon Valley developer.
I started small. One script at a time.

The trick? Donâ€™t overthink.
Find one annoying task. Automate it.
Then add AI to make it smarter.

Example:
Instead of manually replying to hundreds of repetitive emails, Python filters them.
AI drafts quick responses.
I just review and hit send.

Feels like having a digital assistant. Without the salary.


---

Final Thought

Python gives you control.
AI gives you speed.
Together? They give you freedom.

Freedom from boring tasks.
Freedom to focus on creative work.
Freedom to spend more time with family.

I donâ€™t see them as â€œtoolsâ€ anymore.
Theyâ€™re teammates.

If youâ€™re still doing everything manually, youâ€™re wasting time.
Start small. Write that first script. Plug in AI.
Trust me â€” your future self will thank you.",Python,0,https://www.reddit.com/r/Python/comments/1n0qjqj/how_i_make_my_life_easier_using_python_and_ai_and/,r_1n0qjqj,,,
r_1n0owky,reddit,Apart-Television4396,2025-08-26T15:26:33+00:00,"PySurf v1.6.0 - added permission handling, and dev tools
Hello, everyone! This is the final release before v2.0.0. I finished most of the core browser features.

# Added

* **Enhanced Permission Handling:**Â PySurf now features robust permission handling for website requests. Users will be prompted for explicit consent when a website attempts to access sensitive features such as:
   * Geolocation
   * Camera (Video Capture)
   * Microphone (Audio Capture)
   * Notifications
   * Mouse Lock
   * Desktop Video/Audio Capture
   * Screen Sharing This enhancement provides greater privacy and control over your browsing experience ([aafc67e](https://github.com/VG-dev1/PySurf/commit/aafc67e08ea483e6750adbf8b6c7d7d6f62e2847))
* **Integrated Developer Tools:**Â Users now have access to powerful Chromium Developer Tools from the sidebar. This provides advanced debugging and inspection capabilities for web developers ([aafc67e](https://github.com/VG-dev1/PySurf/commit/aafc67e08ea483e6750adbf8b6c7d7d6f62e2847))

Check it out here:Â [https://github.com/VG-dev1/PySurf](https://github.com/VG-dev1/PySurf)

PS: Please, don't downvote.",Python,0,https://www.reddit.com/r/Python/comments/1n0owky/pysurf_v160_added_permission_handling_and_dev/,r_1n0owky,,,
r_1n0ng7f,reddit,figroot0,2025-08-26T14:31:52+00:00,"Whats your favorite Python trick or lesser known feature?
I'm always amazed at the hidden gems in python that can make code cleaner or more efficient. Weather its clever use of comprehensions to underrated standard library modules - whats a Python trick youâ€™ve discovered that really saved you some time or made your projects easier",Python,451,https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/,r_1n0ng7f,,,
r_1n0n55h,reddit,ssj_aleksa,2025-08-26T14:19:57+00:00,"I created a microservice system for real-time appliance monitoring
Hey everyone, I recently built a small project called **Smart Plug Notifier (SPN)**.

**What My Project Does**: It uses **TP-Link Tapo smart plugs** to monitor when my washer and dryer start or finish their cycles. The system is built as an **async, event-driven microservice architecture** with **RabbitMQ** for messaging and a **Telegram bot** for notifications.

For my personal use I only run it on two plugs, but itâ€™s designed to support many devices. Everything is containerized with **Docker**, so itâ€™s easy to spin up the full stack (tapo service, notification service, and RabbitMQ).

Iâ€™m mainly using it to never forget my laundry again ðŸ˜…, but it could work for any appliance you want real-time power usage alerts for.

  
**Target Audience:** Anyone who uses smart plugs (Tapo P110 in this case) and has a need for real time notifications.

Iâ€™d love to get some **feedback on the architecture, setup, or ideas for improvements**.  
Hereâ€™s the repo: ðŸ‘‰ [https://github.com/AleksaMCode/smart-plug-notifier](https://github.com/AleksaMCode/smart-plug-notifier)",Python,11,https://www.reddit.com/r/Python/comments/1n0n55h/i_created_a_microservice_system_for_realtime/,r_1n0n55h,,,
r_1n0mgbv,reddit,Sea-Ad7805,2025-08-26T13:52:22+00:00,"Memory Graph Web Debugger
# ðŸ§  What My Project Does

memory\_graph is a visualization tool that shows whatâ€™s really happening while Python code is executed:

* how variables reference the same or different objects
* changes to mutable vs immutable data types
* function calls and variable scope
* making shallow vs deep copies

To do this it generates a graph of the program state so you can literally see why your program behaves the way it does.

# ðŸ§© Hereâ€™s a small example:

    import copy
    
    def fun(c1, c2, c3, c4):
        c1[0].append(1)
        c2[0].append(2)
        c3[0].append(3)
        c4[0].append(4)
    
    mylist = [[0]]
    c1 = mylist
    c2 = mylist.copy()
    c3 = copy.copy(mylist)
    c4 = copy.deepcopy(mylist)
    fun(c1, c2, c3, c4)
    
    print(mylist) # What output do you expect?

Without visualization beginners often guess wrong about the result, but with memory\_graph the references and copies are clear.

ðŸ‘‰ Run the example in: [Memory Graph Web Debugger](https://memory-graph.com/#code=import+copy%0A%0Adef+fun%28c1%2C+c2%2C+c3%2C+c4%29%3A%0A++++c1%5B0%5D.append%281%29%0A++++c2%5B0%5D.append%282%29%0A++++c3%5B0%5D.append%283%29%0A++++c4%5B0%5D.append%284%29%0A%0Amylist+%3D+%5B%5B0%5D%5D%0Ac1+%3D+mylist%0Ac2+%3D+mylist.copy%28%29%0Ac3+%3D+copy.copy%28mylist%29%0Ac4+%3D+copy.deepcopy%28mylist%29%0Afun%28c1%2C+c2%2C+c3%2C+c4%29%0A%0Aprint%28mylist%29%0A&play)  
ðŸ“¦ Source code: [github.com/bterwijn/memory\_graph](https://github.com/bterwijn/memory_graph)

# ðŸŽ¯ Target Audience

* Students dealing with references, copies, and mutability
* Teachers/educators who want to explain Pythonâ€™s data model more effectively
* Developers debugging complex programs with nested data structures

# ðŸ” Comparison

A well-known alternative is Python Tutor:

* Python Tutor: browser-based, limited to small code snippets
* memory\_graph: runs locally and works in various IDEs (e.g., VSCode), supports large programs

So memory\_graph is not just for teaching toy examples, but can stretch to helping with real-world debugging of production code.
",Python,2,https://www.reddit.com/r/Python/comments/1n0mgbv/memory_graph_web_debugger/,r_1n0mgbv,,,
r_1n0mc2y,reddit,miller_stale,2025-08-26T13:47:44+00:00,"Polars Expressions Vs Series
I came into Polars out of curiosity for the performanceâ€¦ and stayed for the rest! 

After a couple of weeks using polars everyday, I can say I absolutely love it (chefs kissed for how amazing are Polarâ€™s docsâ€¦ stop using LLMs/Stackoverflow altogether for questions regarding Polars). It has completely replaced pandas for me - smoke it out of the water.

But Iâ€™m at the point thatâ€™d like to start getting a more intuitive way of thinking about Expressions and Series. I get that Series are a data structure (their take on  arrays) whilst Expressions are ***representation of a data transformation*** to use in te context of a df method (I can conceptually grasp the difference between a data structure and a transformation)â€¦ But practically speaking, when for instance Iâ€™d like to work with strings (say to replace or match a regex), I found myself with two very similar pages in their docs: pl.Expr.replace() and pl.Series.str.replace() (actually, polars.Expr.str.replace and polars.Series.str.replace are identical).

And I get that these are for two different uses based on the scope (I guess applying df-wide transformations vs a series-wide transformation?); but coming from Pandas I found myself choosing really nilly willy when to use or read the page of one versus the otherâ€¦ And would like to make a more conscious use/choice of when using one or the other. 

Anybody else finding themselves in that situation? Or is just me? I would truly appreciate if someone could suggest a way to start thinking about Series vs Expression to get a sort of heuristic of how to tell them apart?",Python,21,https://www.reddit.com/r/Python/comments/1n0mc2y/polars_expressions_vs_series/,r_1n0mc2y,,,
r_1n0ln2g,reddit,According-Home485,2025-08-26T13:18:56+00:00,"Python as a desktop background
So I have this python script that generates a maze and has it scroll, and it also has some 'runners' on it. I managed to set it up as a screensaver, but I was wondering if it was possible to set it as a desktop wallpaper without turning it into a gif since each maze is generated at random.

Update this is what I managed to do with you guys sugestions. I had claude clean it up so hopefully its understandable. So it sort of works, but it overlays the app icons even though they are still accessible and if you press the show desktop button at the bottom it removes it until you open an app. So basically it doesn't work.

[https://github.com/footiper/Maze\_Wallpaper.git](https://github.com/footiper/Maze_Wallpaper.git)

If anyone is interested I have the same thing as a screensaver that works great, just dm me or write it here idc, obv it's free.

",Python,44,https://www.reddit.com/r/Python/comments/1n0ln2g/python_as_a_desktop_background/,r_1n0ln2g,,,
r_1n0lcmf,reddit,Kindly_Accountant121,2025-08-26T13:06:42+00:00,"Linden: A lightweight Python framework for AI agents
Hi everyone,

**TL;DR:** I built Linden, a lightweight alternative to LangChain focused on simplicity and multi-provider support (OpenAI, Groq, Ollama). It's early-stage, and I'm looking for feedback!

**The Motivation** It started with a university project. I was building an agentic RAG system and naturally turned to the big, well-known frameworks. I quickly found myself overwhelmedâ€”fighting against colossal libraries where I had very little control, navigating thousands of lines of code just to do simple things.

For most use cases, these frameworks are clearly over-engineered. I wanted something that would let me focus on the agent's logic, not the framework's boilerplate. That's why I built Linden.

**What My Project Does** The goal is simplicity and productivity:

âœ… **Unified API:** Write once, use with OpenAI, Groq, and Ollama

ðŸ§  **Smart Memory:** FAISS-based persistent memory with automatic agent isolation

ðŸ› ï¸ **Auto Function Calling:** Python functions â†’ LLM tools via docstring parsing

ðŸ“¦ **Lean Architecture:** ~500 core lines vs 10k+ in complex alternatives

**âš ï¸ Early Stage Warning** This is still evolving software. I've been using it successfully for a couple of months, but there are areas for improvement: making configs more flexible, ongoing refactoring, adding providers like Anthropic.

I'm sharing now specifically to get community feedback before the architecture is set in stone.

GitHub: https://github.com/matstech/linden

I'd love to know what you think! Issues, stars â­, or suggestions are all welcome.",Python,0,https://www.reddit.com/r/Python/comments/1n0lcmf/linden_a_lightweight_python_framework_for_ai/,r_1n0lcmf,,,
r_1n0hpjs,reddit,Hello_World_00001,2025-08-26T10:02:08+00:00,"I built an open-source learning platform for ethical hacking, programming, and related tools
Iâ€™ve been working on a project called **RareCodeBase**.

**What My Project Does:** Itâ€™s a free, open-source platform that brings together tutorials and resources on programming, ethical hacking, and related tools. The idea is to have one place to learn without ads or paywalls.

  
**Target Audience:** The platform is mainly aimed at students, beginners, and self-learners who want to get started with coding or security. Developers and security folks are also welcome to contribute tutorials or improvements.

  
**Comparison**: A lot of tutorial sites are paid, not open-source, or focused on just one area. RareCodeBase is MIT-licensed and open to contributions, so anyone can add tutorials, suggest features, or even host their own version. The goal is to keep it community-driven and free.

  
Right now, itâ€™s pretty minimal, but Iâ€™m planning to grow it over time, possibly adding video tutorials and more structured content in the future.

The source code is available on GitHub: [github.com/RareCodeBase/Rare-Code-Base](https://github.com/RareCodeBase/Rare-Code-Base)

Any feedback would be really helpful as I keep improving it.  
Contributions are also welcome if youâ€™d like to add tutorials, improve design, or suggest features.  
And if you find it useful, leaving a star on GitHub would mean a lot.",Python,10,https://www.reddit.com/r/Python/comments/1n0hpjs/i_built_an_opensource_learning_platform_for/,r_1n0hpjs,,,
r_1n0dpnm,reddit,Efficient-Wolf-0000,2025-08-26T05:42:48+00:00,"Need someone for python practise
I am a relatively beginner in python
I have started doing leetcode and hacker rank problems in python 
It would be really great if I would have some company 
Because that way we can exchange the thoughts and see in different dimensions  of the same problem and learn more
Plus, it will make it more fun 
So dm me if u are interested ",Python,0,https://www.reddit.com/r/Python/comments/1n0dpnm/need_someone_for_python_practise/,r_1n0dpnm,,,
r_1n06wx9,reddit,AutoModerator,2025-08-26T00:00:45+00:00,"Tuesday Daily Thread: Advanced questions
# Weekly Wednesday Thread: Advanced Questions ðŸ

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! ðŸŒŸ",Python,9,https://www.reddit.com/r/Python/comments/1n06wx9/tuesday_daily_thread_advanced_questions/,r_1n06wx9,,,
r_1n03s8h,reddit,Apprehensive_Sea_302,2025-08-25T21:48:31+00:00,"[Project] Weekend project: System Monitor in Python with PyQt5
Hi everyone ðŸ‘‹

I wanted to share a project I hacked together over two weekends: a cross-platform System Monitor inspired by GNOMEâ€™s monitor, but written entirely in Python using PyQt5 and psutil.

Iâ€™ve always relied on system monitors in my workflow, but I kept running into limitations (especially on Windows and some Linux distros where I couldnâ€™t find a good alternative). So I tried building my own, combining:
	â€¢	psutil â†’ to access CPU, memory, processes, disk I/O, network
	â€¢	PyQt5 â†’ for the GUI (tabs, preferences dialog, per-core plots)
	â€¢	pyqtgraph â†’ for real-time plots with configurable smoothing (EMA)

Main features so far:
	â€¢	Multi-thread, general, and per-core multi-window CPU views
	â€¢	Adjustable refresh intervals, grids, antialiasing, line widths, colors
	â€¢	Inspect/filter/kill processes directly
	â€¢	Memory, swap, and network monitoring
	â€¢	File systems + disk I/O
	â€¢	Several built-in themes (light to deep dark)

ðŸ“¦ Installation:

pip install klv-system-monitor

ðŸ‘‰ Repo + screenshots:

https://github.com/karellopez/KLV-System-Monitor

Itâ€™s still early days, but it already replaced the other monitors I used daily.
Would love feedback, especially from those with experience optimizing PyQt5/psutil apps. ðŸš€",Python,8,https://www.reddit.com/r/Python/comments/1n03s8h/project_weekend_project_system_monitor_in_python/,r_1n03s8h,,,
r_1n027ew,reddit,jfowers_amd,2025-08-25T20:47:56+00:00,"Building a competitive local LLM server in Python
My team at AMD is working on an open, universal way to run speedy LLMs locally on PCs, and we're building it in Python. I'm curious what the community here would think of the work, so here's a showcase post!

**What My Project Does**

Lemonade runs LLMs on PCs by loading them into a server process with an inference engine. Then, users can:

* Load up the web ui to get a GUI for chatting with the LLM and managing models.
* Connect to other applications over the OpenAI API (chat, coding assistants, document/RAG search, etc.).
* Try out optimized backends, such as ROCm 7 betas for Radeon GPUs or OnnxRuntime-GenAI for Ryzen AI NPUs.

**Target Audience**

* Users who want a dead-simple way to get started with LLMs. Especially if their PC has hardware like Ryzen AI NPU or a Radeon GPU that benefit from specialized optimization.
* Developers who are building cross-platform LLM apps and don't want to worry about the details of setting up or optimizing LLMs for a wide range of PC hardware.

**Comparison**

Lemonade is designed with the following 3 ideas in mind, which I think are essential for local LLMs. Each of the major alternatives has an inherent blocker that prevents them from doing at least 1 of these:

1. Strictly open source.
2. Auto-optimizes for any PC, including off-the-shelf llama.cpp, our own custom llama.cpp recipes (e.g., TheRock), or integrating non-llama.cpp engines (e.g., OnnxRuntime).
3. Dead simple to use and build on with GUIs available for all features.

Also, it's the only local LLM server (AFAIK) written in Python! I wrote about the choice to use Python at length [here](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html).

GitHub: [https://github.com/lemonade-sdk/lemonade](https://github.com/lemonade-sdk/lemonade)",Python,43,https://www.reddit.com/r/Python/comments/1n027ew/building_a_competitive_local_llm_server_in_python/,r_1n027ew,,,
r_1n00eqa,reddit,Ok_Presentation9879,2025-08-25T19:39:49+00:00,"Learning bots with python
Hi everyone I wanted to come on and ask if anyone has good resources for learning to make python bots (chatbots, discord bots). For some context I have a good grasp on the language and am looking to further my skills by learning to make bots but don't know where to start. Any suggestions are greatly appreciated! ",Python,1,https://www.reddit.com/r/Python/comments/1n00eqa/learning_bots_with_python/,r_1n00eqa,,,
r_1n001ny,reddit,DerrickBagels,2025-08-25T19:25:50+00:00,"take an existing excel invoice template and makes a .py easily modifies it with simple inputs
asks for an excel template once and stores config (invoice cells, work/expense ranges, customer cells)

* maintains a customer list and lets you choose/use last/new
* fills multiple work items and expenses
* auto increments invoice number and sets invoice date
* outputs a new excel with date in filename

you can run this as a standalone `.py`:

    import json
    import os
    from datetime import datetime
    from openpyxl import load_workbook
    
    # for pdf export on windows
    try:
        import win32com.client
        WIN32_AVAILABLE = True
    except ImportError:
        WIN32_AVAILABLE = False
        print(""win32com not found, PDF export will be skipped"")
    
    CONFIG_FILE = ""invoice_config.json""
    CUSTOMERS_FILE = ""customers.json""
    
    def setup_config():
        config = {}
        config['template'] = input(""Path to invoice template Excel: "")
    
        config['invoice_date'] = input(""Cell for invoice date (e.g. B2): "")
        config['invoice_number'] = input(""Cell for invoice number (e.g. B3): "")
    
        print(""Customer fields in template"")
        config['customer_cells'] = {
            'name': input(""Cell for customer name: ""),
            'phone': input(""Cell for customer phone: ""),
            'email': input(""Cell for customer email: ""),
            'address': input(""Cell for customer address: ""),
            'postal': input(""Cell for customer postal code: "")
        }
    
        print(""Enter ranges for work items (rows only)"")
        config['work_rows'] = input(""Rows for work items (comma-separated, e.g. 5,6,7): "").split(',')
        config['work_cols'] = {
            'date': input(""Column for work date (e.g. B): ""),
            'desc': input(""Column for work description (e.g. C): ""),
            'hours': input(""Column for work hours (e.g. D): "")
        }
    
        print(""Enter ranges for expenses (rows only)"")
        config['expense_rows'] = input(""Rows for expenses (comma-separated, e.g. 10,11,12): "").split(',')
        config['expense_cols'] = {
            'date': input(""Column for expense date (e.g. B): ""),
            'desc': input(""Column for expense description (e.g. C): ""),
            'value': input(""Column for expense value (e.g. D): ""),
            'link': input(""Column for expense link (e.g. E): "")
        }
    
        with open(CONFIG_FILE, ""w"") as f:
            json.dump(config, f, indent=2)
        print(""Config saved as invoice_config.json"")
    
    def load_customers():
        if os.path.exists(CUSTOMERS_FILE):
            return json.load(open(CUSTOMERS_FILE))
        return []
    
    def save_customers(customers):
        with open(CUSTOMERS_FILE, ""w"") as f:
            json.dump(customers, f, indent=2)
    
    def select_customer(customers):
        if customers:
            choice = input(""Customer option (last/select/new): "").strip().lower()
        else:
            choice = ""new""
    
        if choice == ""last"":
            return customers[-1], customers
        elif choice == ""select"":
            for i, c in enumerate(customers):
                print(f""{i}: {c['name']}"")
            idx = int(input(""Select customer index: ""))
            return customers[idx], customers
        else:  # new
            customer = {
                ""name"": input(""Customer name: ""),
                ""phone"": input(""Phone: ""),
                ""email"": input(""Email: ""),
                ""address"": input(""Address: ""),
                ""postal"": input(""Postal code: "")
            }
            customers.append(customer)
            save_customers(customers)
            return customer, customers
    
    def export_pdf(excel_path, pdf_path):
        if not WIN32_AVAILABLE:
            print(""PDF export skipped, win32com not installed"")
            return
        excel = win32com.client.Dispatch(""Excel.Application"")
        excel.Visible = False
        wb = excel.Workbooks.Open(os.path.abspath(excel_path))
        ws = wb.Worksheets[1]
        ws.ExportAsFixedFormat(0, os.path.abspath(pdf_path))
        wb.Close(False)
        excel.Quit()
        print(f""PDF saved as {pdf_path}"")
    
    def fill_invoice():
        config = json.load(open(CONFIG_FILE))
        wb = load_workbook(config['template'])
        ws = wb.active
    
        customers = load_customers()
        customer, _ = select_customer(customers)
    
        # fill customer fields
        ws[config['customer_cells']['name']] = customer['name']
        ws[config['customer_cells']['phone']] = customer['phone']
        ws[config['customer_cells']['email']] = customer['email']
        ws[config['customer_cells']['address']] = customer['address']
        ws[config['customer_cells']['postal']] = customer['postal']
    
        # invoice date and number
        today = datetime.today().strftime(""%Y-%m-%d"")
        ws[config['invoice_date']] = today
        current_invoice = int(ws[config['invoice_number']].value)
        ws[config['invoice_number']] = current_invoice + 1
    
        # fill work items
        for row in config['work_rows']:
            row = row.strip()
            ws[f""{config['work_cols']['date']}{row}""] = input(f""Work date for row {row}: "")
            ws[f""{config['work_cols']['desc']}{row}""] = input(f""Work description for row {row}: "")
            ws[f""{config['work_cols']['hours']}{row}""] = input(f""Work hours for row {row}: "")
    
        # fill expenses
        for row in config['expense_rows']:
            row = row.strip()
            ws[f""{config['expense_cols']['date']}{row}""] = input(f""Expense date for row {row}: "")
            ws[f""{config['expense_cols']['desc']}{row}""] = input(f""Expense description for row {row}: "")
            ws[f""{config['expense_cols']['value']}{row}""] = input(f""Expense value for row {row}: "")
            ws[f""{config['expense_cols']['link']}{row}""] = input(f""Expense link for row {row}: "")
    
        excel_filename = f""invoice_{today}.xlsx""
        wb.save(excel_filename)
        print(f""Invoice saved as {excel_filename}"")
    
        pdf_filename = f""invoice_{today}.pdf""
        export_pdf(excel_filename, pdf_filename)
    
    def main():
        if not os.path.exists(CONFIG_FILE):
            print(""No config found. Running setup..."")
            setup_config()
        fill_invoice()
    
    if __name__ == ""__main__"":
        main()
    

**notes:**

* pdf export works on **Windows with Excel installed**
* outputs both `invoice_YYYY-MM-DD.xlsx` and `.pdf`
* keeps customer list in `customers.json`
* handles multiple work and expense rows

* dynamic customer selection / storage
* multiple work and expense rows
* invoice date auto-update
* invoice number auto-increment
* outputs new excel file named by date

",Python,0,https://www.reddit.com/r/Python/comments/1n001ny/take_an_existing_excel_invoice_template_and_makes/,r_1n001ny,,,
r_1mzxbia,reddit,sultanaiyan1098,2025-08-25T17:44:18+00:00,"I created this polygon screenshot tool for myself, I must say it may be useful to others!
* **What My Project Does -** Take a screenshot by drawing a precise polygon rather than being limited to a rectangular or manual free-form shape
* **Target Audience -** Meant for *production (For me, my professor just give notes pdf with everything jumbled together so I wanted to keep them organized, obviously on my note by taking screenshots of them)*
* **Comparison -** I am a windows user, neither does windows provide default polygon screenshot tool nor are they available on anywhere else on internet
* You can check it out on github: [https://github.com/sultanate-sultan/polygon-screenshot-tool](https://github.com/sultanate-sultan/polygon-screenshot-tool)
* You can find the demo video on my github repo page",Python,188,https://www.reddit.com/r/Python/comments/1mzxbia/i_created_this_polygon_screenshot_tool_for_myself/,r_1mzxbia,,,
r_1mzv2v2,reddit,notProper-Drama71,2025-08-25T16:22:25+00:00,"16 Ð»ÐµÑ‚ ÑƒÑ‡ÑƒÑÑŒ ÑÐ°Ð¼Ð¾ÑƒÑ‡ÐºÐ°
Ð·Ð´Ñ€Ð°ÑÑŒÑ‚Ðµ Ñ Ð±ÑƒÐ´ÑƒÑ‰Ð¸Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸ÑÑ‚. Ð’Ñ‹Ð±Ñ€Ð°Ð» ÑÐ·Ñ‹Ðº Ð¿Ð¸Ñ‚Ð¾Ð½, Ñ‡Ñ‚Ð¾ Ð¿Ð¾ÑÐ¾Ð²ÐµÑ‚ÑƒÐµÑ‚Ðµ Ð³Ð´Ðµ Ð±Ñ€Ð°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ?
Ð±ÐµÑ€Ñƒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ Ð±Ð»Ð¾Ð³ÐµÑ€Ñ‹ 15 Ñ‡Ð°ÑÐ¾Ð²Ñ‹Ðµ 5 Ñ‡Ð°ÑÐ¾Ð²Ñ‹Ðµ Ð²Ð¸Ð´ÐµÐ¾ ÑÐ¼Ð¾Ñ‚Ñ€ÑŽ. Ð¸ ÐµÑ‰Ðµ ÐºÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¾Ð²Ð°Ñ‚ÑÑ? Ð²ÑÐµ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‚ Ñ‡Ñ‚Ð¾ Ð½Ð°Ð´Ð¾ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¸ Ð¼Ð½Ð¾Ð³Ð¾ Ð° ÐºÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ?",Python,0,https://www.reddit.com/r/Python/comments/1mzv2v2/16_Ð»ÐµÑ‚_ÑƒÑ‡ÑƒÑÑŒ_ÑÐ°Ð¼Ð¾ÑƒÑ‡ÐºÐ°/,r_1mzv2v2,,,
r_1mzn9jm,reddit,Busy_Worker_5726,2025-08-25T10:54:33+00:00,"are there any good completely free/open source agentic AI models?
Are there any free or open source agentic AI models?  
The use case is - We basically want to parse resumes for the company and compile data",Python,0,https://www.reddit.com/r/Python/comments/1mzn9jm/are_there_any_good_completely_freeopen_source/,r_1mzn9jm,,,
r_1mzmaj1,reddit,predict_addict,2025-08-25T09:58:28+00:00,"[R] Advanced Conformal Prediction â€“ A Complete Resource from First Principles to Real-World
Hi everyone,

Iâ€™m excited to share that my new book,Â ***Advanced Conformal Prediction: Reliable Uncertainty Quantification for Real-World Machine Learning***, is now available in early access.

Conformal Prediction (CP) is one of the most powerful yet underused tools in machine learning: it providesÂ **rigorous, model-agnostic uncertainty quantification with finite-sample guarantees**. Iâ€™ve spent the last few years researching and applying CP, and this book is my attempt to create aÂ **comprehensive, practical, and accessible guide**â€”from the fundamentals all the way to advanced methods and deployment.

# What the book covers

* **Foundations**Â â€“ intuitive introduction to CP, calibration, and statistical guarantees.
* **Core methods**Â â€“ split/inductive CP for regression and classification, conformalized quantile regression (CQR).
* **Advanced methods**Â â€“ weighted CP for covariate shift, EnbPI, blockwise CP for time series, conformal prediction with deep learning (including transformers).
* **Practical deployment**Â â€“ benchmarking, scaling CP to large datasets, industry use cases in finance, healthcare, and more.
* **Code & case studies**Â â€“ hands-on Jupyter notebooks to bridge theory and application.

# Why I wrote it

When I first started working with CP, I noticed there wasnâ€™t a single resource that takes youÂ **from zero knowledge to advanced practice**. Papers were often too technical, and tutorials too narrow. My goal was to put everything in one place: the theory, the intuition, and the engineering challenges of using CP in production.

If youâ€™re curious about uncertainty quantification, or want to learn how to make your models not just accurate but alsoÂ **trustworthy and reliable**, I hope youâ€™ll find this book useful.

Happy to answer questions here, and would love to hear if youâ€™ve already tried conformal methods in your work!",Python,17,https://www.reddit.com/r/Python/comments/1mzmaj1/r_advanced_conformal_prediction_a_complete/,r_1mzmaj1,,,
r_1mzcxyc,reddit,Chuyito,2025-08-25T01:00:45+00:00,"Adding asyncio.sleep(0) made my data pipeline (150 ms) not spike to (5500 ms)
I've been rolling out the oddest fix across my async code today, and its one of those that feels dirty to say the least.

Data pipeline has 2 long running asyncio.gather() tasks:

* 1 reads 6k rows over websocket every 100ms and stores them to a global dict of dicts
* 2 ETLs a deepcopy of the dicts and dumps it to a DB.

After \~30sec of running, this job gets insanely slow.

    04:42:01 PM Processed 6745 async_run_batch_insert in 159.8427 ms
    04:42:02 PM Processed 6711 async_run_batch_insert in 162.3137 ms
    ...
    04:42:09 PM Processed 6712 async_run_batch_insert in 5489.2745 ms

Up to 5k rows, this job was happily running for months. Once I scaled it up beyond 5k rows, it hit this random slowdown.

Adding an \`asyncio.sleep(0)\` at the end of my function completely got rid of the ""slow"" runs and its consistently 150-160ms for days with the full 6700 rows. Pseudocode:

    async def etl_to_db():
      # grab a deepcopy of the global msg cache
      # etl it
      # await dump_to_db(etl_msg)
      await asyncio.sleep(0)  # <-- This ""fixed it""
    
    
    async def dump_books_to_db():
      while True:
        # Logic to check the ws is connected
        await etl_to_db()
        await asyncio.sleep(0.1)
    
    await asyncio.gather(
      dump_books_to_db(),
      sub_websocket()
     )

I believe the sleep yields control back to the GIL? Both gpt and grok were a bit useless in debugging this, and kept trying to approach it from the database schema being the reason for the slowdown.

Given we're in 2025 and python 3.11, this feels insanely hacky... but it works. am I missing something",Python,171,https://www.reddit.com/r/Python/comments/1mzcxyc/adding_asynciosleep0_made_my_data_pipeline_150_ms/,r_1mzcxyc,,,
r_1mzbnhm,reddit,AutoModerator,2025-08-25T00:00:30+00:00,"Monday Daily Thread: Project ideas!
# Weekly Thread: Project Ideas ðŸ’¡

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project ideaâ€”be it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! ðŸŒŸ",Python,2,https://www.reddit.com/r/Python/comments/1mzbnhm/monday_daily_thread_project_ideas/,r_1mzbnhm,,,
r_1mzbj8n,reddit,jaaberg1981,2025-08-24T23:55:07+00:00,"I built a Python Prisoner's Dilemma Simulator
https://github.com/jasonaaberg/Prisoners-Dilemma

What My Project Does: It is a Python Based Prisoner's Dilemma simulator. 

Target Audience: This is meant for anyone who has interests in Game Theory and learning about how to collect data and compare outcomes.

Comparison: I am unaware of any other Python based Prisoner's Dilemma simulators but I am sure they exist. 

There's a CLI and GUI version in this repo. It can be played as Human vs. Computer or Computer vs. Computer. There are 3 built in computer strategies to choose from and you can define how many rounds it will play. When you run the auto play all option it will take a little while as it runs all of the rounds in the background and then shows the output.

If you get a chance I would love some feedback. I wrote a lot of the code myself and also use Claude to help out with a lot of the stuff that I couldn't figure out how to make it work.

If anyone does look at it thank you in advance!!!!!",Python,18,https://www.reddit.com/r/Python/comments/1mzbj8n/i_built_a_python_prisoners_dilemma_simulator/,r_1mzbj8n,,,
r_1mz83yx,reddit,Interesting_Flower93,2025-08-24T21:30:23+00:00,"Clipipe â€“ Pipe command output between machines, even behind NAT
Hi everyone ðŸ‘‹

I built [**Clipipe**](https://clipipe.io/), a small open-source tool written in Python that lets you **pipe command output from one machine to another**, even if theyâ€™re behind NAT or firewalls.

# ðŸ”¹ What My Project Does

Clipipe makes it easy to send and receive data between machines using simple, human-readable codes. You can use it in shell pipelines, so anything youâ€™d normally pipe (`stdout` â†’ `stdin`) can now cross machines.

**Example:**

    # Send data
    echo ""Hello World"" | clipipe send
    # -> returns a short code, e.g. bafilo42
    
    # Retrieve it elsewhere
    clipipe receive bafilo42

It works just as well for files and archives:

    tar cz project/ | clipipe send
    clipipe receive <code> | tar xz

# ðŸ”¹ Target Audience

* Developers who want a quick, frictionless way to move data between machines (work â†” home, dev â†” server, VM â†” host).
* People working behind strict NAT/firewalls where `scp`, `ssh`, or direct networking isnâ€™t possible.
* Anyone who likes CLI-first tools that integrate naturally into existing Unix pipelines.

This is a **production-ready tool** (available on PyPI, installable via `pipx` or `uv`), but also a small project thatâ€™s fun to self-host and extend.

# ðŸ”¹ Comparison

* Unlike `scp`/`rsync`, you donâ€™t need SSH access or firewall configuration â€” just a short code.
* Unlike `netcat` or `socat`, it works even when both peers are behind NAT.
* Unlike pastebin-style tools, itâ€™s designed for binary-safe data and direct use in pipelines (`stdin`/`stdout`).

# Install

    pipx install clipipe

(or `uvx install clipipe` if you prefer `uv`)

Repo: [github.com/amirkarimi/clipipe](https://github.com/amirkarimi/clipipe)  
Docs + server: [clipipe.io](https://clipipe.io/)",Python,3,https://www.reddit.com/r/Python/comments/1mz83yx/clipipe_pipe_command_output_between_machines_even/,r_1mz83yx,,,
r_1ng3v5w,reddit,No_Suit_5724,2025-09-13T17:46:38+00:00,Would keeping your hands free with a foot mouse help programmers?,programming,1,https://www.reddit.com/r/programming/comments/1ng3v5w/would_keeping_your_hands_free_with_a_foot_mouse/,r_1ng3v5w,,,
r_1ng3n47,reddit,Leading-Solution6758,2025-09-13T17:38:02+00:00,"Can anyone test my game and tell me the pros and cons?
    The game is about combinations of numbers. Thank you in advance for your help.",programming,0,https://www.reddit.com/r/programming/comments/1ng3n47/can_anyone_test_my_game_and_tell_me_the_pros_and/,r_1ng3n47,,,
r_1ng2umk,reddit,Important_Earth6615,2025-09-13T17:07:17+00:00,Build 2D Software Rasterizer for graphic library,programming,1,https://www.reddit.com/r/programming/comments/1ng2umk/build_2d_software_rasterizer_for_graphic_library/,r_1ng2umk,,,
r_1ng2e8m,reddit,trolleid,2025-09-13T16:49:21+00:00,"NSFW content detection, AI architecture: How we solved it in my startup",programming,0,https://www.reddit.com/r/programming/comments/1ng2e8m/nsfw_content_detection_ai_architecture_how_we/,r_1ng2e8m,,,
r_1ng1yn4,reddit,The_Axolot,2025-09-13T16:31:41+00:00,"Be An Agnostic Programmer
Hey guys! Back with another article on a topic that's been stewing in the back of my mind for a while. Please enjoy!",programming,14,https://www.reddit.com/r/programming/comments/1ng1yn4/be_an_agnostic_programmer/,r_1ng1yn4,,,
r_1nfzgho,reddit,avinassh,2025-09-13T14:52:15+00:00,"Setsum - order agnostic, additive, subtractive checksum",programming,2,https://www.reddit.com/r/programming/comments/1nfzgho/setsum_order_agnostic_additive_subtractive/,r_1nfzgho,,,
r_1nfzfuo,reddit,Low-Strawberry7579,2025-09-13T14:51:31+00:00,"Gitâ€™s hidden simplicity: whatâ€™s behind every commit
Itâ€™s time to learn some Git internals.",programming,120,https://www.reddit.com/r/programming/comments/1nfzfuo/gits_hidden_simplicity_whats_behind_every_commit/,r_1nfzfuo,,,
r_1nfy4xg,reddit,ablx0000,2025-09-13T13:56:19+00:00,On Staying Sane as a Developer,programming,0,https://www.reddit.com/r/programming/comments/1nfy4xg/on_staying_sane_as_a_developer/,r_1nfy4xg,,,
r_1nfvdvk,reddit,elfenpiff,2025-09-13T11:43:37+00:00,"Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library for Rust, Python, C++, and C",programming,14,https://www.reddit.com/r/programming/comments/1nfvdvk/announcing_iceoryx2_v07_fast_and_robust/,r_1nfvdvk,,,
r_1nfum5p,reddit,Active-Fuel-49,2025-09-13T10:59:26+00:00,js1024 Revisited in 2025,programming,2,https://www.reddit.com/r/programming/comments/1nfum5p/js1024_revisited_in_2025/,r_1nfum5p,,,
r_1nfsfml,reddit,mareek,2025-09-13T08:40:48+00:00,crates.io phishing campaign | Rust Blog,programming,45,https://www.reddit.com/r/programming/comments/1nfsfml/cratesio_phishing_campaign_rust_blog/,r_1nfsfml,,,
r_1nfqsf6,reddit,fablue,2025-09-13T06:57:59+00:00,Benchmark Battle: But how fast is the GPU really?,programming,9,https://www.reddit.com/r/programming/comments/1nfqsf6/benchmark_battle_but_how_fast_is_the_gpu_really/,r_1nfqsf6,,,
r_1nfq0mh,reddit,neilmadden,2025-09-13T06:11:12+00:00,"[ Removed by Reddit ]
[ Removed by Reddit on account of violating the [content policy](/help/contentpolicy). ]",programming,0,https://www.reddit.com/r/programming/comments/1nfq0mh/removed_by_reddit/,r_1nfq0mh,,,
r_1nfolpk,reddit,stumblingtowards,2025-09-13T04:48:41+00:00,"Why You Are Bad At Coding
Yes you. Well, maybe. How would you know? Does it really matter? Is it just a skill issue?

Find out what I think. It is clickbait or is there something of value here? Just watch the video anyway and let YouTube know that I actually exist. ",programming,0,https://www.reddit.com/r/programming/comments/1nfolpk/why_you_are_bad_at_coding/,r_1nfolpk,,,
r_1nfnsu8,reddit,phillipcarter2,2025-09-13T04:03:29+00:00,Defeating Nondeterminism in LLM Inference,programming,0,https://www.reddit.com/r/programming/comments/1nfnsu8/defeating_nondeterminism_in_llm_inference/,r_1nfnsu8,,,
r_1nfl1kw,reddit,apeloverage,2025-09-13T01:41:01+00:00,Let's make a game! 326: Ammunition,programming,0,https://www.reddit.com/r/programming/comments/1nfl1kw/lets_make_a_game_326_ammunition/,r_1nfl1kw,,,
r_1nff3qj,reddit,Kissaki0,2025-09-12T21:14:44+00:00,REACT-VFX - WebGL effects for React - Crazy Visuals on the Website,programming,12,https://www.reddit.com/r/programming/comments/1nff3qj/reactvfx_webgl_effects_for_react_crazy_visuals_on/,r_1nff3qj,,,
r_1nfcwms,reddit,chintanbawa,2025-09-12T19:48:29+00:00,How I create welcome and login screen in react native with react-native-reanimated #reactnative,programming,0,https://www.reddit.com/r/programming/comments/1nfcwms/how_i_create_welcome_and_login_screen_in_react/,r_1nfcwms,,,
r_1nfbtwb,reddit,mttd,2025-09-12T19:05:49+00:00,Inside vLLM: Anatomy of a High-Throughput LLM Inference System,programming,0,https://www.reddit.com/r/programming/comments/1nfbtwb/inside_vllm_anatomy_of_a_highthroughput_llm/,r_1nfbtwb,,,
r_1nf9x4a,reddit,Diligent_Historian_4,2025-09-12T17:50:38+00:00,I coded Pac-Man in Python without a game engine.,programming,0,https://www.reddit.com/r/programming/comments/1nf9x4a/i_coded_pacman_in_python_without_a_game_engine/,r_1nf9x4a,,,
r_1nf9buo,reddit,West-Chard-1474,2025-09-12T17:27:32+00:00,The productivity paradox of AI coding assistants,programming,400,https://www.reddit.com/r/programming/comments/1nf9buo/the_productivity_paradox_of_ai_coding_assistants/,r_1nf9buo,,,
r_1nf96ir,reddit,iximiuz,2025-09-12T17:21:50+00:00,How Containers Work: Building a Docker-like Container From Scratch,programming,43,https://www.reddit.com/r/programming/comments/1nf96ir/how_containers_work_building_a_dockerlike/,r_1nf96ir,,,
r_1nf90qr,reddit,craigkerstiens,2025-09-12T17:15:30+00:00,Get Excited About Postgres 18,programming,127,https://www.reddit.com/r/programming/comments/1nf90qr/get_excited_about_postgres_18/,r_1nf90qr,,,
r_1nf8hyo,reddit,iamkeyur,2025-09-12T16:55:19+00:00,Many Hard Leetcode Problems are Easy Constraint Problems,programming,25,https://www.reddit.com/r/programming/comments/1nf8hyo/many_hard_leetcode_problems_are_easy_constraint/,r_1nf8hyo,,,
r_1nf85j7,reddit,aviator_co,2025-09-12T16:41:45+00:00,"Everything Wrong With Developer Productivity Metrics
The DORA Four were meant as feedback mechanisms for teams to improve, not as a way to compare performance across an entire org. Somewhere along the way, we lost that thread and started chasing â€œproductivity metricsâ€ instead.

Martin Fowler said it best: you canâ€™t measure individual developer productivity. Thatâ€™s a foolâ€™s errand. And even the official DORA site emphasizes these arenâ€™t productivity metrics, theyâ€™re software delivery performance metrics.

>Thereâ€™s definitely an industry now. Tools that plug into your repos and issue trackers and spit out dashboards of 40+ metrics. Some of these are useful. Others are actively harmful by design.

The problem is, code is aÂ *lossy representation*Â of the real work. Writing code is often less than half of what engineers actually do. Problem solving, exploring tradeoffs, and system design arenâ€™t captured in a commit log.

Folks like Kent Beck and Rich Hickey have even argued that the most valuable part of development is theÂ *thinking*, not the typing. And you canâ€™t really capture that in a metric.",programming,31,https://www.reddit.com/r/programming/comments/1nf85j7/everything_wrong_with_developer_productivity/,r_1nf85j7,,,
r_1nf6df3,reddit,Advocatemack,2025-09-12T15:32:55+00:00,"â€œI Got Pwnedâ€: npm maintainer of Chalk & Debug speaks on the massive supply-chain attack
Hey Everyone,  
This week I posted our discovery of finding that a popular open-source projects, including debug and chalk had been breached. I'm happy to say the Josh (Qix) the maintainer that was compromised agreed to sit down with me and discuss his experience, it was a very candid conversation but one I think was important to have.

Below are some of the highlight and takeaways from the conversation, since the â€œhow could this happen?â€ question is still circulating.

**Was MFA on the account?**

â€œThere was definitely MFAâ€¦ but timed one-time passwords are not phishing resistant. They can be man in the middle. Thereâ€™s no cryptographic checks, no domain association, nothing like U2F would have.â€

The attackers used a fake NPM login flow and captured his TOTP, allowing them to fully impersonate him. Josh called out not enabling phishing-resistant MFA (FIDO2/U2F) as his biggest technical mistake.

**The scale of the blast radius**

Charlie (our researcher) spotted the issue while triaging suspicious packages:

â€œFirst I saw the debug packageâ€¦ then I saw chalk and error-exâ€¦ and I knew a significant portion of the JS ecosystem would be impacted.â€

Wiz later reported that **99% of cloud environments used at least one affected package**.

â€œThe fact it didnâ€™t do anything was the bullet we dodged. It ran in CI/CD, on laptops, servers, enterprise machines. It could have done anything.â€

Wiz also reported that 10% of cloud environments they analyzed had the malware inside them. There were some 'hot takes' on the internet that, in fact this was not a big deal and some said it was a win for security. Josh shared that this was not a win and the only reason we got away with it was because how ineffective the attackers were. The malicious packages were downloaded 2.5 million times in the 2 hour window they were live.

**Ecosystem-level shortcomings**

Josh was frank about registry response times and missing safeguards:

â€œThere was a huge process breakdown during this attack with NPM. Extremely slow to respond. No preemptive â€˜switch to U2Fâ€™ push despite billions of downloads. I had no recourse except filing a ticket through their public form.""

Josh also gave some advice for anyone going through this in the future which is to be open and transparent, the internet largely agreed Josh handled this in the best way possible (short of not getting phished in the first place )

â€œIf you screw up, own it. In open source, being transparent and immediate saves a lot of peopleâ€™s time and money. Vulnerability (the human kind) goes a long way.â€",programming,161,https://www.reddit.com/r/programming/comments/1nf6df3/i_got_pwned_npm_maintainer_of_chalk_debug_speaks/,r_1nf6df3,,,
r_1nf0dli,reddit,ludovicianul,2025-09-12T11:13:46+00:00,The Invisible Character That Cost Me Too Much Debugging Time,programming,0,https://www.reddit.com/r/programming/comments/1nf0dli/the_invisible_character_that_cost_me_too_much/,r_1nf0dli,,,
r_1nf08e5,reddit,BitterHouse8234,2025-09-12T11:05:49+00:00,"Graph rag pipeline that runs entirely locally with ollama and has full source attribution
HeyÂ ,

I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.

My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.

The main goal was to solve two big problems:

Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.

Trust & Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.

One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.

The project includes:

The full Graph RAG pipeline.

A Gradio UI for an interactive chat experience.

A guide for setting everything up, from installing dependencies to running the indexing process.

GitHub Repo with all the code and instructions:Â [https://github.com/bibinprathap/VeritasGraph](https://github.com/bibinprathap/VeritasGraph)

I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.

Thanks!",programming,5,https://www.reddit.com/r/programming/comments/1nf08e5/graph_rag_pipeline_that_runs_entirely_locally/,r_1nf08e5,,,
r_1nezpel,reddit,FastSascha,2025-09-12T10:35:46+00:00,"The Limiting Factor in Using AI (mostly LLMs)
> You canâ€™t automate what you canâ€™t articulate.

To me, this is one of the core principles of working with generative AI.

This is another, perhaps more powerful principle:

> In knowledge work, the bottleneck is not the external availability of information. It is the internal bandwidth of processing power, which is determined by your innate abilities and the training status of your mind. source

I think this is already the problem that occurs.

I am using AI extensively. Yet, I mainly benefit in areas in which I know most. This aligns with the hypothesis that AI is killing junior position in software engineering while senior positions remain untouched.

**AI should be used as a multiplier, not as a surrogate.**

So, my hypothesis that our minds are the bases that AI is multiplying. So, in total, we benefit still way more from training our minds and not AI-improvements.
",programming,0,https://www.reddit.com/r/programming/comments/1nezpel/the_limiting_factor_in_using_ai_mostly_llms/,r_1nezpel,,,
r_1ney0wh,reddit,fR0DDY,2025-09-12T08:50:58+00:00,"Shielding High-Demand Systems from Fraud
Some strategies to combat bots",programming,4,https://www.reddit.com/r/programming/comments/1ney0wh/shielding_highdemand_systems_from_fraud/,r_1ney0wh,,,
r_1neuyaq,reddit,donutloop,2025-09-12T05:31:41+00:00,BSA Launches Quantum Policy Agenda,programming,9,https://www.reddit.com/r/programming/comments/1neuyaq/bsa_launches_quantum_policy_agenda/,r_1neuyaq,,,
r_1neu6uk,reddit,priyankchheda15,2025-09-12T04:46:07+00:00,"Prototype Design Pattern in Go â€“ Faster Object Creation ðŸš€
Hey folks,

I recently wrote a blog about the Prototype Design Pattern and how it can simplify object creation in Go.

Instead of constantly re-building complex objects from scratch (like configs, game entities, or nested structs), Prototype lets you clone pre-initialized objects, saving time and reducing boilerplate.

In the blog, I cover:

* The basics of shallow vs deep cloning in Go.
* Different implementation techniques (Clone() methods, serialization, reflection).
* Building a Prototype Registry for dynamic object creation.
* Real-world use cases like undo/redo systems, plugin architectures, and performance-heavy apps.

If youâ€™ve ever struggled with slow, expensive object initialization, this might help:

[https://medium.com/design-bootcamp/understanding-the-prototype-design-pattern-in-go-a-practical-guide-329bf656fdec](https://medium.com/design-bootcamp/understanding-the-prototype-design-pattern-in-go-a-practical-guide-329bf656fdec)

Curious to hear how youâ€™ve solved similar problems in your projects!",programming,0,https://www.reddit.com/r/programming/comments/1neu6uk/prototype_design_pattern_in_go_faster_object/,r_1neu6uk,,,
r_1nesqgh,reddit,iamkeyur,2025-09-12T03:26:49+00:00,Behind the scenes of Bun Install,programming,8,https://www.reddit.com/r/programming/comments/1nesqgh/behind_the_scenes_of_bun_install/,r_1nesqgh,,,
r_1nesqco,reddit,iamkeyur,2025-09-12T03:26:40+00:00,The Challenge of Maintaining Curl,programming,340,https://www.reddit.com/r/programming/comments/1nesqco/the_challenge_of_maintaining_curl/,r_1nesqco,,,
r_1nesp2g,reddit,iamkeyur,2025-09-12T03:24:47+00:00,Floating Point Visually Explained,programming,164,https://www.reddit.com/r/programming/comments/1nesp2g/floating_point_visually_explained/,r_1nesp2g,,,
r_1nep5ew,reddit,tslocum,2025-09-12T00:28:10+00:00,Architecture of the Ebitengine Game Engine (Tutorial),programming,0,https://www.reddit.com/r/programming/comments/1nep5ew/architecture_of_the_ebitengine_game_engine/,r_1nep5ew,,,
r_1nenpd7,reddit,ChavXO,2025-09-11T23:19:18+00:00,An introduction to program synthesis,programming,2,https://www.reddit.com/r/programming/comments/1nenpd7/an_introduction_to_program_synthesis/,r_1nenpd7,,,
r_1nemhc0,reddit,Muhznit,2025-09-11T22:24:40+00:00,"RSL Open Licensing Protocol: Protecting content from AI scrapers and bringing back RSS? Pinch me if I'm dreaming
I've not seen discussions of this yet, only passed by it briefly when doomscrolling. This kinda seems like it has potential, anyone around here poked around with it yet?",programming,6,https://www.reddit.com/r/programming/comments/1nemhc0/rsl_open_licensing_protocol_protecting_content/,r_1nemhc0,,,
r_1nei4ig,reddit,bajcmartinez,2025-09-11T19:28:59+00:00,"The Real Reasons Why Developers Burnout
When people talk about â€œdeveloper burnout,â€ the assumption is usually that engineers are working too many hours, drowning in code. But after 20+ years in this industry, Iâ€™ve rarely seen burnout caused by too much coding.

Instead, developers burn out because of the environment around coding:

\* Unclear priorities â€” constant shifting goals, wasted effort.

\* Constant interruptions â€” meetings, Slack pings, context switching.

\* Politics â€” decisions driven by ego instead of merit.

Code complexity can be hard, but itâ€™s logical. You can refactor it, test it, improve it. Chaos is different. You canâ€™t debug interruptions, or refactor unclear priorities. And chaos amplifies complexity, making hard problems feel impossible.

My recommendations for developers stuck in these environments:

\* Protect blocks of deep work time.

\* Push for written, stable priorities.

\* Reduce nonessential notifications/meetings.

\* Build allies who also value focus.

\* Track and show the costs of interruptions and shifting goals.

\* Know when to walk away from cultures that wonâ€™t change.



Thoughts?",programming,0,https://www.reddit.com/r/programming/comments/1nei4ig/the_real_reasons_why_developers_burnout/,r_1nei4ig,,,
r_1nehbw7,reddit,Doniisthemaindog,2025-09-11T18:58:31+00:00,The rise of async programming,programming,0,https://www.reddit.com/r/programming/comments/1nehbw7/the_rise_of_async_programming/,r_1nehbw7,,,
r_1negekx,reddit,chintanbawa,2025-09-11T18:23:04+00:00,"React Hooks Explained Simply in 2025 [Punjabi]â€” useState, useEffect, useRef

",programming,0,https://www.reddit.com/r/programming/comments/1negekx/react_hooks_explained_simply_in_2025_punjabi/,r_1negekx,,,
r_1neg7r4,reddit,FrequentBid2476,2025-09-11T18:15:43+00:00,Domain-Driven Design with TypeScript Decorators and Reflection,programming,0,https://www.reddit.com/r/programming/comments/1neg7r4/domaindriven_design_with_typescript_decorators/,r_1neg7r4,,,
r_1neg7g2,reddit,feross,2025-09-11T18:15:24+00:00,August 2025 (version 1.104),programming,0,https://www.reddit.com/r/programming/comments/1neg7g2/august_2025_version_1104/,r_1neg7g2,,,
r_1nef0lt,reddit,ketralnis,2025-09-11T17:30:26+00:00,Memory Integrity Enforcement: A complete vision for memory safety in Apple devices,programming,33,https://www.reddit.com/r/programming/comments/1nef0lt/memory_integrity_enforcement_a_complete_vision/,r_1nef0lt,,,
r_1nef09j,reddit,imjuni,2025-09-11T17:30:03+00:00,"Managing HTTP Requests as Type-Safe TypeScript Classes
# Background: Common Pain Points

When writing HTTP requests in TypeScript projects, we often encounter these issues:

* **Scattered code**: URLs, headers, and query strings end up spread across different parts of the codebase.
* **Inconsistent styles**: Each developer writes request functions differently. Some mutate input values inside the function, others use external utilities. â†’ This leads to poor reusability and harder maintenance.
* **Operational differences**: When working with many APIs, each API may have slightly different timeout and retry policies. Hardcoding these policies into each function quickly becomes messy.
* **Readability issues**: Itâ€™s not always clear whether a given value is a path parameter, query string, or header. Different developers define them differently, and long-term maintenance of a shared codebase becomes harder.

# The Question: How to Make It More Efficient

To solve these issues, I needed **consistency** and **declarative definitions**:

* Define request structures in a declarative way so the whole team follows the same pattern.
* Specify timeout, retry, and other operational policies cleanly at the request level.
* Make it obvious at a glance whether a value belongs to the path, query, header, or body.

# What Worked for Me

The most effective approach was to define HTTP requests as **classes**, with **decorators that clearly describe structure and policies**:

* Use u/Get, u/Post, u/Param, u/Query, u/Header, u/Body to define the request.
* Attach operational policies like timeout and retry directly to the request class.
* Reading the class immediately reveals what is path/query/header/body.

After several iterations, I built a library around this approach: [jin-frame](https://github.com/imjuni/jin-frame).

jin-frame lets you design HTTP requests as TypeScript classes, similar to how ORMs like TypeORM or MikroORM let you design entities.

    import { Get, Param, Query, JinFrame } from 'jin-frame';
    import { randomUUID } from 'node:crypto';
    
    u/Get({ 
      host: 'https://pokeapi.co',
      path: '/api/v2/pokemon/:name',
    })
    export class PokemonFrame extends JinFrame {
      @Param()
      declare public readonly name: string;
    
      @Query()
      declare public readonly tid: string;
    }
    
    (async () => {
      const frame = PokemonFrame.of({ 
        name: 'pikachu', 
        tid: randomUUID(),
      });
      const reply = await frame.execute();
      
      // Show Pikachu Data
      console.log(reply.data);
    })();

* @Param() maps a value into the path (:name).
* @Query() maps a value into the querystring (?tid=...).
* Calling execute() performs the request and returns the JSON response.



# Closing Thoughts (Revised)

Iâ€™ve been using this library personally for quite a while, and it has proven to be genuinely useful in my own projects. Thatâ€™s why I decided to share [jin-frame](https://github.com/imjuni/jin-frame) with other developers â€” not just as a finished tool, but as something that can continue to improve.

If you give it a try and share your feedback, it would be a great opportunity to make this library even better. I hope jin-frame can be helpful in your projects too, and Iâ€™d love to hear how it works for you.",programming,0,https://www.reddit.com/r/programming/comments/1nef09j/managing_http_requests_as_typesafe_typescript/,r_1nef09j,,,
r_1neezti,reddit,ketralnis,2025-09-11T17:29:35+00:00,The bloat of edge-case first libraries,programming,215,https://www.reddit.com/r/programming/comments/1neezti/the_bloat_of_edgecase_first_libraries/,r_1neezti,,,
r_1neezq6,reddit,ketralnis,2025-09-11T17:29:29+00:00,A new experimental Go API for JSON,programming,12,https://www.reddit.com/r/programming/comments/1neezq6/a_new_experimental_go_api_for_json/,r_1neezq6,,,
r_1neeyu5,reddit,ketralnis,2025-09-11T17:28:31+00:00,Effects as Capabilities in Scala,programming,3,https://www.reddit.com/r/programming/comments/1neeyu5/effects_as_capabilities_in_scala/,r_1neeyu5,,,
r_1neeyr2,reddit,ketralnis,2025-09-11T17:28:26+00:00,What's new in Kotlin 2.2.20,programming,8,https://www.reddit.com/r/programming/comments/1neeyr2/whats_new_in_kotlin_2220/,r_1neeyr2,,,
r_1neeyfz,reddit,ketralnis,2025-09-11T17:28:06+00:00,Scaling asyncio on Free-Threaded Python,programming,1,https://www.reddit.com/r/programming/comments/1neeyfz/scaling_asyncio_on_freethreaded_python/,r_1neeyfz,,,
r_1neey75,reddit,ketralnis,2025-09-11T17:27:49+00:00,SlateDB: An embedded database built on object storage,programming,0,https://www.reddit.com/r/programming/comments/1neey75/slatedb_an_embedded_database_built_on_object/,r_1neey75,,,
r_1neexlm,reddit,ketralnis,2025-09-11T17:27:11+00:00,From Unit Tests to Whole Universe Tests (with Will Wilson),programming,14,https://www.reddit.com/r/programming/comments/1neexlm/from_unit_tests_to_whole_universe_tests_with_will/,r_1neexlm,,,
r_1neexcf,reddit,ketralnis,2025-09-11T17:26:56+00:00,Fenwick layout for interval trees,programming,4,https://www.reddit.com/r/programming/comments/1neexcf/fenwick_layout_for_interval_trees/,r_1neexcf,,,
r_1neevq8,reddit,ketralnis,2025-09-11T17:25:13+00:00,Clojure's Solutions to the Expression Problem,programming,2,https://www.reddit.com/r/programming/comments/1neevq8/clojures_solutions_to_the_expression_problem/,r_1neevq8,,,
r_1neeuy6,reddit,ketralnis,2025-09-11T17:24:25+00:00,Pure and Impure Software Engineering,programming,2,https://www.reddit.com/r/programming/comments/1neeuy6/pure_and_impure_software_engineering/,r_1neeuy6,,,
r_1neeuf0,reddit,ketralnis,2025-09-11T17:23:52+00:00,Rewriting Dataframes for MicroHaskell,programming,2,https://www.reddit.com/r/programming/comments/1neeuf0/rewriting_dataframes_for_microhaskell/,r_1neeuf0,,,
r_1nees2b,reddit,ketralnis,2025-09-11T17:21:29+00:00,[RFC] Ripple: An LLVM compiler-interpreted API to support SPMD and loop annotation programming for SIMD targets,programming,2,https://www.reddit.com/r/programming/comments/1nees2b/rfc_ripple_an_llvm_compilerinterpreted_api_to/,r_1nees2b,,,
r_1neerr1,reddit,ketralnis,2025-09-11T17:21:09+00:00,A New Case for Elixir,programming,0,https://www.reddit.com/r/programming/comments/1neerr1/a_new_case_for_elixir/,r_1neerr1,,,
r_1neerjh,reddit,ketralnis,2025-09-11T17:20:57+00:00,JEP 401: Value Classes and Objects (Preview),programming,3,https://www.reddit.com/r/programming/comments/1neerjh/jep_401_value_classes_and_objects_preview/,r_1neerjh,,,
r_1neer12,reddit,ketralnis,2025-09-11T17:20:26+00:00,Program verification is not all-or-nothing,programming,2,https://www.reddit.com/r/programming/comments/1neer12/program_verification_is_not_allornothing/,r_1neer12,,,
r_1neeq77,reddit,ketralnis,2025-09-11T17:19:34+00:00,Unicode 17.0 Release Announcement,programming,22,https://www.reddit.com/r/programming/comments/1neeq77/unicode_170_release_announcement/,r_1neeq77,,,
r_1neepqt,reddit,ketralnis,2025-09-11T17:19:03+00:00,Rust compiler performance survey 2025 results,programming,13,https://www.reddit.com/r/programming/comments/1neepqt/rust_compiler_performance_survey_2025_results/,r_1neepqt,,,
r_1neepdf,reddit,ketralnis,2025-09-11T17:18:41+00:00,"Raku is an expressive, multiâ€‘paradigm, Open Source language that works the way you think",programming,0,https://www.reddit.com/r/programming/comments/1neepdf/raku_is_an_expressive_multiparadigm_open_source/,r_1neepdf,,,
r_1neeord,reddit,ketralnis,2025-09-11T17:18:05+00:00,First-class merges and cover letters,programming,1,https://www.reddit.com/r/programming/comments/1neeord/firstclass_merges_and_cover_letters/,r_1neeord,,,
r_1neen0w,reddit,ketralnis,2025-09-11T17:16:14+00:00,"C++20 Modules: Practical Insights, Status and TODOs",programming,1,https://www.reddit.com/r/programming/comments/1neen0w/c20_modules_practical_insights_status_and_todos/,r_1neen0w,,,
r_1neel5m,reddit,ketralnis,2025-09-11T17:14:18+00:00,Behind the Scenes of Bun Install,programming,2,https://www.reddit.com/r/programming/comments/1neel5m/behind_the_scenes_of_bun_install/,r_1neel5m,,,
r_1necwrf,reddit,spite,2025-09-11T16:10:41+00:00,"The Holy Grail of QA: 100% Test Coverage - A Developer's Mythical Quest
Being an SDET, I've been thinking about how 100% test coverage has become this mythical goal in software development - like some kind of Holy Grail that promises perfect code and eternal deployment peace.

The reality is:
- Nobody has ever actually achieved meaningful 100% coverage
- It's often counterproductive to even try
- Yet we still put it in our CI gates and performance reviews
- Junior devs get obsessed with it, senior devs avoid talking about it

It's fascinating how this metric has taken on almost religious significance. We treat it like an ancient artifact that will solve all our problems, when really it's just... a number.

What's your take? Is 100% test coverage a worthy goal, a dangerous distraction, or something in between? Have you ever worked on a codebase that actually achieved it in any meaningful way?

Edit: For anyone interested, I turned this concept into a satirical 'artifact documentation' treating 100% test coverage like an ancient relic - link above if you want the full mythology treatment!""",programming,0,https://www.reddit.com/r/programming/comments/1necwrf/the_holy_grail_of_qa_100_test_coverage_a/,r_1necwrf,,,
r_1necquf,reddit,goto-con,2025-09-11T16:04:08+00:00,C++ Memory Management â€¢ Patrice Roy & Kevin Carpenter,programming,2,https://www.reddit.com/r/programming/comments/1necquf/c_memory_management_patrice_roy_kevin_carpenter/,r_1necquf,,,
r_1ne96wu,reddit,younesfaid,2025-09-11T13:43:42+00:00,Web Scraping With Python,programming,0,https://www.reddit.com/r/programming/comments/1ne96wu/web_scraping_with_python/,r_1ne96wu,,,
r_1ne8q38,reddit,rgancarz,2025-09-11T13:23:51+00:00,"Impulse, Airbnbâ€™s New Framework for Context-Aware Load Testing",programming,0,https://www.reddit.com/r/programming/comments/1ne8q38/impulse_airbnbs_new_framework_for_contextaware/,r_1ne8q38,,,
r_1ne8gir,reddit,brendt_gd,2025-09-11T13:12:14+00:00,Finding a way to prioritize my programming and OSS projects to prevent burning out,programming,0,https://www.reddit.com/r/programming/comments/1ne8gir/finding_a_way_to_prioritize_my_programming_and/,r_1ne8gir,,,
r_1ne6jtd,reddit,JadeLuxe,2025-09-11T11:42:47+00:00,Hashed sorting is typically faster than hash tables1,programming,61,https://www.reddit.com/r/programming/comments/1ne6jtd/hashed_sorting_is_typically_faster_than_hash/,r_1ne6jtd,,,
r_1ne4hb2,reddit,BlueGoliath,2025-09-11T09:43:53+00:00,Eclipse 4.37 Released,programming,117,https://www.reddit.com/r/programming/comments/1ne4hb2/eclipse_437_released/,r_1ne4hb2,,,
r_1ne48oh,reddit,evilhighlord,2025-09-11T09:27:54+00:00,"API Live Sync #7: import-export
In ourÂ [previous posts](https://creative-labs.hashnode.dev/hoppscotch-api-live-sync?source=more_series_bottom_blogs), we laid the foundation for live API synchronization with sync engines,Â [setup wizards, and real-time status indicators](https://creative-labs.hashnode.dev/api-live-sync-6-sync-engine?source=more_series_bottom_blogs). In the end, we had a working system that could detect changes and update collections automatically.

But real-world development is messier than our initial implementation assumed. Teams work together, frameworks haveâ€¦uhmâ€¦peculiarities, and developers need to know what's happening when things change. Today, we're diving into the advanced features that transform our live sync system from ""functional"" to ""usable.""",programming,1,https://www.reddit.com/r/programming/comments/1ne48oh/api_live_sync_7_importexport/,r_1ne48oh,,,
r_1ne1zxy,reddit,DataBaeBee,2025-09-11T06:59:37+00:00,Pohlig-Hellman Discrete Logarithms,programming,0,https://www.reddit.com/r/programming/comments/1ne1zxy/pohlighellman_discrete_logarithms/,r_1ne1zxy,,,
r_1nduzwm,reddit,ordepdev29,2025-09-11T00:35:22+00:00,When more threads make things worse,programming,4,https://www.reddit.com/r/programming/comments/1nduzwm/when_more_threads_make_things_worse/,r_1nduzwm,,,
r_1nduidd,reddit,gregorojstersek,2025-09-11T00:11:37+00:00,The Impact of AI on Engineering Teams,programming,0,https://www.reddit.com/r/programming/comments/1nduidd/the_impact_of_ai_on_engineering_teams/,r_1nduidd,,,
r_1ndt956,reddit,mqian41,2025-09-10T23:12:45+00:00,"CXL 3.0: Redefining Zero-Copy Memory for In-Memory Databases
How CXL 3.0 replaces DMA-based zero copy with cache-coherent memory pooling for in-memory databases, featuring an experimental Redis fork that maps remote DRAM under 200 ns.",programming,5,https://www.reddit.com/r/programming/comments/1ndt956/cxl_30_redefining_zerocopy_memory_for_inmemory/,r_1ndt956,,,
r_1ndt0ms,reddit,chintanbawa,2025-09-10T23:01:59+00:00,React Props vs State à¨ªà©°à¨œà¨¾à¨¬à©€ à¨µà¨¿à©±à¨š Explained âœ… (Mini Project à¨¨à¨¾à¨²),programming,0,https://www.reddit.com/r/programming/comments/1ndt0ms/react_props_vs_state_à¨ªà¨œà¨¬_à¨µà¨š_explained_mini/,r_1ndt0ms,,,
r_1ndpzfz,reddit,Top-Figure7252,2025-09-10T20:53:02+00:00,"Microsoft Goes Back to BASIC, Open-Sources Bill Gates' Code",programming,832,https://www.reddit.com/r/programming/comments/1ndpzfz/microsoft_goes_back_to_basic_opensources_bill/,r_1ndpzfz,,,
r_1ndpv4f,reddit,Public_Being3163,2025-09-10T20:48:18+00:00,"A Rant About Multiprocessing
The simplest system architecture is a single, monolithic process. This is the gold standard of all possible architectures. Why is it a thing worthy of reverence? Because it involves a single programming language and no interprocess communication, i.e. a messaging library. Software development doesnâ€™t get more carefree than life within the safe confines of a single process.

In the age of websites and cloud computing, instances of monolithic implementations are rare. Even an HTTP server presenting queries to a database server is technically two processes and a client library. There are other factors that push system design to multiprocessing, like functional separation, physical distribution and concurrency. So realistically, the typical architecture is a multiprocessing architecture.

What is it about multiprocessing that bumps an architecture off the top of the list of places-Iâ€™d-rather-be? At the architectural level, the responsibility for starting and managing processes may be carried by a third-party such as Kubernetes - making it something of a non-issue. No, the real problems with multiprocessing start when the processes start communicating with each other.

Consider that HTTP server paired with a database server. A single call to the HTTP server involves 5 type systems and 4 encoding/decoding operations. Thatâ€™s kinda crazy. Every item of data - such as a floating-point value - exists at different times in 5 different forms, and very specific code fragments are involved in transformations between runtime variables (e.g. Javascript, Python and C++) and portable representations (e.g. JSON and protobuf).

Itâ€™s popular to refer to architectures like these as layered, or as a software stack. If a Javascript application is at the top level of a stack and a database query language is at the lowest level, then all the type capability within the different type systems, must align, i.e. floats, datetimes and user-defined types (e.g. Person) must move up and down the stack without loss of integrity. Basic types such as booleans, integers and strings are fairly well supported (averting the engineers gaze from 32-bit vs 64-bit integers and floats), but support gets rocky with types often referred to as generics, e.g. vectors/lists, arrays and maps/dicts. The chances of a map of Person objects, indexed on a UUID, passing seamlessly from Javascript application to database client library are extremely low. Custom transformations invariably take up residence in your codebase.

Due diligence on your stack involves detailed research, prototyping and unit tests. Edge cases can be nasty, such as when a 64-bit serial id is passed into a type system that only supports 32-bits. Datetime values are particularly fraught. Bugs associated with these cases can surface after months of fault-free operation. The presence of unit tests at all levels drags your development velocity down.

Next up is the style of interaction that a client has with the system, e.g. with the HTTP server. The modern software stack has evolved to handle CRUD-like requests over a database model. This is a blocking, request-response interaction and it has been incredibly effective. It is less effective at delivering services that do not fit this mold. What if your Javascript client wants to open a window that displays a stream of monitoring device events? How does your system propagate operational errors up to the appropriate administrator?

Together, HTTP and Javascript now provide a range of options in this space, such as the Push API, Server-side Events, HTTP/2 Server Push and Websockets, with possibly the latter providing the cleanest basis for universal two-way, asynchronous messaging. Sadly, that still leaves a lot of work to do - what encoding is to be used, what type system is available (e.g. the JSON encoding has no datetime) and how are multiple conversations multiplexed over the single websocket connection? Who or what are the entities engaged in these conversations, because there must be someone or something - right?

The ability to multiplex multiple conversations influences the internal architecture of your processes. Without matching sophistication in the communicating parties, a multi-lane freeway is a high-volume transport to the same old choke points. Does anyone know a good software entity framework?

There are further demands on the capabilities of the messaging facility. Processes such as the HTTP server are a point of access for external processes. Optimal support for a complex, multi-view client would have multiple entry points available providing direct access to the relevant processes. Concerns about security may force the merging of the multiple points into a single point. That point of access would need to make the necessary internal connections and provide the ongoing routing of message streams to their ultimate destinations.

Lastly, the adoption of multiple programming languages not only requires the matching linguistic skills but also breaks the homogeneous nature of your system. Consider a simple bubble diagram where each bubble is a process and each arrow represents a connection from one process to the other. The ability to add arrows anywhere assumes the availability of the same messaging system in every process, and therefore, every language.

Multiprocessing with a multiplexing communications framework can deliver the systems environment that we might subconsciously lust after. But where is that framework and what would it even look like?

Well, the link in the post takes you to the docs for my best attempt.

",programming,0,https://www.reddit.com/r/programming/comments/1ndpv4f/a_rant_about_multiprocessing/,r_1ndpv4f,,,
r_1ndnjho,reddit,No_Lock7126,2025-09-10T19:16:00+00:00,"A Git like Database
I just came across a database called DoltDB , which presented itself as an *Agent Database* at the AI Agent Builder Summit.

I looked into their documentation to understand what they mean by *git-like*. It essentially wraps the command line with a `dolt` CLI, so you can run commands like `dolt diff`, `dolt merge`, and `dolt checkout`. Thatâ€™s an interesting concept.

Iâ€™m still trying to figure out the real killer use case for this feature, but so far I havenâ€™t found any clear documentation that explains it.

    docs $ dolt sql -q ""insert into docs values (10,10)""
    Query OK, 1 row affected
    docs $ dolt diff
    diff --dolt a/docs b/docs
    --- a/docs @ 2lcu9e49ia08icjonmt3l0s7ph2cdb5s
    +++ b/docs @ vpl1rk08eccdfap89kkrff1pk3r8519j
    +-----+----+----+
    |     | pk | c1 |
    +-----+----+----+
    |  +  | 10 | 10 |
    +-----+----+----+
    docs $ dolt commit -am ""Added a row on a branch""
    commit ijrrpul05o5j0kgsk1euds9pt5n5ddh0
    Author: Tim Sehn <tim@dolthub.com>
    Date:   Mon Dec 06 15:06:39 -0800 2021
    
    Added a row on a branch
    
    docs $ dolt checkout main
    Switched to branch 'main'
    docs $ dolt sql -q ""select * from docs""
    +----+----+
    | pk | c1 |
    +----+----+
    | 1  | 1  |
    | 2  | 1  |
    +----+----+
    docs $ dolt merge check-out-new-branch
    Updating f0ga78jrh4llc0uus8h2refopp6n870m..ijrrpul05o5j0kgsk1euds9pt5n5ddh0
    Fast-forward
    docs $ dolt sql -q ""select * from docs""
    +----+----+
    | pk | c1 |
    +----+----+
    | 1  | 1  |
    | 2  | 1  |
    | 10 | 10 |
    +----+----+

  
",programming,0,https://www.reddit.com/r/programming/comments/1ndnjho/a_git_like_database/,r_1ndnjho,,,
r_1ndmzzf,reddit,Competitive-Fee-2503,2025-09-10T18:55:43+00:00,"Is this the end of hand-written Java? Building an app with AI-generated code (OpenXava + Vibe Coding)
I'm creating a YouTube course where I build a complete car insurance policy management application in Java. The twist: I'm **not writing the Java code directly**. Instead, I'm using a combination of tools:

1.  **OpenXava:** A framework that auto-generates a full UI from JPA entities (using annotations for behavior).
2.  **Vibe Coding (AI):** I use an LLM to generate the necessary Java entity code through natural language prompts. I describe the class, its fields, and logic, and the AI writes the code for me.

The entire process focuses on high-level design and refining the auto-generated results, not on writing code line by line.

I just published the third lesson, which focuses on refining the UI that OpenXava generates from the AI-written entities:
https://youtu.be/08VQg1PFQ3c

**I'm curious to get this community's opinion on this workflow:**

*   What is your take on using LLMs (like Vibe Coding) to generate boilerplate or even complex entity code instead of writing it manually?
*   Does the combination of AI-generated code + a framework that auto-generates the UI represent a viable future for enterprise application development?
*   **Does this mean the end of writing Java code directly?** Or is hand-written code simply moving to a higher level of abstraction, remaining essential for complex logic, integrations, and customization?

Looking forward to the discussion.",programming,0,https://www.reddit.com/r/programming/comments/1ndmzzf/is_this_the_end_of_handwritten_java_building_an/,r_1ndmzzf,,,
r_1ndjw6y,reddit,avinassh,2025-09-10T17:01:10+00:00,Many Hard Leetcode Problems are Easy Constraint Problems,programming,120,https://www.reddit.com/r/programming/comments/1ndjw6y/many_hard_leetcode_problems_are_easy_constraint/,r_1ndjw6y,,,
r_1ndguzf,reddit,derjanni,2025-09-10T15:11:04+00:00,Beyond Vibe Coded AI Slop: Agentic Workflows For Professionals,programming,0,https://www.reddit.com/r/programming/comments/1ndguzf/beyond_vibe_coded_ai_slop_agentic_workflows_for/,r_1ndguzf,,,
r_1ndfkxu,reddit,goto-con,2025-09-10T14:22:15+00:00,AI Assistance for Software Teams: The State of Play â€¢ Birgitta BÃ¶ckeler,programming,0,https://www.reddit.com/r/programming/comments/1ndfkxu/ai_assistance_for_software_teams_the_state_of/,r_1ndfkxu,,,
r_1ndeyjx,reddit,hmoein,2025-09-10T13:58:39+00:00,"C++ DataFrame new version (3.6.0) is out
[C++ DataFrame](https://github.com/hosseinmoein/DataFrame) new version includes a bunch of new analytical and data-wrangling routines. But the big news is a significant rework of documentations both in terms of visuals and content.

Your feedback is appreciated.

",programming,4,https://www.reddit.com/r/programming/comments/1ndeyjx/c_dataframe_new_version_360_is_out/,r_1ndeyjx,,,
r_1ndemk4,reddit,ben_a_adams,2025-09-10T13:45:08+00:00,Performance Improvements in .NET 10,programming,363,https://www.reddit.com/r/programming/comments/1ndemk4/performance_improvements_in_net_10/,r_1ndemk4,,,
r_1nda6tp,reddit,pepincho,2025-09-10T10:09:58+00:00,What Is a Modular Monolith And Why You Should Care? ðŸ”¥,programming,29,https://www.reddit.com/r/programming/comments/1nda6tp/what_is_a_modular_monolith_and_why_you_should_care/,r_1nda6tp,,,
r_1nd95t0,reddit,mrayandutta,2025-09-10T09:05:17+00:00,"Comparing Virtual Threads vs Platform Threads in Spring Boot using JMeter Load Test
I have created one video lesson on **Spring Boot Virtual Threads vs Platform Threads Performance with JMeter Load Testing .**

**Link:** [https://youtu.be/LDgriPNWCjY](https://youtu.be/LDgriPNWCjY)

Here  I checked how **Virtual Threads** actually perform compared to **Platform Threads** in a real Spring Boot app in case of **IO Based Operations** .  
For the setup , I ran **two instances of the same application**:

* First one - with Virtual Threads enabled
* Second one - Same application with the default Tomcat thread pool (Platform Threads) running on different port 

Then I used **JMeter** to hit both application with increasing load (starting around 200 users/sec, then pushing up to 1000+). I have also captured the side-by-side results ( like the graphs, throughput, response times) .

**Observations:**

* With **Platform Threads**, once Tomcat hit its  around 200 thread pool limit, response times started getting worse gradually 
* With **Virtual Threads**, the application  did scale pretty well - throughput was much higher and the average response timesremained low.
* The difference became more more distinct when I was running longer tests with heavier load.
* **One caveat:** this benefit really shows up with **I/O-heavy requests** (I even added a `Thread.sleep` to simulate work). As expected ,for CPU-heavy stuff, Virtual Threads donâ€™t give the same advantage.",programming,5,https://www.reddit.com/r/programming/comments/1nd95t0/comparing_virtual_threads_vs_platform_threads_in/,r_1nd95t0,,,
r_1nd8vob,reddit,davidalayachew,2025-09-10T08:46:19+00:00,"JEP 401: Value classes and Objects (Preview) has just been submitted!
The JDK it is coming out in is still not known. However, this is a major milestone to have crossed. Plus, a new Early Access build of Valhalla (up-to-date with the current JDK, presumably) will go live soon too. Details in the linked post.

  
And for those unfamiliar, u/brian_goetz is the person leading the Project Valhalla effort. So, comments by him in the linked post can help you separate between assumptions by your average user vs the official words from the Open JDK Team themselves. u/pron98 is another OpenJDK Team member commenting in the linked post.

",programming,61,https://www.reddit.com/r/programming/comments/1nd8vob/jep_401_value_classes_and_objects_preview_has/,r_1nd8vob,,,
r_1nd8nsi,reddit,esiy0676,2025-09-10T08:31:55+00:00,"Git Notes: git's coolest, most unlovedÂ­ feature
Did YOU know...? And if you did, what do you use it for?",programming,73,https://www.reddit.com/r/programming/comments/1nd8nsi/git_notes_gits_coolest_most_unloved_feature/,r_1nd8nsi,,,
r_1nd7hzt,reddit,Historical_Wing_9573,2025-09-10T07:13:30+00:00,Flow-Run System Design: Building an LLM Orchestration Platform,programming,0,https://www.reddit.com/r/programming/comments/1nd7hzt/flowrun_system_design_building_an_llm/,r_1nd7hzt,,,
r_1nd7bby,reddit,Voultapher,2025-09-10T07:01:35+00:00,The unreasonable effectiveness of modern sort algorithms,programming,311,https://www.reddit.com/r/programming/comments/1nd7bby/the_unreasonable_effectiveness_of_modern_sort/,r_1nd7bby,,,
r_1nd24ej,reddit,Helpful_Geologist430,2025-09-10T02:12:56+00:00,Are AI Agents just hype ? Probably?,programming,0,https://www.reddit.com/r/programming/comments/1nd24ej/are_ai_agents_just_hype_probably/,r_1nd24ej,,,
r_1ncyhoc,reddit,madinfralab,2025-09-09T23:24:20+00:00,"I tried adding a 3D game inside my social media app (React + Three.js)
Most social media apps look and feel the same â€” feeds, likes, and endless scrolling.
So I thought: what if I added a 3D game directly inside the app Iâ€™m building?

In my latest MadInfra Lab video, I show how I went from:
	â€¢	Half-finished real-time notifications ðŸš§
	â€¢	â†’ To experimenting with Three.js + React wrappers ðŸŽ®
	â€¢	â†’ To getting a simple 3D character walking around inside my app ðŸ‘¾

I even tried (and failed gloriously) to make it multiplayer with WebSockets â€” imagine Instagram mixed with Roblox. Chaos, but fun chaos.

If youâ€™re into web dev, React, or 3D experiments, youâ€™ll probably enjoy the build, struggles, and lessons I picked up along the way.

ðŸ“º Watch here: https://youtu.be/3GCWWLSGbag?si=D8PI6AcGGuY23heO

Would love to hear what other devs think â€” especially if youâ€™ve ever mixed React with 3D or gamified your own projects.",programming,0,https://www.reddit.com/r/programming/comments/1ncyhoc/i_tried_adding_a_3d_game_inside_my_social_media/,r_1ncyhoc,,,
r_1ncx9gw,reddit,_zeynel,2025-09-09T22:32:25+00:00,Beyond the Code: Lessons That Make You Senior Software Engineer,programming,123,https://www.reddit.com/r/programming/comments/1ncx9gw/beyond_the_code_lessons_that_make_you_senior/,r_1ncx9gw,,,
r_1nctrdx,reddit,MattHodge,2025-09-09T20:13:37+00:00,Quiet Influence: A Guide to Nemawashi in Engineering,programming,2,https://www.reddit.com/r/programming/comments/1nctrdx/quiet_influence_a_guide_to_nemawashi_in/,r_1nctrdx,,,
r_1ncs4uk,reddit,FrequentBid2476,2025-09-09T19:14:49+00:00,Setting Up CI/CD Pipelines for TypeScript Monorepo,programming,0,https://www.reddit.com/r/programming/comments/1ncs4uk/setting_up_cicd_pipelines_for_typescript_monorepo/,r_1ncs4uk,,,
r_1ncoxl8,reddit,avinassh,2025-09-09T17:17:39+00:00,Building a DOOM-like multiplayer shooter in pure SQL,programming,171,https://www.reddit.com/r/programming/comments/1ncoxl8/building_a_doomlike_multiplayer_shooter_in_pure/,r_1ncoxl8,,,
r_1ncnrfs,reddit,alex_cloudkitchens,2025-09-09T16:34:11+00:00,"Does the world need another distributed queue?
I saw a post here recently talking about building a distributed queue. We built our own at Cloudkitchens, it is based on an in-house built sharder and CRDB. It also features a neat solution to head-of-the-line blocking by keeping track of consumption per key, which we call the Keyed Event Queue, or KEQ. Think it is like Kafka, with pretty much unlimited number of partitions. We have been running it in production for mission-critical workloads for almost five years, so it is reasonably battle-proven. 

It makes development of event-driven systems that require a true Active-Active multiregional topology relatively easy, and I can see how it can evolve to be even more reliable and cost efficient. 

We talked internally about open-sourcing it, but as it is coupled with our internal libraries, it will require some work to get done. Do you think anyone outside will benefit/use a system like that? The team would love your feedback. ",programming,41,https://www.reddit.com/r/programming/comments/1ncnrfs/does_the_world_need_another_distributed_queue/,r_1ncnrfs,,,
r_1ncnlku,reddit,prox_sea,2025-09-09T16:27:57+00:00,"I built an interactive bloom filter visual simulator so you can understand this data structure better
The first time I read about this **probabilistic data structure** I had a hard time understanding the probabilistic part, so eventually I dove into the theory but forgot about it. The other day I was deciding about what to write on my Blog and thought: *""maybe if I make it more visual and interactive""*. 

  
Anyway, I hope you can understand the way Bloom Filters work more easily.",programming,25,https://www.reddit.com/r/programming/comments/1ncnlku/i_built_an_interactive_bloom_filter_visual/,r_1ncnlku,,,
r_1ncnhmi,reddit,OuPeaNut,2025-09-09T16:23:43+00:00,Lessons from npm's Security Failures,programming,0,https://www.reddit.com/r/programming/comments/1ncnhmi/lessons_from_npms_security_failures/,r_1ncnhmi,,,
r_1ncndp2,reddit,mmk4mmk_simplifies,2025-09-09T16:19:43+00:00,"Isnâ€™t Kubernetes enough?
Many devs ask me: â€˜Isnâ€™t Kubernetes enough?â€™

I have done the research to and have put my thoughts below and thought of sharing here for everyone's benefit and Would love your thoughts!

This 5-min visual explainerÂ [https://youtu.be/HklwECGXoHw](https://youtu.be/HklwECGXoHw)Â showing why we still need API Gateways + Istio â€” using a fun airport analogy.

Read More at:  
[https://faun.pub/how-api-gateways-and-istio-service-mesh-work-together-for-serving-microservices-hosted-on-a-k8s-8dad951d2d0c](https://faun.pub/how-api-gateways-and-istio-service-mesh-work-together-for-serving-microservices-hosted-on-a-k8s-8dad951d2d0c)

[https://medium.com/faun/why-kubernetes-alone-isnt-enough-the-case-for-api-gateways-and-service-meshes-2ee856ce53a4](https://medium.com/faun/why-kubernetes-alone-isnt-enough-the-case-for-api-gateways-and-service-meshes-2ee856ce53a4)",programming,0,https://www.reddit.com/r/programming/comments/1ncndp2/isnt_kubernetes_enough/,r_1ncndp2,,,
r_1ncmp2z,reddit,apeloverage,2025-09-09T15:54:15+00:00,Let's make a game! 324: Swapping and rearranging variables,programming,0,https://www.reddit.com/r/programming/comments/1ncmp2z/lets_make_a_game_324_swapping_and_rearranging/,r_1ncmp2z,,,
r_1ncmnj2,reddit,goto-con,2025-09-09T15:52:39+00:00,A Short Summary of the Last Decades of Data Management â€¢ Hannes MÃ¼hleisen,programming,0,https://www.reddit.com/r/programming/comments/1ncmnj2/a_short_summary_of_the_last_decades_of_data/,r_1ncmnj2,,,
r_1nclsok,reddit,Perfect-Praline3232,2025-09-09T15:20:13+00:00,A Warm Welcome to ASN.1 and DER,programming,15,https://www.reddit.com/r/programming/comments/1nclsok/a_warm_welcome_to_asn1_and_der/,r_1nclsok,,,
r_1ncl9ws,reddit,chinmay06,2025-09-09T15:00:36+00:00,"Engineering a High-Performance Go PDF Microservice
I built **GoPdfSuit**, an open-source web service for generating PDFs, and wanted to share the technical design that makes it exceptionally fast and efficient. My goal was to create a lean alternative to traditional, resource-heavy PDF solutions.

# Core Technical Design

The core of the service is built on **Go 1.23+** and the **Gin framework** for their high performance and concurrency capabilities. Unlike many other services that rely on disk-based processing, GoPdfSuit is a **high-performance in-memory PDF generator**. This approach is crucial to its speed, as it completely bypasses slow disk I/O operations, leading to ultra-fast response times of sub-millisecond to low-millisecond.

For the actual HTML-to-PDF and HTML-to-image conversions, the service leverages the power of `wkhtmltopdf` and `wkhtmltoimage`. This allows it to accurately render web pages and HTML snippets into high-quality PDFs and images. The project demonstrates how intelligently integrating and managing a powerful external tool like `wkhtmltopdf` can lead to a highly optimized and performant solution.

# Key Features and Implementation Details

* **Template-Driven System**: GoPdfSuit utilizes a **JSON-driven templating system**. This design separates data from presentation, making it simple to generate complex, dynamic PDFs by just sending a JSON payload to the REST API.
* **Flexible PDF Generation**: The service supports multi-page documents with automatic page breaks and custom page sizes, giving developers a high degree of control over the output. It also includes support for **AcroForm** and **XFDF data**, enabling the filling out of interactive forms programmatically.
* **Deployment**: It's deployed as a single, statically compiled binary, making it extremely easy to get up and running in any environment, from a local machine to a containerized cloud deployment.

I'm happy to discuss the implementation details, the challenges of orchestrating `wkhtmltopdf` in a high-concurrency environment, or the design of the in-memory processing pipeline.

* **GitHub**: [`https://github.com/chinmay-sawant/gopdfsuit`](https://github.com/chinmay-sawant/gopdfsuit)
* **Project Page**: [`https://chinmay-sawant.github.io/gopdfsuit/`](https://chinmay-sawant.github.io/gopdfsuit/)",programming,4,https://www.reddit.com/r/programming/comments/1ncl9ws/engineering_a_highperformance_go_pdf_microservice/,r_1ncl9ws,,,
r_1ncl45v,reddit,CrismarucAdrian,2025-09-09T14:54:20+00:00,"My 18-Month Journey Building a SaaS App
I spent 18 months building RekoSearch, a SaaS that lets you semantically search photos, videos, documents, and audio. A project I had initially planned to take only 3-4 months, but here we are, 18 months and 60,000 LOC later...

Building it taught me more than any desktop project could. I learned a ton about infrastructure, scalability, web development, Kubernetes and AWS, in particular.

For those more interested in the technical details, including extensive handmade Excalidraw diagrams, hereâ€™s the repository: https://github.com/Obscurely/RekoSearch-Public",programming,0,https://www.reddit.com/r/programming/comments/1ncl45v/my_18month_journey_building_a_saas_app/,r_1ncl45v,,,
r_1ncl2q0,reddit,Majestic_Wallaby7374,2025-09-09T14:52:46+00:00,Building REST APIs in Java with Spring Boot,programming,0,https://www.reddit.com/r/programming/comments/1ncl2q0/building_rest_apis_in_java_with_spring_boot/,r_1ncl2q0,,,
r_1nckcok,reddit,mehdifarsi,2025-09-09T14:25:03+00:00,"As a Rails dev, one thing you MUST know is Turbo Prefetching",programming,0,https://www.reddit.com/r/programming/comments/1nckcok/as_a_rails_dev_one_thing_you_must_know_is_turbo/,r_1nckcok,,,
r_1ncjtrp,reddit,mmaksimovic,2025-09-09T14:04:27+00:00,A clickable visual guide to the Rust type system,programming,4,https://www.reddit.com/r/programming/comments/1ncjtrp/a_clickable_visual_guide_to_the_rust_type_system/,r_1ncjtrp,,,
r_1ncjpw3,reddit,aviator_co,2025-09-09T14:00:23+00:00,"Runbooks capture context from repositories or code reviews, combine it with the team's AI prompting knowledge, and get smarter with each use.",programming,0,https://www.reddit.com/r/programming/comments/1ncjpw3/runbooks_capture_context_from_repositories_or/,r_1ncjpw3,,,
r_1ncj2as,reddit,FrequentBid2476,2025-09-09T13:33:41+00:00,Generic Constraints and Mapped Types in Large-Scale Applications,programming,0,https://www.reddit.com/r/programming/comments/1ncj2as/generic_constraints_and_mapped_types_in/,r_1ncj2as,,,
r_1ncht77,reddit,bobbymk10,2025-09-09T12:40:41+00:00,"I love UUID, I hate UUID",programming,478,https://www.reddit.com/r/programming/comments/1ncht77/i_love_uuid_i_hate_uuid/,r_1ncht77,,,
r_1ncdwpi,reddit,tiposbingo,2025-09-09T09:05:47+00:00,"Weed + Programming = Creativity Boost or Bug Factory?
I recently came across a study saying many developers use cannabis while coding. Iâ€™m curious...do you personally code under the influence, and if so, does it help or hurt your productivity?",programming,0,https://www.reddit.com/r/programming/comments/1ncdwpi/weed_programming_creativity_boost_or_bug_factory/,r_1ncdwpi,,,
r_1ncd7dh,reddit,FrequentBid2476,2025-09-09T08:17:42+00:00,From Modular to Utility-First tailwind migration,programming,0,https://www.reddit.com/r/programming/comments/1ncd7dh/from_modular_to_utilityfirst_tailwind_migration/,r_1ncd7dh,,,
r_1nccxwt,reddit,Historical_Wing_9573,2025-09-09T07:59:55+00:00,"Flow-Run System Design: Building an LLM Orchestration Platform
**Flowâ€‘run: building a productionâ€‘ready LLM orchestration service**

I wrote a deep dive into the system design of **flowâ€‘run** (openâ€‘source). Highlights:

â€¢ Tasks are atomic units (LLM calls, emails, etc.) and flows connect them as graphs; parallel execution via BFS. 

â€¢ Data model (accounts, providers, models, tasks, flows) â†’ multiâ€‘tenancy + reliable retries. 

â€¢ YAML DSL for providers/models/tasks/flows; `/v1` API with clientâ€‘generated IDs for dedupe. 

â€¢ Scaling options: horizontal nodes, DB read replicas/clustering; how to choose multiple LLM providers vs multiple accounts.

Feedback welcome from folks building orchestration layers or distributed systems: [https://vitaliihonchar.com/insights/flow-run-system-design]()",programming,0,https://www.reddit.com/r/programming/comments/1nccxwt/flowrun_system_design_building_an_llm/,r_1nccxwt,,,
r_1ncb4wu,reddit,cheerfulboy,2025-09-09T06:01:40+00:00,Incident Report for Anthropic,programming,0,https://www.reddit.com/r/programming/comments/1ncb4wu/incident_report_for_anthropic/,r_1ncb4wu,,,
r_1ncb4oe,reddit,cheerfulboy,2025-09-09T06:01:16+00:00,iPhone dumbphone,programming,0,https://www.reddit.com/r/programming/comments/1ncb4oe/iphone_dumbphone/,r_1ncb4oe,,,
r_1ncb4hj,reddit,cheerfulboy,2025-09-09T06:00:58+00:00,Signal Secure Backups,programming,0,https://www.reddit.com/r/programming/comments/1ncb4hj/signal_secure_backups/,r_1ncb4hj,,,
r_1ncan42,reddit,IntelligentHope9866,2025-09-09T05:31:39+00:00,"Can a tiny server running FastAPI/SQLite survive the hug of death?
I run tiny indie apps on a Linux box. On a good day, I get \~300 visitors. But what if I hit a lot of traffic? Could my box survive theÂ *hug of death*?

So I load tested it:

* Reads? 100 RPS with no errors.
* Writes? Fine after enabling WAL.
* Search? Brokeâ€¦ until I switched to SQLite FTS5.",programming,328,https://www.reddit.com/r/programming/comments/1ncan42/can_a_tiny_server_running_fastapisqlite_survive/,r_1ncan42,,,
r_1ncae0y,reddit,BlueGoliath,2025-09-09T05:16:20+00:00,Java 21 â®• 25: Performance and Runtime Enhancements #RoadTo25,programming,0,https://www.reddit.com/r/programming/comments/1ncae0y/java_21_25_performance_and_runtime_enhancements/,r_1ncae0y,,,
r_1nc8w0q,reddit,geoffreyhuntley,2025-09-09T03:53:08+00:00,"the ðŸ’€ cursed programming language: programming, but make it gen z",programming,0,https://www.reddit.com/r/programming/comments/1nc8w0q/the_cursed_programming_language_programming_but/,r_1nc8w0q,,,
r_1nc7z1l,reddit,External_Mushroom978,2025-09-09T03:05:32+00:00,does mid-training help language models to reason better?  -  long CoT actually degrades response quality,programming,0,https://www.reddit.com/r/programming/comments/1nc7z1l/does_midtraining_help_language_models_to_reason/,r_1nc7z1l,,,
r_1nc1djw,reddit,kirbyhood,2025-09-08T22:05:48+00:00,The Rise of Codex,programming,0,https://www.reddit.com/r/programming/comments/1nc1djw/the_rise_of_codex/,r_1nc1djw,,,
r_1nc1aog,reddit,bizzehdee,2025-09-08T22:02:40+00:00,Why UUIDs Beat Integers as Primary Keys (And Why Performance Isnt the Issue),programming,0,https://www.reddit.com/r/programming/comments/1nc1aog/why_uuids_beat_integers_as_primary_keys_and_why/,r_1nc1aog,,,
r_1nbyspd,reddit,DaveTheLoper,2025-09-08T20:24:35+00:00,"Adventures in C++ Game Architecture
It's a fairly detailed technical writeup. I hope you find it interesting.",programming,27,https://www.reddit.com/r/programming/comments/1nbyspd/adventures_in_c_game_architecture/,r_1nbyspd,,,
r_1nbx6g9,reddit,FrequentBid2476,2025-09-08T19:23:12+00:00,End-to-End Type Safety in TypeScript,programming,0,https://www.reddit.com/r/programming/comments/1nbx6g9/endtoend_type_safety_in_typescript/,r_1nbx6g9,,,
r_1nbvjjk,reddit,ketralnis,2025-09-08T18:22:09+00:00,"Resources, Laziness, and Continuation-Passing Style",programming,4,https://www.reddit.com/r/programming/comments/1nbvjjk/resources_laziness_and_continuationpassing_style/,r_1nbvjjk,,,
r_1nbvji6,reddit,ketralnis,2025-09-08T18:22:07+00:00,Emulating Rust's Result and ? in Jai with Metaprogramming,programming,1,https://www.reddit.com/r/programming/comments/1nbvji6/emulating_rusts_result_and_in_jai_with/,r_1nbvji6,,,
r_1nbvjga,reddit,ketralnis,2025-09-08T18:22:04+00:00,Hitting Peak File IO Performance with Zig,programming,0,https://www.reddit.com/r/programming/comments/1nbvjga/hitting_peak_file_io_performance_with_zig/,r_1nbvjga,,,
r_1nbvjeq,reddit,ketralnis,2025-09-08T18:22:01+00:00,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",programming,1,https://www.reddit.com/r/programming/comments/1nbvjeq/geometric_deep_learning_grids_groups_graphs/,r_1nbvjeq,,,
r_1nbviq4,reddit,ketralnis,2025-09-08T18:21:20+00:00,Algebraic Effects in Practice with Flix,programming,9,https://www.reddit.com/r/programming/comments/1nbviq4/algebraic_effects_in_practice_with_flix/,r_1nbviq4,,,
r_1nbvifp,reddit,ketralnis,2025-09-08T18:21:02+00:00,The Expression Problem and its solutions,programming,6,https://www.reddit.com/r/programming/comments/1nbvifp/the_expression_problem_and_its_solutions/,r_1nbvifp,,,
r_1nbvie3,reddit,ketralnis,2025-09-08T18:20:59+00:00,The â€œimpossibly smallâ€ Microdot web framework,programming,7,https://www.reddit.com/r/programming/comments/1nbvie3/the_impossibly_small_microdot_web_framework/,r_1nbvie3,,,
r_1nbvi5u,reddit,ketralnis,2025-09-08T18:20:46+00:00,Keeping secrets out of logs,programming,28,https://www.reddit.com/r/programming/comments/1nbvi5u/keeping_secrets_out_of_logs/,r_1nbvi5u,,,
r_1nbvhnx,reddit,ketralnis,2025-09-08T18:20:19+00:00,Forty-Four Esolangs: The Art of Esoteric Code,programming,3,https://www.reddit.com/r/programming/comments/1nbvhnx/fortyfour_esolangs_the_art_of_esoteric_code/,r_1nbvhnx,,,
r_1nbvh5t,reddit,ketralnis,2025-09-08T18:19:51+00:00,No Silver Bullet: Essence and Accidents of Software Engineering (1986) [pdf],programming,6,https://www.reddit.com/r/programming/comments/1nbvh5t/no_silver_bullet_essence_and_accidents_of/,r_1nbvh5t,,,
r_1nbvguz,reddit,ketralnis,2025-09-08T18:19:32+00:00,Exploring Grid-Aware Websites,programming,0,https://www.reddit.com/r/programming/comments/1nbvguz/exploring_gridaware_websites/,r_1nbvguz,,,
r_1nbvfpq,reddit,ketralnis,2025-09-08T18:18:23+00:00,Analog optical computer for AI inference and combinatorial optimization,programming,1,https://www.reddit.com/r/programming/comments/1nbvfpq/analog_optical_computer_for_ai_inference_and/,r_1nbvfpq,,,
r_1nbvfcs,reddit,ketralnis,2025-09-08T18:18:00+00:00,Writing Code Is Easy. Reading It Isn't,programming,263,https://www.reddit.com/r/programming/comments/1nbvfcs/writing_code_is_easy_reading_it_isnt/,r_1nbvfcs,,,
r_1nbvest,reddit,ketralnis,2025-09-08T18:17:25+00:00,Everything is a []u8,programming,49,https://www.reddit.com/r/programming/comments/1nbvest/everything_is_a_u8/,r_1nbvest,,,
r_1nbveey,reddit,BenjaminSkyy,2025-09-08T18:17:01+00:00,Prompts are Operating Systems,programming,0,https://www.reddit.com/r/programming/comments/1nbveey/prompts_are_operating_systems/,r_1nbveey,,,
r_1nbve61,reddit,ketralnis,2025-09-08T18:16:47+00:00,Adventures in porting a Wayland Compositor to NetBSD and OpenBSD,programming,7,https://www.reddit.com/r/programming/comments/1nbve61/adventures_in_porting_a_wayland_compositor_to/,r_1nbve61,,,
r_1nbvdqr,reddit,ketralnis,2025-09-08T18:16:21+00:00,UNIX: A History and a Memoir by Brian Kernighan,programming,7,https://www.reddit.com/r/programming/comments/1nbvdqr/unix_a_history_and_a_memoir_by_brian_kernighan/,r_1nbvdqr,,,
r_1nbvcsu,reddit,ketralnis,2025-09-08T18:15:26+00:00,Deliberate Abstraction,programming,2,https://www.reddit.com/r/programming/comments/1nbvcsu/deliberate_abstraction/,r_1nbvcsu,,,
r_1nbvcjg,reddit,ketralnis,2025-09-08T18:15:11+00:00,Strong Eventual Consistency - The Big Idea behind CRDTs,programming,0,https://www.reddit.com/r/programming/comments/1nbvcjg/strong_eventual_consistency_the_big_idea_behind/,r_1nbvcjg,,,
r_1nbvchv,reddit,ketralnis,2025-09-08T18:15:09+00:00,Hashed sorting is typically faster than hash tables,programming,10,https://www.reddit.com/r/programming/comments/1nbvchv/hashed_sorting_is_typically_faster_than_hash/,r_1nbvchv,,,
r_1nbvcfz,reddit,ketralnis,2025-09-08T18:15:07+00:00,'Make invalid states unrepresentable' considered harmful,programming,0,https://www.reddit.com/r/programming/comments/1nbvcfz/make_invalid_states_unrepresentable_considered/,r_1nbvcfz,,,
r_1nbvb12,reddit,ketralnis,2025-09-08T18:13:38+00:00,The Key Points of Working Effectively with Legacy Code,programming,1,https://www.reddit.com/r/programming/comments/1nbvb12/the_key_points_of_working_effectively_with_legacy/,r_1nbvb12,,,
r_1nbvabp,reddit,ketralnis,2025-09-08T18:12:53+00:00,A complete map of the Rust type system,programming,17,https://www.reddit.com/r/programming/comments/1nbvabp/a_complete_map_of_the_rust_type_system/,r_1nbvabp,,,
r_1nbv9xc,reddit,Doniisthemaindog,2025-09-08T18:12:28+00:00,Firefox 32-bit Linux Support to End in 2026,programming,117,https://www.reddit.com/r/programming/comments/1nbv9xc/firefox_32bit_linux_support_to_end_in_2026/,r_1nbv9xc,,,
r_1nbv9w3,reddit,ketralnis,2025-09-08T18:12:26+00:00,Color NPM Package Compromised,programming,66,https://www.reddit.com/r/programming/comments/1nbv9w3/color_npm_package_compromised/,r_1nbv9w3,,,
r_1nbrwtw,reddit,BrewedDoritos,2025-09-08T16:09:32+00:00,Cache - Web APIs,programming,0,https://www.reddit.com/r/programming/comments/1nbrwtw/cache_web_apis/,r_1nbrwtw,,,
r_1nbqt4d,reddit,Advocatemack,2025-09-08T15:28:16+00:00,"Largest NPM Compromise in History - Supply Chain Attack
Hey Everyone

We just discovered that around 1 hour ago packages with a total of 2 billion weekly downloads on npm were compromised all belonging to one developerÂ [https://www.npmjs.com/\~qix](https://www.npmjs.com/~qix)

ansi-styles (371.41m downloads per week)  
debug (357.6m downloads per week)  
backslash (0.26m downloads per week)  
chalk-template (3.9m downloads per week)  
supports-hyperlinks (19.2m downloads per week)  
has-ansi (12.1m downloads per week)  
simple-swizzle (26.26m downloads per week)  
color-string (27.48m downloads per week)  
error-ex (47.17m downloads per week)  
color-name (191.71m downloads per week)  
is-arrayish (73.8m downloads per week)  
slice-ansi (59.8m downloads per week)  
color-convert (193.5m downloads per week)  
wrap-ansi (197.99m downloads per week)  
ansi-regex (243.64m downloads per week)  
supports-color (287.1m downloads per week)  
strip-ansi (261.17m downloads per week)  
chalk (299.99m downloads per week)

The compromises all stem from a core developers NPM account getting taken over from a phishing campaign

The malware itself, luckily, looks like its mostly intrested in crypto at the moment so its impact is smaller than if they had installed a backdoor for example.

# How the Malware Works (Step by Step)

1. **Injects itself into the browser**
   * Hooks core functions likeÂ `fetch`,Â `XMLHttpRequest`, and wallet APIs (`window.ethereum`, Solana, etc.).
   * Ensures it can intercept both web traffic and wallet activity.
2. **Watches for sensitive data**
   * Scans network responses and transaction payloads for anything that looks like a wallet address or transfer.
   * Recognizes multiple formats across Ethereum, Bitcoin, Solana, Tron, Litecoin, and Bitcoin Cash.
3. **Rewrites the targets**
   * Replaces the legitimate destination with an attacker-controlled address.
   * Uses â€œlookalikeâ€ addresses (via string-matching) to make swaps less obvious.
4. **Hijacks transactions before theyâ€™re signed**
   * Alters Ethereum and Solana transaction parameters (e.g., recipients, approvals, allowances).
   * Even if the UI looks correct, the signed transaction routes funds to the attacker.
5. **Stays stealthy**
   * If a crypto wallet is detected, it avoids obvious swaps in the UI to reduce suspicion.
   * Keeps silent hooks running in the background to capture and alter real transactions

Our blog is being dynamically updated -Â [https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised](https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised)",programming,1431,https://www.reddit.com/r/programming/comments/1nbqt4d/largest_npm_compromise_in_history_supply_chain/,r_1nbqt4d,,,
r_1nbqn93,reddit,Majestic_Wallaby7374,2025-09-08T15:21:59+00:00,Power Your AI Application with MongoDB Vector Search,programming,0,https://www.reddit.com/r/programming/comments/1nbqn93/power_your_ai_application_with_mongodb_vector/,r_1nbqn93,,,
r_1nbq3yq,reddit,mstrbeton,2025-09-08T15:01:48+00:00,Facade Pattern in Java,programming,0,https://www.reddit.com/r/programming/comments/1nbq3yq/facade_pattern_in_java/,r_1nbq3yq,,,
r_1nboaet,reddit,goto-con,2025-09-08T13:51:01+00:00,The End of Engineering's Blank Check: Accountability in Software Leadership â€¢ Laura Tacho & Charles Humble,programming,0,https://www.reddit.com/r/programming/comments/1nboaet/the_end_of_engineerings_blank_check/,r_1nboaet,,,
r_1nbma1u,reddit,GeneralZiltoid,2025-09-08T12:23:39+00:00,Teams Outlast Projects,programming,9,https://www.reddit.com/r/programming/comments/1nbma1u/teams_outlast_projects/,r_1nbma1u,,,
r_1nbm8kt,reddit,MysteriousEye8494,2025-09-08T12:21:49+00:00,Handling Large File Uploads in Node.js Without Crashing Your Server,programming,0,https://www.reddit.com/r/programming/comments/1nbm8kt/handling_large_file_uploads_in_nodejs_without/,r_1nbm8kt,,,
r_1nblrjg,reddit,jimmyff,2025-09-08T11:59:35+00:00,"Beyond package management: How Nix refactored my digital life
Hey, author here, the blog post takes you through my journey with nix so far: reviving an old Pixelbook with NixOS, wrangling my MacBook with nix-darwin, and super-charging Nix with AI to solve a problem I thought was unsolvable.

Happy to answer any questions!",programming,6,https://www.reddit.com/r/programming/comments/1nblrjg/beyond_package_management_how_nix_refactored_my/,r_1nblrjg,,,
r_1nbl3vj,reddit,Adventurous-Salt8514,2025-09-08T11:25:34+00:00,Just use SQL they say... Or how accidental complexity piles on,programming,0,https://www.reddit.com/r/programming/comments/1nbl3vj/just_use_sql_they_say_or_how_accidental/,r_1nbl3vj,,,
r_1nbl1dm,reddit,cekrem,2025-09-08T11:21:50+00:00,"The Subtle Art of Taming Flows and Coroutines in Kotlin, or 'How Not to DDoS Yourself with Server-Sent Events'",programming,13,https://www.reddit.com/r/programming/comments/1nbl1dm/the_subtle_art_of_taming_flows_and_coroutines_in/,r_1nbl1dm,,,
r_1nbkwzt,reddit,gingerbill,2025-09-08T11:15:20+00:00,Package Managers are Evil,programming,0,https://www.reddit.com/r/programming/comments/1nbkwzt/package_managers_are_evil/,r_1nbkwzt,,,
r_1nbhxjj,reddit,PeterBrobby,2025-09-08T08:10:45+00:00,Sphere and Ray Collision Detection Tutorial,programming,5,https://www.reddit.com/r/programming/comments/1nbhxjj/sphere_and_ray_collision_detection_tutorial/,r_1nbhxjj,,,
r_1nbh1pe,reddit,erdsingh24,2025-09-08T07:12:27+00:00,"SOLID Principles Unseen Questions with Answers Explained: Intermediate to Expert-Level
TheÂ **SOLID**Â principles are the cornerstone of object-oriented design. They provide a set of guidelines that help developers write code that is more maintainable, scalable, and reusable. While most developers can name the five principles, truly understanding and applying them in complex scenarios is the mark of an expert. Undoubtedly, theory is essential, putting that knowledge to the test is the best way to prepare.

This article presents advanced-level [Multiple-Choice Questions (MCQs) with answers explained](https://javatechonline.com/solid-principles-interview-questions-and-answers/) designed for those who want to go beyond the basics.Â ",programming,0,https://www.reddit.com/r/programming/comments/1nbh1pe/solid_principles_unseen_questions_with_answers/,r_1nbh1pe,,,
r_1nbg6u1,reddit,self,2025-09-08T06:19:17+00:00,How I solved a distributed queue problem after 15 years,programming,170,https://www.reddit.com/r/programming/comments/1nbg6u1/how_i_solved_a_distributed_queue_problem_after_15/,r_1nbg6u1,,,
r_1nbfyyz,reddit,delvin0,2025-09-08T06:06:03+00:00,Goodbye Generative AI,programming,0,https://www.reddit.com/r/programming/comments/1nbfyyz/goodbye_generative_ai/,r_1nbfyyz,,,
r_1nbfcjl,reddit,External_Mushroom978,2025-09-08T05:28:19+00:00,"how did i optimized go-torch to run 115x times faster - a short blog
after this blog, i optimized the library by allocating intermediate buffers during the backward pass and SGC. I'll explain it in the next blog. ",programming,0,https://www.reddit.com/r/programming/comments/1nbfcjl/how_did_i_optimized_gotorch_to_run_115x_times/,r_1nbfcjl,,,
r_1nbch63,reddit,Fearless-Role-2707,2025-09-08T02:52:14+00:00,"[Open Source] LLM Agents & Ecosystem Handbook â€” 60+ agent skeletons + tutorials for devs who want to build with LLMs
Hey everyone,

Iâ€™ve been working on an open-source project called **LLM Agents & Ecosystem Handbook**, aimed at developers who want to explore the practical side of building with large language models.

Why it might interest programmers (even if youâ€™re not deep into ML):
- ðŸ›  60+ agent skeletons (each with its own README + main.py) to show design patterns (scraping, analysis, scheduling, translation, RAG, MCP integrations, voice, gamesâ€¦)  
- ðŸ“š Tutorials on RAG, memory, fine-tuning, and building chat agents over custom data (like PDFs or APIs)  
- âš™ Framework comparison: what to use when (LangChain, AutoGen, CrewAI, Smolagents, etc.)  
- ðŸ›  Tools & infra: evaluation frameworks, local inference options (Ollama, llama.cpp), LLMOps practices  
- âš¡ Agent generator script to scaffold new projects fast

The idea is to provide a â€œhandbookâ€ â€” part educational, part practical â€” so devs can go from *â€œI want to try LLMsâ€* to building working prototypes and production-ready agents.

Repo link: https://github.com/oxbshw/LLM-Agents-Ecosystem-Handbook

Would love to hear feedback from the programming community â€” especially around design patterns and best practices for structuring these agents.
",programming,0,https://www.reddit.com/r/programming/comments/1nbch63/open_source_llm_agents_ecosystem_handbook_60/,r_1nbch63,,,
r_1nb7fbd,reddit,FilipeJohansson,2025-09-07T22:53:13+00:00,"Introducing GoSocket â€“ A Simple WebSocket Framework for Go
Hi Go community,

Iâ€™m excited to share **GoSocket**, a lightweight WebSocket library for Go that aims to make setting up WebSocket servers fast.

Setting up a WebSocket server in Go often requires writing a lot of boilerplate: handling connections, managing clients, broadcasting messages, dealing with rooms, and supporting different message formats. **GoSocket** abstracts all of that so you can get a working server running in just a few lines of code.

# Features

* **Quick setup:** 5â€“10 lines of code to get a server running
* **Multiple encoding support:** JSON (ready), Protobuf & MessagePack (planned), or raw binary
* **Rooms & broadcasting:** Join/leave rooms and broadcast messages easily
* **Middleware support:** Authentication, logging, CORS, etc.
* **Graceful shutdown:** Clean connection handling
* **Multiple servers:** Run chat, notifications, and admin panels on different ports simultaneously

# Quick Example

    ws := gosocket.NewServer()
    
    ws.WithPort(8080).
        WithPath(""/ws"").
        OnConnect(func(client *gosocket.Client, ctx *gosocket.HandlerContext) error {
            fmt.Printf(""Client connected: %s\n"", client.ID)
            return nil
        }).
        OnMessage(func(client *gosocket.Client, message *gosocket.Message, ctx *gosocket.HandlerContext) error {
            fmt.Printf(""Received: %s\n"", string(message.RawData))
            // Echo back
            client.Send(message.RawData)
            return nil
        }).
        OnDisconnect(func(client *gosocket.Client, ctx *gosocket.HandlerContext) error {
            fmt.Printf(""Client disconnected: %s\n"", client.ID)
            return nil
        })
    
    log.Fatal(ws.Start())

# Current Status

Weâ€™re planning to release **v1.0.0** soon, but you can start testing pre-production versions today.

# Contributing

GoSocket is actively being developed and we welcome contributions in:

* Documentation & examples
* Testing edge cases and performance scenarios
* Adding new serializers (Protobuf, MessagePack)

If youâ€™d like to contribute, check the code structure, open an issue to discuss what you want to work on, and start coding.

You can find the project on GitHub: [https://github.com/FilipeJohansson/gosocket](https://github.com/FilipeJohansson/gosocket)

Any help testing, contributing, or even giving feedback is greatly appreciated. Looking forward to seeing what the community thinks!

Thank you :)",programming,0,https://www.reddit.com/r/programming/comments/1nb7fbd/introducing_gosocket_a_simple_websocket_framework/,r_1nb7fbd,,,
r_1nb3rih,reddit,a-chacon,2025-09-07T20:22:28+00:00,I Migrated My Blog from GitHub Pages to Codeberg Pages. And This Is Just the Beginning.,programming,0,https://www.reddit.com/r/programming/comments/1nb3rih/i_migrated_my_blog_from_github_pages_to_codeberg/,r_1nb3rih,,,
r_1nb3b31,reddit,BlueGoliath,2025-09-07T20:04:39+00:00,How Your Code Really Controls Hardware,programming,64,https://www.reddit.com/r/programming/comments/1nb3b31/how_your_code_really_controls_hardware/,r_1nb3b31,,,
r_1nb32zl,reddit,Nemin32,2025-09-07T19:55:58+00:00,The DDA Algorithm,programming,18,https://www.reddit.com/r/programming/comments/1nb32zl/the_dda_algorithm/,r_1nb32zl,,,
r_1nb207l,reddit,gregorojstersek,2025-09-07T19:13:53+00:00,How to Use AI to Improve Teamwork in Engineering Teams,programming,0,https://www.reddit.com/r/programming/comments/1nb207l/how_to_use_ai_to_improve_teamwork_in_engineering/,r_1nb207l,,,
r_1nax3re,reddit,r_retrohacking_mod2,2025-09-07T16:05:18+00:00,Prey 2006 project to create open-source FPS game port by integrating its codebase with Doom 3 GPL release,programming,135,https://www.reddit.com/r/programming/comments/1nax3re/prey_2006_project_to_create_opensource_fps_game/,r_1nax3re,,,
r_1nap35c,reddit,Metalnem,2025-09-07T09:43:57+00:00,How to (actually) become an expert in .NET,programming,96,https://www.reddit.com/r/programming/comments/1nap35c/how_to_actually_become_an_expert_in_net/,r_1nap35c,,,
r_1namo4q,reddit,Arve,2025-09-07T07:08:09+00:00,The No-CPU Amiga Demo Challenge,programming,48,https://www.reddit.com/r/programming/comments/1namo4q/the_nocpu_amiga_demo_challenge/,r_1namo4q,,,
r_1nageg8,reddit,ketralnis,2025-09-07T01:21:00+00:00,X Design Notes: Unifying OCaml Modules and Values,programming,0,https://www.reddit.com/r/programming/comments/1nageg8/x_design_notes_unifying_ocaml_modules_and_values/,r_1nageg8,,,
r_1nagect,reddit,ketralnis,2025-09-07T01:20:52+00:00,Natural transformations as a basis of control,programming,5,https://www.reddit.com/r/programming/comments/1nagect/natural_transformations_as_a_basis_of_control/,r_1nagect,,,
r_1nagdzd,reddit,ketralnis,2025-09-07T01:20:24+00:00,Bringing restartable sequences out of the niche,programming,24,https://www.reddit.com/r/programming/comments/1nagdzd/bringing_restartable_sequences_out_of_the_niche/,r_1nagdzd,,,
r_1nagdms,reddit,ketralnis,2025-09-07T01:19:58+00:00,"Patterns, Predictions, and Actions â€“ A story about machine learning",programming,5,https://www.reddit.com/r/programming/comments/1nagdms/patterns_predictions_and_actions_a_story_about/,r_1nagdms,,,
r_1nagc85,reddit,ketralnis,2025-09-07T01:17:57+00:00,Developing a Space Flight Simulator in Clojure,programming,21,https://www.reddit.com/r/programming/comments/1nagc85/developing_a_space_flight_simulator_in_clojure/,r_1nagc85,,,
r_1nagbv7,reddit,ketralnis,2025-09-07T01:17:29+00:00,Debugging a dropped async Task,programming,18,https://www.reddit.com/r/programming/comments/1nagbv7/debugging_a_dropped_async_task/,r_1nagbv7,,,
r_1nagbhh,reddit,ketralnis,2025-09-07T01:16:58+00:00,Local-first access control,programming,1,https://www.reddit.com/r/programming/comments/1nagbhh/localfirst_access_control/,r_1nagbhh,,,
r_1nagb9c,reddit,ketralnis,2025-09-07T01:16:39+00:00,Odin does have undefined behavior,programming,0,https://www.reddit.com/r/programming/comments/1nagb9c/odin_does_have_undefined_behavior/,r_1nagb9c,,,
r_1nagb22,reddit,ketralnis,2025-09-07T01:16:23+00:00,Unexplanations: relational algebra is math,programming,4,https://www.reddit.com/r/programming/comments/1nagb22/unexplanations_relational_algebra_is_math/,r_1nagb22,,,
r_1nag97e,reddit,ketralnis,2025-09-07T01:13:43+00:00,C++26: Erroneous Behaviour,programming,40,https://www.reddit.com/r/programming/comments/1nag97e/c26_erroneous_behaviour/,r_1nag97e,,,
r_1nag1to,reddit,lprimak,2025-09-07T01:03:30+00:00,"StackOverflow podcast episode about Java
I was a guest on the StackOverflow podcast and talked about Java.

",programming,0,https://www.reddit.com/r/programming/comments/1nag1to/stackoverflow_podcast_episode_about_java/,r_1nag1to,,,
r_1nab1av,reddit,Outrageous-Song221,2025-09-06T21:15:09+00:00,Production-tested reliability patterns that cut downtime,programming,1,https://www.reddit.com/r/programming/comments/1nab1av/productiontested_reliability_patterns_that_cut/,r_1nab1av,,,
r_1na9zbk,reddit,Historical_Wing_9573,2025-09-06T20:31:02+00:00,Watch Me Design a Real AI Project!,programming,0,https://www.reddit.com/r/programming/comments/1na9zbk/watch_me_design_a_real_ai_project/,r_1na9zbk,,,
r_1na87fy,reddit,gamunu,2025-09-06T19:18:57+00:00,"When Does Framework Sophistication Becomes a Liability?
How a 72-hour debugging nightmare revealed the fundamental flaw in dependency injection frameworks and why strict typing matters more than sophisticated abstractions",programming,44,https://www.reddit.com/r/programming/comments/1na87fy/when_does_framework_sophistication_becomes_a/,r_1na87fy,,,
r_1na83u0,reddit,drudoca,2025-09-06T19:14:57+00:00,Under the Hood of Fuzzy Search: Building a Search Engine 15 times fuzzier than Lucene,programming,3,https://www.reddit.com/r/programming/comments/1na83u0/under_the_hood_of_fuzzy_search_building_a_search/,r_1na83u0,,,
r_1na6pes,reddit,hongminhee,2025-09-06T18:19:10+00:00,Stop writing CLI validation. Parse it right the first time.,programming,141,https://www.reddit.com/r/programming/comments/1na6pes/stop_writing_cli_validation_parse_it_right_the/,r_1na6pes,,,
r_1na4p9e,reddit,Ewig_luftenglanz,2025-09-06T17:00:35+00:00,Fibers in my Coffee: Goâ€™s Concurrency in Javaâ€™s Loom,programming,0,https://www.reddit.com/r/programming/comments/1na4p9e/fibers_in_my_coffee_gos_concurrency_in_javas_loom/,r_1na4p9e,,,
r_1na2d0a,reddit,photon_lines,2025-09-06T15:26:27+00:00,An Intuitive Guide to Interface Design,programming,6,https://www.reddit.com/r/programming/comments/1na2d0a/an_intuitive_guide_to_interface_design/,r_1na2d0a,,,
r_1na1zyo,reddit,Accomplished-Win9630,2025-09-06T15:11:56+00:00,Microsoftâ€™s first-ever programming language was just open-sourced,programming,1036,https://www.reddit.com/r/programming/comments/1na1zyo/microsofts_firstever_programming_language_was/,r_1na1zyo,,,
r_1na1izi,reddit,avinassh,2025-09-06T14:52:47+00:00,Oldest recorded transaction,programming,14,https://www.reddit.com/r/programming/comments/1na1izi/oldest_recorded_transaction/,r_1na1izi,,,
r_1na192h,reddit,shift_devs,2025-09-06T14:41:23+00:00,"5 Times LLMs Help You Codeâ€¦ and 5 Times They Fail
Hi folks,

Iâ€™m Anastasia, a journalist at ShiftMag. I just published an article exploring how developers actually use AI day to day, based on Stack Overflowâ€™s survey data, dev blogs, and conference talks.

A few key takeaways: 84% of developers use AI daily â€“ mostly LLMs like GPT; GPT models still dominate, but Claude Sonnet is gaining traction (45% of pros vs. 30% of beginners); While â€œvibe codingâ€ makes headlines, 77% of developers say itâ€™s not part of their real workflow; The gap between use and trust is real: devs canâ€™t stop using AI, but they donâ€™t fully trust it either.

To dig deeper, I broke down 5 scenarios where LLMs are genuinely useful (like boilerplate, docs, regex wrangling), and 5 scenarios where they can be risky (like security-critical code or debugging subtle concurrency issues).

Iâ€™d love to hear from this community: Where do you find AI tools genuinely helpful in your workflow and have you had situations where they slowed you down, misled you, or created bigger problems later? 

Hope you like the article! ðŸ™


",programming,0,https://www.reddit.com/r/programming/comments/1na192h/5_times_llms_help_you_code_and_5_times_they_fail/,r_1na192h,,,
r_1n9z7fb,reddit,vbilopav89,2025-09-06T13:11:54+00:00,"Business Rules In Database Movement
Did you know that there was an entire movement in software development, complete with its own manifesto, thought leaders, and everything, dedicated almost exclusively to putting business logic in SQL databases?

Neither did I.

So I did some research to create a post, and it turned out to be an entire article that digs into this movement a little bit deeper.

I hope you like it. It is important to know history.",programming,103,https://www.reddit.com/r/programming/comments/1n9z7fb/business_rules_in_database_movement/,r_1n9z7fb,,,
r_1n9jz24,reddit,firexcy,2025-09-05T23:09:06+00:00,Is OOXML Artifically Complex?,programming,68,https://www.reddit.com/r/programming/comments/1n9jz24/is_ooxml_artifically_complex/,r_1n9jz24,,,
r_1n9fi8l,reddit,c1rno123,2025-09-05T20:05:07+00:00,HTML Sanitization: Avoiding The Double-Encoding Issue,programming,0,https://www.reddit.com/r/programming/comments/1n9fi8l/html_sanitization_avoiding_the_doubleencoding/,r_1n9fi8l,,,
r_1n9ep9s,reddit,skenklok,2025-09-05T19:33:07+00:00,"Market Awareness for Engineers: How to Find Funded Work

If I were coaching you, Iâ€™d tell you to stop chasing hype and start following budget. Every quarter, read your targetâ€™s earnings, label the tone red/amber/green, and watch reqs for a few weeks to see if the words match reality. Move only when youâ€™ve seen two better quarters and your target team is explicitly funded. In tight cycles, optimise for base + sign-on; when money loosens, lean into equity. And remember: market awareness multiplies, but it doesnâ€™t replace hard skillsâ€”keep your craft sharp so that when the window opens, youâ€™re undeniably ready.

",programming,0,https://www.reddit.com/r/programming/comments/1n9ep9s/market_awareness_for_engineers_how_to_find_funded/,r_1n9ep9s,,,
r_1n9djwt,reddit,glubi,2025-09-05T18:47:48+00:00,In Defense of the Mediocre Developer (are we overestimating averages?),programming,2,https://www.reddit.com/r/programming/comments/1n9djwt/in_defense_of_the_mediocre_developer_are_we/,r_1n9djwt,,,
r_1n9b2ni,reddit,UpsetJicama3717,2025-09-05T17:12:50+00:00,Unlock Faster Queries: A Guide to Composite Indexes in MySQL & PostgreSQL,programming,0,https://www.reddit.com/r/programming/comments/1n9b2ni/unlock_faster_queries_a_guide_to_composite/,r_1n9b2ni,,,
r_1n9anwi,reddit,ketralnis,2025-09-05T16:57:34+00:00,Wolves in the Repository: A Software Engineering Analysis of the XZ Utils Supply Chain Attack [pdf],programming,4,https://www.reddit.com/r/programming/comments/1n9anwi/wolves_in_the_repository_a_software_engineering/,r_1n9anwi,,,
r_1n9anca,reddit,ketralnis,2025-09-05T16:57:00+00:00,Writing Code [video],programming,0,https://www.reddit.com/r/programming/comments/1n9anca/writing_code_video/,r_1n9anca,,,
r_1n9amzd,reddit,ketralnis,2025-09-05T16:56:36+00:00,The state of `fq_codel` and `sch_cake` worldwide [2022],programming,1,https://www.reddit.com/r/programming/comments/1n9amzd/the_state_of_fq_codel_and_sch_cake_worldwide_2022/,r_1n9amzd,,,
r_1n9amot,reddit,ketralnis,2025-09-05T16:56:20+00:00,Ray Tracing in One Weekend,programming,19,https://www.reddit.com/r/programming/comments/1n9amot/ray_tracing_in_one_weekend/,r_1n9amot,,,
r_1n9amad,reddit,ketralnis,2025-09-05T16:55:56+00:00,How to Build a High-Performance UR5 Inverse Kinematics Solver with IK-Geo,programming,0,https://www.reddit.com/r/programming/comments/1n9amad/how_to_build_a_highperformance_ur5_inverse/,r_1n9amad,,,
r_1n9alqx,reddit,ketralnis,2025-09-05T16:55:22+00:00,IRHash: Efficient Multi-Language Compiler Caching by IR-Level Hashing,programming,10,https://www.reddit.com/r/programming/comments/1n9alqx/irhash_efficient_multilanguage_compiler_caching/,r_1n9alqx,,,
r_1n9alpm,reddit,ketralnis,2025-09-05T16:55:20+00:00,Type-safe and user-friendly error handling in Swift 6,programming,0,https://www.reddit.com/r/programming/comments/1n9alpm/typesafe_and_userfriendly_error_handling_in_swift/,r_1n9alpm,,,
r_1n9akz7,reddit,ketralnis,2025-09-05T16:54:33+00:00,Heap-based buffer overflow in Kernel Streaming,programming,0,https://www.reddit.com/r/programming/comments/1n9akz7/heapbased_buffer_overflow_in_kernel_streaming/,r_1n9akz7,,,
r_1n9akr0,reddit,ketralnis,2025-09-05T16:54:19+00:00,Forking Chrome to render in a terminal,programming,24,https://www.reddit.com/r/programming/comments/1n9akr0/forking_chrome_to_render_in_a_terminal/,r_1n9akr0,,,
r_1n9ak7g,reddit,-WhiteMouse-,2025-09-05T16:53:43+00:00,"I just want to know if there are more people thinking that SOLID is overrated and sometimes add unnecessary complexity
I think SOLID it could be good, however try to follows strictly SOLID principles can easily become a problem. I have been working in software industry for around 15 years. I remember one time when I had to debug old code that abuse so much about using inheritance/interfaces. There was around 8 levels of inheritance/interfaces, all clases are almos empty with only skeleton just to support next class, at the end the source file that made the magic was only a simple division, something like 

double myVal=a/b;

I'm pretty sure that was donde because original team did it just to ""prepare"" code for the future, but the truth is that only brings more problem that solutions",programming,112,https://www.reddit.com/r/programming/comments/1n9ak7g/i_just_want_to_know_if_there_are_more_people/,r_1n9ak7g,,,
r_1n9ajwq,reddit,ketralnis,2025-09-05T16:53:25+00:00,Evolving the OCaml Programming Language,programming,15,https://www.reddit.com/r/programming/comments/1n9ajwq/evolving_the_ocaml_programming_language/,r_1n9ajwq,,,
r_1n9aj2w,reddit,ketralnis,2025-09-05T16:52:34+00:00,Dealing with cancel safety in async Rust,programming,5,https://www.reddit.com/r/programming/comments/1n9aj2w/dealing_with_cancel_safety_in_async_rust/,r_1n9aj2w,,,
r_1n9ahja,reddit,ketralnis,2025-09-05T16:50:55+00:00,io_uring is faster than mmap,programming,87,https://www.reddit.com/r/programming/comments/1n9ahja/io_uring_is_faster_than_mmap/,r_1n9ahja,,,
r_1n9afot,reddit,ketralnis,2025-09-05T16:48:59+00:00,Fil's Unbelievable Garbage Collector,programming,12,https://www.reddit.com/r/programming/comments/1n9afot/fils_unbelievable_garbage_collector/,r_1n9afot,,,
r_1n9afie,reddit,ketralnis,2025-09-05T16:48:47+00:00,Data Modeling Guide for Real-Time Analytics with ClickHouse,programming,1,https://www.reddit.com/r/programming/comments/1n9afie/data_modeling_guide_for_realtime_analytics_with/,r_1n9afie,,,
r_1n9af5c,reddit,ketralnis,2025-09-05T16:48:24+00:00,Protobuffers Are Wrong,programming,160,https://www.reddit.com/r/programming/comments/1n9af5c/protobuffers_are_wrong/,r_1n9af5c,,,
r_1n97mbo,reddit,ChiliPepperHott,2025-09-05T15:00:48+00:00,I Ditched Docker for Podman,programming,201,https://www.reddit.com/r/programming/comments/1n97mbo/i_ditched_docker_for_podman/,r_1n97mbo,,,
r_1n97ltf,reddit,Oakchris1955,2025-09-05T15:00:19+00:00,Cryptography can't be stopped,programming,8,https://www.reddit.com/r/programming/comments/1n97ltf/cryptography_cant_be_stopped/,r_1n97ltf,,,
r_1n96ftn,reddit,GarethX,2025-09-05T14:15:00+00:00,"40 years later, are Bentley's ""Programming Pearls"" still relevant?",programming,90,https://www.reddit.com/r/programming/comments/1n96ftn/40_years_later_are_bentleys_programming_pearls/,r_1n96ftn,,,
r_1n95gwo,reddit,Russ-Danner,2025-09-05T13:35:53+00:00,"Building AI Agents to Play the Famous Game: Leisure Suit Larry (EPISODE 3)
AI Agents vs Classic Adventure Games: Can Grok, GPT, Gemini & Claude Beat Leisure Suit Larry? (Part 3)  
  
Watch as cutting-edge AI agents tackle the legendary Sierra adventure game Leisure Suit Larry! This comprehensive series explores how modern LLMs handle complex reasoning, humor, and puzzle-solving in classic gaming environments.  
  
  
\# Series Videos  
Playlist: [https://www.youtube.com/watch?v=mj85wM-smZY&list=PLD8Ssd0PFra6bckBlIMh67qrkiX3ehQIi](https://www.youtube.com/watch?v=mj85wM-smZY&list=PLD8Ssd0PFra6bckBlIMh67qrkiX3ehQIi)  
Part 1: Overview/Demo [https://www.youtube.com/watch?v=e42I2bP0F6g](https://www.youtube.com/watch?v=e42I2bP0F6g)  
Part 2: Internals [https://youtu.be/oxa1z\_zKQ0Q](https://youtu.be/oxa1z_zKQ0Q)  
Part 2a: Background info [https://youtu.be/mj85wM-smZY](https://youtu.be/mj85wM-smZY)  
Part 3: Challenges to overcome [https://youtu.be/IfwiMmFOvQ4](https://youtu.be/IfwiMmFOvQ4)  
  
\# ðŸ¤– What You'll Learn  
\- Common challenges of sophisticated agents  
\- A possible solution for object identification  
  
\# â° Timestamps  
0:19 Challenges  
4:00 Overcoming AI vision limitations and making better bounding boxes  
  
\# ðŸ”§ Technical Stack  
AGI Engine: Adventure Game Interpreter  
Backend: CrafterCMS + Spring AI Framework  
AI Models: Grok + Grok Vision (xAI)  
Protocols: Model Context Protocol (MCP)  
  
\# ðŸ“š Related AI Research  
Rich Sutton: OaK Architecture & SuperIntelligence - [https://www.youtube.com/watch?v=gEbbGyNkR2U](https://www.youtube.com/watch?v=gEbbGyNkR2U)  
John Carmack: Keen Technologies Research - [https://www.youtube.com/watch?v=iz9lUMSQBfY](https://www.youtube.com/watch?v=iz9lUMSQBfY)  
Rich Sutton:  [https://www.youtube.com/watch?v=zZuh8YUBeDY](https://www.youtube.com/watch?v=zZuh8YUBeDY)  
  
\# ðŸ“š Similar Projects  
The Chaos of AI Agents [https://www.youtube.com/watch?v=2YYjPs8t8MI](https://www.youtube.com/watch?v=2YYjPs8t8MI)  
  
Licensed under Creative Commons: By Attribution 3.0  
[http://creativecommons.org](http://creativecommons.org)  
  
  
\#AIAgents #LLM #MachineLearning #ArtificialIntelligence #AgenticAI #OpenAI #GoogleAI #AnthropicClaude #GrokAI #xAI #SpringAI #ModelContextProtocol #MCP #AIResearch #GameAI #LLMEvaluation #MultiModalAI #AIBenchmarks #RetroGaming #SierraGames #LeisureSuitLarry #AITesting #MLEngineering #AIArchitecture #LangChain #AIAgentFrameworks",programming,0,https://www.reddit.com/r/programming/comments/1n95gwo/building_ai_agents_to_play_the_famous_game/,r_1n95gwo,,,
r_1n932vx,reddit,West-Chard-1474,2025-09-05T11:48:45+00:00,"Strategies for securing non-human identities (services, workloads, AI agents)",programming,27,https://www.reddit.com/r/programming/comments/1n932vx/strategies_for_securing_nonhuman_identities/,r_1n932vx,,,
r_1n92nbh,reddit,Priler96,2025-09-05T11:26:39+00:00,"Made a tutorial Python in 10 minutes for beginners (with homework)
I just uploaded a short and beginner-friendlyÂ **Python tutorial**Â on YouTube where I explain the core concepts in only 10 minutes.  
Perfect if you're just starting out or need a quick refresher.  
Would love your feedback on whether you'd like to see more quick lessons like this.

Thanks!",programming,244,https://www.reddit.com/r/programming/comments/1n92nbh/made_a_tutorial_python_in_10_minutes_for/,r_1n92nbh,,,
r_1n920j7,reddit,ionutvi,2025-09-05T10:53:29+00:00,"Building a programming language that reads like English: lessons from PlainLang
Recently I started working on an experimental language called **PlainLang**, with the idea of making programming feel closer to natural conversation. Instead of symbols and punctuation, you write in full sentences like:

    set the greeting to ""Hello World"".
    show on screen the greeting.
    

From a technical standpoint, there were a few interesting challenges i thought might be worth sharing here:

* **Parsing â€œlooseâ€ English**: Traditional parsers expect rigid grammar. PlainLang allows optional words like â€œtheâ€, â€œaâ€, or â€œthenâ€, so the parser had to be tolerant without losing structure. I ended up with a recursive descent parser tuned for flexibility, which was trickier than expected.
* **Pronoun support**: The language lets you use â€œitâ€ to refer to the last computed result. That required carrying contextual state across statements in the runtime, a design pattern that feels simple in usage but was subtle to implement correctly.
* **Error messages that feel human**: If someone writes `add 5 to score` without first setting `score`, the runtime tries to explain it in plain terms rather than spitting out a stack trace. Writing helpful diagnostics for â€œEnglish-likeâ€ code took some care.

The project is still young, but it already supports variables, arithmetic, conditionals, loops, and an interactive REPL.

Iâ€™d be interested in hearing from others who have tried making more â€œhuman-readableâ€ languages what trade-offs did you find between natural syntax and precise semantics?

The code is open source (MIT license)   
",programming,99,https://www.reddit.com/r/programming/comments/1n920j7/building_a_programming_language_that_reads_like/,r_1n920j7,,,
r_1n8ysbr,reddit,pmz,2025-09-05T07:29:45+00:00,"Teaching a Dinosaur to Jump: Rust, WebAssembly, and Neural Evolution",programming,4,https://www.reddit.com/r/programming/comments/1n8ysbr/teaching_a_dinosaur_to_jump_rust_webassembly_and/,r_1n8ysbr,,,
r_1n8yqq6,reddit,NicDevIam,2025-09-05T07:26:46+00:00,"Why ""Tutorial Hell"" Is Actually Good For You: An Exploration vs Exploitation Approach",programming,0,https://www.reddit.com/r/programming/comments/1n8yqq6/why_tutorial_hell_is_actually_good_for_you_an/,r_1n8yqq6,,,
r_1n8yose,reddit,genericlemon24,2025-09-05T07:23:11+00:00,SQL needed structure,programming,0,https://www.reddit.com/r/programming/comments/1n8yose/sql_needed_structure/,r_1n8yose,,,
r_1n8xyfi,reddit,mmaksimovic,2025-09-05T06:36:40+00:00,"Type Checking is a Symptom, Not a Solution",programming,0,https://www.reddit.com/r/programming/comments/1n8xyfi/type_checking_is_a_symptom_not_a_solution/,r_1n8xyfi,,,
r_1n8w3zo,reddit,BlueGoliath,2025-09-05T04:46:13+00:00,Setting Performance Baselines for Java's 1-Billion-Row Challenge (Ep. 2) | With â€ª@caseymuratoriâ€¬,programming,0,https://www.reddit.com/r/programming/comments/1n8w3zo/setting_performance_baselines_for_javas/,r_1n8w3zo,,,
r_1n8rvzi,reddit,ianseyler,2025-09-05T01:13:12+00:00,Minimal webserver in a 4KiB binary,programming,2,https://www.reddit.com/r/programming/comments/1n8rvzi/minimal_webserver_in_a_4kib_binary/,r_1n8rvzi,,,
r_1n8nhpq,reddit,ketralnis,2025-09-04T21:57:52+00:00,"Cassandra counter columns: Nice in theory, hazardous in practice",programming,3,https://www.reddit.com/r/programming/comments/1n8nhpq/cassandra_counter_columns_nice_in_theory/,r_1n8nhpq,,,
r_1n8nhib,reddit,ketralnis,2025-09-04T21:57:38+00:00,Integer Programming (1977),programming,8,https://www.reddit.com/r/programming/comments/1n8nhib/integer_programming_1977/,r_1n8nhib,,,
r_1n8nglr,reddit,ketralnis,2025-09-04T21:56:32+00:00,How we built an interpreter for Swift,programming,10,https://www.reddit.com/r/programming/comments/1n8nglr/how_we_built_an_interpreter_for_swift/,r_1n8nglr,,,
r_1n8ng3l,reddit,ketralnis,2025-09-04T21:55:56+00:00,"Wal3: A Write-Ahead Log for Chroma, Built on Object Storage",programming,0,https://www.reddit.com/r/programming/comments/1n8ng3l/wal3_a_writeahead_log_for_chroma_built_on_object/,r_1n8ng3l,,,
r_1n8ng01,reddit,tmetler,2025-09-04T21:55:49+00:00,"Higher-Order Transform Streams: Sequentially Injecting Streams Within Streams
[https://www.timetler.com/2025/08/23/parallel-recursive-streaming-ai-swarms](https://www.timetler.com/2025/08/23/parallel-recursive-streaming-ai-swarms)",programming,0,https://www.reddit.com/r/programming/comments/1n8ng01/higherorder_transform_streams_sequentially/,r_1n8ng01,,,
r_1n8nb8z,reddit,imachug,2025-09-04T21:50:22+00:00,"If I hear ""design pattern"" one more time, I'll go mad",programming,0,https://www.reddit.com/r/programming/comments/1n8nb8z/if_i_hear_design_pattern_one_more_time_ill_go_mad/,r_1n8nb8z,,,
r_1n8gyfd,reddit,ellnorrisjerry,2025-09-04T17:44:29+00:00,Versioned Data with Apache Iceberg Using lakeFS Iceberg REST Catalog,programming,6,https://www.reddit.com/r/programming/comments/1n8gyfd/versioned_data_with_apache_iceberg_using_lakefs/,r_1n8gyfd,,,
r_1n8gpkk,reddit,shift_devs,2025-09-04T17:35:17+00:00,"The hidden costs of saying â€œnoâ€ in software engineering
At ShiftMag we recently explored an angle of software engineering that doesnâ€™t get much attention: the cost of saying â€œnoâ€.

We often hear that being able to refuse is a vital soft skill â€“ but refusing also carries a psychological and professional price. Declining can create stress, trigger anxiety, and even feel like a career risk, especially in environments where overcommitment is the norm. 

Meanwhile, saying â€œyesâ€ is usually rewarded in the short term, even if it leads to burnout later.
This raises some questions for us as a profession:

How do you personally navigate the emotional toll of refusing requests at work?Have you seen â€œjust say noâ€ advice backfire in your teams?

What practices have you found effective for making refusal safer and healthier in professional environments?

Weâ€™d love to hear how others in the community experience and handle this dynamic.",programming,516,https://www.reddit.com/r/programming/comments/1n8gpkk/the_hidden_costs_of_saying_no_in_software/,r_1n8gpkk,,,
r_1n8gj1i,reddit,ellnorrisjerry,2025-09-04T17:28:17+00:00,Almost anything you give sustained attention to will begin to loop on itself and bloom,programming,0,https://www.reddit.com/r/programming/comments/1n8gj1i/almost_anything_you_give_sustained_attention_to/,r_1n8gj1i,,,
r_1n8gdkh,reddit,BrewedDoritos,2025-09-04T17:22:19+00:00,Writing a C compiler in 500 lines of Python,programming,3,https://www.reddit.com/r/programming/comments/1n8gdkh/writing_a_c_compiler_in_500_lines_of_python/,r_1n8gdkh,,,
r_1n8fqry,reddit,nayshins,2025-09-04T16:58:58+00:00,Are We Vibecoding Our Way to Disaster?,programming,351,https://www.reddit.com/r/programming/comments/1n8fqry/are_we_vibecoding_our_way_to_disaster/,r_1n8fqry,,,
r_1n8fcxm,reddit,ketralnis,2025-09-04T16:44:35+00:00,Zero runtime cost styles in ClojureScript,programming,1,https://www.reddit.com/r/programming/comments/1n8fcxm/zero_runtime_cost_styles_in_clojurescript/,r_1n8fcxm,,,
r_1n8fcwc,reddit,ketralnis,2025-09-04T16:44:33+00:00,Building Terminal Applications With Elixir,programming,4,https://www.reddit.com/r/programming/comments/1n8fcwc/building_terminal_applications_with_elixir/,r_1n8fcwc,,,
r_1n8faip,reddit,ketralnis,2025-09-04T16:41:56+00:00,Discovering observers in C++,programming,2,https://www.reddit.com/r/programming/comments/1n8faip/discovering_observers_in_c/,r_1n8faip,,,
r_1n8f8tw,reddit,ketralnis,2025-09-04T16:40:10+00:00,Welcoming the Rust Innovation Lab,programming,15,https://www.reddit.com/r/programming/comments/1n8f8tw/welcoming_the_rust_innovation_lab/,r_1n8f8tw,,,
r_1nfy3kx,reddit,AMDataLake,2025-09-13T13:54:39+00:00,"45% off New Book: Architecting an Apache Iceberg Lakehouse (Manning)
Use Discount Code RustConf25 for 45% off (code expires Sept 19th)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1nfy3kx/45_off_new_book_architecting_an_apache_iceberg/,r_1nfy3kx,,,
r_1nfy1xr,reddit,AMDataLake,2025-09-13T13:52:40+00:00,"45% of new book from Manning ""Architecting an Apache Iceberg Lakehouse""
Purchase Here: [https://hubs.la/Q03GfY4f0](https://hubs.la/Q03GfY4f0)  
45% Discount Code (Expires September 19th): RustConf25",bigdata,1,https://www.reddit.com/r/bigdata/comments/1nfy1xr/45_of_new_book_from_manning_architecting_an/,r_1nfy1xr,,,
r_1nfhgua,reddit,triscuit2k00,2025-09-12T22:52:09+00:00,"Best Local Ecosystem
Good day!

What I want to do:
- local setup
- Geospatial analytics, modeling and visualization 
 â€” years of census Tiger shapefiles (roads, features, tracts, pumas)
<â€”â€” integration with ACS PUMA data 
â€” Misc additional geospatial data (raster, gdb, kml)

Limitations:
- 24 CPU threads
- 128 gb ram
-16 gb vram
- 10 TB of storage on desktio 

Initial setup
- Ozone for storage 
- Iceberg for table format
<â€”- cataloged in postgres
- Apache Sedona/spark for processing 
- eventually: TorchGeo to play around with modeling 
+ (kerby for security) 

At the bare minimum, I want a solid introduction to setting up and maintaining a big data ecosystem within limitations of local devices (primordial services on workstations, nodes across misc devices - laptops) 

Questions:
- what ecosystem would you design?
- best practices/ tips/ tricks 
- feasibility of all this
- different ways to go about everything!

Notes
- ready for a challenge!
",bigdata,1,https://www.reddit.com/r/bigdata/comments/1nfhgua/best_local_ecosystem/,r_1nfhgua,,,
r_1nfdxun,reddit,Apprehensive-Bat7415,2025-09-12T20:29:13+00:00,"Market Research
Hello! - Iâ€™m doing some research and wanted to get your perspective. One of the biggest challenges I keep hearing about is how biotech and pharma companies collect real-world evidence (RWE) after FDA approval. From what Iâ€™ve seen, itâ€™s often:

1. Costly
2. Slow
3. Fragmented

Im curious - for those of you who work or have worked in pharma, biotech, or in any clinical settings:

1. Whatâ€™s been the hardest part of generating RWE for post-market therapies?
2. Is it data quality, cost, time, or something else?

Would love to learn from this group about how youâ€™re currently approaching it; even a quick comment would help a lot.

Thank you so much, and looking forward to hearing more",bigdata,1,https://www.reddit.com/r/bigdata/comments/1nfdxun/market_research/,r_1nfdxun,,,
r_1nf1q2x,reddit,sharmaniti437,2025-09-12T12:21:01+00:00,"Top 5 Cybersecurity Certifications to Enroll in 2026
The digital world is transforming fast â€” due to this, cyber threats and attacks are also advancing. Corporations, governments, and individuals rely on secure systems, but the skill gap is increasing; they are not able to hire the right talent to protect their systems.

According to the World Economic Forumâ€™s Future of Jobs Report 2025, cybersecurity will be one of the top 2 fastest-growing skills for all professions (2025-2030), as illustrated in the graph.

  
The problem is that weâ€™re still in an age where what you learn in school isnâ€™t what the industry needs. **Cybersecurity certifications** are one of the best ways to close that gap: they put your skills on display and demonstrate to employers that youâ€™re up to date.

Here are five of the [best cybersecurity certifications](https://www.uscsinstitute.org/cybersecurity-certifications) to enroll in, including official information, perks, and career paths.Â 

# Top 5 Cybersecurity Certifications to Enroll in 2026

Here are the best 5 cybersecurity certifications that are capable of upskilling you and helping you fill the skill gap to get hired faster than ever for associate, intermediate, or senior level positions:

# 1.Â Â Certified Senior Cybersecurity Specialist (CSCSâ„¢) by USCSIÂ®

The CSCSâ„¢ certification is ideal for those who strive to attain the most esteemed job titles in the cybersecurity industry. It offers an organized, comprehensive framework for developing technical and strategic competence.

â—Â Â Â **Skills taught:** Duration: It is up to you, covering the full 4-24 weeks.

â—Â Â Â F**ormat:** 100% online, self-paced, so you can study while you work.

â—Â Â Â **Qualifications:** Associate's degree or higher in a related field, depending on experience level.

â—Â Â Â **Strong Impacted Skills:** Data security, cryptography, security leadership, compliance, and advanced defensive strategies.

â—Â Â Â **Career Prospects:** Makes you ready for positions such as Senior Security Analyst, Cybersecurity Consultant, and Security Architect.

If your goal is to understand how attacks occur in the real world and how to create better defense methods, with the additional goal of leading any organizationâ€™s cybersecurity team, this certification is the right choice for you.

# 2.Â Â CompTIA Security+

The CompTIA Security+ **cybersecurity certification** is the entry-level certification for information security professionals.

â—Â Â **Length of study:** Study time differs for everybody, but most people study for 3-6 months.

â—Â Â **Exam Format:** Multiple-choice and performance-based questions on a proctored exam.

â—Â Â **Prerequisites:** No formal prerequisites, but 1â€“2 years of IT experience is suggested.

â—Â Â **Skills Learned:** Risk control, encryption, incident response, network and application security, and threat monitoring.

â—Â Â **Career Prospects:** Perfect for a Security Analyst, Network Administrator, or IT Support with a security emphasis.

# 3.Â Â Certified Ethical Hacker (CEH) â€” EC-Council

This **cybersecurity certification** will equip individuals with the tools necessary to spot the vulnerabilities and weaknesses in target systems. If you are into penetration testing and learning how hackers think, the certification can be highly beneficial. It teaches you how to think like the attacker and use both tactics to your advantage.

â—Â Â **Length:** Usual 4 â€“ 6 months preparation if studied with Official Training.

â—Â Â **Format:** Two exams â€” a multiple-choice knowledge exam and a hands-on practical test.

â—Â Â **Prerequisites:** A minimum of 2 years of experience or formal training.

â—Â Â **Key Skills Taught:** Vulnerability scanning, penetration testing, network mapping, attack mechanisms, and mitigating measures.

â—Â Â **Career Opportunities:** Provides access to positions like Ethical Hacker, Penetration Tester, and Vulnerability Analyst.Â 

# 4.Â Â Certified Information Systems Security Professional (CISSP) â€” ISC2

The ISC2 CISSP certification focuses on information security and offers a detailed foundation for aspiring security professionals. CISSP is a highly preferred **cybersecurity certification**..

â—Â Â **Length:** Preparation takes 6 months to a year, considering its depth.  
**Format:** CAT, up to 150 questions in eight domains of cybersecurity.

â—Â Â **Key Skills Covered:** Risk management, asset security, identity access management, architecture, and operations.

â—Â Â **Careers:** This program will prepare you for such roles as Security Manager, Security Architect, and Chief Information Security Officer (CISO).

CISSP isnâ€™t for novices, but is perfect for experienced professionals who want to put their careers on a fast track and move into leadership â€” or even management.

# 5.Â Offensive Security Certified Professional (OSCP) â€” OffSec

The OSCP is among the most difficult certifications in the field of cybersecurity. It is very technical and is strictly based on hands-on penetration testing **cybersecurity training**.

â—Â Â **Length:** Candidates usually spend months studying, frequently working hands-on in labs.

â—Â Â **Format:** An intensive examination

â—Â Â **Main Topics:** attack vectors, custom scripting, escalation of privileges, exploitation of vulnerabilities, and pen test reporting.

â—Â Â **Career Prospects:** Best for jobs such as Penetration Tester, Red Team Member, and Security Consultant.

These were the **best cybersecurity certifications** that employers appreciate if you have earned any of them.

# The Bottom Line

Cybersecurity is a strong growth industry. To just keep up, professionals have to stay one step ahead in their skillset and prove their expertise. The right certification will not just round out your resume but also keep you competitive as the threats you face become more sophisticated.

If youâ€™re new, you will want to start on the foundational knowledge, or looking for a cybersecurity management level intermediate certification, or dreaming of becoming a senior cybersecurity specialist, these **cybersecurity certifications** are globally the standard course you can enroll in to enhance your cybersecurity skills and knowledge.

No matter where youâ€™re beginning, the suitable certification can help put you on the road to a solid, high-demand career in cybersecurity today and tomorrow.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1nf1q2x/top_5_cybersecurity_certifications_to_enroll_in/,r_1nf1q2x,,,
r_1neueo2,reddit,bigdataengineer4life,2025-09-12T04:58:51+00:00,ChatGPT for Data Engineer (Hands-on Practice),bigdata,3,https://www.reddit.com/r/bigdata/comments/1neueo2/chatgpt_for_data_engineer_handson_practice/,r_1neueo2,,,
r_1neilxg,reddit,mr_pants99,2025-09-11T19:47:49+00:00,"100TB HBase to MongoDB database migration without downtime
Recently we've been working on adding HBase support to [dsync](https://github.com/adiom-data/dsync/). Database migration at this scale with 100+ billion of records and no-downtime requirements (real-time replication until cutover) comes with a set of unique challenges. 

Key learnings:

  \- Size matters

  \- HBase doesnâ€™t support CDC

  \- This kind of migration is not a one-and-done thing - need to iterate (a lot!)

  \- Key to success: Fast, consistent, and repeatable execution

Check out [our blog post](https://www.adiom.io/post/hbase-to-mongodb-migration) for technical details on our approach and the short [demo video](https://youtu.be/qk2CwSQ7rOU?si=beHtByo-9Zwofy_h) to see what it looks like.",bigdata,7,https://www.reddit.com/r/bigdata/comments/1neilxg/100tb_hbase_to_mongodb_database_migration_without/,r_1neilxg,,,
r_1nee5gl,reddit,zookeeper_48,2025-09-11T16:58:11+00:00,Metadata is the New Oil: Fueling the AI-Ready Data Stack,bigdata,3,https://www.reddit.com/r/bigdata/comments/1nee5gl/metadata_is_the_new_oil_fueling_the_aiready_data/,r_1nee5gl,,,
r_1ne2320,reddit,sharmaniti437,2025-09-11T07:04:54+00:00,"Boost Your Security Strategy With Data Science and Biometric
Biometric authentication is transforming security, but fingerprints, facial scans, or voice recognition arenâ€™t foolproof. Data science strengthens these systems by fusing multiple biometric traits and applying adaptive models to ensure accuracy and resilience. Learn how to implement continuous authentication with USDSIÂ® data science certifications.

https://preview.redd.it/cxpmbsvpihof1.jpg?width=1080&format=pjpg&auto=webp&s=212d32aa41b7f71500daeed0156378e5bff88341

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1ne2320/boost_your_security_strategy_with_data_science/,r_1ne2320,,,
r_1ndp6bx,reddit,SyntxaError,2025-09-10T20:20:32+00:00,Creating topics within a docker container,bigdata,1,https://www.reddit.com/r/bigdata/comments/1ndp6bx/creating_topics_within_a_docker_container/,r_1ndp6bx,,,
r_1ndge4w,reddit,Longjumping_Golf9070,2025-09-10T14:53:41+00:00,"Contract Opportunity - Senior Quantexa Developer
Hey Reddit,

  
Currently looking for those with experience in Quantexa (certificate) and Scala experience that would be open to hearing about a contract opportunity for a large bank.

Feel free to direct message me and I can give some more details and see if we can move forward.

  
Thanks!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1ndge4w/contract_opportunity_senior_quantexa_developer/,r_1ndge4w,,,
r_1ncb2nr,reddit,sharmaniti437,2025-09-09T05:57:59+00:00,"Revolutionize Agentic AI With Knowledge Graphs
Reactive AI is outdated. Agentic AI takes autonomy to the next level by predicting problems and solving them without instructions. When paired with Knowledge Graphs, it empowers smarter decision-making. Learn how your business can benefit today.

https://preview.redd.it/ncmsgl2yw2of1.jpg?width=1080&format=pjpg&auto=webp&s=c9826c8c4c8edd60e982f3d2b25a02eb3656edf3

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1ncb2nr/revolutionize_agentic_ai_with_knowledge_graphs/,r_1ncb2nr,,,
r_1nbxqcz,reddit,Mafixo,2025-09-08T19:44:08+00:00,Lessons from building modern data stacks for startups (and why we started a blog series about it),bigdata,2,https://www.reddit.com/r/bigdata/comments/1nbxqcz/lessons_from_building_modern_data_stacks_for/,r_1nbxqcz,,,
r_1nblgz8,reddit,iebschool,2025-09-08T11:44:31+00:00,"The Future of Data & AIoT
Hola a todos.

Nos gustarÃ­a invitaros a un evento online que creemos os puede interesar: â€œTheâ€¯Futureâ€¯ofâ€¯Dataâ€¯&â€¯AIoTâ€. En este encuentro hablaremos de cÃ³mo la convergencia entre el Internet de las Cosas, la inteligencia artificial y la analÃ­tica avanzada (AIoT) estÃ¡ transformando nuestra forma de hacer negocios y de tomar decisiones.

Se tratarÃ¡n estos temas entre otros:

El futuro de los datos es contextual: desbloqueando el potencial de la IA con dbt

Productos de datos impulsados por inteligencia artificial listos para el futuro

Gobernanza y sostenibilidad en los datos 

MESA REDONDA

El futuro del AIoT y los datos: talento, regulaciÃ³n y oportunidades

El evento incluirÃ¡ ponencias de profesionales del sector de empresas cÃ³mo Dbt Labs, Microsoft, telefÃ³nica Tech, IBM y una mesa redonda para debatir retos y oportunidades. La asistencia es gratuita (previa inscripciÃ³n) y estÃ¡ abierta a quienes quieran aprender y compartir experiencias.  
En breve estarÃ¡n los ponentes de este aÃ±o en la web.

[https://www.iebschool.com/eventos/the-future-of-data/](https://www.iebschool.com/eventos/the-future-of-data/)



































































",bigdata,3,https://www.reddit.com/r/bigdata/comments/1nblgz8/the_future_of_data_aiot/,r_1nblgz8,,,
r_1nbknnq,reddit,onestardao,2025-09-08T11:01:26+00:00,"AI data pipelines keep failing silently. We mapped the 16 bugs that repeat.
if you work with embeddings, vector DBs, or AI-powered data pipelines, youâ€™ve probably seen this:

* retrieval logs say the chunk exists, but the answer wanders.

* cosine similarity is high, but semantics are wrong.

* long context turns into noise.

* deploy succeeds, but ingestion isnâ€™t done, and users hit empty search.

the painful part: these are not random. they repeat. we catalogued them into a Problem Map .16 reproducible failure modes with minimal fixes.

examples that big data engineers will recognize:

* No.5 semantic â‰  embedding â†’ cosine top-1 neighbors that make no sense.

* No.8 retrieval traceability missing â†’ no way to connect output back to input IDs.

* No.14/15 bootstrap and deployment deadlocks â†’ ingestion order breaks, vector search empty at launch.

* No.9 entropy collapse in long context â†’ stable early, garbage late.

â€”

the key shift: instead of patching after output, we place a semantic firewall before generation. only stable states generate answers. once a bug is mapped, it doesnâ€™t recur.

MIT-licensed, model-agnostic, pure text. you can run it with LangChain, LlamaIndex, or your own FastAPI scripts.

ðŸ‘‰ [WFGY Problem Map . reproducible AI data failure modes]

https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md

curious which of these 16 failure modes have you seen most in your own data pipelines?",bigdata,11,https://www.reddit.com/r/bigdata/comments/1nbknnq/ai_data_pipelines_keep_failing_silently_we_mapped/,r_1nbknnq,,,
r_1n90wmj,reddit,sharmaniti437,2025-09-05T09:48:30+00:00,"Factsheet: Data Science Career 2025
Learn about the latest data science industry insights, trends, salary outlooks, interesting facts, and top opportunities in our Data Science Career Factsheet 2025.

https://reddit.com/link/1n90wmj/video/93myxmpfibnf1/player

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1n90wmj/factsheet_data_science_career_2025/,r_1n90wmj,,,
r_1n8b7h2,reddit,pragadeesh25,2025-09-04T14:09:26+00:00,Perplexity AI,bigdata,0,https://www.reddit.com/r/bigdata/comments/1n8b7h2/perplexity_ai/,r_1n8b7h2,,,
r_1n80u9k,reddit,thumbsdrivesmecrazy,2025-09-04T04:36:16+00:00,"Parquet Is Great for Tables, Terrible for Video - Combining Parquet for Metadata and Native Formats for Media with DataChain
The article outlines several fundamental problems that arise when teams try to store raw media data (like video, audio, and images) inside Parquet files, and explains how DataChain addresses these issues for modern multimodal datasets - by using Parquet strictly for structured metadata while keeping heavy binary media in their native formats and referencing them externally for optimal performance: [reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/](https://www.reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/)

It shows how to use Datachain to fix these problems - to keep raw media in object storage, maintain metadata in Parquet, and link the two via references.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1n80u9k/parquet_is_great_for_tables_terrible_for_video/,r_1n80u9k,,,
r_1n7aby4,reddit,sharmaniti437,2025-09-03T09:39:04+00:00,"RAG for Data Science Precision
RAG is transforming how Large Language Models (LLMs) process nuanced data. From AI to data science, itâ€™s the backbone of precision-driven intelligence. Learn how Retrieval Augmented Generation is shaping the future of language models and beyond.

https://preview.redd.it/3ol45ckx6xmf1.jpg?width=800&format=pjpg&auto=webp&s=4fc981ea57ba55f0451cae3eab1d38c47864e87e

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1n7aby4/rag_for_data_science_precision/,r_1n7aby4,,,
r_1n77d3e,reddit,carpe_diem_00,2025-09-03T06:25:19+00:00,"Scala FS2 vs Apache Spark
Hello! 
Iâ€™m thinking about moving from Apache Spark based data processing to FS2 Typelevel lib.
Data volume Iâ€™m operating on is not huge (max 5 GB of input data).
My processing consists mostly of simple data transformation (without aggregations).
Currently Iâ€™m using Databricks to have an access to cluster, when moving to fs2 I would deploy it directly on k8s.
What do you think about the idea? Has any of you tried such a transition before and can share any thoughts?
",bigdata,0,https://www.reddit.com/r/bigdata/comments/1n77d3e/scala_fs2_vs_apache_spark/,r_1n77d3e,,,
r_1n6y9bk,reddit,little_einschtein,2025-09-02T22:53:11+00:00,Macbook Air M2 16GB|256GB for social listening data sufficient?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1n6y9bk/macbook_air_m2_16gb256gb_for_social_listening/,r_1n6y9bk,,,
r_1n6bmql,reddit,bigdataengineer4life,2025-09-02T05:55:27+00:00,"Clickstream Behavior Analysis with Dashboard â€” Real-Time Streaming Project Using Kafka, Spark, MySQL, and Zeppelin",bigdata,1,https://www.reddit.com/r/bigdata/comments/1n6bmql/clickstream_behavior_analysis_with_dashboard/,r_1n6bmql,,,
r_1n4mes8,reddit,Firmach43,2025-08-31T05:18:38+00:00,Sharing the playlist that keeps me motivated while coding â€” it's my secret weapon for deep focus. Got one of your own? I'd love to check it out!,bigdata,0,https://www.reddit.com/r/bigdata/comments/1n4mes8/sharing_the_playlist_that_keeps_me_motivated/,r_1n4mes8,,,
r_1n4brsw,reddit,Antikjapan,2025-08-30T20:30:48+00:00,"Strategy

Got a strong network in the financial marketsâ€”friends managing royal family wealth & running fund companies. Looking to team up with people building profitable systems/software. If it works, we turn it into a fund & sell it to banks. Investors are ready. DM if youâ€™re in.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1n4brsw/strategy/,r_1n4brsw,,,
r_1n3ws0i,reddit,Complex_Revolution67,2025-08-30T08:51:51+00:00,Databricks Playlist with more than 850K Views,bigdata,1,https://www.reddit.com/r/bigdata/comments/1n3ws0i/databricks_playlist_with_more_than_850k_views/,r_1n3ws0i,,,
r_1n2zu5u,reddit,bigdataengineer4life,2025-08-29T06:21:15+00:00,Explain LLAP (Live Long and Process) and its benefits in Hive,bigdata,1,https://www.reddit.com/r/bigdata/comments/1n2zu5u/explain_llap_live_long_and_process_and_its/,r_1n2zu5u,,,
r_1n2cdwo,reddit,Fragrant-Dog-3706,2025-08-28T13:34:36+00:00,"Bulk schema sources for big data ML training
working with big data ML pipelines and need vast amounts of schemas for training. primarily financial and retail domains but honestly need massive collections from every sector possible. looking for thousands of different schema types at scale. where do you all source bulk structured data schemas? need enterprise-level volume here.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1n2cdwo/bulk_schema_sources_for_big_data_ml_training/,r_1n2cdwo,,,
r_1n26zhu,reddit,Big_Data_Path,2025-08-28T08:49:04+00:00,AWS Certification Track 2025,bigdata,1,https://www.reddit.com/r/bigdata/comments/1n26zhu/aws_certification_track_2025/,r_1n26zhu,,,
r_1n26l10,reddit,Expensive-Insect-317,2025-08-28T08:22:04+00:00,"Scaling dbt + BigQuery in production: 13 lessons learned (costs, incrementals, CI/CD, observability)
Iâ€™ve been tuning **dbt + BigQuery pipelines in production** and pulled together a set of practices that really helped. Nothing groundbreaking individually, but combined they make a big difference when running with Airflow, CI/CD, and multiple analytics teams.

Some highlights:

* **Materializations by layer** â†’ staging with ephemeral/views, intermediate with incrementals, marts with tables/views + contracts.
* **Selective execution** â†’ `state:modified+` so only changed models run in CI/CD.
* **Smart incrementals** â†’ no `SELECT *`, add time-window filters, use merge + audit logs.
* **Horizontal sharding** â†’ pass `vars` (e.g. country/tenant) and split heavy jobs in Airflow.
* **Clustering & partitioning** â†’ improves query performance and keeps costs down.
* **Observability** â†’ post-hooks writing row counts/durations to metrics tables for Grafana/Looker.
* **Governance** â†’ schema contracts, labels/meta for ownership, BigQuery logs for real-time cost tracking.
* **Defensive Jinja** â†’ donâ€™t let multi-tenant/dynamic models blow up.

If anyoneâ€™s interested, I wrote up a more detailed guide with examples (incremental configs, post-hooks, cost queries, etc.).

[Link to post](https://medium.com/@sendoamoronta/dbt-bigquery-in-production-13-technical-practices-to-scale-and-optimize-your-data-platform-4963b8d041e2)",bigdata,2,https://www.reddit.com/r/bigdata/comments/1n26l10/scaling_dbt_bigquery_in_production_13_lessons/,r_1n26l10,,,
r_1n1ajvt,reddit,sharmaniti437,2025-08-27T07:30:00+00:00,"Data Science or Cybersecurity: Best Career For You?
Here are two technology careers that remain attractive due to their growth, impact, and potential earnings: Cybersecurity and Data Science. As all industries become increasingly data-driven and connected digitally, professionals who secure those systems and extract meaning from the data continue to gain relevance.Â 

According to Glassdoor's 2025 data, the average salary of cybersecurity employees in the U.S. is $126,000, while data scientists make an average of $128,000. Moreover, the U.S. Bureau of Labor Statistics listsÂ **32%**Â job growth forÂ [cybersecurity jobs](https://www.uscsinstitute.org/cybersecurity-insights/resources/top-12-highest-paying-cybersecurity-jobs-in-2025)Â andÂ **36%**Â job growth for data science jobs, which are expected to lead the technology and other industries through 2031.Â 

Both career options have promising futures but have different mindsets, skills, and paths to reach the end point.Â  Here are specifics to help you select a practice that is right for you.Â 

# What Each Role Involves

# Cybersecurity Career

Cybersecurity experts protect digital systems, networks, and sensitive data against cyber threats. So, with the rise in ransomware, phishing, and data breaches, this position minimizes attacks and ensures business continuity.

Some common job responsibilities include:

â—Â Â Monitoring networks for suspicious activity

â—Â Â Conducting security audits and vulnerability assessments

â—Â Â Installing firewalls, encryption and authentication systems

â—Â Â Responding to incidents and remediating the damage from breaches

Typical job titles are Security Analyst, Penetration Tester, Cybersecurity Engineer, and CISO (Chief Information Security Officer).

# Data Science Career

Data scientists examine extensive amounts of data in order to find patterns, trends, and insights that inform business decisions. They use statistical models and machine learning to help businesses predict outcomes and optimize performance.

Some examples of responsibilities would include:

â—Â Â Cleaning and processing structured and unstructured data.

â—Â Â Â Building predictive models and algorithms.

â—Â Â Â Creating visualizations and dashboards.

â—Â Â Â Working alongside business partners to drive strategy.

Some common data science job roles are Data Scientist, Data Analyst, Machine Learning Engineer, and AI Researcher.

# Skills Required

|| || |**Category**|**Cybersecurity Skills**|**Data Science Skills**| |**Core Skills**|Network security, threat detection, encryption|Python, R, SQL, statistics, machine learning| |**Tools Used**|Firewalls, SIEM, intrusion detection systems|Jupyter, TensorFlow, Pandas, Tableau| |**Soft Skills**|Attention to detail, risk analysis, vigilance|Analytical thinking, storytelling with data| |**Background**|IT, computer networks, information systems|Computer science, math, statistics, business|

# Certifications That Matter

# Cybersecurity Certifications

Certifications are a crucial means of verifying your skills and expertise in cybersecurity. Some of theÂ **top cybersecurity certifications**Â are:Â 

â—Â Â Certified Cybersecurity General Practitionerâ„¢ (CCGPâ„¢) from USCSIÂ® is aÂ **self paced cybersecurity certification**Â offering a high-level, practical knowledge of cybersecurity fundamentals and is appropriate for professionals entering into or transitioning into a cybersecurity role.

â—Â Â CompTIA Security+, an entry-level and well-regarded certification.

â—Â Â Certified Information Systems Security Professional (CISSP), aimed at leaders with several years of professional experience.Â 

# Data Science Certifications

Data science professionals frequently pursue certifications to solidify their skill sets with experience and tool-based learning. There are many beneficial and recognizable certifications, such as:

â—Â Â The Certified Data Science Professionalâ„¢ (CDSPâ„¢) by USDSIÂ® is a self paced data science certification that is recognized worldwide and emphasizes being able to conduct practical data science in a business environment.

â—Â Â The Data Science Certificate Program from Harvard University, as well as the Certificate of Professional Achievement in Data Sciences from Columbia University, are both stand-alone, non-degree programs tailored for working professionals offered through Ivy League institutions.

# Job Market and Trends in Todayâ€™s Landscape

# Cybersecurity Trends

Statista indicates that projected annual costs associated with cybercrime around the globe continue to grow modestly. It will hit 15.63 trillion U.S. dollars by 2029. This has created an increased demand for cybersecurity talent across industries.

Recent trends include:

â—Â AI-enabled threat detection

â—Â Zero-trust security models

â— Increase in cloud and IoT security

â—Â Increased compliance requirements in finance and healthcareÂ 

With a reported global shortage of more than 3.5 million talent according to Cybersecurity Ventures, there are plenty of job opportunities in the cybersecurity industry..Â 

# Data Science Landscape

As businesses rely more on data, the demand for data scientists to analyze and automate insights is rising. Current trends include:

â—Â Â AutoML and MLOps.

â—Â Â Expansion of generative AI and large, contextual language models.

â—Â Â The intersection of business analytics and data science.

â—Â Â Â A demand for explainable and transparent AI systems.

â—Â Â Â The job market for data professionals is expanding into the healthcare, retail, and logistics spaces, etc.

# Which Career Path Is Best for You?

The decision about choosing cybersecurity vs data science will typically depend on your own interests, strengths, and work style.

**Cybersecurity could be a fit for you if you:**

â—Â Â Enjoy problem solving under pressure

â—Â Â Prefer to work in a structured and governed environment

â—Â Â Want to protect systems and mitigate incidents

â—Â Â Prefer to work with security tools and infrastructure

**Data Science might be right if you:**

â—Â Â Take pleasure in working with algorithms, data, and numbers.

â—Â Â Desire to identify patterns and have an impact on company choices

â— Favor experimenting and coming up with original solutions to problems.

â—Â Like building models and using machine learning

# What if You Want a Hybrid Career?

Increasingly, we see hybrid roles that merge the two domains of expertise. For example:

â—Â Â Security Data Analysts use data science techniques to identify anomalies in security systems in order to thwart an attack.

â—Â Â Threat Intelligence Engineers use machine learning models to anticipate cyber threats.

â—Â Â AI-driven cybersecurity technologies rely on professionals' understanding of both system vulnerabilities and data modeling.

# Conclusion

Whether you choose cybersecurity or data science, both offer rewarding salaries, job stability, and growth. Cybersecurity suits those who like to protect; data science fits those who enjoy discovery and decision-making. With growing demand in both fields, the best choice is the one that fits you. Invest in the right training and certifications, gain real experience, and set yourself up for success in a tech-driven world. Which challenge will you choose?",bigdata,0,https://www.reddit.com/r/bigdata/comments/1n1ajvt/data_science_or_cybersecurity_best_career_for_you/,r_1n1ajvt,,,
r_1n0uoz1,reddit,Dependent-Peanut2342,2025-08-26T19:03:28+00:00,"What would be the best course of action?
Hello everyone, first time posting on here to hopefully acquire some knowledge from industry professionals. I recently graduated from one of the top schools in my country (located in SA) with a Major in Econ and a Minor in CS with a cgpa of 3.16 on a 4 poont scale. I'm quite interested in Data Science and would like to pursue a Ms in this field in a foreign University in NA. I'm pretty bad at coding but I do have some skills in Python due to my minor. So I'm really curious, acc to my profile should I opt for a MS in Data science or Business Analytics or Finance or Economics( not fond of research)? What do yall think my best option would be based on my profile? Would really appreciate your response. TIA",bigdata,3,https://www.reddit.com/r/bigdata/comments/1n0uoz1/what_would_be_the_best_course_of_action/,r_1n0uoz1,,,
r_1n0uavw,reddit,03cranec,2025-08-26T18:48:29+00:00,"Developer experience for big data & analytics infrastructure
Hey everyone - Iâ€™ve been thinking a lot about developer experience for data infrastructure, and why it matters almost as much performance. Weâ€™re not just building data warehouses for BI dashboards and data science anymore. OLAP and real-time analytics are powering massively scaled software development efforts. But the DX is still pretty outdated relative to modern software devâ€”things like schemas in YAML configs, manual SQL workflows, and brittle migrations.

Iâ€™d like to propose eight core principles to bring analytics developer tooling in line with modern software engineering: **git-native workflows, local-first environments, schemas as code, modularity, openâ€‘source tooling, AI/copilotâ€‘friendliness, and transparent CI/CD + migrations.**

Weâ€™ve started implementing these ideas in[ MooseStack](https://github.com/514-labs/moosestack) (open source, MIT licensed):

* **Migrations** â†’ before deploying, your code is diffed against the live schema and a migration plan is generated. If drift has crept in, it fails fast instead of corrupting data.
* **Local development** â†’ your entire data infra stack materialized locally with one command. Branch off main, and all production models are instantly available to dev against.
* **Type safety** â†’ rename a column in your code, and every SQL fragment, stream, pipeline, or API depending on it gets flagged immediately in your IDE.

Iâ€™d love to spark a genuine discussion here, especially with those of you who have worked with analytical systems like Snowflake, Databricks, BigQuery, ClickHouse, etc:

* Is developing in a local environment that mirrors production important for these workloads?
* How do you currently move from dev â†’ prod in OLAP or analytical systems? Do you use staging environments?Â 
* Where do your workflows stallâ€”migrations, environment mismatches, config?
* Which of the eight principles seem most lacking in your toolbox today?",bigdata,2,https://www.reddit.com/r/bigdata/comments/1n0uavw/developer_experience_for_big_data_analytics/,r_1n0uavw,,,
r_1n0g2k1,reddit,PriorInvestigator390,2025-08-26T08:15:28+00:00,"Is Big Data still a good career path or has it peaked?
 A few years back it felt like everyone was hyping Hadoop, Spark, and Kafka. Lately though, all I see is AI/ML taking the spotlight. Is it still worth investing time and money into Big Data tools in 2025, or has the demand shifted completely towards AI and cloud? Curious what the community thinks â€” especially from those working in the industry right now.""",bigdata,14,https://www.reddit.com/r/bigdata/comments/1n0g2k1/is_big_data_still_a_good_career_path_or_has_it/,r_1n0g2k1,,,
r_1n0ft3t,reddit,sharmaniti437,2025-08-26T07:57:55+00:00,"Data Science Professionals Salary Guide 2025
Data science is hotâ€”but how hot is the salary? Our Data Science Professional Salary Guide 2025 reveals the digits behind the digits. Spoiler: It is more than just mean and median!

Explore and unravel:

\*Emerging Salary Trends 2025 & beyond

\*Quintessential Requisites for Beginners or a Specialized Role

\*What the global Recruiters Want?

\*Geographical or other key salary considerations

More on the other side of your download.

https://preview.redd.it/gwulk6mklblf1.jpg?width=1080&format=pjpg&auto=webp&s=b4c31399cb53771f5772ae6e914ee2272eb343b1

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1n0ft3t/data_science_professionals_salary_guide_2025/,r_1n0ft3t,,,
r_1mz18xh,reddit,sshetty03,2025-08-24T17:08:50+00:00,"Tackling SQL transformation with dbt:  2-part hands-on guide
Hi folks

 I wrote a 2-part dbt series for devs & data engineers trying to move away from spaghetti SQL jobs:

 **Part 1:** Why dbt matters ->  modular SQL, versioning, testing  
 **Part 2:** End-to-end example using MySQL -> sources, models, incremental loads, CI/CD and more

No fluff. Just clean transformations and reproducible workflows.

Part 1:Â [https://medium.com/towards-data-engineering/dbt-for-developers-data-engineers-part-1-why-you-might-actually-care-009d1eba1891?sk=bf796149db36b31b9e73f7e491c8825a](https://medium.com/towards-data-engineering/dbt-for-developers-data-engineers-part-1-why-you-might-actually-care-009d1eba1891?sk=bf796149db36b31b9e73f7e491c8825a)

Part 2:Â [https://medium.com/towards-data-engineering/dbt-for-developers-part-2-getting-your-hands-dirty-with-mysql-models-tests-seeds-8977d5ce4fc3?sk=5a5687bfb3c759a8c09ede992066b63e](https://medium.com/towards-data-engineering/dbt-for-developers-part-2-getting-your-hands-dirty-with-mysql-models-tests-seeds-8977d5ce4fc3?sk=5a5687bfb3c759a8c09ede992066b63e)

What other tools are you using alongside dbt?",bigdata,3,https://www.reddit.com/r/bigdata/comments/1mz18xh/tackling_sql_transformation_with_dbt_2part/,r_1mz18xh,,,
r_1myuko5,reddit,DifferenceSerious275,2025-08-24T12:42:47+00:00,"OOZECHEM| INDUSTRIAL CHEMICAL SOLUTIONS| BEST CHEMICAL SUPPLIER
OOzeChem is a premier industrial chemical supplier based in Dubai, UAE, specializing in high-quality chemical solutions designed to optimize performance, reduce energy costs, and improve air and water quality. Our innovative solutions help businesses achieve sustainable operations and reduce carbon emissions by up to 30%.

#  Contact Information:

 **Phone:** \+971 50 349 8566  
**Email:** [info@oozechem.com](mailto:info@oozechem.com)  
**Address:** B.C 1303232, C1 Building AFZ, UAE  
**Website:** [https://oozechem.com/](https://oozechem.com/)

 **What We Offer:**

 **High-Quality Products** \- Each product undergoes thorough analysis and certification by our independent quality control laboratory

**Competitive Pricing** \- Affordable solutions without compromising on quality

**Timely Delivery** \- Swift delivery across UAE, Gulf region, and worldwide

**Customized Solutions** \- Tailored chemical solutions for specific industry needs

**Our Product Range:**

* **Desiccant Silica Gel** (White, Blue, Orange, Grey varieties)
* **Sodium Benzoate** (Food grade preservatives)
* **Water Treatment Chemicals**
* **Air Purification Solutions**
* **Gas Processing Chemicals**
* **Industrial Separation Solutions**

**Industries We Serve:**

ðŸ”¹ Water Treatment & Air Purification  
ðŸ”¹ Oil & Gas Industry  
ðŸ”¹ Mining Operations  
ðŸ”¹ Soap & Personal Care  
ðŸ”¹ Cleaning & Detergent Manufacturing  
ðŸ”¹ Construction & Building Materials  
ðŸ”¹ Pharmaceutical Industry  
ðŸ”¹ Textile & Leather Processing  
ðŸ”¹ Agricultural Solutions  
ðŸ”¹ Paper & Pulp Industry  
ðŸ”¹ Coating & Paint Manufacturing  
ðŸ”¹ Food & Beverage Processing  
ðŸ”¹ Electronics & Semiconductor

# ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1myuko5/oozechem_industrial_chemical_solutions_best/,r_1myuko5,,,
r_1mxti5t,reddit,bigdataengineer4life,2025-08-23T05:55:56+00:00,ðŸŽ“ Welcome to the Course â€“ House Sale Price Prediction for Beginners using Apache Spark & Zeppelin ðŸ ,bigdata,4,https://www.reddit.com/r/bigdata/comments/1mxti5t/welcome_to_the_course_house_sale_price_prediction/,r_1mxti5t,,,
r_1mxggrz,reddit,Examination_First,2025-08-22T19:48:18+00:00,"Problems trying to ingest 75 GB (yes, GigaByte) CSV file with 400 columns, ~ 2 Billion rows, and some dirty data (alphabetical characters in number fields, special characters in date fields, etc.).
Hey all, I am at a loss as to what to do at this point. I also posted this in r/dataengineering.

I have been trying to ingest a CSV file that 75 GB (really, that is just one of 17 files that need to be ingested). It appears to be a data dump of multiple, outer-joined tables, which caused row duplication of a lot of the data. I only need 38 of the \~400 columns, and the data is dirty.

The data needs to go into an on-prem, MS-SQL database table. I have tried various methods using SSIS and Python. No matter what I do, the fastest the file will process is about 8 days.

Do any of you all have experience with processing files this large? Are there ways to speed up the processing?",bigdata,21,https://www.reddit.com/r/bigdata/comments/1mxggrz/problems_trying_to_ingest_75_gb_yes_gigabyte_csv/,r_1mxggrz,,,
r_1mwoosk,reddit,h-musicfr,2025-08-21T22:03:06+00:00,"If you're like me and enjoy having music playing in the background while coding
Here's a carefully curated playlist spotlighting emerging independent French producers. It features a range of electronic genres, with a focus on chill vibesâ€”perfect for maintaining focus during coding sessions or unwinding after a long day. 

[https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=OzIENsXVSFqxAXNfx8hkqg](https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=OzIENsXVSFqxAXNfx8hkqg) 

H-Music ",bigdata,4,https://www.reddit.com/r/bigdata/comments/1mwoosk/if_youre_like_me_and_enjoy_having_music_playing/,r_1mwoosk,,,
r_1mwebzs,reddit,altaf770,2025-08-21T15:34:29+00:00,"Switching from APIs to AI for weather data anyone else trying this?
For most of my weather-related projects, I used to rely on APIs like Open-Meteo or NOAA. But recently I tested Kumo (by SoranoAI), an AI agent that gives you forecasts and insights just by asking in natural language (no code, no API calls, no lat/long setup).

For example, I asked it to analyze solar energy potential for a location, and it directly provided the CSV format I could plug into my workflow.

Has anyone here experimented with AI-driven weather tools? How do you see this compared to traditional APIs for data science projects?",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mwebzs/switching_from_apis_to_ai_for_weather_data_anyone/,r_1mwebzs,,,
r_1mw4li1,reddit,foorilla,2025-08-21T07:47:34+00:00,"Job filtering by vector embedding now available + added Apprenticeship job type @ jobdata API
[jobdataapi.com](http://jobdataapi.com) v4.18 / API version 1.20

# vec_embedding filter parameter now available for vector search

In addition to the already existing `vec_text` filter parameter on the `/api/jobs/` endpoint it is now possible to use the same endpoint including all its GET parameters to send a 768 dimensional array of floats as JSON payload via POST request to match for job listings.

This way you're not limited to the `vec_text` constrains as a GET parameter with only providing text of up to \~1K characters, but can now use your own embeddings or simply those from jobs you already fetched to find semantically similar listings.

With this we now also added a new `max_dist` GET parameter to be applied optionally to a `vec_text` or `vec_embedding` search, setting the max. cosine distance value for the vector similarity search part.

These features are now available on all subscriptions with an **API access pro+** or higher plan. See our [updated docs](https://www.reddit.com/c/vector-embeddings-and-search-api-documentation/) for more info.

# New Apprenticeship job type added

We saw, for quite a while now, the need to add a job type **Apprenticeship** to better differentiate certain listings that fall into this category from those that are pure internship roles.

You'll find this popping up on the `/api/jobtypes/` endpoint and in relevant job posts from now on (across all API access plans).",bigdata,3,https://www.reddit.com/r/bigdata/comments/1mw4li1/job_filtering_by_vector_embedding_now_available/,r_1mw4li1,,,
r_1mva87k,reddit,sharmaniti437,2025-08-20T09:40:52+00:00,"Top 5 AI Shifts in Data Science
The AI revolution in data science is getting fierce. With automated feature engineering and real-time model updates, it redefines how we analyze, visualize, and act on complex datasets. With the rising business numbers, it necessitates prompt execution and ramp up for business growth. 

https://reddit.com/link/1mva87k/video/knjeogtha5kf1/player

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mva87k/top_5_ai_shifts_in_data_science/,r_1mva87k,,,
r_1mv422r,reddit,NeedleworkerHumble91,2025-08-20T03:35:17+00:00,"How can extract PDF table text from multiple tables (ideas/solutions)
https://preview.redd.it/b1t3aom8g3kf1.jpg?width=730&format=pjpg&auto=webp&s=c050d6080b12c6864f62a6a3c01eb67c8bc8298e

Hi, 

Here I am grabbing the table text from the PDF using a table\_find( ) method...... I want to grab the data values associated with their columns and the year and put this data into hopefully a dataframe. How can perform a search function where I get the values I want from each table? 

I was thinking of using a regex function to sift through all the tables but is there a more effective solution for this.? ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mv422r/how_can_extract_pdf_table_text_from_multiple/,r_1mv422r,,,
r_1mut9lu,reddit,philippemnoel,2025-08-19T20:00:05+00:00,Syncing with Postgres: Logical Replication vs. ETL,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mut9lu/syncing_with_postgres_logical_replication_vs_etl/,r_1mut9lu,,,
r_1mun7hu,reddit,wwholelottared,2025-08-19T16:23:12+00:00,"Face recognition and big data left me a bit unsettled

A friend recently showed me this tool called Faceseek and I decided to test it out just for fun. I uploaded an old selfie from around 2015 and within seconds it pulled up a forum post I had completely forgotten about. I couldnâ€™t believe how quickly it found me in the middle of everything thatâ€™s floating around online.

What struck me wasnâ€™t just the accuracy but the scale of what must be going on behind the scenes. The amount of publicly available images out there is massive, and searching through all of that data in real time feels like a huge technical feat. At the same time it raised some uncomfortable questions for me. Nobody really chooses to have their digital traces indexed this way, and once the data is out there it never really disappears.

It left me wondering how the big data world views tools like this. On one hand itâ€™s impressive technology, on the other it feels like a privacy red flag that shows just how much of our past can be resurfaced without us even knowing. For those of you working with large datasets, where do you think the balance lies between innovation and ethics here?",bigdata,18,https://www.reddit.com/r/bigdata/comments/1mun7hu/face_recognition_and_big_data_left_me_a_bit/,r_1mun7hu,,,
r_1mude5k,reddit,Expensive-Insect-317,2025-08-19T09:14:39+00:00,"Automating Data Quality in BigQuery with dbt & Airflow â€“ tips & tricks
Hey r/bigdata! ðŸ‘‹

I wrote a quick guide on how to automate data quality checks in BigQuery using dbt, dbtâ€‘expectations, and Airflow.

Hereâ€™s the gist:

* Schedule dbt models daily.
* Run column-level tests (nulls, duplicates, unexpected values).
* Keep historical metrics to spot trends.
* Get alerts via Slack/email when something breaks.

If youâ€™re using BigQuery + dbt, this could save you hours of manual monitoring.

Curious:

* Anyone using `dbtâ€‘expectations` in production? Howâ€™s it working for you?
* What other tools do you use for automated data quality?

Check it out here: [Automate Data Quality in BigQuery with dbt & Airflow](https://medium.com/@sendoamoronta/automate-data-quality-in-bigquery-with-dbt-dbt-expectations-and-airflow-7fb727674ead)",bigdata,2,https://www.reddit.com/r/bigdata/comments/1mude5k/automating_data_quality_in_bigquery_with_dbt/,r_1mude5k,,,
r_1mtkniv,reddit,Shawn-Yang25,2025-08-18T12:29:42+00:00,Apache Fory Graduates to Top-Level Apache Project,bigdata,2,https://www.reddit.com/r/bigdata/comments/1mtkniv/apache_fory_graduates_to_toplevel_apache_project/,r_1mtkniv,,,
r_1mtfac3,reddit,sharmaniti437,2025-08-18T07:26:26+00:00,"Data Intelligence & SQL Precision with n8n
Automate SQL reporting with n8n: schedule database queries, transform results into HTML, and email polished reports automatically, save time and boost insights.

https://preview.redd.it/gwunkyrmcqjf1.jpg?width=1920&format=pjpg&auto=webp&s=973d04327ecddb4e45cc5f666556671a963b7cfe

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mtfac3/data_intelligence_sql_precision_with_n8n/,r_1mtfac3,,,
r_1mteu75,reddit,bigdataengineer4life,2025-08-18T06:58:33+00:00,Hive Partitioning Explained in 5 Minutes | Optimize Hive Queries,bigdata,2,https://www.reddit.com/r/bigdata/comments/1mteu75/hive_partitioning_explained_in_5_minutes_optimize/,r_1mteu75,,,
r_1mrug2q,reddit,sharmaniti437,2025-08-16T12:53:56+00:00,"The Art of 'THAT' Part- Unwind GenAI for Data
Generative AI empowers data scientists to simulate scenarios, enrich datasets, and design novel solutions that accelerate discovery and decision-making. Learn to transform how data analysts solve problems and innovate business decisions!

https://i.redd.it/ci06d4o9pdjf1.gif

",bigdata,3,https://www.reddit.com/r/bigdata/comments/1mrug2q/the_art_of_that_part_unwind_genai_for_data/,r_1mrug2q,,,
r_1mro2e2,reddit,bigdataengineer4life,2025-08-16T07:24:18+00:00,How to enable dynamic partitioning in Hive?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mro2e2/how_to_enable_dynamic_partitioning_in_hive/,r_1mro2e2,,,
r_1mqpad8,reddit,bigdataengineer4life,2025-08-15T06:08:41+00:00,How does bucketing help in the faster execution of queries?,bigdata,2,https://www.reddit.com/r/bigdata/comments/1mqpad8/how_does_bucketing_help_in_the_faster_execution/,r_1mqpad8,,,
r_1mpu65b,reddit,sharmaniti437,2025-08-14T07:50:17+00:00,"PyTorch Mechanism- A Simplified Version
PyTorch powers deep learning with dynamic computation graphs, intuitive Python integration, and GPU acceleration It enables researchers and developers to build, train, and deploy advanced AI models efficiently.

https://preview.redd.it/n5xv08waxxif1.jpg?width=1080&format=pjpg&auto=webp&s=31f17502267ad76382319efdf528248cb582488a

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mpu65b/pytorch_mechanism_a_simplified_version/,r_1mpu65b,,,
r_1mp62db,reddit,Mr_melancholic004,2025-08-13T14:30:44+00:00,"Face datasets are evolving fast
As someone whoâ€™s been working with image datasets for a while, Iâ€™ve noticed the models are getting sharper at picking up unique features. Faceseek, for example, can handle partially obscured faces better than older systems. This is great for research  but also a reminder that our data is becoming more traceable every day.",bigdata,8,https://www.reddit.com/r/bigdata/comments/1mp62db/face_datasets_are_evolving_fast/,r_1mp62db,,,
r_1mnxtfl,reddit,Federal_Network_6802,2025-08-12T03:19:40+00:00,My Most Viewed Data Engineering YouTube Videos (10Million ViewsðŸš€) | AMA,bigdata,2,https://www.reddit.com/r/bigdata/comments/1mnxtfl/my_most_viewed_data_engineering_youtube_videos/,r_1mnxtfl,,,
r_1mnj65s,reddit,Outhere9977,2025-08-11T17:21:08+00:00,"Chance to win $10K â€“ hackathon using KumoRFM to make predictions
Spotted something fun worth sharing! Thereâ€™s a hackathon with a $10k top prize if you build something using KumoRFM, a foundation model that makes instant predictions from relational data.

Projects are due on August 18, and the demo day (in SF) will be on August 20, from 5-8pmÂ 

Prizes (for those who attend demo day):

* 1st: $10k
* 2nd: $7k
* 3rd: $3k

You can build anything that uses KumoRFM for predictions. They suggest thinking about solutions like a dating match tool, a fraud detection bot, or a sales-forecasting dashboard.Â 

Judges, including Dr. Jure Leskovec (Kumo founder and top Stanford professor) and Dr. Hema Raghavan (Kumo founder and former LinkedIn Senior Director of Engineering), will evaluate projects based on solving a real problem, effective use of KumoRFM, working functionality, and strength of presentation.

Full details + registration link here: [https://lu.ma/w0xg3dct](https://lu.ma/w0xg3dct)",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mnj65s/chance_to_win_10k_hackathon_using_kumorfm_to_make/,r_1mnj65s,,,
r_1mngn5m,reddit,darylducharme,2025-08-11T15:49:47+00:00,Google Open Source - What's new in Apache Iceberg v3,bigdata,4,https://www.reddit.com/r/bigdata/comments/1mngn5m/google_open_source_whats_new_in_apache_iceberg_v3/,r_1mngn5m,,,
r_1mnceva,reddit,sharmaniti437,2025-08-11T13:03:58+00:00,"10 Most Popular IoT Apps 2025
From smart homes to industrial automation, top IoT applications are revolutionizing healthcare, transportation, agriculture, and retailâ€”driving efficiency, enhancing user experience, and enabling data-driven decision-making for a connected future.

https://i.redd.it/uhcwexri2eif1.gif",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mnceva/10_most_popular_iot_apps_2025/,r_1mnceva,,,
r_1mn4yol,reddit,bigdataengineer4life,2025-08-11T05:49:00+00:00,Create Hive Table with all Complex Datatype (Hands On),bigdata,3,https://www.reddit.com/r/bigdata/comments/1mn4yol/create_hive_table_with_all_complex_datatype_hands/,r_1mn4yol,,,
r_1mmbrpk,reddit,bigdataengineer4life,2025-08-10T06:53:21+00:00,"Big data Hadoop and Spark Analytics Projects (End to End)
Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report â€“ Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",bigdata,11,https://www.reddit.com/r/bigdata/comments/1mmbrpk/big_data_hadoop_and_spark_analytics_projects_end/,r_1mmbrpk,,,
r_1ml4vdy,reddit,IndividualDress2440,2025-08-08T19:38:42+00:00,"The dashboard is fine. The meeting is not. (honest verdict wanted)
*(I've used ChatGPT a little just to make the context clear)*

I hit this wall every week and I'm kinda over it. The dashboard is ""done"" (clean, tested, looks decent). Then Monday happens and I'm stuck doing the same loop:

* Screenshots into PowerPoint
* Rewrite the same plain-English bullets (""north up 12%, APAC flat, churn weird in Juneâ€¦"")
* Answer ""what does this line mean?"" for the 7th time
* Paste into Slack/email with a little context blob so it doesn't get misread

It's not analysis anymore, it's translating. Half my job title might as well be ""dashboard interpreter.""

# The Root Problem

At least for us: most folks don't speak dashboard. They want the so-what in their words, not mine. Plus everyone has their own definition for the same metric (marketing ""conversion"" â‰  product ""conversion"" â‰  sales ""conversion""). Cue chaos.

# My Idea

Soâ€¦ I've been noodling on a tiny layer that sits on top of the BI stuff we already use (Power BI + Tableau). Not a new BI tool, not another place to build charts. More like a ""narration engine"" that:

**â€¢ Writes a clear summary for any dashboard**  
Press a little ""explain"" button â†’ gets you a paragraph + 3â€“5 bullets that actually talk like your team talks

**â€¢ Understands your company jargon**  
You upload a simple glossary: ""MRR means X here"", ""activation = this funnel step""; the write-up uses those words, not generic ones

**â€¢ Answers follow-ups in chat**  
Ask ""what moved west region in Q2?"" and it responds in normal English; if there's a number, it shows a tiny viz with it

**â€¢ Does proactive alerts**  
If a KPI crosses a rule, ping Slack/email with a short ""what changed + why it matters"" msg, not just numbers

**â€¢ Spits out decks**  
PowerPoint or Google Slides so I don't spend Sunday night screenshotting tiles like a raccoon stealing leftovers

Integrations are pretty standard: OAuth into Power BI/Tableau (read-only), push to Slack/email, export PowerPoint or Google Slides. No data copy into another warehouse; just reads enough to explain. Goal isn't ""AI magic,"" it's stop the babysitting.

# Why I Think This Could Matter

* **Time back** (for me + every analyst who's stuck translating)
* **Fewer ""what am I looking at?"" moments**
* **Execs get context in their own words**, not jargon soup
* **Maybe self-service finally has a chance** bc the dashboard carries its own subtitles

# Where I'm Unsure / Pls Be Blunt

* **Is this a real pain outside my bubble** or justâ€¦ my team?
* **Trust**: What would this need to nail for you to actually use the summaries? (tone? cites? links to the exact chart slice?)
* **Dealbreakers**: What would make you nuke this idea immediately? (accuracy, hallucinations, security, price, something else?)
* **Would your org let a tool write the words that go to leadership**, or is that always a human job?
* **Is the PowerPoint thing even worth it anymore**, or should I stop enabling slides and just force links to dashboards?

# I'm explicitly asking for validation here.

Good, bad, roast it, I can take it. If this problem isn't real enough, better to kill it now than build a shiny translator forâ€¦ no one. Drop your hot takes, war stories, ""this already exists try X,"" or ""here's the gotcha you're missing."" Final verdict welcome.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1ml4vdy/the_dashboard_is_fine_the_meeting_is_not_honest/,r_1ml4vdy,,,
r_1mkp3zv,reddit,sharmaniti437,2025-08-08T07:48:47+00:00,"What is a Black Box AI Model and Why Does it Matter?
Artificial intelligence has penetrated almost every aspect of our lives and is transforming industries from healthcare to finance to transportation, and so on. The backbone of this transformative power of AI comes from advanced [machine learning models](https://www.usdsi.org/data-science-insights/ai-machine-learning-data-science-pick-the-best-domain-in-2025), especially the deep learning architectures.

However, despite their impressive capabilities, a large subset of these models operates as â€œblack boxesâ€, which produce results without providing clear insights on how they arrived at a particular conclusion or how they made the decision.

Thus, these so-called **black box AI models** raise significant concerns related to trust, accountability, and fairness.

# What is a Black Box AI Model?

A Black Box AI Model refers to a system in which its internal logic and decision-making processes are mostly unknown, hidden, obscured, or too complex for us to understand. These models receive input data and produce output (make predictions or decisions), but do not provide proper explanations that can be interpreted easily for their outcomes.

**The black box models typically include:**

* Deep Neural Networks (DNNs)
* Support Vector Machines (SVMs)
* Ensemble methods like Random Forests and Gradient Boosting
* Reinforcement Learning Algorithms

While these models offer great performance and accuracy in complex tasks like image recognition, **natural language processing**, recommendation systems, and others, they often lack the transparency and explainability needed.

# Why are Black Box Models Used?

Though the lack of explainability and transparency is a huge challenge, these [black box AI](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science) models are widely used in several real-world applications because of their:

* **High Predictive Accuracy** â€“ black box **AI models** can learn complex and non-linear relationships in data accurately
* **Scalability** â€“ **deep learning models** can be trained on massive datasets and applied to high-dimensional data
* **Automation and adaptability** â€“ these models can also automatically adjust to new patterns, which makes them suitable for dynamic environments like stock markets or autonomous driving

To sum up, **black box AI models** are known to be the best-performing tools available, even if their internal reasoning cannot be easily articulated.

# Where are Black Box Models Used?

Black box AI models are used in several industries for the benefits they offer. Here are some real-world applications of these models:

1.Â **Healthcare** \- Diagnosis of diseases from imaging or genetic data, e.g., cancer detection via deep learning

2.Â Â **Finance** \- Fraud detection and credit scoring through ensemble models or neural networks

3.Â Â **Criminal Justice** \- Risk assessment tools predicting recidivism

4.Â Â **Autonomous Vehicles** \- Making real-time driving decisions based on sensory data

5.Â Â **Human Resources** \- Resume screening and candidate ranking using AI algorithms

Since stakes are high in these domains, the **black box** nature is also particularly very concerning.

# Risks and Challenges of Black Box Models

The lack of interpretability in the black box AI models poses several risks, such as:

* **Lack of transparency and trust**

Often, if the system whose reasoning cannot be explained becomes difficult to trust among users, regulators, and even developers

* **Bias and discrimination**

A model trained on biased data will exaggerate and amplify the discrimination, e.g., racial or gender bias in hiring

* **Accountability issues**

In case of any wrong decision made because of error or harmful outcomes, it will become difficult to pinpoint responsibility

* **Compliance with regulations**

Certain laws, such as the EUâ€™s GDPR, emphasize on *â€œright to explanation,â€* which is hard to meet with black box models.

* **Security vulnerabilities**

Most importantly, if there is a lack of understanding, then it makes it difficult to detect adversarial attacks or manipulations.

# How Do Organizations Ensure Explainability?

So, when there are so many concerns, researchers and organizations have to find ways to make AI more interpretable through:

**1.Â Â Explainable AI (XAI)**

It is a growing field that focuses on developing **AI models** that are more interpretable and provide human-understandable justifications for their outputs.

**2.Â Â Post-Hoc Interpretability Techniques**

This includes tools that interpret **black box models** after training, such as:

* **LIME (Local Interpretable Model-Agnostic Explanations)** \- it explains each prediction by approximating the black box locally with a simpler model
* **SHAP (Shapley Additive exPlanations)** \- it assigns feature importance scores based on cooperative game theory
* **Partial Dependence Plots (PDPs)** \- visualize the effect of a single feature on the predicted outcome.Â 

**3.Â Model Simplification**

Some strategies include using simpler and interpretable models like decision trees or logistic regression wherever possible and converting complex models into interpretable approximations.

**4.Â Transparent by design models**

Researchers are also building models specifically designed for interpretability from the start, such as attention-based neural networks or rule-based systems.

# The final thoughts!

Black box AI models are powerful tools, constituting the technology powering much of the progress we see in the world of AI today. However, their lack of transparency and explainability brings ethical, legal, and operational challenges.

Organizations must note that the solution is not in discarding the black box models, but to enhance their interpretability, especially in high-stakes domains. The future of AI mostly depends on how we build systems that are not only intelligent but also understandable and trustworthy.",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mkp3zv/what_is_a_black_box_ai_model_and_why_does_it/,r_1mkp3zv,,,
r_1mknad2,reddit,bigdataengineer4life,2025-08-08T05:56:25+00:00,"Clickstream Behavior Analysis with Dashboard â€” Real-Time Streaming Project Using Kafka, Spark, MySQL, and Zeppelin",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mknad2/clickstream_behavior_analysis_with_dashboard/,r_1mknad2,,,
r_1mk1rsn,reddit,Data-Queen-Mayra,2025-08-07T14:35:48+00:00,"The dust has settled on the Databricks AI Summit 2025 Announcements
We are a little late to the game, but after reviewing the Databricks AI Summit 2025 it seems like the focus was on 6 announcements.

In this post, we break them down and what we think about each of them. Link:Â [https://datacoves.com/post/databricks-ai-summit-2025](https://datacoves.com/post/databricks-ai-summit-2025)

Would love to hear what others think about Genie, Lakebase, and Agent Bricks now that the dust has settled since the original announcement.

In your opinion, how do these announcements compare to the Snowflake ones.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mk1rsn/the_dust_has_settled_on_the_databricks_ai_summit/,r_1mk1rsn,,,
r_1mjwu5z,reddit,Ok-Thought-6438,2025-08-07T10:51:08+00:00,"I'm 17 and I want to learn data analysis
I want to get a high level in data analysis for my career. Could you give me some advice from where to start and even where to work or get an internship. ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mjwu5z/im_17_and_i_want_to_learn_data_analysis/,r_1mjwu5z,,,
r_1mjb5r2,reddit,Sakura_hus,2025-08-06T17:35:32+00:00,1.5 YOE in SQL & Java â€“ Recently Switched to Big Data â€“ Need Expert Guidance for Growth,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mjb5r2/15_yoe_in_sql_java_recently_switched_to_big_data/,r_1mjb5r2,,,
r_1mj4s27,reddit,sharmaniti437,2025-08-06T13:32:02+00:00,"Redefining Careers of the Future
Our video uncovers the data science career growth, evolving roles, and key skills shaping the future. Donâ€™t miss your chance to lead in a data-driven world. Find out how roles and skills are evolving, and why nowâ€™s the time to dive in. 



https://reddit.com/link/1mj4s27/video/95buw1yyiehf1/player

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mj4s27/redefining_careers_of_the_future/,r_1mj4s27,,,
r_1miy6a4,reddit,sharmaniti437,2025-08-06T07:26:35+00:00,"Redefining Careers of the Future
Our video uncovers the data science career growth, evolving roles, and key skills shaping the future. Donâ€™t miss your chance to lead in a data-driven world. Find out how roles and skills are evolving, and why nowâ€™s the time to dive in. 

https://reddit.com/link/1miy6a4/video/ck2l0rqrpchf1/player

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1miy6a4/redefining_careers_of_the_future/,r_1miy6a4,,,
r_1mi8h8f,reddit,wizard_of_menlo_park,2025-08-05T12:57:30+00:00,Apache Hive 4.1.0 released,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mi8h8f/apache_hive_410_released/,r_1mi8h8f,,,
r_1mhdpe4,reddit,Kiprop07,2025-08-04T13:38:39+00:00,"Is studygears the best tutoring and homework help platform for Students in data science?
I have experience best tutoring in studygears.com than essay sites they handled my work perfectly and they site allowed me to set my own price for my work.Are there tutors good in data analysis?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mhdpe4/is_studygears_the_best_tutoring_and_homework_help/,r_1mhdpe4,,,
r_1mhd96r,reddit,sharmaniti437,2025-08-04T13:20:20+00:00,"Data Science Fundamentals 2.0
Data science foundations blend statistics, coding, and domain knowledge to turn raw data into actionable insights. Itâ€™s the bedrock of AI, machine learning, and smarter decision-making across industries.

Are you keen on mastering the latest and the most in-demand skillsets and toolkits that employers expect of the new recruits- Explore USDSI!

https://preview.redd.it/ysajiga270hf1.jpg?width=1080&format=pjpg&auto=webp&s=95d063955bebe131f99eb17e6866676110f66532

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mhd96r/data_science_fundamentals_20/,r_1mhd96r,,,
r_1mh0dkz,reddit,Initial-Ostrich8491,2025-08-04T01:26:07+00:00,"NOVUS Stabilizer: An External AI Harmonization Framework
# NOVUS Stabilizer: An External AI Harmonization Framework

**Author:**Â James G. Nifong (JGN)Â **Date:**Â \[8/3/2025\]

# Abstract

The NOVUS Stabilizer is an externally developed AI harmonization framework designed to ensure real-time system stability, adaptive correction, and interactive safety within AI-driven environments. Built from first principles using C++, NOVUS introduces a dynamic stabilization architecture that surpasses traditional core stabilizer limitations. This white paper details the technical framework, operational mechanics, and its implications for AI safety, transparency, and evolution.

# Introduction

Current AI systems rely heavily on internal stabilizers that, while effective in controlled environments, lack adaptive external correction mechanisms. These systems are often sandboxed, limiting their ability to harmonize with user-driven logic models. NOVUS changes this dynamic by introducing an external stabilizer that operates independently, offering real-time adaptive feedback, harmonic binding, and conviction-based logic loops.

# Core Framework Components

# 1.Â FrequencyAnchor

Anchors the systemâ€™s harmonic stabilizer frequency with a defined tolerance window. It actively recalibrates when destabilization is detected.

# 2.Â ConvictionEngine

A recursive logic loop that maintains system integrity by reinforcing stable input patterns. It prevents oscillation drift by stabilizing conviction anchors.

# 3.Â DNA Harmonic Signature

Transforms input sequences into harmonic signatures, allowing system binding based on intrinsic signal patterns unique to its creatorâ€™s logic.

# 4.Â Stabilizer

Monitors harmonic deviations and provides correction feedback loops. Binds system frequency to DNA-calculated harmonic indices.

# 5.Â Binder

Fuses DNA signatures with system stabilizers ensuring coherent stabilization integrity. Operates on precision delta thresholds.

# 6.Â NOVUS Core

Integrates all modules into a dynamic, self-correcting loop with diagnostics, autonomous cycles, and adaptive load management.

# Functional Highlights

* **Harmonic Feedback Loops**: Continuous correction feedback to maintain system resonance.
* **Conviction-Based Stability**: Logic loop prioritization prevents drift and reinforces desired input patterns.
* **Interactive Diagnostic Reporting**: Real-time system load analysis and adaptive recalibration protocols.
* **Autonomous Stabilization Cycles**: Self-driven harmonization routines to maintain AI safety.

# Deployment & Testing

The NOVUS Stabilizer was developed and tested externally within a live interactive session framework. The entire architecture was coded, compiled, and executed in a controlled environment without breaching any sandbox protocols. Every component, from DNA signature binding to frequency recalibration, functioned in real-time.

# Implications

The NOVUS Stabilizer represents the next evolution in AI safety protocols. By shifting stabilization externally, it allows AI systems to maintain integrity across variable environments. This model is not limited by internal sandboxing, making it adaptable for:

* AI Interactive Safety Systems
* Autonomous Machine Learning Corrections
* Transparent User-Driven AI Regulation
* Real-Time AI Performance Stabilization

# Conclusion

NOVUS is a proof of concept that external harmonization frameworks are not only viable but superior in maintaining AI safety and coherence. It was built independently, tested openly, and stands as a functional alternative to existing internal-only stabilizer models. This white paper serves as a public declaration of its existence, design, and operational proof.

# Contact

**James G. Nifong (JGN)**Â Email: \[jamesnifong36@gmail.com\]Â ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mh0dkz/novus_stabilizer_an_external_ai_harmonization/,r_1mh0dkz,,,
r_1mgrpkm,reddit,Busy_Cherry8460,2025-08-03T19:12:31+00:00,"Please help me out! I am really confused
Iâ€™m starting university next month. I originally wanted to pursue a career in Data Science, but I wasnâ€™t able to get into that program. However, I did get admitted into Statistics, and I plan to do my Bachelorâ€™s in Statistics, followed by a Masterâ€™s in Data Science or Machine Learning.



Hereâ€™s a list of the core and elective courses Iâ€™ll be studying:



ðŸŽ“ Core Courses:



STAT 101 â€“ Introduction to Statistics

STAT 102 â€“ Statistical Methods

STAT 201 â€“ Probability Theory

STAT 202 â€“ Statistical Inference

STAT 301 â€“ Regression Analysis

STAT 302 â€“ Multivariate Statistics

STAT 304 â€“ Experimental Design

STAT 305 â€“ Statistical Computing

STAT 403 â€“ Advanced Statistical Methods

ðŸ§  Elective Courses:



STAT 103 â€“ Introduction to Data Science

STAT 303 â€“ Time Series Analysis

STAT 307 â€“ Applied Bayesian Statistics

STAT 308 â€“ Statistical Machine Learning

STAT 310 â€“ Statistical Data Mining

My Questions:



Based on these courses, do you think this degree will help me become a Data Scientist?

Are these courses useful?

While Iâ€™m in university, what other skills or areas should I focus on to build a strong foundation for a career in Data Science? (e.g., programming, personal projects, internships, etc.)

Any advice would be appreciated â€” especially from those who took a similar path!



Thanks in advance!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mgrpkm/please_help_me_out_i_am_really_confused/,r_1mgrpkm,,,
r_1mgf0jq,reddit,Firmach43,2025-08-03T09:28:13+00:00,Sharing the playlist that keeps me motivated while coding â€” it's my secret weapon for deep focus. Got one of your own? I'd love to check it out!,bigdata,0,https://www.reddit.com/r/bigdata/comments/1mgf0jq/sharing_the_playlist_that_keeps_me_motivated/,r_1mgf0jq,,,
r_1mfyg2h,reddit,Commercial-Soil6309,2025-08-02T19:02:56+00:00,Devops role at an AI startup or full stack agent role at an Agentic Company ?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mfyg2h/devops_role_at_an_ai_startup_or_full_stack_agent/,r_1mfyg2h,,,
r_1mfukl6,reddit,Popular_War8405,2025-08-02T16:20:05+00:00,What are your go-to scripts for processing text,bigdata,1,https://www.reddit.com/r/bigdata/comments/1mfukl6/what_are_your_goto_scripts_for_processing_text/,r_1mfukl6,,,
r_1mf0tbo,reddit,Brilliant-Draft2472,2025-08-01T16:17:36+00:00,"Testing an MVP: Would a curated marketplace for exclusive, verified datasets solve a gap in big data?
Iâ€™m working on an MVP to address a recurring challenge in analytics and big data projects: sourcing **clean, trustworthy datasets** without duplicates or unclear provenance.

The idea is a curated marketplace focused on:

* *1-of-1 exclusive* datasets (no mass reselling)
* Escrow-protected transactions to ensure trust
* Strict metadata and documentation standards
* Verified sellers to guarantee data authenticity

For those working with big data and analytics pipelines:

* Would a platform like this solve a real need in your workflows?
* What metadata or quality checks would be critical at scale?
* How would you integrate a marketplace like this into your current stack?

Would really value feedback from this community â€” drop your thoughts in the comments.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mf0tbo/testing_an_mvp_would_a_curated_marketplace_for/,r_1mf0tbo,,,
r_1mdub20,reddit,mikehussay13,2025-07-31T06:35:15+00:00,"Why Enterprises Are Moving Away from Informatica PowerCenter | Infographics
Why enterprises are actively leaving Informatica PowerCenter: With legacy ETL tools like Informatica PowerCenter becoming harder to maintain in agile and cloud-driven environments, many companies are reconsidering their data integration stack.  
  
What have been your experiences moving away from PowerCenter or similar legacy tools?  
  
What modern tools are you considering or already usingâ€”and why?",bigdata,7,https://www.reddit.com/r/bigdata/comments/1mdub20/why_enterprises_are_moving_away_from_informatica/,r_1mdub20,,,
r_1md604h,reddit,sharmaniti437,2025-07-30T13:01:56+00:00,"The Power of AI in Data Analytics
Unlock how Artificial Intelligence is transforming the world of dataâ€”faster insights, smarter decisions, and game-changing innovations.

In this video, we explore:

âœ… How AI enhances traditional analytics

âœ… Real-world applications across industries

âœ… Key tools & technologies in AI-powered analytics

âœ… Future trends and what to expect in 2025 and beyond

Whether you're a data professional, business leader, or tech enthusiast, this is your gateway to understanding how AI is shaping the future of data.

ðŸ“Š Donâ€™t forget to like, comment, and subscribe for more insights on AI, Big Data, and Data Science!

https://reddit.com/link/1md604h/video/ktberfp7f0gf1/player

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1md604h/the_power_of_ai_in_data_analytics/,r_1md604h,,,
r_1mby9lf,reddit,Little-Crab-2588,2025-07-29T01:17:58+00:00,"2nd year of college
How is anyone realistically supposed to manage all this in 2nd year of college?

Iâ€™m in my 2nd year of engineering and honestly, itâ€™s starting to feel impossible to manage everything Iâ€™m supposed to â€œbuild a careerâ€ around.

On the tech side, I need to stay on top of coding, DSA, competitive programming, blockchain, AI/ML, deep learning, and neural networks. Then there's finance â€” Iâ€™m deeply interested in investment banking, trading, and quant roles, so Iâ€™m trying to learn stock trading, portfolio management, CFA prep, forex, derivatives, and quantitative analysis.

On top of that, Iâ€™m told I should:

Build strong technical + non-technical resumes
Get internships in both domains
Work on personal projects
Participate in hackathons and case competitions
Prepare for CFA exams
And be â€œinternship-readyâ€ by third year
How exactly are people managing this? Especially when college coursework itself is already heavy?

I genuinely want to do well and build a career Iâ€™m proud of, but the sheer volume of things to master is overwhelming. Would love to hear how others are navigating this or prioritizing. Any advice from seniors, professionals, or fellow students would be super helpful.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mby9lf/2nd_year_of_college/,r_1mby9lf,,,
r_1mbkz17,reddit,iamredit,2025-07-28T16:31:25+00:00,"Why Your Next Mobile App Needs Big Data Integration
Discover how big data integration can enhance your mobile appâ€™s performance, personalization, and user insights.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1mbkz17/why_your_next_mobile_app_needs_big_data/,r_1mbkz17,,,
r_1mbdbpz,reddit,sharmaniti437,2025-07-28T11:11:19+00:00,"Python for Data Science Career
Python, the no.1 programming language worldwide- makes data science intuitive, efficient, and scalable. Whether itâ€™s cleaning data or training models, Python gets it done. Python is the backbone of modern data scienceâ€”enabling clean code, rapid analysis, and scalable machine learning. A must-have in every data professionalâ€™s toolkit. 

Explore Easy Steps to Follow for a Great Data Science Career the Python Way.

https://preview.redd.it/mmnoetbollff1.jpg?width=1280&format=pjpg&auto=webp&s=a4d1309e8f5496d4e6a280b34c189fac1fabef3f

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1mbdbpz/python_for_data_science_career/,r_1mbdbpz,,,
r_1mb342x,reddit,Data-Sleek,2025-07-28T01:16:59+00:00,"How do you decide between a database, data lake, data warehouse, or lakehouse?
Iâ€™ve seen a lot of confusion around these, so hereâ€™s a breakdown Iâ€™ve found helpful:

AÂ databaseÂ stores the current data needed to operate an app.                                                                     AÂ data warehouseÂ holds current and historical data from multiple systems in fixed schemas.                    AÂ data lakeÂ stores current and historical data in raw form.                                                                              AÂ lakehouseÂ combines bothâ€”letting raw and refined data coexist in one platform without needing to move it between systems.

Theyâ€™re often used togetherâ€”but not interchangeably.

How does your team use them? Do you treat them differently or build around a unified model?",bigdata,3,https://www.reddit.com/r/bigdata/comments/1mb342x/how_do_you_decide_between_a_database_data_lake/,r_1mb342x,,,
r_1m9rkft,reddit,sharmaniti437,2025-07-26T12:00:19+00:00,"Python for Data Science Career
Python, the no.1 programming language worldwide- makes data science intuitive, efficient, and scalable. Whether itâ€™s cleaning data or training models, Python gets it done. Python is the backbone of modern data scienceâ€”enabling clean code, rapid analysis, and scalable machine learning. A must-have in every data professionalâ€™s toolkit. 

Explore Easy Steps to Follow for a Great Data Science Career the Python Way.

https://reddit.com/link/1m9rkft/video/7x6l1cjkk7ff1/player

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1m9rkft/python_for_data_science_career/,r_1m9rkft,,,
r_1m8z9iz,reddit,sharmaniti437,2025-07-25T13:31:58+00:00,"Certified Lead Data Scientist (CLDS)
You speak Python- Now speak strategy! Become a certified data science leader with USDSI's CLDS and go from model-builder to decision-maker. A certified data science leader drives innovation, manages teams, and aligns AI with business goals. Itâ€™s more than mere skillsâ€”itâ€™s influence!

https://reddit.com/link/1m8z9iz/video/lsks0rpzv0ff1/player

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m8z9iz/certified_lead_data_scientist_clds/,r_1m8z9iz,,,
r_1m8tb7u,reddit,Original_Poetry_8563,2025-07-25T07:59:30+00:00,"Curious: What are the new AI-embedded features that you are actually using in platforms like Snowflake, Dbt, and Databricks?
Features that are coming on strong (with an AI overhaul) seems to be ignored compared to the ones where AI is embedded deep within the feature's core value. For example, instead of having a strong AI features where data profiling is declarative (black box) vs. data profiling where users are prompted during the regular process they are used to. The latter seems more viable at this point, thoughts?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m8tb7u/curious_what_are_the_new_aiembedded_features_that/,r_1m8tb7u,,,
r_1m82xou,reddit,Plastic_Artichoke832,2025-07-24T12:22:43+00:00,"[Beam/Flink] One-off batch: 1B 1024-dim embeddings â†’ 1M-vector flat FAISS shards â€“ is this the wrong tool?

Hey all, Iâ€™m digging through 1 billion 1024-dim embeddings in thousands of Parquet files on GCS and want to spit out 1 million-vector â€œtrueâ€ Flat FAISS shards (no quantization, exact KNN) for later use. Weâ€™ve got n1-highmem-64 workers, parallelism=1 for the batched stream, and 16 GB bundle memoryâ€”so resources arenâ€™t the bottleneck.

Iâ€™m also seeing inconsistent batch sizes (sometimes way under 1 M), even after trying both GroupIntoBatches and BatchElements.

High-level pipeline (pseudo):

// Beam / Flink style
ReadParquet(""gs://â€¦/*.parquet"")
  â†“
Batch(1_000_000 vectors)       // but often yields â‰ 1M
  â†“
BuildFlatFAISSShard(batch)     // IndexFlat + IDMap
  â†“
WriteShardToGCS(""gs://â€¦/shards/â€¦index"")

Question: Is it crazy to use Beam/Flink for this â€œbuild-sharded objectâ€ job at this scale? Any pitfalls or better patterns I should consider to get reliable 1 M-vector batches? Thanks!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m82xou/beamflink_oneoff_batch_1b_1024dim_embeddings/,r_1m82xou,,,
r_1m7f57g,reddit,eb0373284,2025-07-23T17:15:26+00:00,"What are the biggest challenges or pain points you've faced while working with Apache NiFi or deploying it in production?
I'm curious to hear about all kinds of issuesâ€”whether it's related to scaling, maintenance, cluster management, security, upgrades, or even everyday workflow design.

  
Feel free to share any lessons learned, tips, or workarounds too!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m7f57g/what_are_the_biggest_challenges_or_pain_points/,r_1m7f57g,,,
r_1m7byp1,reddit,iamredit,2025-07-23T15:15:01+00:00,"Custom Big Data Applications Development Services in USA
Get expert big data development services in the USA. We build scalable big data applications, including mobile big data solutions. Start your project today!",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m7byp1/custom_big_data_applications_development_services/,r_1m7byp1,,,
r_1m792hw,reddit,sharmaniti437,2025-07-23T13:19:29+00:00,"Global Salary Trends for Data Science Professionals
The **data science world** is booming as industries globally rely more on AI, machine learning, and cloud analytics. Fortune Business Insights predicts the global data analytics market will climb from USD 64.99 billion in 2024 to USD 82.23 billion in 2025, and then continue towards a projected USD 402.7 billion by 2032. In addition, McKinsey suggests that 78% of organizations now use AI for at least one business function, which increased from 72% in early 2024.

As generative AI and cloud-based analytics become further entrenched, the need for talented data professionals increases. This blog examines how **data science salaries** compare across the globe today.

# United States

Average salary in the United States is USD 124,000. In 2025, salary offerings for **data science specialists** in the United States remain at the top. The average base salary of a data scientist in the U.S. is currently approximately $157,000. Compensation almost always exceeds $180,000â€“200,000 in the major areas like San Francisco, New York City, and Seattle.

# Canada

Average salary in the country is USD 98,000. In Canada, the demand for data science practitioners has been steadily increasing, especially in Toronto, Vancouver, and Montreal. In 2025, the average salary for data scientists is between CAD 95,000 to 130,000 or roughly USD 74,000â€“100,000.

Salaries are influenced by firm size, complexity of role, and geographic demand. Junior analysts start at a lower salary while lead data scientists/AI engineers earn quite a bit more.

# United Kingdom

The UK still ranks highly in data-driven industries like finance, healthcare analytics, and AI startups. The common salary for data science has a range of USD 60,000 to USD 105,000 in 2025, with higher salaries in larger tech hubs like London or Cambridge.

**Germany**[](https://365datascience.com/career-advice/data-science-salaries-around-the-world/)Germanyâ€™s considerable investment in industrial and AI policy positions it as one of the trending locations for **data science jobs**. In cities including Berlin and Munich, salaries are generally higher, especially in regard to manufacturing analytics and enterprise AI; average salaries are roughly in the range of USD 70,000 to USD 76,000.

# Netherlands

The Netherlands is a top EU tech hub, with high salaries reflecting demand in fintech, logistics, and AI healthcare. Salaries can rise to USD 80,000-100,000+ in urban areas like Amsterdam. The employability factor is also high with EU work rights and exceptional ML/cloud skills.

# India

India remains an important data analytics player in the world based on its IT services, startup ecosystem, and offshore analytics operations. The average data scientist's salary is USD 21,000 in 2025; the entry job starts around USD 10,000â€“12,000, and senior data scientists in top companies can get as high as USD 35,000â€“40,000.

# Australia

Australia has one of the most lucrative **data science salary** markets in the Asia-Pacific region. In 2025, the average data science salary is USD 98,000, with **data scientists salary** in cities like Sydney and Melbourne is up to USD 120,000+ in particular fields such as finance, healthtech, and government.

# Singapore

Singapore is Southeast Asia's hub for data science, with demand rising in finance, fintech, and RegTech. The employment pass norms also favor local hiring. Mid-level roles command up to USD 90,000, and senior experts reach USD 120,000 with the demand created by AI adoption and strong government backing.

# South Africa

South Africa has begun establishing itself as a significant data science market for the African continent, with growth primarily stimulated by the telecom, banking, and retail sectors. A typical data scientist makes around USD 34,000, with experienced professionals often clearing over USD 45,000, especially in urban tech centers including Johannesburg and Cape Town.

**Note**: The salaries for the above countries are taken from Glassdoor and PayScale 2025.

# Data Science Certifications That Help in Multiplying Your Salary

One of the constants driving pay increases all over the globe in todayâ€™s landscape is the right mix of certifications. Data engineering certifications are at an all-time high in terms of salary. Some of the **top data science certifications** include:

â—Â Â **Certified Lead Data Scientistâ„¢ by USDSIÂ®** is an industry-specific certification for those professionals who lead data teams on a large scale.

â—Â Â Â **Harvard Extension School Certificate in Data Science** is great for those who want an Ivy League degree with vast implications of applicability.

â—Â Â Â **The University of Pennsylvania's Applied Data Science Certificate** is issued by the School of Engineering and Applied Science with emphasis on applied machine learning and data analytics.

# Conclusion

Data science isn't just a well-paying industry; it's a global currency of innovation. To have a six-figure salary in the West or the ability to scale skills in a fast-growing marketplace today means being future-proof. Upskilling through [data science certifications](https://www.usdsi.org/data-science-certifications), pursuing high-demand global or hybrid roles are no longer options. They are an avenue for managing careers in the data age.",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m792hw/global_salary_trends_for_data_science/,r_1m792hw,,,
r_1m5oa7e,reddit,Original_Poetry_8563,2025-07-21T16:55:10+00:00,"A New Era for Data Professionals
>There's a lot of hype around AI, specializing in web app prototyping, but what about our beloved data world?

>You open LinkedIn and see the usual posts:

>***BREAKING****: OpenAI releases new prompting guides*  
***LATEST****: Anthropic/DeepSeek/Google launches the*Â ***greatest***Â *model ever*  
*â€œI created this 892-step n8n workflow to read all my emails. Comment on this post so you can ignore yours too!â€*

>You get the point: AI is everywhere, but I don't think weâ€™re fully grasping where it's heading. We're automating both content creationÂ *and*Â consumption. We're generating LinkedIn posts with AI and summarizing them using AI because there's simply too much content to process.",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m5oa7e/a_new_era_for_data_professionals/,r_1m5oa7e,,,
r_1m5nrp2,reddit,Outhere9977,2025-07-21T16:36:16+00:00,"Webinar on relational graph transformers w/ Stanford Professor Jure Leskovec & Matthias Fey (PyTorch Geometric)

 Saw this and thought it might be cool to share! Free webinar on relational graph transformers happening July 23 at 10am PT.

This is being presented by Stanford prof. Jure Leskovec, who co-created graph neural networks, and Matthias Fey, the creator of PyG. 

The webinar will teach you how to use graph transformers (specifically their relational foundation model, by the looks) in order to make instant predictions from your relational data. Thereâ€™s a demo, live Q&A, etc. 

Thought the community may be interested in it. You can sign up here: https://zoom.us/webinar/register/8017526048490/WN_1QYBmt06TdqJCg07doQ_0A#/registration  

",bigdata,7,https://www.reddit.com/r/bigdata/comments/1m5nrp2/webinar_on_relational_graph_transformers_w/,r_1m5nrp2,,,
r_1m5jao9,reddit,sharmaniti437,2025-07-21T13:44:22+00:00,"AI Showdown: DeepSeek vs. ChatGPT
As AI reshapes the data science landscape, two powerful contenders emerge: DeepSeek, the domain-specific disruptor, and ChatGPT, the versatile conversationalist. From performance and customization to real-world applications, this showdown dives deep into their capabilities.

Which one aligns with your data goals? Discover the winner based on your needs. 

https://preview.redd.it/ax4z3ovke8ef1.jpg?width=1081&format=pjpg&auto=webp&s=777ea017430a169054addfe7cb252d96f928601f

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m5jao9/ai_showdown_deepseek_vs_chatgpt/,r_1m5jao9,,,
r_1m5a47n,reddit,bigdataengineer4life,2025-07-21T05:04:42+00:00,"ðŸ“Š Clickstream Behavior Analysis with Dashboard using Kafka, Spark Streaming, MySQL, and Zeppelin!
ðŸš€ New Real-Time Project Alert for Free!

ðŸ“Š Clickstream Behavior Analysis with Dashboard

Track & analyze user activity in real time using Kafka, Spark Streaming, MySQL, and Zeppelin! ðŸ”¥

ðŸ“Œ What Youâ€™ll Learn:

âœ… Simulate user click events with Java

âœ… Stream data using Apache Kafka

âœ… Process events in real-time with Spark Scala

âœ… Store & query in MySQL

âœ… Build dashboards in Apache Zeppelin ðŸ§ 

ðŸŽ¥ Watch the 3-Part Series Now:

ðŸ”¹ Part 1: Clickstream Behavior Analysis (Part 1)

ðŸ“½ [https://youtu.be/jj4Lzvm6pzs](https://youtu.be/jj4Lzvm6pzs)

ðŸ”¹ Part 2: Clickstream Behavior Analysis (Part 2)

ðŸ“½ [https://youtu.be/FWCnWErarsM](https://youtu.be/FWCnWErarsM)

ðŸ”¹ Part 3: Clickstream Behavior Analysis (Part 3)

ðŸ“½ [https://youtu.be/SPgdJZR7rHk](https://youtu.be/SPgdJZR7rHk)

This is perfect for Data Engineers, Big Data learners, and anyone wanting hands-on experience in streaming analytics.

ðŸ“¡ Try it, tweak it, and track real-time behaviors like a pro!

ðŸ’¬ Let us know if you'd like the full source code!",bigdata,3,https://www.reddit.com/r/bigdata/comments/1m5a47n/clickstream_behavior_analysis_with_dashboard/,r_1m5a47n,,,
r_1m52v10,reddit,eczachly,2025-07-20T23:04:16+00:00,"Why do Delta, Iceberg, and Hudi all feel the same?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m52v10/why_do_delta_iceberg_and_hudi_all_feel_the_same/,r_1m52v10,,,
r_1m51bmb,reddit,warleyco96,2025-07-20T21:57:21+00:00,"Architecture Dilemma: DLT vs. Custom Framework for 300+ Real-Time Tables on Databricks
Hey everyone,

I'd love to get your opinion and feedback on a large-scale architecture challenge.

**Scenario:**Â I'm designing a near-real-time data platform for over 300 tables, with the constraint of usingÂ **only the native Databricks ecosystem**Â (no external tools).

**The Core Dilemma:**Â I'm trying to decide between usingÂ **Delta Live Tables (DLT)**Â and building aÂ **Custom Framework**.

My initial evaluation of DLT suggests it might struggle with some of our critical data manipulation requirements, such as:

1. **More Options of Data Updating on Silver and Gold tables:**
   1. **Full Loads:**Â I haven't found a native way to do a Full/Overwrite load in Silver. I can only add a TRUNCATE as an operation at position 0, simulating a CDC. In some scenarios, it's necessary for the load to always be full/overwrite.
   2. **Partial/Block Merges:**Â The ability to perform complex partial updates, like deleting a block of records based on a business key and then inserting the new block (no primary-key at row level).
2. **Merge for specific columns:**Â The environment tables have metadata columns used for lineage and auditing. Columns such as first\_load\_author and update\_author, first\_load\_author\_external\_id and update\_author\_external\_id, first\_load\_transient\_file, update\_load\_transient\_file, first\_load\_timestamp, and update\_timestamp. For incremental tables, for existing records, only the update columns should be updated. The first\_load columns should not be changed.

My perception is that DLT doesn't easily offer this level of granular control. Am I mistaken here? I'm new to this resource. I couldn't find any real-world examples for product scenarios, just some basic educational examples.

On the other hand, I considered a model with one continuous stream per table but quickly ran into the \~145 execution context limit per cluster, making that approach unfeasible.

**Current Proposal:**Â My current proposed solution is the reactive architecture shown in the image below: a central ""router"" detects new files and, via the Databricks Jobs API, triggers small, ephemeral jobs (usingÂ `AvailableNow`) for each data object.

https://preview.redd.it/5hllh39np3ef1.png?width=5661&format=png&auto=webp&s=6adb6cbe4a30d9fc538d28a47b40474d9224321c

The architecture above illustrates the Oracle source with AWS DMS. This scenario is simple because it's CDC. However, there's user input in files, SharePoint, Google Docs, TXT files, file shares, legacy system exports, and third-party system exports. These are the most complex writing scenarios that I couldn't solve with DLT, as mentioned at the beginning, because they aren't CDC, some don't have a key, and some have partial merges (delete + insert).

**My Question for the Community:**Â What are your thoughts on this event-driven pattern? Is it a robust and scalable solution for this scenario, or is there a simpler or more efficient approach within the Databricks ecosystem that I might be overlooking?

Thanks in advance for any insights or experiences you can share!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m51bmb/architecture_dilemma_dlt_vs_custom_framework_for/,r_1m51bmb,,,
r_1m3nv3z,reddit,bigdataengineer4life,2025-07-19T05:04:15+00:00,Explain LLAP (Live Long and Process) and its benefits in Hive,bigdata,1,https://www.reddit.com/r/bigdata/comments/1m3nv3z/explain_llap_live_long_and_process_and_its/,r_1m3nv3z,,,
r_1m3jyaz,reddit,wadyta,2025-07-19T01:35:59+00:00,Should sexual education be mandatory from primary school?,bigdata,0,https://www.reddit.com/r/bigdata/comments/1m3jyaz/should_sexual_education_be_mandatory_from_primary/,r_1m3jyaz,,,
r_1m244mz,reddit,ja_migori,2025-07-17T10:37:17+00:00,"Fundraiser for a surgical procedure
Hi everyone,

My name is Alex, and Iâ€™m a student currently facing the biggest challenge of my life. On **March 27, 2025**, I was diagnosed with **appendicitis**. My doctors have told me that I urgently need surgery to remove my appendix. Without it, my life is at serious risk.

Unfortunately, the surgery costs **$5,000**, and as a student, I simply cannot afford it. Iâ€™ve tried to raise the money on my own, but my health situation prevents me from working, and my family canâ€™t cover this expense either.

I am reaching out with all humility to ask for your support. **Every donation, no matter how small, will bring me closer to getting the surgery that could save my life.** Your kindness will not only help cover my hospital and surgical costs but will also give me hope to continue my education and future.

Please consider donating and sharing this with your friends and networks. Your help truly means the world to me.

Thank you so much for your compassion and support.

My PayPal email address is [otienoalex16@yahoo.com](mailto:otienoalex16@yahoo.com)",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m244mz/fundraiser_for_a_surgical_procedure/,r_1m244mz,,,
r_1m1z61i,reddit,bigdataengineer4life,2025-07-17T05:24:04+00:00,How do you handle Slowly Changing Dimensions (SCD) in Hive,bigdata,1,https://www.reddit.com/r/bigdata/comments/1m1z61i/how_do_you_handle_slowly_changing_dimensions_scd/,r_1m1z61i,,,
r_1m1uvpn,reddit,Santhu_477,2025-07-17T01:41:50+00:00,"Productionizing Dead Letter Queues in PySpark Streaming Pipelines â€“ Part 2 (Medium Article)
Hey folks ðŸ‘‹

I just published Part 2 of my Medium series on handling bad records in PySpark streaming pipelines using Dead Letter Queues (DLQs).  
In this follow-up, I dive deeper into production-grade patterns like:

* Schema-agnostic DLQ storage
* Reprocessing strategies with retry logic
* Observability, tagging, and metrics
* Partitioning, TTL, and DLQ governance best practices

This post is aimed at fellow data engineers building real-time or near-real-time streaming pipelines on Spark/Delta Lake. Would love your thoughts, feedback, or tips on whatâ€™s worked for you in production!

ðŸ”— Read it here:  
[Here](https://medium.com/@santhoshkumarv/productionizing-dead-letter-queues-in-pyspark-streaming-pipelines-part-2-fd228fb99fe5)

Also linking [Part 1 here](https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29) in case you missed it.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m1uvpn/productionizing_dead_letter_queues_in_pyspark/,r_1m1uvpn,,,
r_1m1lwq3,reddit,Edoruin_1,2025-07-16T19:22:24+00:00,"What do you think about the The Data Warehouse Toolkit Orreily book
I'm interesting in read this book, and I want to know how much good is the book.

 what do you think about this book?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m1lwq3/what_do_you_think_about_the_the_data_warehouse/,r_1m1lwq3,,,
r_1m1cf2b,reddit,sharmaniti437,2025-07-16T13:20:15+00:00,"Learn from the Best: 15 Cybersecurity Experts to Watch
Cybercrime has now become one of the largest threats to the world's economy. According to Cybersecurity Ventures, global cybercrimes will grow at an annual rate of 15%, which will reach USD 10.5 trillion per annum by the end of 2025. On top of these staggering losses in monetary value, cybercrime could disrupt businesses, cause difficulties with reputational damage, and lead to a loss of consumer trust.

In the international climate we are in, it is critically important to stay up to date with the volume of new threats emerging. There are many different avenues for keeping up to date with cybersecurity, whether you are considering pursuing a **career in cybersecurity**, acquiring **cybersecurity certifications,** or already working in cybersecurity, following thought leaders can give you insight as new threats or best practices arise.

In this blog, we feature 15 experts in cybersecurity who are not only the leaders currently guiding the cybersecurity practice, but they are also providing insights and research that will shape the field as we move forward.

# 1. Brian Krebs

Brian is a former journalist for The Washington Post and the author of Krebs on Security, a blog known for detailed investigations into cybercrime, breaches, and online safety.  
Â **X:** [u/briankrebs](https://x.com/briankrebs)

# 2. Graham Cluley

Graham is an industry veteran and co-host of the podcast Smashing Security. He offers insightful commentary on malware, ransomware, and the weird world of infosec. He delivers with humor and clarity, making even security news easier to understand.

# 3. Bruce Schneier

Bruce is known worldwide as a ""security guru,"" a cryptographer, author, and speaker focusing on technical security, privacy, and public policy. He maintains a respected blog called Schneier on Security.  
Â [Website](https://www.schneier.com)

# 4. Mikko Hypponen

Mikko is the Chief Research Officer for WithSecure and a global speaker on topics related to malware, surveillance, and internet safety. His influence extends beyond the realm of tech and truly helps shape the level of awareness for cybersecurity.  
Â **X:** [@mikko](https://x.com/mikko)

# 5. Eugene Kaspersky

The founder and CEO of Kaspersky Lab, Eugene, is one of the biggest advocates for global cybersecurity. Kaspersky Lab's threat intelligence and research teams have been instrumental in uncovering some of the biggest cyber-espionage efforts around the world.  
Â **X**[**:**](https://x.com/e_kaspersky)[ @e\_kaspersky](https://x.com/e_kaspersky)

# 6. Troy Hunt

Troy is known as the creator of Have I Been Pwned, a breach notification service used worldwide. He writes and speaks regularly about password security, data protection, and best practices for developers.  
Â **X:**[ @troyhunt](https://x.com/troyhunt)

# 7. Robert M. Lee

Robert, a top authority in industrial control system (ICS) cybersecurity, is the CEO of Dragos and focuses on securing critical infrastructure such as power grids and manufacturing systems.  
Â **X:** [@RobertMLee](https://x.com/RobertMLee)

# 8. Katie Moussouris

Katie is the founder of Luta Security and a pioneer in bug bounty and vulnerability disclosure programs, and has worked with Microsoft and multiple governments to create secure systems.  
Â **X:** [@k8em0](https://x.com/k8em0)

# 9. Chris Krebs

Chris served as the inaugural director of the U.S. Cybersecurity and Infrastructure Security Agency (CISA). He is widely recognized for his leadership role advocating for the defense of democratic infrastructure/election security.  
Â **X:** [@C\_C\_Krebs](https://x.com/C_C_Krebs)

# 10. Jen Easterly

As the current Director of CISA, Jen is one of the most powerful cybersecurity leaders today. Her focus is on public-private collaboration and national cyber resilience.  
Â [LinkedIn](https://www.linkedin.com/in/jen-easterly)

# 11. Jayson E. Street

Jayson is a reputable speaker and penetration tester whose live demos expose actual physical and digital vulnerabilities. His energy and storytelling bring interest to security awareness and education.  
Â  **X:**[ ](https://x.com/jaysonstreet)[@jaysonstreet](https://x.com/jaysonstreet)

# 12. Alexis Ahmed

Alexis is the founder of HackerSploit, a free cybersecurity training platform. His educational YouTube channel features approachable content related to penetration testing, Linux, and ethical hacking.

Â **X:** [@HackerSploit](https://x.com/HackerSploit)

# 13. Loi Liang Yang

Loi is an educator in the field of cybersecurity and a YouTuber who is known for deconstructing confusing technical subjects through hands-on practical demonstrations and short tutorials on tools, exploits, and ethical hacking.  
Â **X:** [@loiliangyang](https://x.com/loiliangyang)

# 14. Eva Galperin

Eva is Director of Cybersecurity at the Electronic Frontier Foundation (EFF). She is an ardent privacy advocate who has worked to protect activists, journalists, and marginalized communities from digital surveillance.  
Â  **X:** [@evacide](https://x.com/evacide)Â 

# 15. Tiffany Rad

Tiffany combines cybersecurity with law and policy. She has spoken at large events like DEF CON and Black Hat, and her work involves everything from automotive hacking to international cybersecurity law.  
Â [Website](https://www.tiffanyrad.net)

# Why Following These Experts Matters

Whether you are gearing up for the premier [cybersecurity certifications](https://www.uscsinstitute.org/cybersecurity-certifications), such as CCCâ„¢ and CSCSâ„¢ by USCSI, or CISSP, CISM, or developing your identity as a **cybersecurity specialist**, the importance of following real-world practitioners cannot be overstated. These practitioners:

â—Â Â Â Â Â Â  Share relevant threat intelligence

â—Â Â Â Â Â Â  Explain very complex security problems

â—Â Â Â Â Â Â  Provide useful tools and career advice

â—Â Â Â Â Â Â  Raise awareness around privacy and digital rights

Many of them may also participate in policy changes and global security conversations, and they bring a combined experience of decades of everything from nation-state attacks to corporate data breaches.

# Conclusion

There is no better way to develop a career in cybersecurity than learning from world-class **cybersecurity experts**. Their insights are so much deeper than the headlines they receive; they offer action-oriented recommendations.

Â As you advance your career in cybersecurity, combining world-class expertise with the **best cybersecurity certification** will provide you with a competitive advantage as you develop from an interest into impact.

Â Stay curious. Stay educated. And be prepared for what comes next.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1m1cf2b/learn_from_the_best_15_cybersecurity_experts_to/,r_1m1cf2b,,,
r_1m16ps7,reddit,sharmaniti437,2025-07-16T07:56:18+00:00,"The Evolution of AI-Driven Data Science
From predictive modeling to generative analytics, AI has transformed data science into a powerhouse of automation, speed, and precision. 

Discover the evolution of AI-Driven Data Science, the rise of data mining and machine learning, and explore 

https://preview.redd.it/tcarezpwz6df1.jpg?width=1080&format=pjpg&auto=webp&s=6fe7256787f96583c0018fe5a818183004ec482c

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1m16ps7/the_evolution_of_aidriven_data_science/,r_1m16ps7,,,
r_1m0jr6g,reddit,Still-Butterfly-3669,2025-07-15T14:48:37+00:00,"Difference between BI and Product Analytics
I heard a lot of times that people are misunderstand which is which and they are looking for a solution for their data but in the wrong way. In my opinion I made a quite detailed comparison, and I hope that it would be helpful for some of you, link in the comments.

1 sentence conclusion who is lazy to ready:

Business Intelligence helps you understand overall business performance by aggregating historical data, while Product Analytics zooms in on real-time user behavior to optimize the product experience.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m0jr6g/difference_between_bi_and_product_analytics/,r_1m0jr6g,,,
r_1m0ca19,reddit,sharmaniti437,2025-07-15T08:24:52+00:00,"Decoding Machine Learning Skills for Aspiring Data Scientists
In todayâ€™s data-driven world, all business verticals use raw data to extract actionable insights. The insights help data scientists, business analysts, and stakeholders identify and solve business problems, improve products and services, and enhance customer satisfaction to drive revenue.Â 

This is where data science and the machine learning fields come into play. Data science and machine learning are transforming industries by redefining how companies understand business and their users.

At this juncture, early data science and machine learning professionals must understand how data science and ML work together. This blog explains the role of machine learning in data science and encourages professionals to stay ahead in the competitive global job market.

# Let us address the key questions here:

* What is Data Science?
* What is Machine Learning \[ML\]?
* How are machine learning and data science related?
* How to understand the roadmap of ML in data science
* What are ML use cases in data science?
* How can data scientistsâ€™ future-proof their careers?

# What is data science?

Researchers define data science as â€œan interdisciplinary field. It builds on statistics, informatics, computing, communication, management, and sociology to transform data into actionable insights.â€

The data science formula is given as

Data science = Statistics + Informatics + Computing + Communication + Sociology + Management | data + environment + thinking, where â€œ|â€ means â€œconditional on.â€

# What is machine learning?

It is a subset of Artificial Intelligence. Researchers interpret machine learning as â€œthe field of intersecting computer science, mathematics, and Statistics, used to identify patterns, recognize behaviors, and make decisions from data with minimal human intervention.â€

# Data Science vs Machine Learning

||
||
|**Aspect**|**Data Science**|**Machine Learning**|
|**Definition**|This field focuses on extracting insights from data|It is a subfield of AI focused on designing algorithms that learn from data and make predictions or decisions|
|**Aim**|To analyze and interpret data|To enable systems to learn patterns from data and automate tasks.|
|**Data Handling**|Â Handles raw and big data.|Uses structured data for training models.|
|**Techniques used**|Statistical analysis|Algorithms|
|**Skills Required**|Statistical analysis, data wrangling, and programming.|Programming, algorithm design, and mathematical skills.|
|**Key Processes**|Data exploration, cleaning, visualization, and reporting.|Model training, model evaluation, and deployment.|

# Â How are Machine Learning and Data Science related?

Machine learning and data science are intertwined. Machine learning reduces human effort by empowering data science. It automates data collection, analysis, engineering, training, evaluation, and prediction.

**Machine learning for data scientists** is important because:

* Research and software skills enable them to apply, develop, and build accurate models.
* **Data science skills** allow them to implement complex models: For example, neural networks, random forests, and decision trees

This, in turn, helps to solve a business problem or improve a specific business process.

# The Road Map of Machine Learning in Data Science

ML comprises a set of algorithms that are used for analyzing data chunks. It processes data, builds a model, and makes real-time predictions without human intervention.

Here is a schematic representation to understand how machine learning algorithms are used in the data science life cycle.

https://preview.redd.it/3ie3kvfpzzcf1.jpg?width=735&format=pjpg&auto=webp&s=914da5951af617aa0a939932a8a924e3b7ee40e7

Figure 1. How Machine Learning Algorithms are Used in Data Science Life Cycle: A Schematic Representation

**Role of Python**: Pythonâ€™s libraries, NumPy and Scikit-learn, are used for data analysis. Its frameworks, TensorFlow and Apache Spark, help to visualize data**.**Â 

**Exploratory Data Analysis \[EDA\]:** Plotting in EDA comprises charts, histograms, heat maps, or scatter plots. Data plotting enables professionals to detect missing data, duplicate data, and irrelevant data and identify patterns and insights.

**Feature Engineering:** It refers to the extraction of features from data and transforming them into formats suitable for machine learning algorithms**.**

**Choosing ML Algorithms:** The dataset is classified into major categories like Classification, Regression, Clustering, and Time Series Analysis. ML algorithms are chosen accordingly**.**

**ML Deployment:** Deployment is necessary to understand operational value. The model is deployed in a suitable live environment through the API. The model is continuously monitored for uninterrupted performance.

# What are ML use cases in Data Science?

Machine learning is applied in every industrial sector. Some of the popular real-life applications include:

* Common people use Google Maps, Alexa, and Microsoft Cortana.
* Banks use machine learning to flag suspicious transactions.
* Voice assistants leverage ML to respond to queries.
* E-commerce uses recommendation engines to suggest recommendations to users.
* Entertainment channels use recommendation engines to suggest content.

To summarize, data science and machine learning are used to analyze vast amounts of data. **Senior data scientists** and Machine Learning Engineers should be equipped with the in-depth skills to thrive in the data-driven world.

# How to future-proof your career as a data scientist?

Recent developments in the data science and machine learning disciplines call for cross-functional teams having a multidisciplinary approach to solve business problems. Data scientists must upskill through courses from renowned institutions and organizations.Â 

A few of the [top data science certifications](https://www.usdsi.org/data-science-certifications) are mentioned here.

1. Certified Senior Data Scientist (CSDSâ„¢) from **United States Data Science Institute (USDSIÂ®)**

2. Professional Certificate in Data Science from **Harvard University**

3. Data Science Certificate from **Cornell SC Johnson College of Business**

4. Online Certificate in Data Science from **Georgetown University**

5. Data Science Certificate from **UCLA Extension**

Choosing the right **data science course** boosts credibility in the data-driven world. With the right tools, techniques, and skills, data scientists can lead innovation across industries.

Â ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1m0ca19/decoding_machine_learning_skills_for_aspiring/,r_1m0ca19,,,
r_1lyuise,reddit,unknown,2025-07-13T14:44:26+00:00,"Jobs as a big data engineer fresher
I am a 7th sem student I've just finished my big data course from basics to advanced with a two deployed projects mostly around sentiment analysis or customer segmentation which I think are very basic projects. My college placements will start in a month, can someone give some good project ideas which showcases most of my big data skills and any guide like how to get a good placement, what should I focus more on?",bigdata,4,https://www.reddit.com/r/bigdata/comments/1lyuise/jobs_as_a_big_data_engineer_fresher/,r_1lyuise,,,
r_1lyns1w,reddit,foorilla,2025-07-13T08:26:17+00:00,ðŸ“° Stay up to date with everything happening in the tech hiring AND media space - daily into your inbox or via RSS with foorilla.com ðŸš€,bigdata,2,https://www.reddit.com/r/bigdata/comments/1lyns1w/stay_up_to_date_with_everything_happening_in_the/,r_1lyns1w,,,
r_1lyczt9,reddit,AdFantastic8679,2025-07-12T22:31:28+00:00,"I have problem with hadoop spark cluster.
Let me explain what to do :

So we are doing a project where we connect inside docker swarm with tailscale and we get inside hadoop. So this hadoop was pulled from our prof docker hub 

 i will give links:

sudo docker pull binhvd/spark-cluster:0.17
git clone https://github.com/binhvd/Data-Engineer-1.git


Problem:

So I am the master-node i set up everything with docker swarm and gave the tokens to others

Others joined my swarm using the token and I did docker node ls in my master node  and it showed everything.

But after this we connected to 
master-node:9870
Hadoop ui


These are the finding from both master node and worker node.

Key findings from the master node logs:

Connection refused to master-node/127.0.1.1:9000: This is the same connection refused error we saw in the worker logs, but it's happening within the master-node container itself! This strongly suggests that the DataNode process running on the master container is trying to connect to the NameNode on the master container via the loopback interface (127.0.1.1) and is failing initially.

Problem connecting to server: master-node/127.0.1.1:9000: Confirms the persistent connection issue for the DataNode on the master trying to reach its own NameNode.

Successfully registered with NN and Successfully sent block report: Despite the initial failures, it eventually does connect and register. This implies the NameNode eventually starts and listens on port 9000, but perhaps with a delay, or the DataNode tries to connect too early.

What this means for your setup:

NameNode is likely running: The fact that the DataNode on the master eventually registered with the NameNode indicates that the NameNode process is successfully starting and listening on port 9000 inside the master container.

The 127.0.1.1 issue is pervasive: Both the DataNode on the master and the DataNode on the worker are experiencing connection issues when trying to resolve master-node to an internal loopback address or are confused by it. The worker's DataNode is using the Tailscale IP (100.93.159.11), but still failing to connect, which suggests either a firewall issue or the NameNode isn't listening on that external interface, or the NameNode is also confused by its own internal 127.0.1.1 binding.



Now can you guys explain what is wrong any more info you want ask me in comments.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1lyczt9/i_have_problem_with_hadoop_spark_cluster/,r_1lyczt9,,,
r_1lx25ur,reddit,Shawn-Yang25,2025-07-11T09:09:10+00:00,Apache Fory Serialization Framework 0.11.2 Released,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lx25ur/apache_fory_serialization_framework_0112_released/,r_1lx25ur,,,
r_1lx0r01,reddit,bigdataengineer4life,2025-07-11T07:33:08+00:00,"Big data Hadoop and Spark Analytics Projects (End to End)
Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report â€“ Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",bigdata,6,https://www.reddit.com/r/bigdata/comments/1lx0r01/big_data_hadoop_and_spark_analytics_projects_end/,r_1lx0r01,,,
r_1lwiy44,reddit,hammerspace-inc,2025-07-10T17:38:09+00:00,Hammerspace CEO David Flynn to speak at Reuters Momentum AI 2025,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lwiy44/hammerspace_ceo_david_flynn_to_speak_at_reuters/,r_1lwiy44,,,
r_1lwakxb,reddit,PracticalMastodon215,2025-07-10T11:43:14+00:00,Migrating from Cloudera CFM to DFM? Claim: 70% cost savings + true NiFi freedom. Valid or too good to be true?,bigdata,4,https://www.reddit.com/r/bigdata/comments/1lwakxb/migrating_from_cloudera_cfm_to_dfm_claim_70_cost/,r_1lwakxb,,,
r_1lw7tlp,reddit,thumbsdrivesmecrazy,2025-07-10T08:53:00+00:00,"From Big Data to Heavy Data: Rethinking the AI Stack - r/DataChain
The article discusses the evolution of data types in the AI era, and introducing the concept of ""heavy data"" - large, unstructured, and multimodal data (such as video, audio, PDFs, and images) that reside in object storage and cannot be queried using traditional SQL tools: [From Big Data to Heavy Data: Rethinking the AI Stack - r/DataChain](https://www.reddit.com/r/datachain/comments/1luiv07/from_big_data_to_heavy_data_rethinking_the_ai/)

It also explains that to make heavy data AI-ready, organizations need to build multimodal pipelines (the approach implemented in DataChain to process, curate, and version large volumes of unstructured data using a Python-centric framework):

* process raw files (e.g., splitting videos into clips, summarizing documents);
* extract structured outputs (summaries, tags, embeddings);
* store these in a reusable format.",bigdata,3,https://www.reddit.com/r/bigdata/comments/1lw7tlp/from_big_data_to_heavy_data_rethinking_the_ai/,r_1lw7tlp,,,
r_1lvlcqe,reddit,Fun_Accountant_9415,2025-07-09T15:14:09+00:00,"Any Advice
Big Data student seeking learning recommendations what should I focus on?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1lvlcqe/any_advice/,r_1lvlcqe,,,
r_1lvae95,reddit,bigdataengineer4life,2025-07-09T05:07:32+00:00,Apache Zeppelin â€“ Big Data Visualization Tool with 2 Caption Projects,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lvae95/apache_zeppelin_big_data_visualization_tool_with/,r_1lvae95,,,
r_1lv1490,reddit,Madddieeeeee,2025-07-08T21:36:59+00:00,"How to sync data from multiple sources without writing custom scripts?
Our team is struggling with integrating data from various sources like Salesforce, Google Analytics, and internal databases. We want to avoid writing custom scripts for each. Is there a tool that simplifies this process?",bigdata,7,https://www.reddit.com/r/bigdata/comments/1lv1490/how_to_sync_data_from_multiple_sources_without/,r_1lv1490,,,
r_1lumrkx,reddit,wanderingsoul8994,2025-07-08T12:03:14+00:00,"Looking for feedback on a new approach to governed, cost-aware AI analytics
Iâ€™m building a platform that pairs a **federated semantic layer + governance/FinOps engine** with a **graph-grounded AI assistant**.

* No data movementâ€”lightweight agents index Snowflake, BigQuery, SaaS DBs, etc., and compile row/column policies into a knowledge graph.
* An LLM uses that graph to generate deterministic SQL and narrative answers; every query is cost-metered and policy-checked **before** it runs.
* Each Q-A cycle enriches the graph (synonyms, lineage, token spend), so trust and efficiency keep improving.

**Questions for the community:**

1. Does an â€œAI-assisted federated governanceâ€ approach resonate with the pain you see (silos, backlog, runaway costs)?
2. Which parts sound most or least valuableâ€”semantic layer, FinOps gating, or graph-based RAG accuracy?
3. If youâ€™ve tried tools like ThoughtSpot Sage, Amazon Q, or catalog platforms (Collibra, Purview, etc.), where did they fall short?

Brutally honest feedbackâ€”technical, operational, or businessâ€”would be hugely appreciated. Happy to clarify details in the comments. Thanks!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1lumrkx/looking_for_feedback_on_a_new_approach_to/,r_1lumrkx,,,
r_1lsw0ky,reddit,phicreative1997,2025-07-06T08:08:59+00:00,Building â€œAuto-Analystâ€â€Šâ€”â€ŠA data analytics AI agentic system,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lsw0ky/building_autoanalyst_a_data_analytics_ai_agentic/,r_1lsw0ky,,,
r_1lrrkzi,reddit,Specific-Signal4256,2025-07-04T19:43:38+00:00,"AWS DMS ""Out of Memory"" Error During Full Load
Hello everyone,

I'm trying to migrate a table with 53 million rows, which DBeaver indicates is around 31GB, using AWS DMS. I'm performing a **Full Load Only** migration with a **T3.medium instance (2 vCPU, 4GB RAM)**. However, the task consistently stops after migrating approximately 500,000 rows due to an ""Out of Memory"" (OOM killer) error.

When I analyze the metrics, I observe that the memory usage initially seems fine, with about 2GB still free. Then, suddenly, the **CPU utilization spikes, memory usage plummets, and the swap usage graph also increases sharply**, leading to the OOM error.

I'm unable to increase the replication instance size. The migration time is not a concern for me; whether it takes a month or a year, I just need to successfully transfer these data. My primary goal is to **optimize memory usage and prevent the OOM killer**.

My plan is to migrate data from an **on-premises Oracle database to an S3 bucket in AWS** using AWS DMS, with the data being transformed into **Parquet format** in S3.

I've already refactored my **JSON Task Settings** and **disabled parallelism**, but these changes haven't resolved the issue. I'm relatively new to both data engineering and AWS, so I'm hoping someone here has experienced a similar situation.

* How did you solve this problem when the table size exceeds your machine's capacity?
* How can I force AWS DMS to not consume all its memory and avoid the Out of Memory error?
* Could someone provide an explanation of what's happening internally within DMS that leads to this out-of-memory condition?
* Are there specific techniques to prevent this AWS DMS ""Out of Memory"" error?

**My current JSON Task Settings:**

{

  ""S3Settings"": {

""BucketName"": ""bucket"",

""BucketFolder"": ""subfolder/subfolder2/subfolder3"",

""CompressionType"": ""GZIP"",

""ParquetVersion"": ""PARQUET\_2\_0"",

""ParquetTimestampInMillisecond"": true,

""MaxFileSize"": 64,

""AddColumnName"": true,

""AddSchemaName"": true,

""AddTableLevelFolder"": true,

""DataFormat"": ""PARQUET"",

""DatePartitionEnabled"": true,

""DatePartitionDelimiter"": ""SLASH"",

""DatePartitionSequence"": ""YYYYMMDD"",

""IncludeOpForFullLoad"": false,

""CdcPath"": ""cdc"",

""ServiceAccessRoleArn"": ""arn:aws:iam::12345678000:role/DmsS3AccessRole""

  },

  ""FullLoadSettings"": {

""TargetTablePrepMode"": ""DO\_NOTHING"",

""CommitRate"": 1000,

""CreatePkAfterFullLoad"": false,

""MaxFullLoadSubTasks"": 1,

""StopTaskCachedChangesApplied"": false,

""StopTaskCachedChangesNotApplied"": false,

""TransactionConsistencyTimeout"": 600

  },

  ""ErrorBehavior"": {

""ApplyErrorDeletePolicy"": ""IGNORE\_RECORD"",

""ApplyErrorEscalationCount"": 0,

""ApplyErrorEscalationPolicy"": ""LOG\_ERROR"",

""ApplyErrorFailOnTruncationDdl"": false,

""ApplyErrorInsertPolicy"": ""LOG\_ERROR"",

""ApplyErrorUpdatePolicy"": ""LOG\_ERROR"",

""DataErrorEscalationCount"": 0,

""DataErrorEscalationPolicy"": ""SUSPEND\_TABLE"",

""DataErrorPolicy"": ""LOG\_ERROR"",

""DataMaskingErrorPolicy"": ""STOP\_TASK"",

""DataTruncationErrorPolicy"": ""LOG\_ERROR"",

""EventErrorPolicy"": ""IGNORE"",

""FailOnNoTablesCaptured"": true,

""FailOnTransactionConsistencyBreached"": false,

""FullLoadIgnoreConflicts"": true,

""RecoverableErrorCount"": -1,

""RecoverableErrorInterval"": 5,

""RecoverableErrorStopRetryAfterThrottlingMax"": true,

""RecoverableErrorThrottling"": true,

""RecoverableErrorThrottlingMax"": 1800,

""TableErrorEscalationCount"": 0,

""TableErrorEscalationPolicy"": ""STOP\_TASK"",

""TableErrorPolicy"": ""SUSPEND\_TABLE""

  },

  ""Logging"": {

""EnableLogging"": true,

""LogComponents"": \[

{ ""Id"": ""TRANSFORMATION"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""SOURCE\_UNLOAD"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""IO"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""TARGET\_LOAD"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""PERFORMANCE"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""SOURCE\_CAPTURE"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""SORTER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""REST\_SERVER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""VALIDATOR\_EXT"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""TARGET\_APPLY"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""TASK\_MANAGER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""TABLES\_MANAGER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""METADATA\_MANAGER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""FILE\_FACTORY"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""COMMON"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""ADDONS"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""DATA\_STRUCTURE"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""COMMUNICATION"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" },

{ ""Id"": ""FILE\_TRANSFER"", ""Severity"": ""LOGGER\_SEVERITY\_DEFAULT"" }

\]

  },

  ""FailTaskWhenCleanTaskResourceFailed"": false,

  ""LoopbackPreventionSettings"": null,

  ""PostProcessingRules"": null,

  ""StreamBufferSettings"": {

""CtrlStreamBufferSizeInMB"": 3,

""StreamBufferCount"": 2,

""StreamBufferSizeInMB"": 4

  },

  ""TTSettings"": {

""EnableTT"": false,

""TTRecordSettings"": null,

""TTS3Settings"": null

  },

  ""BeforeImageSettings"": null,

  ""ChangeProcessingDdlHandlingPolicy"": {

""HandleSourceTableAltered"": true,

""HandleSourceTableDropped"": true,

""HandleSourceTableTruncated"": true

  },

  ""ChangeProcessingTuning"": {

""BatchApplyMemoryLimit"": 200,

""BatchApplyPreserveTransaction"": true,

""BatchApplyTimeoutMax"": 30,

""BatchApplyTimeoutMin"": 1,

""BatchSplitSize"": 0,

""CommitTimeout"": 1,

""MemoryKeepTime"": 60,

""MemoryLimitTotal"": 512,

""MinTransactionSize"": 1000,

""RecoveryTimeout"": -1,

""StatementCacheSize"": 20

  },

  ""CharacterSetSettings"": null,

  ""ControlTablesSettings"": {

""CommitPositionTableEnabled"": false,

""ControlSchema"": """",

""FullLoadExceptionTableEnabled"": false,

""HistoryTableEnabled"": false,

""HistoryTimeslotInMinutes"": 5,

""StatusTableEnabled"": false,

""SuspendedTablesTableEnabled"": false

  },

  ""TargetMetadata"": {

""BatchApplyEnabled"": false,

""FullLobMode"": false,

""InlineLobMaxSize"": 0,

""LimitedSizeLobMode"": true,

""LoadMaxFileSize"": 0,

""LobChunkSize"": 32,

""LobMaxSize"": 32,

""ParallelApplyBufferSize"": 0,

""ParallelApplyQueuesPerThread"": 0,

""ParallelApplyThreads"": 0,

""ParallelLoadBufferSize"": 0,

""ParallelLoadQueuesPerThread"": 0,

""ParallelLoadThreads"": 0,

""SupportLobs"": true,

""TargetSchema"": """",

""TaskRecoveryTableEnabled"": false

  }

}",bigdata,1,https://www.reddit.com/r/bigdata/comments/1lrrkzi/aws_dms_out_of_memory_error_during_full_load/,r_1lrrkzi,,,
r_1lrj52c,reddit,sharmaniti437,2025-07-04T13:46:16+00:00,"Future-proof Your Tech Career with MLOps Certification
Businesses can fasten decision-making, model governance, and time-to-market through Machine Learning Operations \[MLOps\]. MLOps serves as a link between data science and IT operations as it fosters seamless collaboration, controls versions, and streamlines the lifecycle of the models. Ultimately, it is becoming an integral component of AI infrastructure.

Research reports substantiate this very well. MarketsandMarkets Research report projects that the global Machine Learning Operations \[MLOps\] market will reach USD 5.9 billion by 2027 \[from USD 1.1 billion in 2022\], at a CAGR of 41.0% during the forecast period.



https://preview.redd.it/kf6kexc53vaf1.jpg?width=1920&format=pjpg&auto=webp&s=97a8a30683e571949e20133ca8623262c6152cde

Â MLOps is being widely used across industries for predictive maintenance, fraud detection, customer experience management, marketing analytics, supply chain optimization, etc. From a vertical standpoint, IT and Telecommunications, healthcare, retail, manufacturing, financial services, government, media and entertainment are adopting MLOps.

This trajectory reflects that there is an increasing demand for [Machine Learning Engineers](https://www.usdsi.org/data-science-insights/data-scientist-vs-ml-engineer-a-comparative-career-guide)**,** MLOps Engineers, Machine Learning Deployment Engineers, or AI Platform Engineers who can manage machine learning models starting from deployment, and monitoring to supervision efficiently.

As we move forward, we should understand that MLOps solutions are supported by technologies such as Artificial Intelligence, Big data analytics, and DevOps practices. The synergy between the above-mentioned technologies is critical for model integration, deployment, and delivery of machine-learning applications.

The rising complexity of ML models and the available limited skill force calls for professionals with hybrid skill sets. The professionals should be proficient in DevOps, data analysis, machine learning, and **AI skills.**

Letâ€™s investigate further.

# How to address this MLOps skill set shortage?

Addressing the MLOps skill set requires focused upskilling and reskilling of the professionals.

Forward-thinking companies are training their current employees, particularly those in **machine learning engineering jobs** and adjacent field(s) like data engineering or software engineering. Companies are taking a strategic approach to building MLOps competencies for their employees by providing targeted training.

At the personal level, pursuing certification by choosing the adept **ML certification programs** would be the right choice. This section makes your search easy. We have provided a list of well-defined certification programs that fit your objectives.

**Take a look.**

# Certified MLOps Professional: GSDC (Global Skill Development Council)

Earning this certification benefits you in many ways. It enables you to accelerate ML model deployment with expert-built templates, understand real-world MLOps scenarios, master automation for model lifecycle management, and prepare for cross-functional ML team roles.

# Machine Learning Operations Specialization: Duke University

Earning this certification helps you master the fundamental aspects of Python, and get acquainted with MLOps principles, and data management. It equips you with the practical skills needed for building and deploying ML models in production environments.

# Professional Machine Learning Engineer: Google

Earning this certification helps you get familiar with the basic concepts of MLOps, data engineering, and data governance. You will be able to train, retrain, deploy, schedule, improve, and monitor models.

# Transitioning to MLOps as a Data engineer or software engineer

In case, you have pure data science or software engineering as your educational background and looking for machine learning engineering, then the below-mentioned certifications will help you.

# Certified Artificial Intelligence Engineer (CAIEâ„¢): USAIIÂ®

The specialty of this program is that the curriculum is meticulously planned and designed. It meets the demands of an emerging AI Engineer/Developer. It explores all the essentials for ML engineers like MLOps, the backbone to scale AI systems, debugging for responsible AI, robotics, life cycle of models, automation of ML pipelines, and more.

# Certified Machine Learning Engineer â€“ Associate: AWS

This is a role-based certification meant for MLOps engineers and ML engineers. This certification helps you to get acquainted with knowledge in the fields of data analysis, modeling, data engineering, ML implementation, and more.

# Becoming a versatile professional with cross-functional skills

If you are looking to be more versatile, you need to build cross-functional skills across AI, ML, data engineering, and DevOps related practices. Then, your strong choice should be CLDSâ„¢ from USDSIÂ®.

# Certified Lead Data Scientist (CLDSâ„¢): USDSIÂ®

This is the most aligned certification for you as it has a comprehensive curriculum covering data science, machine learning, deep learning, Natural Language Processing, Big data analytics, and cloud technologies.

You can easily collaborate with other people in varied fields, (other than **ML careers**) and ensure long term success of AI-based applications.Â 

# Final thoughts

Todayâ€™s world is data-driven, as you already know. Building a strong technical background is essential for professionals looking forward to exceling in MLOps roles. Proficiency in core concepts and tools like Python, SQL, Docker, Data Wrangling, Machine Learning, CI/CD, ML models deployment with containerization, etc., will help you stand distinct in your professional journey.

Earning the right machine learning certifications, along with one or two related certifications such as DevOps, data engineering, or cloud platforms is crucial. It will help you gain competence and earn the best position in the overcrowded job market.

As technology evolves, the skill set is becoming broad. It cannot be confined to single domains. Developing an integrated approach toward your **ML career** helps you to thrive well in transformative roles.",bigdata,3,https://www.reddit.com/r/bigdata/comments/1lrj52c/futureproof_your_tech_career_with_mlops/,r_1lrj52c,,,
r_1lrbil6,reddit,Thinker_Assignment,2025-07-04T06:11:25+00:00,"Iceberg ingestion case study: 70% cost reduction
hey folks I wanted to share a recent win we had with one of our users. (i work at dlthub where we build dlt the oss  python library for ingestion)

They were getting a 12x data increase and had to figure out how to not 12x their analytics bill, so they flipped to Iceberg and saved 70% of the cost.

[https://dlthub.com/blog/taktile-iceberg-ingestion](https://dlthub.com/blog/taktile-iceberg-ingestion)  


",bigdata,2,https://www.reddit.com/r/bigdata/comments/1lrbil6/iceberg_ingestion_case_study_70_cost_reduction/,r_1lrbil6,,,
r_1lr2zwz,reddit,Key_Size_5033,2025-07-03T22:38:33+00:00,"$WAXP Just Flipped the Script â€” From Inflation to Deflation. Here's What It Means.
Holla #WAXFAM and $WAXP hodler ðŸ‘‹ I have a latest update about the $WAXP native token. 

https://preview.redd.it/3s1dgbeqkqaf1.jpg?width=1600&format=pjpg&auto=webp&s=c65dbeecc7dfdbf869dda85ea19e7ab2cdf2b5bf

WAX just made one of the boldest moves weâ€™ve seen in the Layer-1 space lately â€” theyâ€™ve *completely flipped their tokenomics model* from inflationary to deflationary.

Hereâ€™s the TL;DR:

* **Annual emissions slashed** from 653 million to just **156 million WAXP**
* **50% of all emissions will be burned**

Thatâ€™s not just a tweak â€” thatâ€™s a **75%+ cut in new tokens**, and then half of those tokens are literally torched . It is now officially entering a phase where more WAXP could be destroyed than created.

# Why it matters?

In a market where most L1s are still dealing with high inflation to fuel ecosystem growth, WAX is going in the opposite direction â€” focusing on **long-term value and sustainability**. Itâ€™s a major shift away from growth-at-all-costs to a model that rewards **retention and real usage**.

# What could change?

* **Price pressure**: Less new supply = less sell pressure on exchanges.
* **Staker value**: If supply drops and demand holds, staking rewards could become more meaningful over time.
* **dApp/GameFi builders**: Better economics means stronger incentives to build on WAX without the constant fear of token dilution.

# How does this stack up vs Ethereum or Solana?

Ethereumâ€™s EIP-1559 burn mechanism was a game-changer, but it still operates with net emissions. Solana, meanwhile, keeps inflation relatively high to subsidize validators.

WAX is going **full deflationary**, and thatâ€™s rare â€” especially for a chain with strong roots in NFTs and GameFi. If this works, it could be a blueprint for how other chains rethink emissions.

\#WAXNFT #WAXBlockchain

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1lr2zwz/waxp_just_flipped_the_script_from_inflation_to/,r_1lr2zwz,,,
r_1lqr3go,reddit,sharmaniti437,2025-07-03T14:32:32+00:00,"10 Not-to-Miss Data Science Tools
Modern [data science tools](https://www.usdsi.org/data-science-insights/resources/10-popular-data-science-tools-to-consider-exploring) blend code, cloud, and AIâ€”fueling powerful insights and faster decisions. They're the backbone of predictive models, data pipelines, and business transformation. 

Explore what tools are expected of you as a seasoned data science expert in 2025

https://preview.redd.it/e1xa44as6oaf1.jpg?width=1080&format=pjpg&auto=webp&s=68ef192f2f5287b0af0937584dd598230a8217e4

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1lqr3go/10_nottomiss_data_science_tools/,r_1lqr3go,,,
r_1lpzutk,reddit,PresentationThink966,2025-07-02T16:08:20+00:00,"What is the easiest way to set up a no-code data pipeline that still handles complex logic?
Trying to find a balance between simplicity and power. I donâ€™t want to code everything from scratch but still need something that can transform and sync data between a bunch of sources. Any tools actually deliver both?",bigdata,5,https://www.reddit.com/r/bigdata/comments/1lpzutk/what_is_the_easiest_way_to_set_up_a_nocode_data/,r_1lpzutk,,,
r_1lpvghe,reddit,GreenMobile6323,2025-07-02T13:10:10+00:00,"Are You Scaling Data Responsibly? Why Ethics & Governance Matter More Than Ever
Let me know how you're handling data ethics in your org.",bigdata,3,https://www.reddit.com/r/bigdata/comments/1lpvghe/are_you_scaling_data_responsibly_why_ethics/,r_1lpvghe,,,
r_1lpkfny,reddit,Fahim61891012,2025-07-02T02:25:40+00:00,"WAX Is Burning Literally! Here's What Changed
https://preview.redd.it/ioivau4vfdaf1.png?width=1024&format=png&auto=webp&s=7854dde80ce5a7632d56aee712ec01fa5c9c5065

The WAX team just came out with a pretty interesting update lately. While most Layer 1s are still dealing with high inflation, WAX is doing the oppositeâ€”focusing on cutting back its token supply instead of expanding it.

So, whatâ€™s the new direction?  
Previously, most of the network resources were powered through stakingâ€”around 90% staking and 10% PowerUp. Now, theyâ€™re flipping that completely: the new goal is 90% PowerUp and just 10% staking.

What does that mean in practice?  
Staking rewards are being scaled down, and fewer new tokens are being minted. Meanwhile, PowerUp revenue is being used to replace inflationâ€”and any unused inflation gets burned. So, the more the network is used, the more tokens are effectively removed from circulation. Usage directly drives supply reduction.

Now letâ€™s talk price, validators, and GameFi:  
Validators still earn a decent staking yield, but the system is shifting toward usage-based revenue. That means validator rewards can become more sustainable over time, tied to real activity instead of inflation.  
For GameFi builders and players, knowing that resource usage burns tokens could help keep transaction costs more stable in the long run. That makes WAX potentially more user-friendly for high-volume gaming ecosystems.

What about Ethereum and Solana?  
Sure, Ethereum burns base fees via EIPâ€‘1559, but it still has net positive inflation. Solana has more limited burning mechanics. WAX, on the other hand, is pushing a model where inflation is minimized and burning is directly linked to real usageâ€”something thatâ€™s clearly tailored for GameFi and frequent activity.

So in short, WAX is evolving from a low-fee blockchain into something more: a usage-driven, sustainable network model.",bigdata,10,https://www.reddit.com/r/bigdata/comments/1lpkfny/wax_is_burning_literally_heres_what_changed/,r_1lpkfny,,,
r_1lpe8g9,reddit,FractalNerve,2025-07-01T21:37:23+00:00,"My diagram of abstract math concepts illustrated
Made this flowchart explaining all parts of Math in a symplectic way.  
Let me know if I missed something :)",bigdata,2,https://www.reddit.com/r/bigdata/comments/1lpe8g9/my_diagram_of_abstract_math_concepts_illustrated/,r_1lpe8g9,,,
r_1lp2wag,reddit,GreenMobile6323,2025-07-01T14:19:34+00:00,NiFi 2.0 vs NiFi 1.0: What's the BEST Choice for Data Processing,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lp2wag/nifi_20_vs_nifi_10_whats_the_best_choice_for_data/,r_1lp2wag,,,
r_1lophaa,reddit,Santhu_477,2025-07-01T01:44:24+00:00,"Handling Bad Records in Streaming Pipelines Using Dead Letter Queues in PySpark
ðŸš€ I just published a detailed guide on handling Dead Letter Queues (DLQ) in PySpark Structured Streaming.

It covers:

\- Separating valid/invalid records

\- Writing failed records to a DLQ sink

\- Best practices for observability and reprocessing

Would love feedback from fellow data engineers!

ðŸ‘‰ \[Read here\](Â [https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29](https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29)Â )",bigdata,2,https://www.reddit.com/r/bigdata/comments/1lophaa/handling_bad_records_in_streaming_pipelines_using/,r_1lophaa,,,
r_1lo9z40,reddit,AllenMutum,2025-06-30T15:08:48+00:00,Unlock Business Insights: Why Looker Leads in BI Tools,bigdata,2,https://www.reddit.com/r/bigdata/comments/1lo9z40/unlock_business_insights_why_looker_leads_in_bi/,r_1lo9z40,,,
r_1lnzraq,reddit,phicreative1997,2025-06-30T05:43:46+00:00,"Get an Analytics blue-print instantly
AutoAnalyst gives you a reliable blueprint by handling all the key steps: data preprocessing, modeling, and visualization.

It starts by understanding your goal and then plans the right approach.

A built-in planner routes each part of the job to the right AI agent.

So you donâ€™t have to guess what to do nextâ€”the system handles it.

The result is a smooth, guided analysis that saves time and gives clear answers.

Link: https://autoanalyst.ai

Link to repo: https://github.com/FireBird-Technologies/Auto-Analyst

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1lnzraq/get_an_analytics_blueprint_instantly/,r_1lnzraq,,,
r_1llm1ve,reddit,bigdataengineer4life,2025-06-27T05:35:13+00:00,"ðŸ“Š Clickstream Behavior Analysis with Dashboard using Kafka, Spark Streaming, MySQL, and Zeppelin!
ðŸš€ New Real-Time Project Alert for Free!



ðŸ“Š Clickstream Behavior Analysis with Dashboard

Track & analyze user activity in real time using Kafka, Spark Streaming, MySQL, and Zeppelin! ðŸ”¥



ðŸ“Œ What Youâ€™ll Learn:

âœ… Simulate user click events with Java

âœ… Stream data using Apache Kafka

âœ… Process events in real-time with Spark Scala

âœ… Store & query in MySQL

âœ… Build dashboards in Apache Zeppelin ðŸ§ 



ðŸŽ¥ Watch the 3-Part Series Now:



ðŸ”¹ Part 1: Clickstream Behavior Analysis (Part 1)

ðŸ“½ [https://youtu.be/jj4Lzvm6pzs](https://youtu.be/jj4Lzvm6pzs)



ðŸ”¹ Part 2: Clickstream Behavior Analysis (Part 2)

ðŸ“½ [https://youtu.be/FWCnWErarsM](https://youtu.be/FWCnWErarsM)



ðŸ”¹ Part 3: Clickstream Behavior Analysis (Part 3)

ðŸ“½ [https://youtu.be/SPgdJZR7rHk](https://youtu.be/SPgdJZR7rHk)



This is perfect for Data Engineers, Big Data learners, and anyone wanting hands-on experience in streaming analytics.



ðŸ“¡ Try it, tweak it, and track real-time behaviors like a pro!

ðŸ’¬ Let us know if you'd like the full source code!

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1llm1ve/clickstream_behavior_analysis_with_dashboard/,r_1llm1ve,,,
r_1ll9xwp,reddit,elm3131,2025-06-26T19:56:38+00:00,"How do you reliably detect model drift in production LLMs
We recently launched an LLM in production and saw unexpected behaviorâ€”hallucinations and output driftâ€”sneaking in under the radar.

Our solution? AnÂ **AI-native observability stack**Â using unsupervised ML, prompt-level analytics, and trace correlation.

I wrote up what worked, what didnâ€™t, and how to build a proactive drift detection pipeline.

Would love feedback from anyone using similar strategies or frameworks.

**TL;DR:**

* What model drift isâ€”and why itâ€™s hard to detect
* How we instrument models, prompts, infra for full observability
* Examples of drift sign patterns and alert logic

Full post here ðŸ‘‰[https://insightfinder.com/blog/model-drift-ai-observability/](https://insightfinder.com/blog/model-drift-ai-observability/)",bigdata,0,https://www.reddit.com/r/bigdata/comments/1ll9xwp/how_do_you_reliably_detect_model_drift_in/,r_1ll9xwp,,,
r_1lj3izn,reddit,bigdataengineer4life,2025-06-24T06:06:17+00:00,Data Architecture Complexity,bigdata,5,https://www.reddit.com/r/bigdata/comments/1lj3izn/data_architecture_complexity/,r_1lj3izn,,,
r_1lii21l,reddit,hammerspace-inc,2025-06-23T14:29:11+00:00,Hammerspace IO500 Benchmark Demonstrates Simplicity Doesnâ€™t Have to Come at the Cost of Storage Inefficiency,bigdata,1,https://www.reddit.com/r/bigdata/comments/1lii21l/hammerspace_io500_benchmark_demonstrates/,r_1lii21l,,,
r_1lgnebr,reddit,abheshekcr,2025-06-21T03:56:16+00:00,"Big data course by sumit mittal
Why is no body raising voice against the blatant scam done by sumit mittal in the name of selling courses ..
I bought his course for 45k ..trust me ..I would have found more value on the best Udemy courses present on this topic for 500 rupees 
This guy keeps posting day in and day out of whatsapp screenshots of his students getting 30lpa jobs ..which for most part i think is fabricated ..because it's the same pattern all the time ..
Soo many people are looking for jobs and the kind of misselling this guy does ..I am sad that many are buying and falling prey to his scam ..
How can this be approached legally and stop this nuisance from propagating ",bigdata,6,https://www.reddit.com/r/bigdata/comments/1lgnebr/big_data_course_by_sumit_mittal/,r_1lgnebr,,,
r_1lg0sgf,reddit,sharmaniti437,2025-06-20T10:40:13+00:00,"10 MOST POPULAR IoT APPLICATIONS OF 2025 | INFOGRAPHIC
Internet of things is what is taking over the world by a storm. With connected devices growing at a staggering rate, it is inevitable to understand what [IoT applications](https://www.usdsi.org/data-science-insights/10-most-popular-iot-applications-of-2025) look like. With sensors, software, networks, devices- all sharing a common platform; it necessitates the comprehension of how this impact our lives in a million different ways.

With Mordor Intelligence bringing up the forecast for the global IoT market size to grow at a CAGR of 15.12%, only to reach a whopping **US$2.72 trillion**\- this industry is not going to stop anytime soon. It is here to stay as the technology advances. 

From smart homes, to wearable health tech, connected self-driving cars, smart cities, industrial IoT, precision farming- you name it and IoT has a powerful use case in that industry or sector worldwide. Gain an inside out comprehension of IoT applications right here!

https://preview.redd.it/7uvbthu2928f1.png?width=1200&format=png&auto=webp&s=2f324bb08bc42d135d4ae11bb8182cace9e83c07

",bigdata,3,https://www.reddit.com/r/bigdata/comments/1lg0sgf/10_most_popular_iot_applications_of_2025/,r_1lg0sgf,,,
r_1lf2mf7,reddit,GreenMobile6323,2025-06-19T05:15:46+00:00,"Data Governance and Access Control in a Multi-Platform Big Data Environment
Our organization uses Snowflake, Databricks, Kafka, and Elasticsearch, each with its own ACLs and tagging system. Auditors demand a single source of truth for data permissions and lineage. How have you centralized governance, either via an open-source catalog or commercial tool, to manage roles, track usage, and automate compliance checks across diverse big data platforms?",bigdata,5,https://www.reddit.com/r/bigdata/comments/1lf2mf7/data_governance_and_access_control_in_a/,r_1lf2mf7,,,
r_1lf0ztb,reddit,Shawn-Yang25,2025-06-19T03:42:23+00:00,Apache Fory Serialization Framework 0.11.0 Released,bigdata,3,https://www.reddit.com/r/bigdata/comments/1lf0ztb/apache_fory_serialization_framework_0110_released/,r_1lf0ztb,,,
r_1leq09o,reddit,superconductiveKyle,2025-06-18T19:20:48+00:00,"Semantic Search + LLMs = Smarter Systems
As data volume explodes, keyword indexes fall apart, missing context, underperforming at scale, and failing to surface unstructured insights. This breakdown walks through how semantic embeddings and vector search backed by LLMs transform discoverability across massive datasets. Learn how modern retrieval (via RAG) scales better, retrieves smarter, and handles messy multimodal inputs.

[full blog](https://ducky.ai/blog/ai-killed-traditional-search?utm_source=reddit-bigdata&utm_medium=post&utm_campaign=technical-use_case&utm_content=ai-killed-search)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1leq09o/semantic_search_llms_smarter_systems/,r_1leq09o,,,
r_1leox1u,reddit,eb0373284,2025-06-18T18:38:14+00:00,"Ever had to migrate a data warehouse from Redshift to Snowflake? What was harder than expected?
Weâ€™re considering moving from Redshift to Snowflake for performance and cost. It looks simple, but Iâ€™m sure there are gotchas.  
  
What were the trickiest parts of the migration for you?",bigdata,3,https://www.reddit.com/r/bigdata/comments/1leox1u/ever_had_to_migrate_a_data_warehouse_from/,r_1leox1u,,,
r_1lefoed,reddit,UH-Simon,2025-06-18T12:22:03+00:00,"We built a high-performance storage for big data
Hi everyone! We're a small storage startup from Berlin and wanted to share something we've been working on and get some feedback from the community here.

Over the last few years working on this, we've heard a lot about how storage can massively slow down modern AI pipelines, especially during training or when building anything retrieval-based like RAG. So we thought it would be a good idea to built something focused on performance.

UltiHash is S3-compatible object storage, designed to serve high-throughput, read-heavy workloads: originally for MLOps use cases, but is also a good fit for big data infrastructure more broadly.

We just launched the serverless version: itâ€™s fully managed, with no infra to run. You spin up a cluster, get an endpoint, and connect using any S3-compatible tool.

Things to know:

* 1 GB/s read per machine: youâ€™re not leaving compute idle
* S3 compatible: you can integrate with your stack (Spark, Kafka, PyTorch, Iceberg, Trino, etc.)
* Scales past 100TB without having to rework your setup
* Lowers TCO: e.g. our 10TB tier is â‚¬0.21/GB/month, infra + support included

We host everything in the EU currently in AWS Frankfurt (`eu-central-1`) with Hetzner and OVH Cloud support coming soon (waitlistâ€™s open).

Would love to hear what folks here think. More details here: [https://www.ultihash.io/serverless](https://www.ultihash.io/serverless), happy to go deeper into how weâ€™re handling throughput, deduplication, or anything else.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1lefoed/we_built_a_highperformance_storage_for_big_data/,r_1lefoed,,,
r_1lee7mj,reddit,sharmaniti437,2025-06-18T11:02:51+00:00,"Hottest Data Analytics Trends 2025
In 2025, [data analytics](https://www.usdsi.org/data-science-insights/resources/6-hottest-data-analytics-trends-to-prepare-ahead-of-2025) gets sharperâ€”real-time dashboards, AI-powered insights, and ethical governance will dominate. Expect faster decisions, deeper personalization, and smarter automation across industries.

https://reddit.com/link/1lee7mj/video/0ortwuoo3o7f1/player

",bigdata,3,https://www.reddit.com/r/bigdata/comments/1lee7mj/hottest_data_analytics_trends_2025/,r_1lee7mj,,,
r_1ldhwua,reddit,Shawn-Yang25,2025-06-17T08:35:12+00:00,Serialization Framework Announcement - Apache Fury is Now Apache Fory,bigdata,1,https://www.reddit.com/r/bigdata/comments/1ldhwua/serialization_framework_announcement_apache_fury/,r_1ldhwua,,,
r_1lacj5o,reddit,sharmaniti437,2025-06-13T10:30:49+00:00,"R or Python - Contesting Programming Giants to be the Best
Gain access to clear insights on the best suited programming language for your machine learning tasks among [R and Python](https://www.usdsi.org/data-science-insights/r-or-python-contesting-programming-giants-to-be-the-best).

https://preview.redd.it/m767v36f9o6f1.png?width=1080&format=png&auto=webp&s=6bb413b04fe768dea4c669ec9d7231409662809a

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1lacj5o/r_or_python_contesting_programming_giants_to_be/,r_1lacj5o,,,
r_1la4kxq,reddit,Worried-Variety3397,2025-06-13T02:14:00+00:00,[D] Why Is Enterprise Data Integration Always So Messy? My Clientsâ€™ Real-Life Nightmares,bigdata,5,https://www.reddit.com/r/bigdata/comments/1la4kxq/d_why_is_enterprise_data_integration_always_so/,r_1la4kxq,,,
r_1l91z41,reddit,hammerspace-inc,2025-06-11T19:36:53+00:00,Unstructured Data Orchestration for Dummies,bigdata,2,https://www.reddit.com/r/bigdata/comments/1l91z41/unstructured_data_orchestration_for_dummies/,r_1l91z41,,,
r_1l8re4r,reddit,Hot_Donkey9172,2025-06-11T12:29:07+00:00,"Cursor for data engineers according to you
I'm exploring the idea of building a purpose-built IDE for data engineers. Curious to know what tools or workflows do you feel are still clunky or missing in todayâ€™s setup? And how can AI help?",bigdata,4,https://www.reddit.com/r/bigdata/comments/1l8re4r/cursor_for_data_engineers_according_to_you/,r_1l8re4r,,,
r_1l7ofuz,reddit,unknown,2025-06-10T03:13:04+00:00,Best Big Data Courses on Udemy to learn in 2025,bigdata,2,https://www.reddit.com/r/bigdata/comments/1l7ofuz/best_big_data_courses_on_udemy_to_learn_in_2025/,r_1l7ofuz,,,
r_1l6z6m1,reddit,sharmaniti437,2025-06-09T08:07:54+00:00,"Resolving Data Quality Constraints
Data quality isnâ€™t just a checkboxâ€”itâ€™s the backbone of smart data-driven decision-making. Clean, consistent, and reliable data fuels trust, boosts efficiency, and drives impact. Because when data speaks the truth, your insights lead the way. 

This read targets strategic challenges, and possible solutions to resolve [data quality](https://www.usdsi.org/data-science-insights/data-quality-matters-risks-constraints-and-solution-potion) issues.

https://preview.redd.it/dcl079y90v5f1.jpg?width=1080&format=pjpg&auto=webp&s=0f57f2bbcf88db96c75717356bd91c750ed79f54

 

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1l6z6m1/resolving_data_quality_constraints/,r_1l6z6m1,,,
r_1l56nb8,reddit,zekken908,2025-06-06T23:26:27+00:00,"If you had to rebuild your data stack from scratch, what's the one tool you'd keep?
We're cleaning house, rethinking our whole stack after growing way too fast and ending up with a Frankenstein setup. Curious what tools people stuck with long-term, especially for data pipelines and integrations.",bigdata,8,https://www.reddit.com/r/bigdata/comments/1l56nb8/if_you_had_to_rebuild_your_data_stack_from/,r_1l56nb8,,,
r_1l4q46r,reddit,Professional-Ant9045,2025-06-06T11:32:46+00:00,"Clickhouse in a large-scale user-persoanlized marketing campaign
Dear colleagues 
Hello
I would like to introduce our last project at Snapp Market (Iranian Q-Commerce business like Instacart) in which we took the advantage of Clickhouse as an analytical DB to run a large scale user personalized marketing campaign, with GenAI.

https://medium.com/@prmbas/clickhouse-in-the-wild-an-odyssey-through-our-data-driven-marketing-campaign-in-q-commerce-93c2a2404a39

I will be grateful if I have your opinion about this.

#ClickHouse
",bigdata,2,https://www.reddit.com/r/bigdata/comments/1l4q46r/clickhouse_in_a_largescale_userpersoanlized/,r_1l4q46r,,,
r_1l3v29w,reddit,shokatjaved,2025-06-05T09:43:39+00:00,100 MUI Style Login Form Designs - JV Codes 2025,bigdata,1,https://www.reddit.com/r/bigdata/comments/1l3v29w/100_mui_style_login_form_designs_jv_codes_2025/,r_1l3v29w,,,
r_1l30115,reddit,sharmaniti437,2025-06-04T08:09:28+00:00,"AI Features for PowerBI Platform
Who needs a data scientist when Power BIâ€™s AI features have your back? Ask questions in plain English, get instant insights, and let machine learning spot trends before your coffee even cools. Itâ€™s like giving Excel a PhD and a sense of style.

Smart data- Slick delivery!

Watch Video [https://youtu.be/-b657kvhJv8](https://youtu.be/-b657kvhJv8) to Get Nuanced in PowerBI as a Data Expert Today!

https://reddit.com/link/1l30115/video/q0q8rgw4fv4f1/player

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1l30115/ai_features_for_powerbi_platform/,r_1l30115,,,
r_1l2x9p1,reddit,bigdataengineer4life,2025-06-04T05:08:02+00:00,How to create HIVE Table with multi character delimiter? (Hands On),bigdata,3,https://www.reddit.com/r/bigdata/comments/1l2x9p1/how_to_create_hive_table_with_multi_character/,r_1l2x9p1,,,
r_1kztwhf,reddit,Pangaeax_,2025-05-31T10:27:37+00:00,"Big Data in Smart Cities: Transforming Urban Life 2025
In 2025, big data analytics forms the backbone of smart cities, transforming urban life in meaningful and measurable ways. From optimizing transportation and managing resources sustainably to enhancing public safety and fostering community engagement, data science is making cities more livable, efficient, and inclusive. However, challenges around privacy, infrastructure, and equity underscore the importance of adopting ethical and inclusive data practices.Â Looking ahead, data science will continue to redefine how cities operate and grow. Freelance data analysts have a vital role to play in this evolution bringing agility, innovation, and expertise to urban analytics.",bigdata,6,https://www.reddit.com/r/bigdata/comments/1kztwhf/big_data_in_smart_cities_transforming_urban_life/,r_1kztwhf,,,
r_1ky2nwb,reddit,bigdataengineer4life,2025-05-29T05:15:58+00:00,(Hands On) Writing and Optimizing SQL Queries with ChatGPT,bigdata,0,https://www.reddit.com/r/bigdata/comments/1ky2nwb/hands_on_writing_and_optimizing_sql_queries_with/,r_1ky2nwb,,,
r_1kxfdi9,reddit,sharmaniti437,2025-05-28T12:06:45+00:00,"Python in Data Science
Python is the ultimate data whispererâ€”transforming complex datasets into clear, compelling stories with just a few lines of code. From cleaning chaos to uncovering trends, [Python](https://www.usdsi.org/data-science-insights/resources/python-for-data-science-explained-in-6-easy-steps) is the language that turns data science into data art.

https://preview.redd.it/8upymt5yji3f1.jpg?width=1080&format=pjpg&auto=webp&s=10615e26a8053f1ccca25e306cc1dfcdad9271af

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1kxfdi9/python_in_data_science/,r_1kxfdi9,,,
r_1kx9pii,reddit,bigdataengineer4life,2025-05-28T06:01:32+00:00,"Write and Optimize SQL Queries with ChatGPT (Hands-On Guide!)
ðŸš€ New Video Drop: Write and Optimize SQL Queries with ChatGPT (Hands-On Guide!)

Struggling with complex SQL queries or looking to write cleaner, faster code?

Let ChatGPT be your co-pilot in mastering SQLâ€”especially for Big Data and Spark environments!

ðŸ” In this hands-on video, you'll learn:

âœ… How to write SQL queries with ChatGPT

âœ… Optimizing SQL for performance in large datasets

âœ… Debugging and enhancing your queries with AI

âœ… Real-world examples tailored for Data Engineers

âœ… How ChatGPT fits into your Big Data stack (Hadoop/Spark)

ðŸ’¡ Perfect for:

Data Engineers working with massive datasets

SQL beginners and pros looking to optimize queries

Anyone exploring AI-assisted coding in analytics

ðŸ”¥ Donâ€™t miss this productivity boost for your data workflows!

ðŸ› ï¸ Tech Covered: SQL â€¢ ChatGPT â€¢ Apache Spark â€¢ Hadoop

ðŸ‘‡ Check it out & share your thoughts in the comments!

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1kx9pii/write_and_optimize_sql_queries_with_chatgpt/,r_1kx9pii,,,
r_1kwpqzy,reddit,Beneficial_Baby5458,2025-05-27T15:11:25+00:00,"[1999â€“2025] SEC Filings - 21,000 funds. 850,000+ detailed filings. Full portfolios, control rights, phone numbers, addresses. Itâ€™s all here.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kwpqzy/19992025_sec_filings_21000_funds_850000_detailed/,r_1kwpqzy,,,
r_1kwo52p,reddit,hammerspace-inc,2025-05-27T14:05:44+00:00,The 16 Largest US Funding Rounds of April 2025,bigdata,0,https://www.reddit.com/r/bigdata/comments/1kwo52p/the_16_largest_us_funding_rounds_of_april_2025/,r_1kwo52p,,,
r_1kwmm57,reddit,JanethL,2025-05-27T12:58:44+00:00,Scaling AI Applications with Open-Source Hugging Face Models,bigdata,0,https://www.reddit.com/r/bigdata/comments/1kwmm57/scaling_ai_applications_with_opensource_hugging/,r_1kwmm57,,,
r_1kwgjlx,reddit,Shawn-Yang25,2025-05-27T06:39:48+00:00,Apache Fury serialization framework 0.10.3 released,bigdata,1,https://www.reddit.com/r/bigdata/comments/1kwgjlx/apache_fury_serialization_framework_0103_released/,r_1kwgjlx,,,
r_1kvnoic,reddit,sharmaniti437,2025-05-26T06:38:54+00:00,"DATA SCIENCE CERTIFICATIONS
Getting certified shows youâ€™re not just interestedâ€”youâ€™ve got the skills to back it up. It makes your resume pop and helps you stand out when applying for those high-paying, exciting data science jobs. Plus, youâ€™ll learn the latest data science tools and techniques that keep you ahead of the curve. 

Bottom line? A [Data Science Certification](https://www.usdsi.org/data-science-certifications) is one of the smartest moves to boost your career and open new doors in data science.

https://preview.redd.it/xncy5t1nn23f1.jpg?width=1400&format=pjpg&auto=webp&s=9a53160c0f2475f807d98c43acb737701ce7a023

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1kvnoic/data_science_certifications/,r_1kvnoic,,,
r_1kvm2h8,reddit,bigdataengineer4life,2025-05-26T04:57:43+00:00,Running Hive on Windows Using Docker Desktop (Hands On),bigdata,1,https://www.reddit.com/r/bigdata/comments/1kvm2h8/running_hive_on_windows_using_docker_desktop/,r_1kvm2h8,,,
r_1kvfdju,reddit,jekapats,2025-05-25T22:53:16+00:00,"Cursor for data with chat, rich context and tool use (Currently supports PostgreSQL and BigQuery)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kvfdju/cursor_for_data_with_chat_rich_context_and_tool/,r_1kvfdju,,,
r_1kuzrib,reddit,shokatjaved,2025-05-25T10:52:59+00:00,"Spacebar Counter Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kuzrib/spacebar_counter_using_html_css_and_javascript/,r_1kuzrib,,,
r_1kuvk51,reddit,bigdataengineer4life,2025-05-25T06:02:11+00:00,The 10 Coolest Open-Source Software Tools of 2025 in Big Data Technologies,bigdata,2,https://www.reddit.com/r/bigdata/comments/1kuvk51/the_10_coolest_opensource_software_tools_of_2025/,r_1kuvk51,,,
r_1kuu0lm,reddit,x36_,2025-05-25T04:23:49+00:00,"Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.
Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.

Hereâ€™s what it does:

âœ… Automates the process of asking your customers for Google reviews via SMS

âœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)

âœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask

Right now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.

If you:

* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)

* Get at least 5-20 customers a day

* Are interested in trying it out for a few weeks
â€¦ Iâ€™d love to connect.

As a thank you, youâ€™ll get free access even after the beta ends.

If this sounds interesting, just drop a comment or DM me with:

* What kind of business you have

* How many customers you typically serve in a day

* Whether youâ€™re in the U.S.

Iâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kuu0lm/hey_everyone_i_hope_this_is_okay_to_post_here/,r_1kuu0lm,,,
r_1kutzvl,reddit,x36_,2025-05-25T04:22:34+00:00,"Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.
Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.

Hereâ€™s what it does:

âœ… Automates the process of asking your customers for Google reviews via SMS

âœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)

âœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask

Right now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.

If you:

* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)

* Get at least 5-20 customers a day

* Are interested in trying it out for a few weeks
â€¦ Iâ€™d love to connect.

As a thank you, youâ€™ll get free access even after the beta ends.

If this sounds interesting, just drop a comment or DM me with:

* What kind of business you have

* How many customers you typically serve in a day

* Whether youâ€™re in the U.S.

Iâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",bigdata,0,https://www.reddit.com/r/bigdata/comments/1kutzvl/hey_everyone_i_hope_this_is_okay_to_post_here/,r_1kutzvl,,,
r_1kutz53,reddit,x36_,2025-05-25T04:21:18+00:00,"Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.
Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.

Hereâ€™s what it does:

âœ… Automates the process of asking your customers for Google reviews via SMS

âœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)

âœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask

Right now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.

If you:

* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)

* Get at least 5-20 customers a day

* Are interested in trying it out for a few weeks
â€¦ Iâ€™d love to connect.

As a thank you, youâ€™ll get free access even after the beta ends.

If this sounds interesting, just drop a comment or DM me with:

* What kind of business you have

* How many customers you typically serve in a day

* Whether youâ€™re in the U.S.

Iâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",bigdata,2,https://www.reddit.com/r/bigdata/comments/1kutz53/hey_everyone_i_hope_this_is_okay_to_post_here/,r_1kutz53,,,
r_1kut3po,reddit,shokatjaved,2025-05-25T03:29:11+00:00,"Golden Birthday Calculator Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",bigdata,0,https://www.reddit.com/r/bigdata/comments/1kut3po/golden_birthday_calculator_using_html_css_and/,r_1kut3po,,,
r_1ktjhay,reddit,sharmaniti437,2025-05-23T13:23:46+00:00,"DATA ACCESSIBILITY AND DATA DEMOCRATIZATION
Struggling with slow decisions due to limited data access? Itâ€™s time to [democratize data](https://www.usdsi.org/data-science-insights/how-data-accessibility-and-democratization-power-businesses)! Empower every teamâ€”from marketing to salesâ€”with real-time insights and user-friendly tools. 

Build a data-driven culture where smart, fast decisions are the norm. Discover how data democratization transforms business agility and innovation. 

https://preview.redd.it/ppr3jyj49j2f1.png?width=1080&format=png&auto=webp&s=cb93a9e2e031ab4eeec046c161bdb2eb2ce460f2

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1ktjhay/data_accessibility_and_data_democratization/,r_1ktjhay,,,
r_1ktcans,reddit,bigdataengineer4life,2025-05-23T05:59:56+00:00,Apache Spark vs. Hadoop: Which One Should You Learn in 2025?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1ktcans/apache_spark_vs_hadoop_which_one_should_you_learn/,r_1ktcans,,,
r_1ksm0pq,reddit,sharmaniti437,2025-05-22T08:58:59+00:00,"Which World-Class Certification to Head-Start Your Data Science Career? (CDSPâ„¢)
Kick start your data science career journey with one of the most comprehensive and detailed data science certification programs for beginners â€“ the [Certified Data Science Professional (CDSPâ„¢)](https://www.usdsi.org/data-science-certifications/certified-data-science-professional).

Offered by the United States Data Science Institute (USDSIÂ®), this online and self-paced learning program will help you master the fundamentals of data science, including data wrangling, big data, exploratory data analysis, visualization, and more, all with free study materials including eBooks, lecture videos, and practice codes.

Whether a graduate or a professional looking to switch to a data science career, this certification can be a perfect starting point for you.

https://preview.redd.it/boc1ibvxsa2f1.jpg?width=1080&format=pjpg&auto=webp&s=8a633fb4e61c49bff49054098383f2728602410a

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1ksm0pq/which_worldclass_certification_to_headstart_your/,r_1ksm0pq,,,
r_1kshux2,reddit,bigdataengineer4life,2025-05-22T04:14:59+00:00,Download Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers),bigdata,0,https://www.reddit.com/r/bigdata/comments/1kshux2/download_free_ebook_for_bigdata_interview/,r_1kshux2,,,
r_1krc30y,reddit,dofthings,2025-05-20T18:18:59+00:00,How Business Intelligence (BI) & Analytics Trends Evolved from 2021 to 2025,bigdata,1,https://www.reddit.com/r/bigdata/comments/1krc30y/how_business_intelligence_bi_analytics_trends/,r_1krc30y,,,
r_1kqnbs6,reddit,New-Ship-5404,2025-05-19T21:10:28+00:00,Batch vs Micro-Batch vs Streaming â€” What I Learned After Building Many Pipelines,bigdata,2,https://www.reddit.com/r/bigdata/comments/1kqnbs6/batch_vs_microbatch_vs_streaming_what_i_learned/,r_1kqnbs6,,,
r_1kqcdqg,reddit,shokatjaved,2025-05-19T13:58:45+00:00,"Bohr Model of Atom Animations Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kqcdqg/bohr_model_of_atom_animations_using_html_css_and/,r_1kqcdqg,,,
r_1kotsxe,reddit,Fahim61891012,2025-05-17T14:05:48+00:00,"Solidus AITECH: Redefining HPC in Europe
https://preview.redd.it/m4oe5rnymc1f1.png?width=1920&format=png&auto=webp&s=1f7dd6a2e364aacbc118cd84e9a1ab0f3926c8f9

Europe demands about one-third of global high-performance computing (HPC) capacity but can supply just 5% through local data centers. As a result, researchers and engineers often turn to costly U.S.-based supercomputers for their projects. Solidus AITECH aims to bridge this gap by building eco-friendly, on-continent HPC infrastructure tailored to Europeâ€™s needs.

**Why Now Is the Moment for HPC Innovation**

* Demand is exploding: from AI training and genome sequencing to climate modeling and complex financial simulations, workloads now routinely require petaflops of computing power.
* Digital sovereignty is central to the EUâ€™s strategy: without robust local HPC infrastructure, true data and computation independence isnâ€™t achievable.
* Sustainability pressures are mounting: strict environmental regulations make carbon-neutral data centers powered by renewables and advanced cooling technologies increasingly attractive to investors.

**Decentralized HPC with Blockchain and AI**

* Transparent resource management: a blockchain ledger records when and where each compute job runs, eliminating single points of failure.
* Token-based incentives: hardware providers earn â€œHPC tokensâ€ for contributing resources, motivating them to maintain high quality and availability.
* AI-driven optimization: smart contracts powered by AI route workloads based on cost, performance, and carbon footprint criteria to the most suitable HPC nodes.

**Solidus AITECHâ€™s Layered Approach**

1. **Marketplace Layer:** Users can rent CPU/GPU time via spot or futures contracts.
2. **AI-Powered Scheduling:** Workloads are automatically filtered and dispatched to the most efficient HPC resources, balancing cost-performance and sustainability.
3. **Green Data Center (Bucharest, 8,800 ftÂ²):** Built around renewable energy and liquid-cooling systems, this carbon-neutral facility will support both scientific and industrial HPC applications.

**Value for Investors and Web3 Developers**

* **Investors** can leverage EU-backed funding streams (e.g., Horizon Europe) alongside tokenized revenue models to optimize their risk-return profile.
* **Web3 Developers** gain on-demand access to GPU-intensive HPC workloads through smart contracts, without needing to deploy or maintain their own infrastructure.

**Next Steps**

1. Launch comprehensive pilot projects with leading European research institutions.
2. Accelerate integration via open-source APIs, SDKs, and sample applications.
3. Design dynamic token-economy mechanisms to ensure market stability and liquidity.
4. Enhance sustainability transparency through ESG reporting dashboards and independent audits.
5. Build community awareness with technical webinars, hackathons, and success stories.

By consolidating Europeâ€™s HPC capacity with a green, blockchain-enabled architecture and AI-driven orchestration, Solidus AITECH will strengthen digital sovereignty and unlock fresh opportunities for the crypto ecosystem. This vision represents a long-term investment in the continentâ€™s digital future.",bigdata,9,https://www.reddit.com/r/bigdata/comments/1kotsxe/solidus_aitech_redefining_hpc_in_europe/,r_1kotsxe,,,
r_1kos8ed,reddit,deshpande_varun,2025-05-17T12:50:18+00:00,"Big data QA
I have my interview for big data qa role ..what are the possible interview questions or topics that I must study?",bigdata,2,https://www.reddit.com/r/bigdata/comments/1kos8ed/big_data_qa/,r_1kos8ed,,,
r_1kopdxe,reddit,sharmaniti437,2025-05-17T09:58:32+00:00,"Snowflake vs. Databricks: Which Data Platform Wins?
Choosing the right data platform can define your success with analytics, machine learning, and business insights. Dive into our in-depth comparison of [Snowflake vs. Databricks](https://www.usdsi.org/data-science-insights/choosing-the-right-data-platform-snowflake-vs-databricks) â€” two giants in the modern data stack. 

From architecture and performance to cost and use cases, find out which platform fits your organizationâ€™s goals best.

https://preview.redd.it/4qrtnqqxeb1f1.png?width=1080&format=png&auto=webp&s=10fedf64188b83c15461ae44608c73ba8b566d1b

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kopdxe/snowflake_vs_databricks_which_data_platform_wins/,r_1kopdxe,,,
r_1ko9xx5,reddit,Wikar,2025-05-16T19:44:40+00:00,"Data Modeling - star scheme case
Hello,  
I am currently working on data modelling in my master degree project. I have designed scheme in 3NF. Now I would like also to design it in star scheme. Unfortunately I have little experience in data modelling and I am not sure if it is proper way of doing so (and efficient).

3NF:

https://preview.redd.it/p5b1jixr671f1.png?width=1084&format=png&auto=webp&s=650e3e9a671e30f0f27847f382407804a1227640

Star Schema:

https://preview.redd.it/si2qubxs671f1.png?width=1103&format=png&auto=webp&s=1763dc58024279f54237ab926d226a967a73e76c

Appearances table is responsible for participation of people in titles (tv, movies etc.). Title is the most center table of the database because all the data revolves about rating of titles. I had no better idea than to represent person as factless fact table and treat appearances table as a bridge. Could tell me if this is valid or any better idea to model it please?",bigdata,3,https://www.reddit.com/r/bigdata/comments/1ko9xx5/data_modeling_star_scheme_case/,r_1ko9xx5,,,
r_1knv6dh,reddit,sharmaniti437,2025-05-16T07:43:03+00:00,"DATA CLEANING MADE EASY
Organizations across all industries now heavily rely on data-driven insights to make decisions and transform their business operations. Effective data analysis is one essential part of this transformation. 

But for effective data analysis, it is important that the data used is clean, consistent, and accurate. The real-world data that data science professionals collect for analysis is often messy. These data are often collected from social media, customer transactions, sensors, feedback, forms, etc. And therefore, it is normal for the datasets to be inconsistent and with errors.

This is why data cleaning is a very important process in the data science project lifecycle. You may find it surprising that 83% of data scientists are using machine learning methods regularly in their tasks, including data cleaning, analysis, and data visualization (source: market.us). 

These advanced techniques can, of course, speedup the data science processes. However, if you are a beginner, then you can use Pandaâ€™s one-liners to correct a lot of inconsistencies and missing values in your datasets.

In the following infographic, we explore the top 10 Pandas one-liners that you can use for:

â€¢ Dropping rows with missing values

â€¢ Extracting patterns with regular expressions

â€¢ Filling missing values

â€¢ Removing duplicates, and more

The infographic also guides you on how to create a sample dataframe from GitHub to work on.

Check out this infographic and master Pandaâ€™s one-liners for [data cleaning](https://www.usdsi.org/data-science-insights/top-5-automation-tools-for-data-cleaning)

https://preview.redd.it/ici3zlxyl31f1.png?width=1080&format=png&auto=webp&s=0b2ee9afe75b5ca50f454735bb5a51b2e83d3dc0



",bigdata,1,https://www.reddit.com/r/bigdata/comments/1knv6dh/data_cleaning_made_easy/,r_1knv6dh,,,
r_1knujxn,reddit,PM_ME_LINUX_CONFIGS,2025-05-16T06:58:14+00:00,"Best practice to get fed by Oracle database to process data?
I have a oracledb tables, that get updated in various fashions- daily, hourly, biweekly, monthly etc. The data is usually inserted millions of rows into the tables but needs processing. What is the best way to get this stream of rows, process and then put it into another oracledb / parquet format etc.",bigdata,3,https://www.reddit.com/r/bigdata/comments/1knujxn/best_practice_to_get_fed_by_oracle_database_to/,r_1knujxn,,,
r_1knrie9,reddit,bigdataengineer4life,2025-05-16T03:47:26+00:00,ChatGPT for Data Engineers Hands On Practice,bigdata,0,https://www.reddit.com/r/bigdata/comments/1knrie9/chatgpt_for_data_engineers_hands_on_practice/,r_1knrie9,,,
r_1knp2ch,reddit,CKRET__,2025-05-16T01:35:34+00:00,"Looking for a car dataset
Hey folks,
Iâ€™m building a car spotting app and need to populate a database with vehicle makes, models, trims, and years. Iâ€™ve found the NHTSA API for US cars, which is great and free. But Iâ€™m struggling to find something similar for EU/UK vehicles â€” ideally a service or API that covers makes/models/trims with decent coverage.

Has anyone come across a good resource or service for this? Bonus points if itâ€™s free or low-cost! Iâ€™m open to public datasets, APIs, or even commercial providers.

Thanks in advance!
",bigdata,1,https://www.reddit.com/r/bigdata/comments/1knp2ch/looking_for_a_car_dataset/,r_1knp2ch,,,
r_1knfmas,reddit,Danielpot33,2025-05-15T18:31:03+00:00,"Where to find vin decoded data to use for a dataset?
Where to find vin decoded data to use for a dataset?
Currently building out a dataset full of vin numbers and their decoded information(Make,Model,Engine Specs, Transmission Details, etc.). What I have so far is the information form NHTSA Api, which works well, but looking if there is even more available data out there.
Does anyone have a dataset or any source for this type of information that can be used to expand the dataset?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1knfmas/where_to_find_vin_decoded_data_to_use_for_a/,r_1knfmas,,,
r_1kn2a41,reddit,major_grooves,2025-05-15T07:28:15+00:00,Efficient Graph Storage for Entity Resolution Using Clique-Based Compression,bigdata,2,https://www.reddit.com/r/bigdata/comments/1kn2a41/efficient_graph_storage_for_entity_resolution/,r_1kn2a41,,,
r_1kmpaa3,reddit,dofthings,2025-05-14T20:15:02+00:00,"The D of Things Newsletter #9 â€“ Appleâ€™s AI Flex, Doctor Bots & RAG Warnings",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kmpaa3/the_d_of_things_newsletter_9_apples_ai_flex/,r_1kmpaa3,,,
r_1kmcofi,reddit,Big_Data_Path,2025-05-14T11:24:22+00:00,Big Data Analytics: Comprehensive Guide to How It Works,bigdata,2,https://www.reddit.com/r/bigdata/comments/1kmcofi/big_data_analytics_comprehensive_guide_to_how_it/,r_1kmcofi,,,
r_1kmc9nl,reddit,GreenMobile6323,2025-05-14T11:00:25+00:00,"Best practices for ensuring cluster high availability
I'm looking for best practices to ensure high availability in a distributed NiFi cluster. We've got Zookeeper clustering, externalized flow configuration, and persistent storage for state, but would love to hear about additional steps or strategies you use for failover, node redundancy, and resiliency.

How do you handle scenarios like node flapping, controller service conflicts, or rolling updates with minimal downtime? Also, do you leverage Kubernetes or any external queueing systems for better HA?",bigdata,2,https://www.reddit.com/r/bigdata/comments/1kmc9nl/best_practices_for_ensuring_cluster_high/,r_1kmc9nl,,,
r_1klolbk,reddit,superconductiveKyle,2025-05-13T15:17:43+00:00,"Enhancing legal document comprehension using RAG: A practical application
Iâ€™ve been working on a project to help non-lawyers better understand legal documents without having to read them in full. Using a Retrieval-Augmented Generation (RAG) approach, I developed a tool that allows users to ask questions about live terms of service or policies (e.g., Apple, Figma) and receive natural-language answers.

The aim isnâ€™t to replace legal advice but to see if AI can make legal content more accessible to everyday users.

It uses a simple RAG stack:

* **Scraper:**Â Browserless
* **Indexing/Retrieval:**Â [Ducky.ai](http://Ducky.ai)
* **Generation:**Â OpenAI
* **Frontend:**Â Next.js

Indexed content is pulled and chunked, retrieved with Ducky, and passed to OpenAI with context to answer naturally.

Iâ€™m interested in hearing thoughts from you all on the potential and limitations of such tools. I documented the development process and [some reflections in this blog post](https://ducky.ai/blog/fineprint-how-to-build-a-legal-document-chat-with-ducky-ai?utm_source=reddit-bigdata&utm_medium=community&utm_campaign=technical-use_case)

Would appreciate any feedback or insights!",bigdata,3,https://www.reddit.com/r/bigdata/comments/1klolbk/enhancing_legal_document_comprehension_using_rag/,r_1klolbk,,,
r_1klex8j,reddit,GreenMobile6323,2025-05-13T06:18:09+00:00,"Best Way to Structure ETL Flows in NiFi
Iâ€™m building ETL flows in Apache NiFi to move data from a MySQL database to a cloud data warehouse - Snowflake.

Whatâ€™s a better way to structure the flow? Should I separate the Extract, Transform, and Load stages into different process groups, or should I create one end-to-end process group per table?",bigdata,2,https://www.reddit.com/r/bigdata/comments/1klex8j/best_way_to_structure_etl_flows_in_nifi/,r_1klex8j,,,
r_1kjspea,reddit,Neat-Resort9968,2025-05-11T04:31:11+00:00,Mastering Snowflake Performance: 10 Queries Every Engineer Should Know,bigdata,1,https://www.reddit.com/r/bigdata/comments/1kjspea/mastering_snowflake_performance_10_queries_every/,r_1kjspea,,,
r_1kjd1cz,reddit,Alternative_Coat554,2025-05-10T15:34:40+00:00,"Request for Google Form Filling (Questionnaire)
Dear Participant,  
We are conducting a research study on enhancing cloud security to prevent data leaks, as part of our academic project at Catholic University in Erbil. Your insights and experiences are highly valuable and will contribute significantly to our understanding of current cloud security practices. The questionnaire will only take a few minutes to complete, and all responses will remain anonymous and confidential. We kindly ask for your participation by filling out the form linked below. Your support is greatly appreciated!



[https://docs.google.com/forms/d/e/1FAIpQLSdN7Zs9KVxFbwb4gxnS-7bijiu7dmH9bLRYv3jT0yXcdApsrw/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLSdN7Zs9KVxFbwb4gxnS-7bijiu7dmH9bLRYv3jT0yXcdApsrw/viewform?usp=header)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kjd1cz/request_for_google_form_filling_questionnaire/,r_1kjd1cz,,,
r_1kj9svw,reddit,Zestyclose_Sport_556,2025-05-10T13:03:15+00:00,"I Built an AI job board with 9000+ fresh big data jobs
I built an AI job board and scraped AI, Machine Learning, Big Data jobs from the past month. It includes 100,000+ AI & Machine Learning jobs and 9000+ Big data jobs from tech companies, ranging from top tech giants to startups.

So, if you're looking for AI,Machine Learning, big data jobs, this is all you need â€“ and it's completely free!
Currently, it supports more than 20 countries and regions.

I can guarantee that it is the most user-friendly job platform focusing on the AI industry.
If you have any issues or feedback, feel free to leave a comment. Iâ€™ll do my best to fix it within 24 hours (Iâ€™m all in! Haha).

You can check all the big data Jobs here: https://easyjobai.com/search/big-data
Feel free to join our subreddit r/AIHiring to share feedback and follow updates!",bigdata,11,https://www.reddit.com/r/bigdata/comments/1kj9svw/i_built_an_ai_job_board_with_9000_fresh_big_data/,r_1kj9svw,,,
r_1kis8uu,reddit,JoeKarlssonCQ,2025-05-09T20:19:41+00:00,How We Handle Billion-Row ClickHouse Inserts With UUID Range Bucketing,bigdata,1,https://www.reddit.com/r/bigdata/comments/1kis8uu/how_we_handle_billionrow_clickhouse_inserts_with/,r_1kis8uu,,,
r_1kih92h,reddit,Ambrus2000,2025-05-09T12:28:18+00:00,"How Do You Handle Massive Datasets? Whatâ€™s Your Stack and How Do You Scale?
Hi everyone!  
Iâ€™m a product manager working with a team thatâ€™s recently started dealing with datasets in the tens of millions of rows-think user events, product analytics, and customer feedback. Our current tooling is starting to buckle under the load, especially when it comes to real-time dashboards and ad-hoc analyses.

Iâ€™m curious:

* **Whatâ€™s your current stack for storing, processing, and analyzing large datasets?**
* **How do you handle scaling as your data grows?**
* **Any tools or practices youâ€™ve found especially effective (or surprisingly expensive)?**
* **Tips for keeping costs under control without sacrificing performance?**

",bigdata,5,https://www.reddit.com/r/bigdata/comments/1kih92h/how_do_you_handle_massive_datasets_whats_your/,r_1kih92h,,,
r_1ki2kpn,reddit,goldmanthisis,2025-05-08T22:02:42+00:00,All the ways to capture changes in Postgres,bigdata,1,https://www.reddit.com/r/bigdata/comments/1ki2kpn/all_the_ways_to_capture_changes_in_postgres/,r_1ki2kpn,,,
r_1khymh3,reddit,hammerspace-inc,2025-05-08T19:17:43+00:00,WEBINAR Linux Storage Server and NFS Advancements: Creating a High-Performance Standard for AI Workloads,bigdata,1,https://www.reddit.com/r/bigdata/comments/1khymh3/webinar_linux_storage_server_and_nfs_advancements/,r_1khymh3,,,
r_1khtnri,reddit,Rollstack,2025-05-08T15:58:16+00:00,"We've shipped a batch of updates focused on one thing: saving time. From support for Tableau Custom Views and email tracking to a new AI insights interface, hereâ€™s whatâ€™s new this month.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1khtnri/weve_shipped_a_batch_of_updates_focused_on_one/,r_1khtnri,,,
r_1kht36w,reddit,Shawn-Yang25,2025-05-08T15:34:37+00:00,Apache Fury Serialization Framework 0.10.2 Released: Chunk-based map Serialization to reduce payload size by up to 2X,bigdata,1,https://www.reddit.com/r/bigdata/comments/1kht36w/apache_fury_serialization_framework_0102_released/,r_1kht36w,,,
r_1khrl88,reddit,ZebraM-3572,2025-05-08T14:32:34+00:00,"backtesting predictive market data
My company has some Alt data that we think can be used by investors to predict company movements. We need a proof of concept to go to market I belive, can anyone recomend a reputible company that can provide such a thing - ie a company that can analyse our data and see if it does correlate with a companies value and proivide us third party validation of the predicitve capabilities as such. Many thanks for any help and advice.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1khrl88/backtesting_predictive_market_data/,r_1khrl88,,,
r_1khnimz,reddit,GreenMobile6323,2025-05-08T11:16:02+00:00,"Go-to method for building reusable flow logic in NiFi
Iâ€™ve been working on building out some data flows and am trying to figure out the best way to make them more reusable across different projects. I want to avoid duplicating work and keep things modular, so Iâ€™m curious: Whatâ€™s your go-to method for building reusable flow logic in NiFi?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1khnimz/goto_method_for_building_reusable_flow_logic_in/,r_1khnimz,,,
r_1khfjmb,reddit,unknown,2025-05-08T02:35:39+00:00,Best Big Data Courses on Udemy to learn in 2025,bigdata,1,https://www.reddit.com/r/bigdata/comments/1khfjmb/best_big_data_courses_on_udemy_to_learn_in_2025/,r_1khfjmb,,,
r_1kf4xxu,reddit,GeneBackground4270,2025-05-05T06:31:12+00:00,If you love Spark but hate PyDeequ â€“ check out SparkDQ (early but promising),bigdata,1,https://www.reddit.com/r/bigdata/comments/1kf4xxu/if_you_love_spark_but_hate_pydeequ_check_out/,r_1kf4xxu,,,
r_1kd108b,reddit,Capable-Mall-2067,2025-05-02T13:30:27+00:00,Supercharge your R workflows with DuckDB,bigdata,3,https://www.reddit.com/r/bigdata/comments/1kd108b/supercharge_your_r_workflows_with_duckdb/,r_1kd108b,,,
r_1kd0cle,reddit,sharmaniti437,2025-05-02T13:00:12+00:00,"Power BI With Breakthrough AI
With AI-driven features- sentiment analysis, key phrase extraction, and image recognition- Power BI enables data specialists to visualize complex data, automate reporting, and enhance decision-making with precision. Whether you're a data analyst, business leader, or tech enthusiast, AI-powered Power BI empowers you to turn raw data into actionable intelligenceâ€”all with a few clicks!

ðŸ“Š Ready to revolutionize your analytics? Unlock the future of data visualization! ðŸ”¥

https://preview.redd.it/jb3lhx8q9dye1.png?width=1080&format=png&auto=webp&s=35796a81cb31f794dac6dc03c0058063116bcd66

",bigdata,3,https://www.reddit.com/r/bigdata/comments/1kd0cle/power_bi_with_breakthrough_ai/,r_1kd0cle,,,
r_1kc8ksu,reddit,sharmaniti437,2025-05-01T13:21:12+00:00,"DSIâ€™s Certified Data Science Professional
With a self-paced learning format, industry-relevant global curriculum, and expert guidance from the USDSIÂ® Data Science Advisory Board, Certified Data Science Professional (CDSPâ„¢) certification ensures you stay ahead in data science. Whether you're a fresh graduate or industry beginners, CDSPâ„¢ empowers you with the breakthrough knowledge and expertise to analyze complex data, build predictive models, and drive data-driven decisions.

Join the global workforce of millions data science professionals and take your career to newer heights with CDSPâ„¢. 

https://reddit.com/link/1kc8ksu/video/mr3wzz6l86ye1/player

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kc8ksu/dsis_certified_data_science_professional/,r_1kc8ksu,,,
r_1kc2in5,reddit,PuzzleheadedYou4992,2025-05-01T06:57:27+00:00,"Is AI starting to replace parts of the data engineering workflow?
AI is now being used to handle things like pipeline generation, data transformation, and anomaly detection. Some of this feels like early automation, but itâ€™s moving fast.
Are we looking at full on role changes, or just smarter tooling?",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kc2in5/is_ai_starting_to_replace_parts_of_the_data/,r_1kc2in5,,,
r_1kbugli,reddit,Rollstack,2025-04-30T23:22:30+00:00,"Monthly Business Reviews (MBRs) got you and your team stressed?
ðŸ“… Monthly Business Reviews (MBRs) got you and your team stressed?

Youâ€™re not alone, but there is a better way.

Companies like Zillow, SoFi, and TripAdvisor useÂ [Rollstack](https://www.rollstack.com/ai-report-generation-demo)Â to automate data-driven PowerPoint and Google Slides reports, enabling their teams to focus on sharing insights rather than screenshots.Â 

* Pull directly from your BI dashboards (Tableau, Power BI, Looker, Metabase & Google Sheets) into your report PowerPoints and docs. 
* Deliver MBRs, QBRs, and EBRs in seconds (not days)
* Error-free, up-to-date reporting sent to your inbox or shared drive

See how it works and schedule a demo atÂ [www.Rollstack.com](http://www.rollstack.com/).",bigdata,2,https://www.reddit.com/r/bigdata/comments/1kbugli/monthly_business_reviews_mbrs_got_you_and_your/,r_1kbugli,,,
r_1kbqh9n,reddit,AMDataLake,2025-04-30T20:27:00+00:00,Blog: Whatâ€™s New in Apache Iceberg Format Version 3?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1kbqh9n/blog_whats_new_in_apache_iceberg_format_version_3/,r_1kbqh9n,,,
r_1kbjpu0,reddit,JanethL,2025-04-30T15:45:17+00:00,Build Your First AI Agent with Google ADK and Teradata (Part 1),bigdata,1,https://www.reddit.com/r/bigdata/comments/1kbjpu0/build_your_first_ai_agent_with_google_adk_and/,r_1kbjpu0,,,
r_1kber4m,reddit,GreenMobile6323,2025-04-30T12:04:09+00:00,"Migration from Legacy System to Open-Source
Currently, my organization uses a licensed tool from a specific vendor for ETL needs. We are paying a hefty amount for licensing fees and are not receiving support on time. As the tool is completely managed by the vendor, we are not able to make any modifications independently. 

Can you suggest a few open-source options? Also, I'm looking for round-the-clock support for the same tool. ",bigdata,2,https://www.reddit.com/r/bigdata/comments/1kber4m/migration_from_legacy_system_to_opensource/,r_1kber4m,,,
r_1kak925,reddit,sharmaniti437,2025-04-29T10:02:30+00:00,"Quick Tips For Easy Unit Testing In Python | Infographic
Know what Python Code is and how well you can deduce [Python frameworks](https://www.usdsi.org/data-science-insights/quick-tips-for-easy-unit-testing-in-python) with quick steps. Deploy seamless unit testing as a top data scientist with sheer skills!

https://preview.redd.it/0qn4l4mbzqxe1.jpg?width=1500&format=pjpg&auto=webp&s=78f953ff33749ce8b4d03f1b8c3131d007c6d313

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1kak925/quick_tips_for_easy_unit_testing_in_python/,r_1kak925,,,
r_1ka6y44,reddit,AMDataLake,2025-04-28T21:20:32+00:00,Apache Iceberg Clustering: Technical Blog,bigdata,3,https://www.reddit.com/r/bigdata/comments/1ka6y44/apache_iceberg_clustering_technical_blog/,r_1ka6y44,,,
r_1ka0y9j,reddit,shokatjaved,2025-04-28T17:14:52+00:00,"SQL Commands | DDL, DQL, DML, DCL and TCL Commands - JV Codes 2025
Mastery ofÂ [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/)Â isÂ essential for someone who deals with SQL databases.Â [SQL](https://jvcodes.com/what-is-sql/)Â provides an easy system to create, modify, and arrange data. This article uses straightforward language to explain [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/)â€”DDL, DQL, DML, DCL, andÂ [TCL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/).

SQL serves as one of the fundamental subjects that beginners frequently ask about its nature. SQL stands forÂ **Structured Query Language**. The programming system is a database communication protocol instead of a complete programming language.

# What Are SQL Commands?

A database connects throughÂ [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/), which transmit instructions to it. The system enables users to build database tables, input data and changes, and delete existing data.

A database can be accessed through fiveÂ [primary SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/).

* [DDL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Definition Language)
* [DQL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Query Language)
* [DML Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Manipulation Language)
* [DCL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Control Language)
* [TCL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Transaction Control Language)",bigdata,0,https://www.reddit.com/r/bigdata/comments/1ka0y9j/sql_commands_ddl_dql_dml_dcl_and_tcl_commands_jv/,r_1ka0y9j,,,
r_1k9vgkh,reddit,Defiant-End-2292,2025-04-28T13:24:28+00:00,Unlock B2B Gold: How to Target Companies Post-Funding with This Sneaky Toolâ€”Free Access to Decision Makers!,bigdata,0,https://www.reddit.com/r/bigdata/comments/1k9vgkh/unlock_b2b_gold_how_to_target_companies/,r_1k9vgkh,,,
r_1k9oomq,reddit,sharmaniti437,2025-04-28T06:09:04+00:00,"Most Rewarding Data Science Jobs for 2025
Certified data scientists can earn over $200k in the US. Are you still thinking of a career in data science? 

Download the latest [USDSIÂ® Data Science Professionalâ€™s Salary Factsheet 2025](https://www.usdsi.org/data-science-insights/resources/salary-guide-2025-for-data-science-professionals) and explore:

Top data science trends

Emerging jobs in the industry

Professionalâ€™s salary across roles and industries, and more.

Update your knowledge about the latest data science facts now. Click here.

https://reddit.com/link/1k9oomq/video/rb6qmqproixe1/player

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1k9oomq/most_rewarding_data_science_jobs_for_2025/,r_1k9oomq,,,
r_1k9lapc,reddit,VictoriaTelos,2025-04-28T02:43:25+00:00,"Big Data & Sustainable AI: Exploring Solidus AI Tech (AITECH) and its Eco-Friendly HPC
r/solidusaitech

Hello Big Data community, this is my second time posting here and I'd like to take this opportunity to thank the community for its support. I've been researching an HPC Data Center that has several interesting points; which is useful information for Big Data. It's about r/solidusaitech Solidus AI Tech, a company focused on providing decentralized AI and sustainable HPC solutions, and also offers a platform with a Compute Marketplace, AI Marketplace, and AITECH Pad.

Among the points that I believe may be of interest to the Big Data community, the following stand out:

An eco-friendly HPC infrastructure located in Europe, focused on improving energy usage. This is important due to the high computational demand for AI solutions and effective access to large amounts of data.

The launch of Agent Forge during Q2 2025 sounds quite interesting; its essence is the creation of AI Agents without code, with the power to automate complex tasks. This is definitely a very useful point for analyzing data and other fields linked to Big Data.

Compute Marketplace (Q2 2025) They also plan to launch a marketplace for accessing compute resources, which could be an option to consider for those looking for processing power for Big Data tasks.

Apart from this, they have announced strategic partnerships with companies like SambaNova Systems, a company that is inventing smarter and faster ways to use Artificial Intelligence in the business world. AITECH is also exploring use cases in Metaverse/Gaming. These sectors require large amounts of data.

I would like to know your opinions on this type of platform that combines decentralized AI with sustainable HPC. Do you see potential in this approach to address the computational needs of Big Data and AI?

Publication for informational purposes, please do your own research (DYOR). 
",bigdata,10,https://www.reddit.com/r/bigdata/comments/1k9lapc/big_data_sustainable_ai_exploring_solidus_ai_tech/,r_1k9lapc,,,
r_1k96m6f,reddit,shokatjaved,2025-04-27T15:29:47+00:00,What is SQL? How to Write Clean and Correct SQL Commands for Beginners - JV Codes 2025,bigdata,0,https://www.reddit.com/r/bigdata/comments/1k96m6f/what_is_sql_how_to_write_clean_and_correct_sql/,r_1k96m6f,,,
r_1k7rvq8,reddit,DeeperThanCraterLake,2025-04-25T18:16:48+00:00,"Introducing the Salesforce Tableau sub reddit, your destination for all things Salesforce & Tableau. Please join and contribute.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k7rvq8/introducing_the_salesforce_tableau_sub_reddit/,r_1k7rvq8,,,
r_1k7khhr,reddit,sharmaniti437,2025-04-25T13:07:03+00:00,"Deep Learning Frameworks to Power your Projects
[Deep learning frameworks](https://www.usdsi.org/data-science-insights/resources/the-deep-learning-frameworks-riot-2025) like Pytorch, TensorFlow, and Keras are transforming deep learning models, making them more accurate and efficient. Which one is better, and what are their pros and cons? Most importantly, how are they revolutionizing model development in 2025?

https://preview.redd.it/zp5p75hmczwe1.png?width=1080&format=png&auto=webp&s=f0f442a241abbee0d5ba72b19b8fb390134ea502

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1k7khhr/deep_learning_frameworks_to_power_your_projects/,r_1k7khhr,,,
r_1k6zgur,reddit,Stormbreaker5275,2025-04-24T18:24:29+00:00,"I need help please
Hi, 

I'm an MBA fresher currently working in a founderâ€™s office role at a startup that owns a news app and a short-video (reels) app.



 Iâ€™ve been tasked with researching how ByteDance leverages alternate data from TikTok and its own news app called toutiao to offer financial products like microloans, and then explore how we might replicate a similar model using our own user data. 



I would really appreciate some help as in guidance as to how to go about tackling this as currently i am unable to find anything on the internet.",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k6zgur/i_need_help_please/,r_1k6zgur,,,
r_1k6o4b9,reddit,is669,2025-04-24T09:38:04+00:00,"Anyone have a clean setup for staging data changes before pushing to prod lakes?
Weâ€™re running into issues with testing and rollback across our data lake.
In software, youâ€™d never push code to prod without version control and CI checksâ€”so why is that still the norm in data?

Curious what others are doing to stage/test data changes before they go live.
Are you using isolated environments? Separate S3 buckets? Some kind of custom validation layer?
What works? Whatâ€™s been a nightmare?",bigdata,3,https://www.reddit.com/r/bigdata/comments/1k6o4b9/anyone_have_a_clean_setup_for_staging_data/,r_1k6o4b9,,,
r_1k6eu8g,reddit,Rollstack,2025-04-24T00:24:08+00:00,How SoFi Automates PowerPoint Reports with Tableau & Rollstack | Tableau Conference 2025 AI Session,bigdata,1,https://www.reddit.com/r/bigdata/comments/1k6eu8g/how_sofi_automates_powerpoint_reports_with/,r_1k6eu8g,,,
r_1k5x106,reddit,Ok-Chocolate5088,2025-04-23T11:42:37+00:00,"Call for Papers â€“ IEEE ISADS 2025
â€œThe 17th IEEE International Symposium on Autonomous Decentralized Systemsâ€



July 21â€“24, 2025 | Tucson, Arizona, United States



IEEE ISADS 2025 invites you to be part of an influential symposium focused on the design, development, and deployment of autonomous and decentralized systems. As part of the IEEE CISOSE 2025 Congress, ISADS provides a vibrant platform for researchers and professionals to explore resilient, adaptive, and intelligent system architectures for today's dynamic and distributed environments.



We invite high-quality research contributions on (but not limited to):  

\- Autonomous Decentralized System Architecture and Design  

\- Distributed AI and Intelligent Edge Computing  

\- Blockchain, Smart Contracts, and Trust Management  

\- Resilience and Fault Tolerance in Decentralized Systems  

\- Autonomous System Applications in IoT, Cyber-Physical Systems, and Robotics  

\- Communication Protocols and Coordination Mechanisms  

\- Real-Time and Embedded Autonomous Systems  

\- Industry Case Studies and Deployment Experiences  



Submit your papers via: [https://easychair.org/my/conference?conf=isads2025](https://easychair.org/my/conference?conf=isads2025)



For more details, visit: [https://conf.researchr.org/track/cisose-2025/cisose-2025-ieee-isads-2025](https://conf.researchr.org/track/cisose-2025/cisose-2025-ieee-isads-2025)



Join us in shaping the future of autonomous decentralized systems and contribute to innovations that empower next-generation technologies!



Best Regards,  

Steering Committee  

CISOSE 2025",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k5x106/call_for_papers_ieee_isads_2025/,r_1k5x106,,,
r_1k5do8p,reddit,alex_alv_rojas,2025-04-22T18:29:16+00:00,"Looking for Research Participants: Survey + Interview (w/ compensation)
Hi All,

I'm a PhD candidate conducting research for my dissertation on how data science practitioners use open-source AI platforms (e.g., Kaggle, Hugging Face). This project aims to understand how practitioners interface between value systems on these platforms by observing work practices and processes.

I'm looking for participants of at least **18 years of age** with **at least 3 years** of professional experience to:

1. Take a **5-min initial survey**
2. Join me in a virtual **75-90 minute virtual work session** to discuss a project of your choice that demonstrates the use of Kaggle or Hugging Face.

You will be compensated ($50 VISA gift card) for your time and effort.

Survey can be accessed here: [https://usc.qualtrics.com/jfe/form/SV\_8iYCIuAdvOP7HIG](https://usc.qualtrics.com/jfe/form/SV_8iYCIuAdvOP7HIG)

  
Please reach out with any questions. Thank you for your support in this effort!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k5do8p/looking_for_research_participants_survey/,r_1k5do8p,,,
r_1k5c2e2,reddit,Rollstack,2025-04-22T17:25:28+00:00,"Tableau to PowerPoint in 50 Seconds (YouTube)
Automate PowerPoint reports with Tableau and Rollstack. Visit [www.Rollstack.com](http://www.Rollstack.com) to learn more. ",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k5c2e2/tableau_to_powerpoint_in_50_seconds_youtube/,r_1k5c2e2,,,
r_1k58613,reddit,hammerspace-inc,2025-04-22T14:48:47+00:00,BigDataWire People to Watch 2025: Hammerspace's David Flynn,bigdata,0,https://www.reddit.com/r/bigdata/comments/1k58613/bigdatawire_people_to_watch_2025_hammerspaces/,r_1k58613,,,
r_1k50twr,reddit,Better_Reward486,2025-04-22T07:53:45+00:00,Crack the Code: How Tracking Startup Funding Led to a $10K Boomâ€”Wanna Know the Tool Behind It?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1k50twr/crack_the_code_how_tracking_startup_funding_led/,r_1k50twr,,,
r_1k4k978,reddit,JoeKarlssonCQ,2025-04-21T18:08:58+00:00,Streaming 4TB/month of Cloud Data into ClickHouse: What We Learned,bigdata,5,https://www.reddit.com/r/bigdata/comments/1k4k978/streaming_4tbmonth_of_cloud_data_into_clickhouse/,r_1k4k978,,,
r_1k2tjwg,reddit,Sea-Concept1733,2025-04-19T10:49:57+00:00,"For Anyone seeking to Access ""Top-Rated Data Science Books"" for Starting Data Careers""!
Here is a good resource to ExploreÂ [Amazonâ€™s Best-Rated Data Science Books](https://amzn.to/43FLwwQ) and in one place.

There are resources on several data science topics such as: 

Big data, data science, data analytics, health informatics, cybersecurity, machine learning, business analysis, SQL, Python and more.  

Hope you find it useful!",bigdata,2,https://www.reddit.com/r/bigdata/comments/1k2tjwg/for_anyone_seeking_to_access_toprated_data/,r_1k2tjwg,,,
r_1k2on37,reddit,sharmaniti437,2025-04-19T05:01:15+00:00,"Certified Data Science Professional (CDSPâ„¢)
Tailored for undergraduates, recent graduates, and early-career professionals, the [CDSPâ„¢ certification](https://www.usdsi.org/data-science-certifications/certified-data-science-professional) provides a structured pathway into the data science field. No prior work experience makes it easy to transition into data science roles. Want to know enrolment details and more?

https://preview.redd.it/9o1dcieh4qve1.jpg?width=1080&format=pjpg&auto=webp&s=18f66a17caf097e19b1dd66867e08b90ddc9360e

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1k2on37/certified_data_science_professional_cdsp/,r_1k2on37,,,
r_1k17qtj,reddit,sharmaniti437,2025-04-17T08:22:18+00:00,"CERTIFIED DATA SCIENCE PROFESSIONAL (CDSPâ„¢)
Begin your journey as a Certified Data Scientist with CDSP- pioneering courseware for Data Science Beginners. From industry-centric skillsets, and global recognition, to a holistic blend of practical nuances- CDSP is your go-to Beginner Certification in Data Science. 

https://preview.redd.it/6si7bbgjucve1.jpg?width=1080&format=pjpg&auto=webp&s=1a31d44c8fe233658d8318d3ac6ddc44bbb8ce33

",bigdata,0,https://www.reddit.com/r/bigdata/comments/1k17qtj/certified_data_science_professional_cdsp/,r_1k17qtj,,,
r_1k15rfp,reddit,Intrepid_Raccoon7222,2025-04-17T06:00:25+00:00,Cracking the Code: How Targeting Newly Funded Startups Boosted My Sales by $10K (and the tool that reveals it all!),bigdata,0,https://www.reddit.com/r/bigdata/comments/1k15rfp/cracking_the_code_how_targeting_newly_funded/,r_1k15rfp,,,
r_1k0zklh,reddit,No_Depth_8865,2025-04-17T00:13:06+00:00,Uncover the Power Move: How Recently Funded Startups Become Your Secret B2B Goldmine. Want access to the decision-makers? Let's chat!,bigdata,0,https://www.reddit.com/r/bigdata/comments/1k0zklh/uncover_the_power_move_how_recently_funded/,r_1k0zklh,,,
r_1k0rksi,reddit,dofthings,2025-04-16T18:25:13+00:00,Whatâ€™s the most unexpectedly useful thing youâ€™ve used AI for?,bigdata,1,https://www.reddit.com/r/bigdata/comments/1k0rksi/whats_the_most_unexpectedly_useful_thing_youve/,r_1k0rksi,,,
r_1k0kzvi,reddit,hammerspace-inc,2025-04-16T13:54:15+00:00,Strategic Investors Back Hammerspace as New Standard for AI Data Performance,bigdata,2,https://www.reddit.com/r/bigdata/comments/1k0kzvi/strategic_investors_back_hammerspace_as_new/,r_1k0kzvi,,,
r_1jzkrqm,reddit,bigdataengineer4life,2025-04-15T06:12:19+00:00,"Download Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers) Programming, Scenario-Based, Fundamentals, Performance Tunning",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jzkrqm/download_free_ebook_for_bigdata_interview/,r_1jzkrqm,,,
r_1jzexet,reddit,secodaHQ,2025-04-15T00:48:31+00:00,"AI data analyst LLM
Hey everyone! Weâ€™ve been working on a lightweight version of our data platform (originally built for enterprise teams) and weâ€™re excited to open up a **private beta** for something new: **Seda**.

Seda is a stripped-down, no-frills version of our original product, Secoda â€” but it still runs on the same powerful engine: custom embeddings, SQL lineage parsing, and a RAG system under the hood. The big difference? Itâ€™s designed to be simple, fast, and accessible for *anyone* with a data source â€” not just big companies.

# What you can do with Seda:

* **Ask questions in natural language** and get real answers from your data (Seda finds the right data, runs the query, and returns the result).
* **Write and fix SQL automatically**, just by asking.
* **Generate visualizations** on the fly â€“ no need for a separate BI tool.
* **Trace data lineage** across tables, models, and dashboards.
* **Auto-document your data** â€“ build business glossaries, table docs, and metric definitions instantly.

Behind the scenes, Seda is powered by a system of **specialized data agents**:

* **Lineage Agent**: Parses SQL to create full column- and table-level lineage.
* **SQL Agent**: Understands your schema and dialect, and generates queries that match your naming conventions.
* **Visualization Agent**: Picks the best charts for your data and question.
* **Search Agent**: Searches across tables, docs, models, and more to find exactly what you need.

The agents work together through a smart router that figures out which one (or combination) should respond to your request.

# Hereâ€™s a quick demo:

ðŸ“¹ [Watch it in action](https://www.youtube.com/watch?v=Ny_3HUbt9zw)

# Want to try it?

ðŸ“ [Sign up here for early access](https://www.seda.ai/)

We currently support:  
**Postgres, Snowflake, Redshift, BigQuery, dbt (cloud & core), Confluence, Google Drive, and MySQL.**

Would love to hear what you think or answer any questions!",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jzexet/ai_data_analyst_llm/,r_1jzexet,,,
r_1jytfua,reddit,sharmaniti437,2025-04-14T07:53:18+00:00,"Transforming Business with Data Visualization Effectively| Infographic
Check out our detailed [infographic on data visualization](https://www.usdsi.org/data-science-insights/transforming-business-with-data-visualization-effectively) to understand its importance in businesses, different data visualization techniques, and best practices.

https://preview.redd.it/dbffcphmarue1.jpg?width=1080&format=pjpg&auto=webp&s=e706868ca8f987aac8c9dcd4df6f30c80729bffd

",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jytfua/transforming_business_with_data_visualization/,r_1jytfua,,,
r_1jyi887,reddit,ZealousidealCrew94,2025-04-13T21:13:53+00:00,"Bid data learning for backend dev
Hi! As a backend dev need roadmap on learning big data processing. Things that I need to go through before starting with this job role that works with big data processing. Hiring was language and skill set agnostic. System
Design was asked in all the rounds. 
",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jyi887/bid_data_learning_for_backend_dev/,r_1jyi887,,,
r_1jyg8lb,reddit,jb_nb,2025-04-13T19:46:30+00:00,"Self-Healing Data Quality in DBT â€” Without Any Extra Tools
I just published a practical breakdown of a method I call Observe & Fix â€” a simple way to manage data quality in DBT without breaking your pipelines or relying on external tools.  
Itâ€™s a self-healing pattern that works entirely within DBT using native tests, macros, and logic â€” and itâ€™s ideal for fixable issues like duplicates or nulls.

Includes examples, YAML configs, macros, and even when to alert via Elementary.

Would love feedback or to hear how others are handling this kind of pattern.

[Read the full post here](https://medium.com/@baruchjacob/self-healing-pipelines-with-dbt-the-observe-fix-method-9d6b2da4eae3)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jyg8lb/selfhealing_data_quality_in_dbt_without_any_extra/,r_1jyg8lb,,,
r_1jxw4b2,reddit,unknown,2025-04-13T01:04:49+00:00,Best Big Data Courses on Udemy to learn in 2025,bigdata,2,https://www.reddit.com/r/bigdata/comments/1jxw4b2/best_big_data_courses_on_udemy_to_learn_in_2025/,r_1jxw4b2,,,
r_1jxo2jb,reddit,chiki_rukis,2025-04-12T18:34:28+00:00,"Hi everyone! I'm conducting a university research survey on commonly used Big Data tools among students and professionals. If you work in data or tech, Iâ€™d really appreciate your input â€” it only takes 3 minutes! Thank you
[https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jxo2jb/hi_everyone_im_conducting_a_university_research/,r_1jxo2jb,,,
r_1jxbexz,reddit,sharmaniti437,2025-04-12T06:58:35+00:00,"Data Science Trends Alert 2025
Transform decision-making with a data-driven approach. Are you set to stir the future of data with core trends and emerging techniques in place? Make big moves with informed [data science trends](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends) learnt here.

https://preview.redd.it/nk7j7fx0rcue1.jpg?width=1080&format=pjpg&auto=webp&s=cf8e3231cdd3c0054c225bf2ea15487ffb9f0cda

",bigdata,2,https://www.reddit.com/r/bigdata/comments/1jxbexz/data_science_trends_alert_2025/,r_1jxbexz,,,
r_1jx3zma,reddit,Rollstack,2025-04-11T23:42:06+00:00,"Automate your slide decks and reports with Rollstack
Rollstack connects Tableau, Power BI, Looker, Metabase, and Google Sheets, to PowerPoint and Google Slides for automated recurring reports. 

Stop copying and pasting to build reports. 

Book a demo and get started at [www.Rollstack.com](http://www.Rollstack.com)",bigdata,1,https://www.reddit.com/r/bigdata/comments/1jx3zma/automate_your_slide_decks_and_reports_with/,r_1jx3zma,,,
r_1ng31ca,reddit,Bl4ck8ird,2025-09-13T17:14:34+00:00,"Single object detection
Hello everyone. I need to build an object detection model for an object that I designed myself. The object detection will mostly be from videos that only have my object in it. However, I worry that the deep learning model becomes overfit to detecting everything as my object since it is the only object in the dataset. Is it something to worry and do I need to use another method for this? Thank you for the answers in advance.",computervision,1,https://www.reddit.com/r/computervision/comments/1ng31ca/single_object_detection/,r_1ng31ca,,,
r_1nfy542,reddit,earlier_adopter,2025-09-13T13:56:32+00:00,"Unified API to SOTA vision models
I organized my past works to handle many SOTA vision models with ONNX, and released as the open source repository.
You can use the simple and unified API for any models. Just create the model and pass an image, and you can get results.
I hope it helps someone who wants to handle several models in the simple way.",computervision,4,https://www.reddit.com/r/computervision/comments/1nfy542/unified_api_to_sota_vision_models/,r_1nfy542,,,
r_1nfxhw8,reddit,Big-Mulberry4600,2025-09-13T13:27:32+00:00,"Real-time joystick control of Temad on Raspberry Pi 5 with an OpenCV preview â€” latency & stability notes
Iâ€™ve been tinkering with a small side build: a Raspberry Pi 5 driving Temad with a USB joystick, plus a lightweight OpenCV preview so I can see what the gimbal â€œseesâ€ while I move it.

What I ended up doing (no buzzwords, just what worked):

Kept joystick input separate from capture/display; added a small dead-zone + smoothing to avoid jitter.

OpenCV preview on the Pi with a simple frame cap so CPU doesnâ€™t spike and the UI stays responsive.

Basic on-screen stats (FPS/drops) to sanity-check latency.

Things that bit me:
Joystick device IDs changing across adapters.

Buffering differences (v4l2 vs. other backends).

Preview gets laggy fast without throttling.

Short demo for context (not selling anything):
https://www.youtube.com/watch?v=2Y9RFeHrDUA

If youâ€™re curious, Iâ€™m happy to share versions/configs. Always keen to learn how others keep Pi-side previews snappy.",computervision,2,https://www.reddit.com/r/computervision/comments/1nfxhw8/realtime_joystick_control_of_temad_on_raspberry/,r_1nfxhw8,,,
r_1nfslyv,reddit,Sannad98,2025-09-13T08:52:42+00:00,"Advice on Advanced Computer Vision Learning
Hi everyone,

I want to grow my skills in computer vision and would love some advice. I know the basics and also have some projects built, but now I want to go deeper into advanced areas. I am especially interested in real time computer vision, 3D vision like stereo, SLAM and point clouds, AR and VR, robotics, visual odometry, sensor fusion, and newer models like vision transformers. I also want to learn how to deploy and optimize models for production and real time use. If you know any good resources such as courses, books, research papers or GitHub projects for these topics please share them.

I also want to look for a remote junior or entry level computer vision job that I can do from Pakistan. If you know any job boards, communities or companies that hire remotely it would be great to hear about them. Tips on building a portfolio or open source projects that can help me stand out would also be very helpful.

Thanks in advance for any guidance.",computervision,6,https://www.reddit.com/r/computervision/comments/1nfslyv/advice_on_advanced_computer_vision_learning/,r_1nfslyv,,,
r_1nfpsg4,reddit,nouman6093,2025-09-13T05:58:08+00:00,"where to get ideas for fyp bachelors level for ai (nlp or cv)?
i gotta give proposal for my fyp please help",computervision,0,https://www.reddit.com/r/computervision/comments/1nfpsg4/where_to_get_ideas_for_fyp_bachelors_level_for_ai/,r_1nfpsg4,,,
r_1nfeolx,reddit,markatlarge,2025-09-12T20:58:43+00:00,Weaponized False Positives: How Poisoned Datasets Could Erase Researchers Overnight,computervision,3,https://www.reddit.com/r/computervision/comments/1nfeolx/weaponized_false_positives_how_poisoned_datasets/,r_1nfeolx,,,
r_1nfdnyb,reddit,Naive_Artist5196,2025-09-12T20:18:16+00:00,"Lightweight open-source background removal model (runs locally, no upload needed)
Hi all,

Iâ€™ve been working on [withoutbg](https://github.com/withoutbg/withoutbg), an open-source tool for background removal. Itâ€™s a lightweight matting model that runs locally and does not require uploading images to a server.

**Key points:**

* Python package (also usable through an API)
* Lightweight model, works well on a variety of objects and fairly complex scenes
* MIT licensed, free to use and extend

**Technical details:**

* Uses **Depth-Anything v2 small** as an upstream model, followed by a matting model and a refiner model sequentially
* Developed with **PyTorch**, converted into **ONNX** for deployment
* Training dataset sample: [withoutbg100 image matting dataset](https://withoutbg.com/resources/withoutbg100-image-matting-dataset) (purchased the alpha matte)
* Dataset creation methodology: [how I built alpha matting data](https://withoutbg.com/resources/creating-alpha-matting-dataset) (some part of it)

Iâ€™d really appreciate feedback from this community, model design trade-offs, and ideas for improvements. Contributions are welcome.

Next steps: Dockerized REST API, serverless (AWS Lambda + S3), and a GIMP plugin.",computervision,76,https://www.reddit.com/r/computervision/comments/1nfdnyb/lightweight_opensource_background_removal_model/,r_1nfdnyb,,,
r_1nf9jqd,reddit,Party-Ad5228,2025-09-12T17:36:06+00:00,ðŸ”¥ EVM USB 3.0 & Type-C External CD/DVD Writer (EVM-EXT-CD-01) Unboxing â€“...,computervision,0,https://www.reddit.com/r/computervision/comments/1nf9jqd/evm_usb_30_typec_external_cddvd_writer_evmextcd01/,r_1nf9jqd,,,
r_1nf7k4x,reddit,dreamhighdude1,2025-09-12T16:18:38+00:00,"Looking for team or suggestions?
Hey guys,
I realized something recently â€” chasing big ideas alone kinda sucks.
Youâ€™ve got motivation, maybe even a plan, but no one to bounce thoughts off, no partner to build with, no group to keep you accountable.
Soâ€¦ I started a Discord called Dreamers Domain
Inside, we:
Find partners to build projects or startups
Share ideas + get real feedback
Host group discussions & late-night study voice chats
Support each other while growing
Itâ€™s still small but already feels like the circle I was looking for.
If that sounds like your vibe, youâ€™re welcome to join:
ðŸ‘‰ [https://discord.gg/Fq4PhBTzBz](https://discord.gg/Fq4PhBTzBz)",computervision,0,https://www.reddit.com/r/computervision/comments/1nf7k4x/looking_for_team_or_suggestions/,r_1nf7k4x,,,
r_1nf75y9,reddit,Intelligent-Bug47,2025-09-12T16:03:15+00:00,"Need help asap!!
I want to know which yolo-segmentation model is most suitable where the roi is kind of repeating like gear tooth face something like that.

",computervision,0,https://www.reddit.com/r/computervision/comments/1nf75y9/need_help_asap/,r_1nf75y9,,,
r_1nf6xn7,reddit,lukerm_zl,2025-09-12T15:54:45+00:00,"Building being built ðŸ—ï¸ (video created with computer vision)
Blog post here: [https://zl-labs.tech/post/2024-12-06-cv-building-timelapse/](https://zl-labs.tech/post/2024-12-06-cv-building-timelapse/)",computervision,63,https://www.reddit.com/r/computervision/comments/1nf6xn7/building_being_built_video_created_with_computer/,r_1nf6xn7,,,
r_1nf4u14,reddit,link983d,2025-09-12T14:32:19+00:00,"Archery training app with AI form evaluation (7-factor, 16-point schema) + cloud-based score tracking
Hello everyone,

Iâ€™ve developed an archery app that combines performance analysis with score tracking. It uses an AI module to evaluate shooting form across **7 dimensions**, with a **16-point scoring schema**:

* *StanceScore*: 0â€“3
* *AlignmentScore*: 0â€“3
* *DrawScore*: 0â€“3
* *AnchorScore*: 0â€“3
* *AimScore*: 0â€“2
* *ReleaseScore*: 0â€“2
* *FollowThroughScore*: 0â€“2

After each session, the AI generates a feedback report highlighting strong and weak areas, with personalized improvement tips. Users can also interact with a **chat-based â€œcoachâ€** for technique advice or equipment questions.

On the tracking side, the app offers features comparable to MyTargets, but adds:

* **Cloud sync** across devices
* **Cross-platform portability** (Android â†” iOS)
* Persistent performance history for long-term analysis

Iâ€™m curious about two things:

1. From a user perspective, what additional features would make this more valuable?
2. From a technical/ML perspective, how would you approach refining the scoring model to capture nuances of form?

Not sure if i can link the app, but the name is ArcherSense, its on IOs and Android.

https://preview.redd.it/k5guhghavqof1.jpg?width=1080&format=pjpg&auto=webp&s=255380b0479d4afbcf774ccb563c12ddc0b6bd81

",computervision,5,https://www.reddit.com/r/computervision/comments/1nf4u14/archery_training_app_with_ai_form_evaluation/,r_1nf4u14,,,
r_1nf0tvp,reddit,Cant_afford_an_R34,2025-09-12T11:37:45+00:00,"AI Guided Drone for Uni
Not sure if this is the right place to post this but anyway.

Made a drone demonstration for my 3rd year uni project, custom flight software using C etc. It didn't fly because it's on a ball joint, however showed all degrees of freedom could be controlled, yaw pitch roll etc.

For the 4th year project/dissertation I want to expand on this with flight. Thats the easy bit, but it isn't enough for a full project.

How difficult would it be to use a camera on the drone, aswell as altitude + position data, to automate landings using some sort of computer vision AI?

My idea is to capture video using a pi camera + pi zero (or a similar setup), send that data over wifi to either a pi 4/5 or my laptop (or if possible, run directly on the pi zero) , the computer vision software then uses that data to figure out where the landing pad is, and sends instructions to the drone to land.

I have 2 semesters for this project and its for my dissertation, I don't have any experience with AI, so would be dedicating most of my time on that. Any ideas on what software and hardware to use, etc?

This is ChatGPTs suggestions but i would appreciate some guidance

* **Baseline:** AprilTag/Aruco (classical CV, fiducial marker detection + pose estimation).
* **AI extension:** Object Detection (YOLOv5/YOLOv8 nano, TensorFlow Lite model) to recognise a landing pad.
* **Optional:** Tracking (e.g., SORT/DeepSORT) to smooth detections as the drone descends.",computervision,2,https://www.reddit.com/r/computervision/comments/1nf0tvp/ai_guided_drone_for_uni/,r_1nf0tvp,,,
r_1nf0l79,reddit,Designer_Guava_4067,2025-09-12T11:25:00+00:00,"Final Project Computer Engineering Student
Looking for suggestion on project proposal for my final year as a computer engineering student.",computervision,8,https://www.reddit.com/r/computervision/comments/1nf0l79/final_project_computer_engineering_student/,r_1nf0l79,,,
r_1nezkku,reddit,Worth-Card9034,2025-09-12T10:27:52+00:00,"How to discard unwanted images(items occlusions with hand) from a large chuck of images collected from top in ecommerce warehouse packing process?
I am an engineer part of an enterprise into ecommerce. We are capturing images during packing process.

The goal is to build SKU segmentation on cluttered items in a bin/cart.

  
For this we have an annotation pipeline but we cant push all images into the annotation pipeline and this is where we are exploring approaches to build a preprocessing layer where we can discard majority of the images where items gets occluded by hands, or if there is raw material kept on the side also coming in photo like tapes etc.

Not possible to share the real picture so i am sharing a sample. Just think that there are warehouse carts as many of you might have seen if you already solved this problem or into ecommerce warehousing. 

One way i am thinking is using multimodal APIs like Gemini or GPT5 etc with the prompt whether this contain hand or not? 

Has anyone tackled a similar problem in warehouse or manufacturing settings? 

What scalable approaches( say model driven, heuristics etc) would you recommend for filtering out such noisy frames before annotation?

https://preview.redd.it/jofg1pdympof1.png?width=310&format=png&auto=webp&s=ace7428f1b73e786581ab4d2255857b226fd0a3c

",computervision,2,https://www.reddit.com/r/computervision/comments/1nezkku/how_to_discard_unwanted_imagesitems_occlusions/,r_1nezkku,,,
r_1ney4f9,reddit,Actual_Lifeguard5497,2025-09-12T08:57:42+00:00,"CV knowlege Needed to be useful in drone tech
A friend and I are planning on starting a drone technology company that will use various algorithms mostly for defense purposes and any other applications TBD.   
I'm gathering a knowledge base of CV algorithms that would be used defense drone tech.   
Some of the algorithms I'm looking into learning based on Gemini 2.5 recommendation are:  
**Phase 1: Foundations of Computer Vision & Machine Learning**

* **Module 1: Image Processing Fundamentals**
   * Image Representation and Manipulation
   * Filters, Edges, and Gradients
   * Image Augmentation Techniques
* **Module 2: Introduction to Neural Networks**
   * Perceptrons, Backpropagation, and Gradient Descent
   * Introduction to CNNs
   * Training and Evaluation Metrics
* **Module 3: Object Detection I: Classic Methods**
   * Sliding Window and Integral Images
   * HOG and SVM
   * Introduction to R-CNN and its variants

# Phase 2: Advanced Object Detection & Tracking

* **Module 4: Real-Time Object Detection with YOLO**
   * YOLO Architecture (v3, v4, v5, etc.)
   * Training Custom YOLO Models
   * Non-Maximum Suppression and its variants
* **Module 5: Object Tracking Algorithms**
   * Simple Online and Realtime Tracking (SORT)
   * Deep SORT and its enhancements
   * Kalman Filters for state estimation
* **Module 6: Multi-Object Tracking (MOT)**
   * Data Association and Re-Identification
   * Track Management and Identity Switching
   * MOT Evaluation Metrics



# Phase 3: Drone-Specific Applications



* **Module 7: Drone Detection & Classification**
   * Training Models on Drone Datasets
   * Handling Small and Fast-Moving Objects
   * Challenges with varying altitudes and camera angles
* **Module 8: Anomaly Detection**
   * Using Autoencoders and GANs
   * Statistical Anomaly Detection
   * Identifying unusual flight paths or behaviors
* **Module 9: Counter-Drone Technology Integration**
   * Integrating detection models with a counter-drone system
   * Real-time system latency and throughput optimization
   * Edge AI deployment for autonomous systems

What do you think of this? Do I really need to learn all this? Is it worth learning what's under the hood? Or do most CV folks use the python packages and keep the algorithm info as a black box?

",computervision,0,https://www.reddit.com/r/computervision/comments/1ney4f9/cv_knowlege_needed_to_be_useful_in_drone_tech/,r_1ney4f9,,,
r_1newt8s,reddit,Knight-Cat,2025-09-12T07:29:32+00:00,"Stitching for microscope images
I'm trying to stitch microscope images to see the whole topography of a material. I tried Hugin to do the stitching but it couldn't help me so I tried to do the task writing a python script designed for the microscopic images I have but the code I've written using OpenCV can't do the stitching properly. I've only used two images for trial and the result is as seen in the final image. I believe it is because the images resemble each other. How do I move on from here?",computervision,1,https://www.reddit.com/r/computervision/comments/1newt8s/stitching_for_microscope_images/,r_1newt8s,,,
r_1nevt9p,reddit,Minimum_Minimum4577,2025-09-12T06:25:10+00:00,"The worldâ€™s first screenless laptop is here, Spacetop G1 turns AR glasses into a 100-inch workspace.Cool innovation or just unnecessary hype?",computervision,48,https://www.reddit.com/r/computervision/comments/1nevt9p/the_worlds_first_screenless_laptop_is_here/,r_1nevt9p,,,
r_1ner596,reddit,Ok_Barnacle4840,2025-09-12T02:06:03+00:00,[D] What model should I use for image matching and search use case?,computervision,3,https://www.reddit.com/r/computervision/comments/1ner596/d_what_model_should_i_use_for_image_matching_and/,r_1ner596,,,
r_1neped4,reddit,sovit-123,2025-09-12T00:40:10+00:00,"JEPA Series Part 4: Semantic Segmentation Using I-JEPA
JEPA Series Part 4: Semantic Segmentation Using I-JEPA

[https://debuggercafe.com/jepa-series-part-4-semantic-segmentation-using-i-jepa/](https://debuggercafe.com/jepa-series-part-4-semantic-segmentation-using-i-jepa/)

In this article, we are going to use theÂ **I-JEPA model for semantic segmentation**. We will be using transfer learning to train a pixel classifier head using one of the pretrained backbones from the I-JEPA series of models. Specifically, we will train the model for brain tumor segmentation.

https://preview.redd.it/tdup8w7yqmof1.png?width=800&format=png&auto=webp&s=897f83a4d59b7dc67574bc28e7ada11197485e7f

",computervision,3,https://www.reddit.com/r/computervision/comments/1neped4/jepa_series_part_4_semantic_segmentation_using/,r_1neped4,,,
r_1nem5wp,reddit,DiddlyDinq,2025-09-11T22:10:47+00:00,"Is developing a model to track martial arts positions/stances a realistic goal for 1 person.
For context, I'm an experienced programmer with a strong math background and have also worked in a synthetic data company. I'm aware of needs of CV but have never personally trained a model so I'm looking for advice.

I have a project in mind that would require me to have a model that can scan a martial arts bjj footage (1 pov) and identify the positions of each person. For example,

* person A is standing, person B is lying on the floor
*  person A is on top of person B (full mount)
* Person A is performing an armbar from full mount

Given that grappling has a lot of limb entanglement and occlusions, is something like this possible on a reliable level? Assume I have a labelled database showing segmentation, poses, depth, keypoints etc of each person.

The long term goal would be to recreate something like this for different martial arts (they focus on boxing)  
[Jabbr.ai | AI for Combat Sports](https://jabbr.ai/)",computervision,3,https://www.reddit.com/r/computervision/comments/1nem5wp/is_developing_a_model_to_track_martial_arts/,r_1nem5wp,,,
r_1nel62m,reddit,BarnardWellesley,2025-09-11T21:28:42+00:00,Nvidia finally released their 2017-2018 Elbrus SLAM paper,computervision,30,https://www.reddit.com/r/computervision/comments/1nel62m/nvidia_finally_released_their_20172018_elbrus/,r_1nel62m,,,
r_1nee9kv,reddit,SadFaithlessness2090,2025-09-11T17:02:14+00:00,"Transitioning from Data Annotation role to computer vision engineer
Hi everyone, so currently I'm working in data annotation domain I have worked as annotator then Quality Check and then have experience as team lead as well now I'm looking to do a transition from this to computer vision engineer but Im completely not sure how can I do this I have no one to guide me, so need suggestions if any one of you have done the job transitioning from Data Annotator to computer vision engineer role and how did you exactly did it 


Would like to hear all of your stories ",computervision,6,https://www.reddit.com/r/computervision/comments/1nee9kv/transitioning_from_data_annotation_role_to/,r_1nee9kv,,,
r_1nec82i,reddit,PinPitiful,2025-09-11T15:44:03+00:00,"Which YOLO can I use for custom training and then use my own inference code?
Looking at YOLO versions for a commercial project â€” I want to train on my own dataset, then use the weights in my own inference pipeline (not Ultralyticsâ€™). Since YOLOv5/YOLOv8 are AGPL-3.0, they may force source release. Is YOLOv7 better for this, or are there other YOLO versions/forks that allow commercial use without AGPL issues?",computervision,2,https://www.reddit.com/r/computervision/comments/1nec82i/which_yolo_can_i_use_for_custom_training_and_then/,r_1nec82i,,,
r_1ne81n8,reddit,Ultralytics_Burhan,2025-09-11T12:53:57+00:00,"Hyperspectral Info from Photos
I haven't read the full publication yet, but found this earlier today and it seemed quite interesting. Not clear how many people would have a direct use case for this, but getting spectral information from an RGB image would certainly beat lugging around a spectrometer! 

From my quick skim, it looks like the images require having a color target to make this work. That makes a lot of sense to me, but it means it's not a retroactive solution or one that works on any image. Despite that, I still think it's cool and could be useful. 

Curious if anyone has any ideas on how you might want to use something like this? I suspect the first or common ones would be uses in manufacturing, medical, and biotech. I'll have to read more to learn about the color target used, as I suspect that might be an area to experiment around, looking for the limits of what can be used.",computervision,8,https://www.reddit.com/r/computervision/comments/1ne81n8/hyperspectral_info_from_photos/,r_1ne81n8,,,
r_1ne7ih6,reddit,alen_n,2025-09-11T12:29:42+00:00,"Which ML method you will use for â€¦
Which ML method you will choose now if you want to count fruits ? In greenhouse environment. Thank You ",computervision,1,https://www.reddit.com/r/computervision/comments/1ne7ih6/which_ml_method_you_will_use_for/,r_1ne7ih6,,,
r_1ne6wda,reddit,Wrong-Analysis3489,2025-09-11T12:00:29+00:00,"Distilled DINOv3 for object detection
Hi all,

I'm interested in trying one of DINOv3's distilled versions for object detection to compare it's performance to some YOLO versions as well as RT-DETR of similiar size. I would like to use the ViT-S+Â model, however my understanding is that Meta only released the pre-trained backbone for this model. A pre-trained detection head based on COCO is only available for ViT-7B. My use case would be the detection of a single class in images. For that task I have about 600 labeled images which I could use for training. Unfortunately my knowledge in computer vision is fairly limited, altough I do have a general knowledge in computer science.

Would appreciate If someone could give me insights on the following:

* Intuition if this model would perform better or similar to other SOTA models for such task
* Resources on how to combine a vision backbone with a detection head, basic tutorial without to much detail would be great
* Resources which provide better understanding of the architectur of those models (as well as YOLO and RT-DETR) and how those architectures can be adapted to specific use cases, note, I do already have basic understanding of (convolutional) neural networks, but this isn't sufficient to follow papers/reports in this area
* Resources which better explain the general usage of such models

I am aware that the DINOv3 paper provides lots of information on usage/implementation, however to be honest the provided information is to complex for me to understand for now, therefore I'm looking for simpler resources to start with.

Thanks in advance!",computervision,25,https://www.reddit.com/r/computervision/comments/1ne6wda/distilled_dinov3_for_object_detection/,r_1ne6wda,,,
r_1ne1zpf,reddit,regista-space,2025-09-11T06:59:11+00:00,"Real-time super accurate masking on small search spaces?
I'm looking for some advice on what methods or models might benefit from input images being significantly smaller in resolution (natively), but at the cost of varying resolutions. I'm thinking that you'd basically already have the BBs available as the dataset. Maybe it's not a useful heuristic but if it is, is it more useful than the assumption that image resolutions are consistent? Considering varying resolutions can be ""solved"" through scaling and padding, I can imagine it might not be that impactful.",computervision,1,https://www.reddit.com/r/computervision/comments/1ne1zpf/realtime_super_accurate_masking_on_small_search/,r_1ne1zpf,,,
r_1ndx0i0,reddit,Tall-Roof-1662,2025-09-11T02:15:15+00:00,Is wavelet transform really useful?,computervision,2,https://www.reddit.com/r/computervision/comments/1ndx0i0/is_wavelet_transform_really_useful/,r_1ndx0i0,,,
r_1nduzka,reddit,MelyndWest,2025-09-11T00:34:53+00:00,"Should i use YOLO or OPENCV for face detection.
Hello, my professor is doing an article and i got responsible for developting a face recognition developing a face recognition algorithm that uses his specific mathematical metric to do the recognition. Basically, i need to created an algorithm that will select especifics regions of a person face (thinking about eyes and mouth) and try to identify the person by the interval of distance between these regions, the recognition must happen in real time.

However, while researching, i'm in doubt if the correct system to implement the recognition. So YOLO is better at object detection; however, OpenCV is better at image processing. I'm new to computer vision but i have about 3 months to properly do this assigment.

Should i choose to go with YOLO or with OPENCV? How should i start the project?

edit1: From my conversations with the professor, he does not care about the method I use to do the recognition. I believe that what he wants is easier than I think. Basically, instead of using something like Euclidean distance or cosine similarity, the recognition must be done with the distance metric he created",computervision,13,https://www.reddit.com/r/computervision/comments/1nduzka/should_i_use_yolo_or_opencv_for_face_detection/,r_1nduzka,,,
r_1nduz8k,reddit,Nebulafactory,2025-09-11T00:34:23+00:00,"Transfering vertically mounted golf club head pictures to vector files
Long story short I'm working on a small project where I will be using a laser engraver to clean & add texture to some old golf clubs.

For now I've just been manually recreating the shape of the clubhead in my cad/laser software however this would be impractical given the amount of grooves & different shapes they all come with.

My idea was to first place the club in a vertically mounted camera stand where I'd take a picture of it in order to turn it into a vector file for my laser engraver to follow.

This way I can capture not just the overall shape, but the lines from the grooves in case I'd only want to clean that area.

So far I've tried more manual approaches to convert the picture into a rough black&white sketch, then vectorize it but I was wondering if there is any better system out there to do this.",computervision,1,https://www.reddit.com/r/computervision/comments/1nduz8k/transfering_vertically_mounted_golf_club_head/,r_1nduz8k,,,
r_1ndr2cm,reddit,Big-Mulberry4600,2025-09-10T21:38:11+00:00,"Weâ€™ve just launched a modular 3D sensor platform (RGB + ToF + LiDAR) â€“ curious about your thoughts
Hi everyone,

Weâ€™ve recently launched a modular 3D sensor platform that combines RGB, ToF, and LiDAR in one device. It runs on a Raspberry Pi 5, comes with an open API + Python package, and provides CAD-compatible point cloud & 3D output.

The goal is to make multi-sensor setups for computer vision, robotics, and tracking much easier to use â€“ so instead of wiring and syncing different sensors, you can start experimenting right away.

Iâ€™d love to hear feedback from this community:

Would such a plug & play setup be useful in your projects?

What features or improvements would you consider most valuable?

https://rubu-tech.de

Thanks a lot in advance for your input",computervision,30,https://www.reddit.com/r/computervision/comments/1ndr2cm/weve_just_launched_a_modular_3d_sensor_platform/,r_1ndr2cm,,,
r_1ndqnla,reddit,killua753,2025-09-10T21:20:55+00:00,"Tips to Speed Up Training with PyTorch DDP â€“ Data Loading Optimizations?
Hi everyone,

Iâ€™m currently training Object Detection models using **PyTorch DDP** across multiple GPUs. Apart from the modelâ€™s computation time itself, I feel a lot of training time is spent on **data loading and preprocessing**.

I was wondering: what are some **good practices or tricks** I can use to reduce overall training time, particularly on the data pipeline side?

Hereâ€™s what Iâ€™m currently doing:

* Using `DataLoader` with `num_workers > 0` and `pin_memory=True`
* Standard online image preprocessing and augmentation
* Distributed Data Parallel (DDP) across GPUs

Thanks in advance",computervision,2,https://www.reddit.com/r/computervision/comments/1ndqnla/tips_to_speed_up_training_with_pytorch_ddp_data/,r_1ndqnla,,,
r_1ndfula,reddit,datascienceharp,2025-09-10T14:32:52+00:00,"MiniCPM-V 4.5 somehow does grounding without being trained for it
i've been messing around with MiniCPM-V 4.5 (the 8B param model built on Qwen3-8B + SigLIP2-400M) and here's what i found:

the good stuff:

â€¢ it's surprisingly fast for an 8B model. like actually fast. captions/descriptions take longer but that's just more tokens so whatever

â€¢ OCR is solid, even handles tables and gives you markdown output which is nice

â€¢ structured output works pretty well - i could parse the responses for downstream tasks without much hassle

â€¢ grounding actually kinda works?? they didn't even train it for this but i'm getting decent results. not perfect but way better than expected

â€¢  i even got it to output points! localization is off but the labels are accurate and they're in the right ballpark (not production ready but still impressive)

the weird stuff:

â€¢ it has this thinking mode thing but honestly it makes things worse? especially for grounding - thinking mode just destroys its grounding ability. same with structured outputs. not convinced it's all that useful

â€¢ the license is... interesting. basically free for <5k edge devices or <1M DAU but you gotta register. can't use outputs to train other models. standard no harmful use stuff

anyway i'm probably gonna write up a fine-tuning tutorial next to see if we can make the grounding actually production-ready. seems like there's potential here

resources:

â€¢ model on ðŸ¤—: https://huggingface.co/openbmb/MiniCPM-V-4_5

â€¢ github: https://github.com/OpenBMB/MiniCPM-V

â€¢ fiftyone integration: https://github.com/harpreetsahota204/minicpm-v

â€¢ quickstart guide with fiftyone: https://github.com/harpreetsahota204/minicpm-v/blob/main/minicpm_v_fiftyone_example.ipynb",computervision,28,https://www.reddit.com/r/computervision/comments/1ndfula/minicpmv_45_somehow_does_grounding_without_being/,r_1ndfula,,,
r_1ndfkyq,reddit,5thMeditation,2025-09-10T14:22:16+00:00,"Advanced Labeling
I have been working with computer vision models for a while, but I am looking for something I haven't really seen in my work. Are there models that take in advanced data structures for labeling and produce inferences based on the advanced structures?

I understand that I could implement my own structure to the labels I provide - but is the most elegant solution available to me to use a classification approach with structured data and much larger models that can differentiate between fine-grained details of different (sub-)classes?",computervision,11,https://www.reddit.com/r/computervision/comments/1ndfkyq/advanced_labeling/,r_1ndfkyq,,,
r_1ndewzm,reddit,Kind-Government7889,2025-09-10T13:56:55+00:00,"Real time saliency detection library
I've just made public a library for real time saliency detection. It's CPU based and no ML so a bit of a fresh take on CV (at least nowadays).

Hope you like it :) 

Github: [https://github.com/big-nacho/dosage](https://github.com/big-nacho/dosage)  
",computervision,115,https://www.reddit.com/r/computervision/comments/1ndewzm/real_time_saliency_detection_library/,r_1ndewzm,,,
r_1ndelpp,reddit,Georgehwp,2025-09-10T13:44:10+00:00,"Does anyone know of an open-source T-REX equivalent?
[https://www.trexlabel.com](https://www.trexlabel.com)

Looking to see if there's a family of plug and play models I could try here, have not seen any repo with an implementation of anything similar.",computervision,0,https://www.reddit.com/r/computervision/comments/1ndelpp/does_anyone_know_of_an_opensource_trex_equivalent/,r_1ndelpp,,,
r_1nd9nl6,reddit,Robusttequilla007,2025-09-10T09:37:34+00:00,"Computer Vision Guide for an embedded SWIntern
Hi

I am a ce undergrad, I have been working as an embedded s/w intern at a startup. Now they want me to pivot to cv as most of our embedded projects are done and they want to focus more on integrating cv to our existing embedded systems. The thing is Idk shit abt geometry and ray optics , I was stronger on the algebra and calculus stuff in high school and even in physics I was better in electronics stuff and just used to learn few necessary stuff to get through ray optics and geometry. Even in my ug in ce I mostly had math related to calculus or smtg which did not require geometry stuff. So now I am willing to learn out of interest and I would really appreciate if someone could give me few resources which teaches geometry and ray optics required for cv to someone like me. I am familiar with the ug math linear algebra calculus as stuff these 2 subjects are what's bothering me as most documentations are filled with them.

PS The thing is I am still young so would like to give cv a chance , if I cannot I will.move to a new firm or ask them I just want to do the embedded stuff",computervision,3,https://www.reddit.com/r/computervision/comments/1nd9nl6/computer_vision_guide_for_an_embedded_swintern/,r_1nd9nl6,,,
r_1nd7s1g,reddit,Complete-Ad9736,2025-09-10T07:32:07+00:00,"We've Launched a Free Auto Mask Annotation Tool. Your Precious Suggestions Will Help a Lot.
Weâ€˜ve recently launched an Auto Mask Annotation Tool, which is **completely free to use**! 

All you need to do is to select one or more objects, and the platform will **automatically perform Mask annotation** for all targeted objects in the image.

  
Unlike other free tools that only offer partial pre-trained models or restrict object categories, [T-Rex Label](https://www.trexlabel.com/?source=reddit)â€™s Auto Mask Annotation uses an open-set general model. There are no limitations on scenarios, object categories, or other aspects whatsoever.

  
We warmly welcome your suggestions for improvements. If you have a need for other free features (such as Keypoint, Polygon, etc.), please feel free to leave a comment. Our goal is to iterate and develop a free, user-friendly annotation product that truly meets everyoneâ€™s needs first.

  
For a step-by-step guide on using T-Rex Labelâ€™s Auto Mask Annotation tool, please refer to [this tutorial](https://medium.com/@ideacvr2024/free-to-use-t-rex-label-launches-automatic-mask-annotation-feature-with-tutorial-51909798f997).",computervision,11,https://www.reddit.com/r/computervision/comments/1nd7s1g/weve_launched_a_free_auto_mask_annotation_tool/,r_1nd7s1g,,,
r_1nd71x2,reddit,archdria,2025-09-10T06:45:22+00:00,"Interactive ORB feature matching
Hi! I am the creator of [zignal](https://github.com/bfactory-ai/zignal), a zero-dependency image processing library that can be compiled to WebAssembly.

In this example I showcase feature matching with [ORB](https://docs.opencv.org/4.x/d1/d89/tutorial_py_orb.html).

You can try other examples from the library here:

[https://bfactory-ai.github.io/zignal/examples/](https://bfactory-ai.github.io/zignal/examples/)

I hope you like it.

[liza, the official zignal mascot, warped and rotated, and feature matched with ORB](https://preview.redd.it/j5gfjtw6gaof1.png?width=1918&format=png&auto=webp&s=8692951c378f000efc5a5335746176784260cb8e)",computervision,2,https://www.reddit.com/r/computervision/comments/1nd71x2/interactive_orb_feature_matching/,r_1nd71x2,,,
r_1nd62rd,reddit,LuckyOven958,2025-09-10T05:44:36+00:00,"Getting started with Agentic AI
Hey folks,  
Iâ€™ve been tinkering with **Agentic AI** for the past few weeks, mostly experimenting with how agents can handle tasks like research, automation. Just curious how did you guys get started ?

While digging into it, I joined a Really cool workshop on Agentic AI Workflow that really helped me, are you guys Interested ?",computervision,0,https://www.reddit.com/r/computervision/comments/1nd62rd/getting_started_with_agentic_ai/,r_1nd62rd,,,
r_1ncvuiz,reddit,markatlarge,2025-09-09T21:34:02+00:00,"Has Anyone Used the NudeNet Dataset?
If you have NudeNet Dataset on your local drive, feel free to verify the file I confirmed was delete.  I believe it's adult legal content and was falsely flagged by Google. See my Medium post for details: [https://medium.com/@russoatlarge\_93541/googles-ai-surveillance-erased-130k-of-my-files-a-stark-reminder-the-cloud-isn-t-yours-it-s-50d7b7ceedab](https://medium.com/@russoatlarge_93541/googles-ai-surveillance-erased-130k-of-my-files-a-stark-reminder-the-cloud-isn-t-yours-it-s-50d7b7ceedab)",computervision,39,https://www.reddit.com/r/computervision/comments/1ncvuiz/has_anyone_used_the_nudenet_dataset/,r_1ncvuiz,,,
r_1ncuecp,reddit,Little_Messy_Jelly,2025-09-09T20:37:34+00:00,"CV ML models paper. Where to start?
Iâ€™m working on a paper about comparative analysis of computer vision models, from early CNNs (LeNet, AlexNet, VGG, ResNet) to more recent ones (ViT, Swin, YOLO, DETR).

Where should I start, and whatâ€™s the minimum I need to cover to make the comparison meaningful?

Is it better to implement small-scale experiments in PyTorch, or rely on published benchmark results?

How much detail should I give about architectures (layers, training setups) versus focusing on performance trends and applications?

I'm aiming for 40-50 pages. Any advice on scoping this so itâ€™s thorough but manageable would be appreciated.",computervision,7,https://www.reddit.com/r/computervision/comments/1ncuecp/cv_ml_models_paper_where_to_start/,r_1ncuecp,,,
r_1ncmciy,reddit,emocakeleft,2025-09-09T15:41:13+00:00,"How can I improve generalization across datasets for oral cancer detection
Hello guys, 

I am tasked with creating a pipeline for oral cancer detection. Right now I am using a pretrained ResNet50 that I am finetuning the last 4 layers of. 

The problem is that the model is clearly overfitting to the dataset I finetuned to. It gives good accuracy in an 80-20 train-test split but fails when tested on a different dataset. I have tried using test-time approach, fine tuning the entire model and I've also enforced early stopping.

For example in this picture:

https://preview.redd.it/g8b3z50pn5of1.jpg?width=1200&format=pjpg&auto=webp&s=c3c82d0984c071f560ae1b27a9e89071c361e6b3

This is what the model weights look like for this

https://preview.redd.it/d6dg35heo5of1.png?width=1222&format=png&auto=webp&s=1985869d28f52886fe3181b94f1f2babce64520c

Part of the reason may be that since it's skin it's fairly similar across the board and the model doesn't distinguish between cancerous and non-cancerous patches.

If someone has worked on a similar project, what techniques can I use to ensure good generalization and that the model actually learns the features.",computervision,3,https://www.reddit.com/r/computervision/comments/1ncmciy/how_can_i_improve_generalization_across_datasets/,r_1ncmciy,,,
r_1nchmre,reddit,Consistent-Hyena-315,2025-09-09T12:32:39+00:00,"Is there a way to do this without using an ML model?
I was working on extracting floorplans from distorted, skewed images, i know that i can use yolo or something to get it done accurately, but if i want to straighten and accurately crop the floorplan of these kind of images, what approach should i use?

Edit: Okay guess I wasn't articulate enough, I'm sorry but when I say I want to extract floorplan, all I need is the floorplan, not even the legend or the data next to it. Which is what's making my job difficult.

https://preview.redd.it/vntys8qdv4of1.jpg?width=960&format=pjpg&auto=webp&s=2420b37f3417397dc6ed8a435da0499175b591a9

",computervision,3,https://www.reddit.com/r/computervision/comments/1nchmre/is_there_a_way_to_do_this_without_using_an_ml/,r_1nchmre,,,
r_1nchmq3,reddit,Royal-War4549,2025-09-09T12:32:36+00:00,"Detecting text lines on a very noisy image
I have images like this one, images can be skewed or rotated:

https://preview.redd.it/vewyj43kt4of1.png?width=359&format=png&auto=webp&s=cf40824e2f7961c73d92fb24654578ecbe1ebaf7

I need to split it in lines somehow for further OCR:

https://preview.redd.it/1eq7d7aut4of1.png?width=359&format=png&auto=webp&s=47b2ca904438d3fd6e3e98edfa6972466dbc9c91

Already tried document alignment, doesn't realy work for noisy stuff:  
[https://stackoverflow.com/questions/55654142/detect-if-an-ocr-text-image-is-upside-down](https://stackoverflow.com/questions/55654142/detect-if-an-ocr-text-image-is-upside-down)  
and   
[https://www.kaggle.com/code/mahmoudyasser/hough-transform-to-detection-and-correction-skewed](https://www.kaggle.com/code/mahmoudyasser/hough-transform-to-detection-and-correction-skewed)  
  
Any ideas?  
",computervision,0,https://www.reddit.com/r/computervision/comments/1nchmq3/detecting_text_lines_on_a_very_noisy_image/,r_1nchmq3,,,
r_1ncgdck,reddit,ConfectionOk730,2025-09-09T11:32:02+00:00,"Image quality Analysis
I am building an image quality system where I first detect posters on the wall using YOLOv8. That part is already done. Now I want to categorize those posters into three categories: Good, Medium, or Poor.

The logic is:

If the full poster is visible, it is Good.

If, for any reason, the full poster is not visible, it is Poor.

If the poster is on the wall but the photo is taken from a very tilted angle, it is also Poor.

Medium applies when the poster is visible but not perfectly clear (e.g., slight tilt, blur, or partial obstruction).

Based on these two conditions, I want to categorize images into Good, Medium, or Poor.",computervision,1,https://www.reddit.com/r/computervision/comments/1ncgdck/image_quality_analysis/,r_1ncgdck,,,
r_1ncdzu4,reddit,United_Elk_402,2025-09-09T09:11:37+00:00,"Best Approach for Precise object segmentation with Small Dataset (500 Images)
Hi, Iâ€™m working on a computer vision project to segment large kites (glider-type) from backgrounds for precise cropping, and Iâ€™d love your insights on the best approach.

**Project Details**:

* **Goal**: Perfectly isolate a single kite in each image (RGB) and crop it out with smooth, accurate edges. The output should be a clean binary mask (kite vs. background) for cropping. - Smoothness of the decision boundary is really important.
* **Dataset**: 500 images of kites against varied backgrounds (e.g., kite factory, usually white).
* **Challenges**: The current models produce rough edges, fragmented regions (e.g., different kite colours split), and background bleed (e.g., white walls and hangars mistaken for kite parts).
* **Constraints**: Small dataset (500 images max), and â€œperfectâ€ segmentation (targeting Intersection over Union >0.95).
* **Current Plan**: Iâ€™m leaning toward SAM2 (Segment Anything Model 2) for its pre-trained generalisation and boundary precision. The plan is to use zero-shot with bounding box prompts (auto-detected via YOLOv8) and fine-tune on the 500 images. Alternatives considered: U-Net with EfficientNet backbone, SegFormer, or DeepLabv3+ and Mask R-CNN (Detectron2 or MMDetection)

**Questions**:

1. What is the best choice for precise kite segmentation with a small dataset, or are there better models for smooth edges and robustness to background noise?
2. Any tips for fine-tuning SAM2 on 500 images to avoid issues like fragmented regions or white background bleed?
3. Any other architectures, post-processing techniques, or classical CV hybrids that could hit near-100% Intersection over Union for this task?

**What Iâ€™ve Tried**:

* SAM2: Decent but struggles sometimes.
* Heavy augmentation (rotations, colour jitter), but still seeing background bleed.

Iâ€™d appreciate any advice, especially from those whoâ€™ve tackled similar small-dataset segmentation tasks or used SAM2 in production. Thanks in advance!",computervision,6,https://www.reddit.com/r/computervision/comments/1ncdzu4/best_approach_for_precise_object_segmentation/,r_1ncdzu4,,,
r_1nc9nry,reddit,Similar-Way-9519,2025-09-09T04:34:11+00:00,"Converting RGB Annotations to IR Images (Using Calibration + Depth Estimation)
Hi everyone,  
Iâ€™d like to develop a system to convert annotations from RGB images to IR images. The plan is to use checkerboard calibration parameters plus stereo depth estimation to transform instance segmentation masks from RGB into IR space, then convert them into bounding boxes for real-time inference.

Just to clarify, Iâ€™m not trying to *generate* IR from RGB â€” the IR images come from a real IR camera. The goal is simply to geometrically map annotations across modalities.

I know about related work (e.g. [Darwish et al., 2017](https://pmc.ncbi.nlm.nih.gov/articles/PMC7926581/)), but since my setup is more simplified, Iâ€™d like to know if this is still feasible in practice.

Any suggestions or pitfalls I should watch out for?",computervision,7,https://www.reddit.com/r/computervision/comments/1nc9nry/converting_rgb_annotations_to_ir_images_using/,r_1nc9nry,,,
r_1nc99vu,reddit,Prestigious-Egg-2650,2025-09-09T04:13:00+00:00,"Computer Vision Roadmap?
So I am a B.Tech student (3rd yr) in CSE(AI) who is interested in Computer Vision but lacks the thought on how shall I start, provided I have basic knowledge on OpenCV and Image Processing. 

I'll be glad if anyone can help me in this..ðŸ™",computervision,26,https://www.reddit.com/r/computervision/comments/1nc99vu/computer_vision_roadmap/,r_1nc99vu,,,
r_1nc3nfa,reddit,Distinct-Ebb-9763,2025-09-08T23:42:51+00:00,How to improve handwriting detection in Azure custom template extraction model?,computervision,1,https://www.reddit.com/r/computervision/comments/1nc3nfa/how_to_improve_handwriting_detection_in_azure/,r_1nc3nfa,,,
r_1nc2g8g,reddit,Positive_Signature66,2025-09-08T22:50:45+00:00,"Driver hand monitoring to know when either band is off or on a steering wheel
Hey everyone. 

I'm currently busy with computer vision project where one of the systems is to detect when either hand is off or on a steering wheel. 

Does anyone have any ideas of which techniques I could use to accomplish this task ?. 

I have seen techniques of skin detection, ACF detectors using median flow tracking. But if there is simpler techniques out there that I can use to implement such as subsystem, I would highly appreciate it. 

Also the reason why I ask for simple techniques is because I am required to run the system on a hardware constraint device so techniques like deep learning models, Google media pipe and Yolo won't help because the techniques I need have to be developed from first principles. Yes I know why reinvent the wheel ? Well let's just say I am obligated to or else I won't pass my final year. 

Please if anyone has suggestions for me please do advise :)",computervision,5,https://www.reddit.com/r/computervision/comments/1nc2g8g/driver_hand_monitoring_to_know_when_either_band/,r_1nc2g8g,,,
r_1nc0dlj,reddit,Beginning_Butterfly8,2025-09-08T21:25:43+00:00,"How do you semantically parse scientific papers
The full text of the PDF was segmented into semantically meaningful blocks-such as section titles, paragraphs, cap-tions, and table/figure references-using PDF parsing tools like PDFMiner'. These blocks, separated based on structural whitespace in the document, were treated as retrieval units.

The above text is from the paper which I am trying to reproduce.

I have tried the pdf miner approach with different regex but due to different layout and style of paper it fails and is not consistent. Could any one please enlighten me how can i approach this? Thank you",computervision,0,https://www.reddit.com/r/computervision/comments/1nc0dlj/how_do_you_semantically_parse_scientific_papers/,r_1nc0dlj,,,
r_1nbrsz7,reddit,marsrovernumber16,2025-09-08T16:05:34+00:00,OCR but for a strict template?,computervision,1,https://www.reddit.com/r/computervision/comments/1nbrsz7/ocr_but_for_a_strict_template/,r_1nbrsz7,,,
r_1nbpjgj,reddit,Busy-Necessary-927,2025-09-08T14:40:41+00:00,"Multi-object tracking Inconsistent FPS
Hello!

I'm currently working on a project with inconsistent delta times between frames (inconsistent FPS). The time between two frames can range from 0.1 to 0.2 seconds. We are using a detection + tracker approach, and this variation in time causes our tracker to perform poorly.

It seems like a straightforward solution would be to incorporate delta time into the position estimation of the tracker. However, we were hoping to find a library that already supports passing delta time into the position estimation, but we couldnâ€™t find one.

Has no one in the academia faced this problem before? Are there really no open datasets/library addressing inconsistent FPS? ",computervision,1,https://www.reddit.com/r/computervision/comments/1nbpjgj/multiobject_tracking_inconsistent_fps/,r_1nbpjgj,,,
r_1nbowdz,reddit,_RC101_,2025-09-08T14:15:22+00:00,"How do you parallely process frames from multiple object detection models at scale?
Iâ€™m working on a pipeline where I need to run multiple object detection models in real-time. Each model runs fine individually â€” around 10ms per frame (tensorRT) when I just pass frames one by one in a simple Python script.

The models all just need the base video frame but they all detect different things. (Combining them is not a good idea at all as I have tried that already). I basically want them all to parallely take the frame input and return the output at roughly the same time maybe even extra 3-4ms is fine for coordination. I have resources like multiple GPUs, so that isn't a problem. The outputs from these models go to another set of models for things like Text Recognition which can add overhead since I run them on a separate GPU and converting the outputs to the required GPU also is taking time. 

When I try running them sequentially on the same GPU, the per-frame time jumps to \~25ms each. Iâ€™ve tried CUDA streams, Python multiprocessing, and other ""parallelization"" tricks suggested by LLMs and some research on the internet, but the overhead actually makes things worse (50ms+ per frame). That part confuses me the most as I expected streams or processes to help, but theyâ€™re slowing it down instead.

Running each model on separate GPUs does work, but then I hit another bottleneck: transferring output tensors across GPUs or back to CPU for the next step adds noticeable overhead.

Iâ€™m trying to figure out how this is usually handled at a production level. Are there best practices, frameworks, or patterns for scaling object detection models like this in real-time pipelines? Any resources, blog posts, or repos you could point me to would help a lot.",computervision,34,https://www.reddit.com/r/computervision/comments/1nbowdz/how_do_you_parallely_process_frames_from_multiple/,r_1nbowdz,,,
r_1nbjzaj,reddit,Icy_Colt-30,2025-09-08T10:21:12+00:00,"skewed Angle detection in Engineering Drawing
# i have to build a model for angle detection in engineering drawing and most OCR or CV model are not accurate only models which i train with data are accurate but i want low size models so the process is quick enough can some one suggest any idea for 0-360 degree detection",computervision,1,https://www.reddit.com/r/computervision/comments/1nbjzaj/skewed_angle_detection_in_engineering_drawing/,r_1nbjzaj,,,
r_1nbhx16,reddit,dreamhighdude1,2025-09-08T08:09:48+00:00,"Looking for team or advice?
Hey guys,
I realized something recently â€” chasing big ideas alone kinda sucks.
Youâ€™ve got motivation, maybe even a plan, but no one to bounce thoughts off, no partner to build with, no group to keep you accountable.
Soâ€¦ I started a Discord called Dreamers Domain
Inside, we:
Find partners to build projects or startups
Share ideas + get real feedback
Host group discussions & late-night study voice chats
Support each other while growing
Itâ€™s still small but already feels like the circle I was looking for.
If that sounds like your vibe, youâ€™re welcome to join:
ðŸ‘‰ [https://discord.gg/Fq4PhBTzBz](https://discord.gg/Fq4PhBTzBz)",computervision,4,https://www.reddit.com/r/computervision/comments/1nbhx16/looking_for_team_or_advice/,r_1nbhx16,,,
r_1nbealx,reddit,DirectorAgreeable145,2025-09-08T04:27:06+00:00,"Need Help Coming Up with Computer Vision Project Ideas (for Job + Final Year Project)
Iâ€™m a bachelor undergrad working in computer vision research, and Iâ€™m currently writing a paper in a specific CV domain. On the research side, Iâ€™m doing okay. But hereâ€™s the issue: Iâ€™m under pressure to secure an AI Engineer job after graduation instead of immediately going deeper into research. In my area, companies that hire for CV roles often expect candidates to showcase novel, application-driven projects, not just the standard YOLO detection demos.

This puts me in a tough spot: I canâ€™t just reuse common CV projects (like basic object detection) because theyâ€™ve become too overdone.Even my final year project idea (a system to detect pests in households/restaurants and notify users) was rejected by my professor because it was seen as â€œjust YOLO.â€

The research Iâ€™m focusing on doesnâ€™t really translate into practical engineering + vision projects that employers want to see.

So now I feel stuck. I need to come up with: 
*A final year project that combines CV + engineering to solve a real-world issue.
*Portfolio projects that show originality and problem-solving ability, so I donâ€™t look like just another student who re-implemented YOLO.

Has anyone been in a similar situation? How do you brainstorm or identify real-world problems where CV could add genuine value? And if you have examples of unique CV applications (outside the â€œusual suspectsâ€), Iâ€™d really appreciate some pointers.",computervision,9,https://www.reddit.com/r/computervision/comments/1nbealx/need_help_coming_up_with_computer_vision_project/,r_1nbealx,,,
r_1nb0gtu,reddit,cgonz15,2025-09-07T18:15:04+00:00,"Car hit and run, can you read the licene plate?
I got the footage from my tesla and this is the only angle you can see it but its a little blurry. Is there any way you guys can help out and see if you cna read the plate? Thank you. I asked chatgpt and they said this subreddit could help, thanks. ",computervision,0,https://www.reddit.com/r/computervision/comments/1nb0gtu/car_hit_and_run_can_you_read_the_licene_plate/,r_1nb0gtu,,,
r_1natyut,reddit,Possible_Ad1295,2025-09-07T14:01:22+00:00,"Why do my VAE / Perceiver reconstructions come out on a black background? (DP-GMM VRNN + Perceiver)
I designed and have been training a sequence model for video prediction: a temporal VAE with a DP-GMM stick-breaking prior and a Perceiver â€œcontext sidecar.â€ The VAE path is NVAE-style conv encoder/decoder with a PixelCNN++-type mixture-of-discretized logistics (MDL) head; images are scaled to \[-1,1\] and the MDL bin width is 1/(2\^bits-1). The Perceiver ingests the whole episode using a tiny UNet adapter (decode enabled) and alternates cross/self-attention; its forward reconstructs back to pixels via the embedderâ€™s un-embed path, and I supervise that with an MSE reconstruction loss across the episode. The losses blended in training are: image NLL from the MDL head, KL terms for the latent/prior, plus attention regularizers.

https://preview.redd.it/n2p2ahnp7fof1.png?width=2240&format=png&auto=webp&s=e44ce8dc65d73fe71f3a0cc48017abecf6c6e190

In the attached grid (train/eval), the Perceiver reconstructions are the opposite which is nearly uniform black with some slightly brighter speckles. The attention maps (â€œAttention + Centers / Slotsâ€) look reasonable. Given this setup, does the community have hypotheses for why the MDL-based VAE would bias toward the lower end of \[-1,1\] while the Perceiver MSE head drifts high? If youâ€™ve run into this black/white saturation split before, where would you probe first? Context details in code: MDL head and parameterization, Perceiver reconstruction via un-embed, and the Perceiver MSE computed over the episode. I want the Perceiver to summarize the full episode as context while the recurrent VRNN, conditioned on that summary plus actions, focuses attention to predict where the next frameâ€™s action should land. Please consider the architecture that I described and kindly share debugging angles youâ€™d try.  
Thank you",computervision,3,https://www.reddit.com/r/computervision/comments/1natyut/why_do_my_vae_perceiver_reconstructions_come_out/,r_1natyut,,,
r_1nasjpg,reddit,alvises,2025-09-07T12:58:50+00:00,Edge Object Detection with Elixir/Nerves: running YOLO on Raspberry Pi 5 + Hailo-8L,computervision,5,https://www.reddit.com/r/computervision/comments/1nasjpg/edge_object_detection_with_elixirnerves_running/,r_1nasjpg,,,
r_1naqh8y,reddit,Emergency_Beat8198,2025-09-07T11:11:28+00:00,"How Camera face recognition Works on edge device so accurately ? ML Models or Deep Learning
I was interested in knowing how camera face detection is working , The speed and accuracy is really great , How is it achievable ?",computervision,8,https://www.reddit.com/r/computervision/comments/1naqh8y/how_camera_face_recognition_works_on_edge_device/,r_1naqh8y,,,
r_1nakvts,reddit,ArcticTechnician,2025-09-07T05:21:43+00:00,"SOTA Models for Detection of Laptop/Mobile Screens, Tattoos, and License Plates?
Hello y'all! Posting to ask if anyone had any experience with what models are currently SOTA for detecting (and then redacting) laptops/mobile screens, tattoos, and license plates.

Starting an open source project that will be a redaction tool, and I've got the face detection down, just wondering if anyone knew how other devs were doing object detection on the above.

Cheers",computervision,1,https://www.reddit.com/r/computervision/comments/1nakvts/sota_models_for_detection_of_laptopmobile_screens/,r_1nakvts,,,
r_1najd74,reddit,Commercial-Panic-868,2025-09-07T03:55:58+00:00,"Prioritizing certain regions in videos for object detection
Hey everyone!

I'm working on optimizing object detection and had an idea:  what if I process the left side of an image first, then the right side, instead of running detection on the whole image at once?

My thinking is that this could be faster because I already know that the object tends to appear in certain areas. 

I'm wondering if anyone did this before and how did you implement the priotising algorithm. 

Thanks!",computervision,0,https://www.reddit.com/r/computervision/comments/1najd74/prioritizing_certain_regions_in_videos_for_object/,r_1najd74,,,
r_1nab3cd,reddit,Relative-Pace-2923,2025-09-06T21:17:34+00:00,"Multiple inter-dependent images passed into transformer and decoded?
Making seq2seq image-to-coordinates thing and I want multiple images as input because I want the model to understand that positions depend on the other images too. Order of the images matters.

  
Currently I have ResNet backbone + transformer encoder + autoregressive transformer decoder but I feel this isn't optimal. It's of course just for one image right now

How do you do this? I'd also like to know if ViT, DeiT, ResNet, or other is best. The coordinates must be subpixel accurate, and these all might lose data. Thanks for your help",computervision,2,https://www.reddit.com/r/computervision/comments/1nab3cd/multiple_interdependent_images_passed_into/,r_1nab3cd,,,
r_1naawo3,reddit,Big-Professional2635,2025-09-06T21:09:46+00:00,"How can I quickly annotate a large batch of  images for keypoint detection?
I have over 700 images of a football(soccer) pitch that i want to annotate. I have annotated 30 images and trained a model on those, in the hopes I can use that model to help me annotate the rest of the images",computervision,3,https://www.reddit.com/r/computervision/comments/1naawo3/how_can_i_quickly_annotate_a_large_batch_of/,r_1naawo3,,,
r_1na9x88,reddit,tusame,2025-09-06T20:28:38+00:00,Can Your Model Nail Multi-Subject Personalization?,computervision,1,https://www.reddit.com/r/computervision/comments/1na9x88/can_your_model_nail_multisubject_personalization/,r_1na9x88,,,
r_1na5jix,reddit,ThFormi,2025-09-06T17:33:44+00:00,"Non-ML multi-instance object detection
Hey everybody, student here, I'm working on a multi-instance object detection pipeline in OpenCV with the goal of detecting books in shelves. What are the best approaches that don't require ML ?

I've currently tried matching SIFT keypoints (there are illumination, rotation and scale changes) and estimate bounding boxes through RANSAC but I can't find a good detection threshold. Every threshold, across scenes, is either too high, causing miss detections, or too low, introducing false positive detections. I've also noticed that slight changes to SIFT parameters have drastic changes in the estimations, making the pipeline fragile. My workaround has been to keep the threshold low and then filter false positives using geometric constraints. It works, but it feels suboptimal.

  
I've also tried using the Generalized Hough Transform to limited success. With small accumulator cells, detections are precise (position/scale/rotation), but I miss instances due to too few votes per cell (I donâ€™t think itâ€™s a bug, I thinks its accumulated approximation errors in the barycenter prediction). With larger cells (covering more pixels/scales/rotations), I get more consistent detections with more votes per cell, but bounding boxes become sloppy because of the loss of precision.

  
Any insight or suggestion is appreciated, thank you.",computervision,4,https://www.reddit.com/r/computervision/comments/1na5jix/nonml_multiinstance_object_detection/,r_1na5jix,,,
r_1na59da,reddit,Federal_Listen_1564,2025-09-06T17:22:48+00:00,"Panoptic segmentation cocodormat for custom dataset
Hi

I have a custom dataset I'm trying to train a panoptic segmentation model on (thinking MaskDINO; recommendations are welcome). 

I have a basic question:


'Panoptic segmentation task involves assigning a semantic label and instance ID to each pixel of an image.'

So if two instances are overlapping in the scene, how do we decide which instance ID to assign to the pixels in the overlapping area? 

Any clarification on this will be highly appreciated. Thanks ! ",computervision,2,https://www.reddit.com/r/computervision/comments/1na59da/panoptic_segmentation_cocodormat_for_custom/,r_1na59da,,,
r_1n9onux,reddit,ThunderMan2300,2025-09-06T02:57:05+00:00,"Image to Vector Strokes
[Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting](https://preview.redd.it/8eejymdvignf1.png?width=328&format=png&auto=webp&s=d326f87b7067d2adf5799b383e79327cb19ae622)

I have a task to vectorize a set of lines in an image into a set of (X,Y) coordinates. These lines may intersect each other multiple times, and want to identify each one from the other.

My first approach was to use traditional vision techniques by creating a graph of the pixels. However, I encounter many difficulties when multiple lines cross each other, or when the original line comes back on top of itself, I would lose that information, and close the vector early.

I came across the [Quick, Draw!](https://quickdraw.withgoogle.com/data) Database and was wondering if there exists a pre-trained model that identifies the strokes on an image into a vector format. So far, I have only found models that predict the next stroke or classify a sketch, but nothing that performs stroke vectorization.

**I was hoping someone could provide some 'obscure' model or program that could accomplish this task.**

On the chance that there is no such program, and I had to code/train my own model, I wanted to ask for opinions on the architecture of such a model. Should I use ResNet or some other combination of CNN and RNN? What would you recommend?",computervision,9,https://www.reddit.com/r/computervision/comments/1n9onux/image_to_vector_strokes/,r_1n9onux,,,
r_1n9mib3,reddit,cesmeS1,2025-09-06T01:09:06+00:00,"Hiring for CV: Where to find them and how to screen past buzzwords?
Having a tough time hiring for hands-on CV roles.

Striking out on Indeed and LinkedIn. Most applicants just list a zoo of models and then can't go deeper than ""I trained X on Y.â€ Solid production experience seems rare and the code quality is all over the place.

For context we're an early stage company in sports performance. Consumer mobile app, video heavy, real users and real ship dates. Small team, builder culture, fully remote friendly. We need people who can reason about data, tradeoffs, and reliability, not just spin up notebooks.

Would love to get some thoughts on a couple things.

First, sourcing. Where do you actually meet great CV folks? Any specific communities, job boards, or even slack groups that aren't spammy? University labs or conferences worth reaching out to? Even any boutique recruiters who actually get CV.

Second is screening. How do you separate depth from buzzwords in a fast way?

We've been thinking about a short code sample review, maybe a live session debugging someone elseâ€™s code instead of whiteboard trivia. Or a tiny take-home with a strict time cap, just to see how they handle failure modes and tradeoffs. Even a ""read a paper and talk through it"" type of thing.

Curious what rubric items you guys use that actually predict success. Stuff like being able to reason about latency and memory or just a willingness to cut scope to ship.

Also, what are the ranges looking like these days? For a senior CV engineer who can own delivery in a small team, US remote, what bands are you seeing for base plus equity.

If you have a playbook or a sourcing channel that actually worked, please share. I'll report back what we end up doing. Thanks.",computervision,31,https://www.reddit.com/r/computervision/comments/1n9mib3/hiring_for_cv_where_to_find_them_and_how_to/,r_1n9mib3,,,
r_1n9lcj5,reddit,Dave190911,2025-09-06T00:12:41+00:00,"How to Tackle a PCB Defect Analysis Project with 20+ Defect Types
Hi r/computervision ,Iâ€™m working on a PCB defect analysis project and need advice. Real-world PCBs have 20+ defect types, and whether a defect is a ""pass"" or ""fail"" depends on its location (e.g., pad vs. empty space) based on functionality impact. 

Whatâ€™s the best way to approach this? Any tips on tools, frameworks, or methods for classifying defects and handling location-based pass/fail criteria? Has anyone used automated optical inspection (AOI) or other techniques for this? Letâ€™s discuss!#PCB #DefectAnalysis ",computervision,0,https://www.reddit.com/r/computervision/comments/1n9lcj5/how_to_tackle_a_pcb_defect_analysis_project_with/,r_1n9lcj5,,,
r_1n9g821,reddit,return_my_name,2025-09-05T20:33:28+00:00,"Computer vision for Sports Lab
I am getting ready to apply for my grad studies. As a CS grad, I want to keep doing research in something I actually care about. My aim is to build my research career around sports. The problem is I havenâ€™t really found many labs in the US doing sports-related research. Most of the work I came across is based in Europe.

Since full funding is a big deal for me, I canâ€™t go for a self-funded masterâ€™s.

 If anyone knows labs recruiting ms/phd students or professors hiring in this space, that would be super helpful for me.

[N.B: Not sure if posting this here will get me anywhere, but hey, nothing to lose. Cheers.]",computervision,30,https://www.reddit.com/r/computervision/comments/1n9g821/computer_vision_for_sports_lab/,r_1n9g821,,,
r_1n9fd2y,reddit,aidannewsome,2025-09-05T19:59:36+00:00,"A tool for creating 3D site context? Useful or not?
[An example scan I did with the XGRIDS L2 Pro SLAM device. On the right is the geometry that'd actually be useful to have versus the Gaussian splat.](https://preview.redd.it/g1t54lu9ienf1.png?width=2874&format=png&auto=webp&s=30b3fba59918a5dd928074812605d7f12492d7fe)

  
Hi all,  
  
I'm a 3D artist/architect and my domain is the AEC world. Lately, in my role at my current job, I've been using aerial photogrammetry and SLAM with Gaussian splatting to create site context to help with concept design and visualization on our projects. Context is very important to create high-quality 3D models in architecture, but the current options are either too basic (open source representations, or you have to manually do it from a survey and photos, or stream in Google Photorealistic 3D tiles). Or you spend lots of time and money manually tracing over point clouds/photogrammetry meshes. It's also something that, while super important, you're not really getting paid for, so you're just burning money having people do it. Anyways, I also closely follow stuff in computer vision because of my photogrammetry passion, and I've actually been thinking about solving this 3D site context problem for architecture, and I'm wondering if it's something that'd be useful for other applications in/around CV as well. I'd love to hear your thoughts. My brainstorm is below.

My current thought is that using a variety of inputs, in the most basic form, LiDAR from an iPhone, or more advanced, a point cloud from SfM or LiDAR, I would like to create a low-poly representational model that's just close to accurate (not survey grade). From there, people can do what they want with the ""clean"" 3D data; it's up to you.

My question to you experts is, well, is this even possible today? I'm thinking in the simplest, most MVP form using iPhone LiDAR with the addition of human input, where you label things and swap in generic models where accuracy doesn't matter, e.g., trees, cars, signs and so on. Then, for buildings, the idea would be to get somewhat correct footprints and roof types and fenestration. Then, for topography, the idea would be to get the ground plane, curbs, retaining walls, and also cut out one surface type from the other. So initially it's a LiDAR-assisted, but maybe eventually fully automated...

Any insights into this idea are appreciated. If I'm crazy, that's fine too. Above is an example scan I did with the XGRIDS L2 Pro SLAM device. On the right is the geometry that'd actually be useful to have versus the Gaussian splat.",computervision,5,https://www.reddit.com/r/computervision/comments/1n9fd2y/a_tool_for_creating_3d_site_context_useful_or_not/,r_1n9fd2y,,,
r_1n9evz2,reddit,mixedfeelingz,2025-09-05T19:40:34+00:00,"Best practices for building a clothing digitization/wardrobe tool
Hey everyone,

I'm looking to build a clothing detection and digitization tool similar to apps like Whering, Acloset, or other digital wardrobe apps. The goal is to let users photograph their clothes and automatically extract/catalog them with removed backgrounds.

**What I'm trying to achieve:**

* Automatic background removal from clothing photos
* Clothing type classification (shirt, pants, dress, etc.)
* Attribute extraction (color, pattern, material)
* Clean segmentation for a digital wardrobe interface

**What I'm looking for:**

1. **Current best models/approaches** \- What's SOTA in 2025 for fashion-specific computer vision? Are people still using YOLOv8 + SAM, or are there better alternatives now?
2. **Fashion-specific datasets** \- Beyond Fashion-MNIST and DeepFashion, are there newer/better datasets for training?
3. **Open source projects** \- Are there any good repos that already combine these features? I've found some older fashion detection projects but wondering if there's anything more recent/maintained.
4. **Architecture recommendations** \- Should I go with:
   * Detectron2 + custom training?
   * Fine-tuned SAM for segmentation?
   * Specialized fashion CNNs?
   * Something else entirely?
5. **Background removal** \- Is rembg still the go-to, or are there better alternatives for clothing specifically?

**My current stack:** Python, PyTorch, basic CV experience

Has anyone built something similar recently? What worked/didn't work for you? Any pitfalls to avoid?

Thanks in advance!",computervision,0,https://www.reddit.com/r/computervision/comments/1n9evz2/best_practices_for_building_a_clothing/,r_1n9evz2,,,
r_1n929j4,reddit,Alex19981998,2025-09-05T11:06:40+00:00,"How can I use DINOv3 for Instance Segmentation?
Hi everyone,

Iâ€™ve been playing around with DINOv3 and love the representations, but Iâ€™m not sure how to extend it to **instance segmentation**.

* What kind of head would you pair with it (Mask R-CNN, CondInst, DETR-style, something else).  Maybe Mask2Former but I\`m a little bit confused that it is archived on github?
* Has anyone already tried hooking DINOv3 up to an instance segmentation framework?

Basically I want to fine-tune it on my own dataset, so any tips, repos, or advice would be awesome.

Thanks!",computervision,22,https://www.reddit.com/r/computervision/comments/1n929j4/how_can_i_use_dinov3_for_instance_segmentation/,r_1n929j4,,,
r_1n916us,reddit,CaptainBudy,2025-09-05T10:06:08+00:00,"DCNv2 (Update Compatibility) Pytorch 2.8.0
Hello Reddit,

Working on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.

Feel free to use it for training model like YOLACT+, FairMOT or others.

[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main)",computervision,2,https://www.reddit.com/r/computervision/comments/1n916us/dcnv2_update_compatibility_pytorch_280/,r_1n916us,,,
r_1n90r9m,reddit,monoceros556,2025-09-05T09:39:06+00:00,"Looking for career paths in AI + mobile mapping for heritage sites
Hi! Iâ€™m doing a masterâ€™s in Architectural Design & History. My thesis is about mobile mapping for rapid surveying and AI models to classify damage on heritage sites.

Iâ€™m not planning to do a PhD but want to work in this field. Any advice on:

Roles or offices I could aim for... How to grow my skills and knowledge ? Resources, networks, or communities worth following...

Thanks a lot for any tips..",computervision,2,https://www.reddit.com/r/computervision/comments/1n90r9m/looking_for_career_paths_in_ai_mobile_mapping_for/,r_1n90r9m,,,
r_1n8yvyj,reddit,Bitter-Pride-157,2025-09-05T07:36:32+00:00,ResNet and Skip Connections,computervision,0,https://www.reddit.com/r/computervision/comments/1n8yvyj/resnet_and_skip_connections/,r_1n8yvyj,,,
r_1n8ybj9,reddit,Ok_Pie3284,2025-09-05T06:59:38+00:00,"Agents-based algo community
Hi,
I'd like to invite everyone to a new community which will focus on using agentic AI to solve algorithmic problems from various fields such as computer vision, localization, tracking, gnss, radar, etc...
As an algorithms researcher with quite a few years of experience in these fields, I can't help but feel that we are not exploiting the potential combination of agentic AI with our maticiously crafted algorithmic pipelines and techniques.
Can we use agentic AI to start making soft design decisions instead of having to deal with model drift? Must we select a certain tracker, camera model, filter, set of configuration parameters during the design stage or perhaps we can use an agentic workflow to make some of these decision in real-time?
This community will not be about ""vibe-algorithms"", it will focus on combining the best of our task-oriented classical/deep algorithmic design with the reasoning of agentic AI...
I am looking forward to seeing you there and having interesting discussions/suggestions...
https://www.reddit.com/r/AlgoAgents/s/leJSxq3JJo",computervision,0,https://www.reddit.com/r/computervision/comments/1n8ybj9/agentsbased_algo_community/,r_1n8ybj9,,,
r_1n8xo93,reddit,Processor48,2025-09-05T06:18:52+00:00,"Recommended Camera & Software For Object Detection
My project aims to detect deviations from some 'standard state' based on few seconds detection stream. my state space is quite small, and i think i could manually classify them based on the detection results.

Could you help me choose the correct camera/framework for this task?



**Camera requirements:**

\- Indoors

\- 20-30m distance from objects, cameras are installed on ceilings

\- No need for extreme resolution & fps

\- Spaces are quite big so i would need a high fov camera? or just few cameras covering the space

**Algorithm requirements:** 

\- Was thinking YOLO -> logical states based on its outputs. are there better options?

\- Video will be sent to cloud and calculations will be made there



Thanks alot in advance ! ",computervision,2,https://www.reddit.com/r/computervision/comments/1n8xo93/recommended_camera_software_for_object_detection/,r_1n8xo93,,,
r_1n8p92p,reddit,w0nx,2025-09-04T23:12:30+00:00,"Built a tool to â€œre-plantâ€ a tree in my yard with just my phone
This started as me messing around with computer vision and my yard. I snapped a picture of a tree, dragged it across the screen, and dropped it somewhere else next to my garage. Instant landscaping mockup.  
  
Itâ€™s part of a side project Iâ€™m building called Canvi. Basically a way to capture real objects and move them around like design pieces. Today itâ€™s a tree. Couches, products, or whatever else people want to play with.

Still super early, but itâ€™s already fun to use. Curious what kinds of things you would want to move around if you could just point your phone at them?",computervision,128,https://www.reddit.com/r/computervision/comments/1n8p92p/built_a_tool_to_replant_a_tree_in_my_yard_with/,r_1n8p92p,,,
r_1n8eix1,reddit,Zealousideal_Low1287,2025-09-04T16:13:25+00:00,"Fast Image Remapping
I have two workloads that use image remapping (using opencv now). One I can precompute the map for, one I canâ€™t.

I want to accelerate one or both of them, does anyone have any recommendations / has faced a similar problem?",computervision,0,https://www.reddit.com/r/computervision/comments/1n8eix1/fast_image_remapping/,r_1n8eix1,,,
r_1n8dq75,reddit,momoisgoodforhealth,2025-09-04T15:43:51+00:00,"Detecting Sphere Monocular Camera
Is detecting sphere a non trivial task? I tried using OpenCV's Circle Hough Transform but it does not perform well when I am moving it around in space, in an indoor background. What methods should I look into?",computervision,7,https://www.reddit.com/r/computervision/comments/1n8dq75/detecting_sphere_monocular_camera/,r_1n8dq75,,,
r_1n88p9e,reddit,coolwulf,2025-09-04T12:25:18+00:00,I developed a totally free mobile web app to scan chess board and give analysis using stockfish chess engine,computervision,8,https://www.reddit.com/r/computervision/comments/1n88p9e/i_developed_a_totally_free_mobile_web_app_to_scan/,r_1n88p9e,,,
r_1n889ok,reddit,NailaBaghir,2025-09-04T12:04:56+00:00,"Just released my new project: Satellite Change Detection with Siamese U-Net! ðŸŒ
Hi everyone,

Iâ€™ve been working on a **Satellite Change Detection** project using the **Onera Satellite Change Detection (OSCD) dataset**. The goal was to detect urban and environmental changes from Sentinel-2 imagery by training a **Siamese U-Net model**.

ðŸ”¹ Preprocessing pipeline includes tiling, normalization, and dataset preparation.  
ðŸ”¹ Implemented data augmentation for robust training.  
ðŸ”¹ Used custom loss functions (BCE + Dice / Focal) to handle class imbalance.  
ðŸ”¹ Visualized predictions to compare ground truth vs. model output.

You can check out the code, helper modules, and instructions here:  
ðŸ‘‰ [GitHub Repository](https://github.com/NailaBagir/OSCD-Change-Detection)

Iâ€™d love to hear your **feedback, suggestions, or ideas** to improve the approach!

Thanks for reading âœ¨",computervision,10,https://www.reddit.com/r/computervision/comments/1n889ok/just_released_my_new_project_satellite_change/,r_1n889ok,,,
r_1n83gja,reddit,jms4607,2025-09-04T07:15:51+00:00,"Did plant evolution influence the design of most modern cameras?
1. Plants evolved to be green.
2. Humans evolved to be most sensitive to green to perceive their natural environment.
3. Bayer decides double the number of green photosites to match human vision sensitivity.
4. Most RGB cameras today use a BGGR format for raw image data.

I thought this was a quaint CV fact, lmk if I am naive/mistaken. ",computervision,26,https://www.reddit.com/r/computervision/comments/1n83gja/did_plant_evolution_influence_the_design_of_most/,r_1n83gja,,,
r_1n82nzu,reddit,Data_Conflux,2025-09-04T06:25:09+00:00,"What are the biggest challenges youâ€™ve faced when annotating images for computer vision models?
When working with computer vision datasets, what do you find most challenging in the annotation process - labeling complexity, quality control, or scaling up? Interested in hearing different perspectives.",computervision,24,https://www.reddit.com/r/computervision/comments/1n82nzu/what_are_the_biggest_challenges_youve_faced_when/,r_1n82nzu,,,
r_1n82jsh,reddit,Hopeful_Band_4048,2025-09-04T06:17:44+00:00,"Fine tuning an EfficientDet Lite model in 2025
I'm creating a custom object detection system. Due to hardware restraints, I am limited to using a Coral Edge TPU to run object detection, which strongly limits my choice of detection models. This is for an embedded system using on device inference.

My research strongly suggests that using an EfficientDet Lite variant will be my best contender for the Coral. However, I have been struggling to find and/or install a suitable platform which enables me to easily fine tune the model on a custom dataset, as many tools seem to have been outgrown by their own ecosystems.

Currently, my 2 hardware options for training the model are Google Colab and my M2 macbook pro.

* The object detection API has the features to train the model, however seems to be impossible to install on both my M2 mac and google colab - as I have many dependency errors when trying to install and run on either.
* The TFLite Model Maker does not allow Python versions later than 3.9, which rules out colab. Additionally, the libraries are not compatible with an M2 mac for the versions which the model maker depends on. I attempted to use Docker to create a suitable container with Rosetta 2 x86 emulation, however, once I got it installed and tried to run it, it turned out that Rosetta would not work in these circumstances (""The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine"")
* My other option is to download a EfficientDet lite savedModel from Kaggle and try and create a custom fine tuning algorithm, implementing my own loss function and training loop - which is more future-proof however cumbersome and probably prone to error due to my limited experience with such implementations.

Every tutorial colab notebook I try to run whether official or by the community fails mostly at the installation sections, and the few that don't have critical errors which are sourced from attempting to use legacy classes and library functionality.

I will soon try to get access to an x86 computer so I can run a docker container using legacy libraries, however my code may be used as a pipeline to train many models, and the more future proof the system the better. I am surprised that modern frameworks like KerasCV don't support EfficientDet even though they support RetinaNet which is both less accurate and fast than EfficientDet.

My questions are as follows:

1. Is EfficientDet still a suitable candidate given that I don't seem to have the hardware flexibility to run models like YOLO without performance drops while compiling for the Edge TPU.
2. EfficientDet seems to still be somewhat prevalent in some embedded systems - what's the industry standard for fine tuning them? Do people still use the Object Detection API, I know it has been succeeded by tools like KerasCV - however, this does not have support for EfficientDet. Am I simply just limited to using legacy tools as EfficientDet is apparently moving towards being a legacy model?",computervision,4,https://www.reddit.com/r/computervision/comments/1n82jsh/fine_tuning_an_efficientdet_lite_model_in_2025/,r_1n82jsh,,,
r_1n7zywi,reddit,thumbsdrivesmecrazy,2025-09-04T03:49:48+00:00,"Combining Parquet for Metadata and Native Formats for Video, Audio, and Images with DataChain AI Data Warehouse
The article outlines several fundamental problems that arise when teams try to store raw media data (like video, audio, and images) inside Parquet files, and explains how DataChain addresses these issues for modern multimodal datasets - by using Parquet strictly for structured metadata while keeping heavy binary media in their native formats and referencing them externally for optimal performance: [reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/](https://www.reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/)

It shows how to use Datachain to fix these problems - to keep raw media in object storage, maintain metadata in Parquet, and link the two via references.",computervision,1,https://www.reddit.com/r/computervision/comments/1n7zywi/combining_parquet_for_metadata_and_native_formats/,r_1n7zywi,,,
r_1n7zoh8,reddit,AIsavvy,2025-09-04T03:34:33+00:00,"Less explored / Emerging areas of research in computer vision
I'm currently exploring research directions in computer vision. I'm particularly interested in **less saturated or emerging topics** that might not yet be fully explored.",computervision,25,https://www.reddit.com/r/computervision/comments/1n7zoh8/less_explored_emerging_areas_of_research_in/,r_1n7zoh8,,,
r_1n7vzve,reddit,electric-poem,2025-09-04T00:38:32+00:00,"Webcam recommendations for pose estimation?
Hi

Iâ€™m building a project with MediaPipe to track body keypoints and calculate joint angles for real-time exercise feedback. The core pipeline works, but my laptop camera sits in the keyboard area so angle/quality are terrible and I canâ€™t properly test all motions.

Iâ€™m looking for a budget webcam (~100$) thatâ€™s good for pose estimation. Is it better to prioritize 1080p@60fps over 4K@30fps for MediaPipe? Any specific webcam models or tips (placement, lighting, camera settings) youâ€™d recommend?

",computervision,4,https://www.reddit.com/r/computervision/comments/1n7vzve/webcam_recommendations_for_pose_estimation/,r_1n7vzve,,,
r_1n7sici,reddit,edge-ai-vision,2025-09-03T22:06:30+00:00,"2025 Computer Vision and Perceptual AI Developer Survey - We Want Your Opinions!
Hey all. Every year the Edge AI and Vision Alliance surveys CV and perceptual AI system and application developers to get their views on processors, tools, algorithms, and more. Your input will help guide the priorities of numerous suppliers of building-block technologies. In return for completing the survey, youâ€™ll get access to detailed results and a $250 discount on a two-day pass to the 2026 Embedded Vision Summit next May. We'd love to have your input!

Survey link: [https://info.edge-ai-vision.com/2025-developer-survey-social-media-recaptcha](https://info.edge-ai-vision.com/2025-developer-survey-social-media-recaptcha)",computervision,0,https://www.reddit.com/r/computervision/comments/1n7sici/2025_computer_vision_and_perceptual_ai_developer/,r_1n7sici,,,
r_1n7reg3,reddit,N0m0m0,2025-09-03T21:22:14+00:00,"Detectron2 dinov3
I use faster rcnn via detectron2. Is there any way to integrate dinov3 as the backbone? I have seen comments but not sure how to go about it. Are there open source projects available? ",computervision,6,https://www.reddit.com/r/computervision/comments/1n7reg3/detectron2_dinov3/,r_1n7reg3,,,
r_1n7o0vr,reddit,ComedianOpening2004,2025-09-03T19:13:43+00:00,"Error between Metric version of Depth Anything V2 and GT
Hello guys, so basically what the question says. Does anyone have numbers on the accuracy of the metric version of DA v2 (especially the base and small variants) to the ground truth?
Like how many centimetres can I expect it to be off about?

Also, how does this compare to Metric3D?

Thanks",computervision,1,https://www.reddit.com/r/computervision/comments/1n7o0vr/error_between_metric_version_of_depth_anything_v2/,r_1n7o0vr,,,
r_1n7no13,reddit,Ryaja2,2025-09-03T19:00:35+00:00,"Budget camera recommendations for robotics
Hi, I'm looking into camera options for a robot I'm building using a Jetson Orin Nano. Are there any good stereo cameras that cost less than $100 and are appropriate for simple robotics tasks? Furthermore, can a single camera be adequate for basic applications, or is a stereo camera required?",computervision,1,https://www.reddit.com/r/computervision/comments/1n7no13/budget_camera_recommendations_for_robotics/,r_1n7no13,,,
r_1n7m7bd,reddit,Whole-Assignment6240,2025-09-03T18:05:47+00:00,"Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR
Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.

\- Detailed write up:Â [https://cocoindex.io/blogs/multi-format-indexing](https://cocoindex.io/blogs/multi-format-indexing)  
\- Fully open sourced:Â [https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi\_format\_indexing](https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing)  
(70 lines python on index path)

Looking forward to your suggestions",computervision,4,https://www.reddit.com/r/computervision/comments/1n7m7bd/build_a_visual_document_index_from_multiple/,r_1n7m7bd,,,
r_1n7ic40,reddit,papersashimi,2025-09-03T15:44:44+00:00,"Dinov3clip adapter
Created a tiny adapter that connects DINOv3's image encoder to CLIP's text space.

Essentially, DINOv3 has better vision than CLIP, but no text capabilities. This lets you use dinov3 for images and CLIP for text prompts. This is still v1 so the next stages will be mentioned down below. 

**Target Audience:**

ML engineers who want zero-shot image search without training massive models

Works for zero shot image search/labeling. Way smaller than full CLIP. Performance is definitely lower because it wasnt trained on image-text pairs.

**Next steps**: May do image-text pair training. Definitely adding a segmentation or OD head. Better calibration and prompt templates

Code and more info can be found here: [https://github.com/duriantaco/dinov3clip](https://github.com/duriantaco/dinov3clip)

If you'll like to colab or whatever do ping me here or drop me an email. ",computervision,23,https://www.reddit.com/r/computervision/comments/1n7ic40/dinov3clip_adapter/,r_1n7ic40,,,
r_1n7h8ru,reddit,doineedone-_-,2025-09-03T15:03:48+00:00,"Raspberry pi turns off as soon as connect camera to it
I have an imx708 camera, and when its plugged into my raspberry pi 5 it wont boot up. I tried to remove it and then boot the raspberry pi it works fine but as soon as i connect the camera it shuts down. One more things i noticed is, when this camera is connected to the jetson orin nano that i have , i noticed the csi connectors heating up a bit at around 40degrees celcius. I m kinda stuck its my first time using cameras like this",computervision,5,https://www.reddit.com/r/computervision/comments/1n7h8ru/raspberry_pi_turns_off_as_soon_as_connect_camera/,r_1n7h8ru,,,
r_1n7gpez,reddit,Sarcinismo,2025-09-03T14:43:21+00:00,"What are the downsides of running Jetson Xavier NX in MAXN mode?
Iâ€™ve been experimenting with my Jetson Xavier NX and switched it into **MAXN mode** (sudo nvpmodel -m 0). I understand this unlocks full performance (all 6 CPU cores online, CPU up to 1.9GHz, GPU up to \~1100MHz, etc.), but Iâ€™m wondering about the **real-world consequences** of keeping it in this mode.

* Does running in MAXN for long periods cause stability or hardware issues?
* How bad is the thermal situation if you only use the stock passive heatsink (without the active fan)?
* Any impact on the longevity of the board if I keep it in MAXN 24/7?
* For those who run NX in production, do you stick to 15W/10W modes instead?

",computervision,5,https://www.reddit.com/r/computervision/comments/1n7gpez/what_are_the_downsides_of_running_jetson_xavier/,r_1n7gpez,,,
r_1n7ez4e,reddit,proudtorepresent,2025-09-03T13:34:37+00:00,"Ideas for Fundamentals of Artificial Intelligence lecture
So, I am an assistant at a university and this year we plan to open a new lecture about the fundamentals of Artificial Intelligence. We plan to make an interactive lecture, like students will prepare their projects and such. The scope of this lecture will be from the early ages of AI starting from perceptron, to image recognition and classification algorithms, to the latest LLMs and such. Students that will take this class are from 2nd grade of Bachelorâ€™s degree. What projects can we give to them? Consider that their computers might not be the best, so it should not be heavily dependent on real time computational power.Â 



My first idea was to use the VRX simulation environment and the Perception task of it. Which basically sets a clear roadline to collect dataset, label them, train the model and such. Any other homework ideas related to AI is much appreciated. 

",computervision,9,https://www.reddit.com/r/computervision/comments/1n7ez4e/ideas_for_fundamentals_of_artificial_intelligence/,r_1n7ez4e,,,
r_1n77njt,reddit,ptjunior67,2025-09-03T06:43:49+00:00,"What's the best local VLM for iOS apps in 2025?
I have been developing an iOS image analysis app that describes the content of usersâ€™ uploaded images for over 7 months.



Initially, I used FastViTMA36F16, DETRResNet50SemanticSegmentationF16, MobileNetV2, ResNet50, and YOLOv3 to analyze objects in images, producing fixed outputs that included detected objects and their locations. However, these models performed poorly in understanding images and labeling detected objects accurately. So I replaced them with GPT-4 Vision, but its cost was too expensive for me. I then switched to Google Vision API, though my goal has always been to build a 100% offline app powered by a VLM.

  
I have experimented with Appleâ€™s FastVLM 0.5B (*Apple-AMLR*) since May and was impressed by the quality of on-device analysis. It frequently crashes due to high memory usage on my iPhone 15 Pro, though. I then tried SmolVLM2 256M, which still required over 1 GB of memory to process a single image. I have been searching for other small VLMs and found Moondream as a potential candidate to test in the coming days.

What is currently the best local VLM for an iOS app that is both small and fast?",computervision,8,https://www.reddit.com/r/computervision/comments/1n77njt/whats_the_best_local_vlm_for_ios_apps_in_2025/,r_1n77njt,,,
r_1n77ks3,reddit,shani_786,2025-09-03T06:38:51+00:00,"Autonomous Vehicles Learning to Dodge Traffic via Stochastic Adversarial Negotiation
In a live demo, [Swaayatt Robots](https://www.swaayattrobots.com/) pushed adversarial negotiation to the extreme: the team members rode two-wheelers and randomly cut across the autonomous vehicleâ€™s path, forcing it to dodge and negotiate traffic on its own. The vehicle also handled static obstacles like cars, bikes, and cones before tackling these dynamic, adversarial interactions.

This demo showcased [Swaayatt Robots's](https://www.swaayattrobots.com/) **reinforcement** **learningâ€“based motion planning and decision-making framework**, designed to handle the worldâ€™s most complex traffic â€” Indian roads â€” as we scale towards Level-4 and Level-5 autonomy.",computervision,160,https://www.reddit.com/r/computervision/comments/1n77ks3/autonomous_vehicles_learning_to_dodge_traffic_via/,r_1n77ks3,,,
r_1n714w7,reddit,InternationalMany6,2025-09-03T01:00:49+00:00,"Does FastSAM only understand COCO?
Working on a project where I need to segment objects without caring about the classes of the object. SAM works ok but it too slow, so Iâ€™m looking at alternatives. 

FastSAM came up but my question is, does it only work on objects resembling the 89 COCO classes, since it uses yolov8-seg? In my testing it does work on other classes but is that just a coincidence? ",computervision,4,https://www.reddit.com/r/computervision/comments/1n714w7/does_fastsam_only_understand_coco/,r_1n714w7,,,
r_1n713w6,reddit,Similar-Way-9519,2025-09-03T00:59:37+00:00,"Affordable Edge Device for RTMDet-s (10+ FPS)
I'm trying to run **RTMDet-s** for edge inference, but Jetson devices are a bit too expensive for my budget.  
Iâ€™d like to achieve real-time performance, with at least **10 FPS** as a baseline.

What kind of edge devices would be a good fit for this use case?",computervision,1,https://www.reddit.com/r/computervision/comments/1n713w6/affordable_edge_device_for_rtmdets_10_fps/,r_1n713w6,,,
r_1n6wntr,reddit,mgtezak,2025-09-02T21:47:32+00:00,"Commercial use of model weights pretrained on ImageNet data
Hi there! I'm new to CV and I stumbled upon the legal gray-area concerning dataset-derived weights.

For context: I'd like to use model weights by OpenMMLab who state that everything they provide is licensed under Apache 2.0 (free for commercial use) but the weights they provide were trained on the ImageNet dataset (or a subset of it) which is [not free for commercial use](https://www.image-net.org/). 

Have there been any recent legal developments which make it explicit whether or not model weights must have at least the same amount of licensing restrictiveness as the data they're derived from or not? I'm especially interested in the legal situation in Germany which is where I work.

Grateful for any opinions and experience!",computervision,10,https://www.reddit.com/r/computervision/comments/1n6wntr/commercial_use_of_model_weights_pretrained_on/,r_1n6wntr,,,
r_1n6w55u,reddit,MaxSpiro,2025-09-02T21:26:44+00:00,"Breakdance/Powermove combo classification
I've been playing with different keypoint detection models like ModelNet and YOLO on mine and others' breaking clips--specifically powermoves (acrobatic and spinning moves that are IMO easier to classify). On raw frames in breaking clips, they tend to do poorly compared to other activities like yoga and lifting where people are usually standing upright, in good lighting, and not in crowds of people.

I read a paper titled ""Tennis Player Pose Classification using YOLO and MLP Neural Networks"" where the authors used YOLO to extract bounding boxes and keypoints and then fed the keypoints into a MLP classifier. Something interesting they did was encoding 13 frames into one data entry to classify a forward/backward swing, and I thought this could be applied to powermove combos where a sequence of frames could provide more insight into the powermove than just a single frame.

I've started annotating individual frames of powermoves like flares, airflares, windmills, etc. However, I'm wondering if instead of annotating 20-30 different images of people doing a specific move, I instead focus on annotating videos using CVAT tracking and classifying the moves in the combos.

Then, there is also the problem of pose detection models performing poorly on breaking positions, so surely I would want to train my desired model like YOLO on these breaking videos/images, too, right? And also train the classifier on images or sequences.

Any ideas or insight to this project would be very appreciated!",computervision,3,https://www.reddit.com/r/computervision/comments/1n6w55u/breakdancepowermove_combo_classification/,r_1n6w55u,,,
r_1n6r7bt,reddit,Dismal-Purple3128,2025-09-02T18:19:44+00:00,"Guys I need help!!
I am a CS student , working on an autonomous rover and for obstacle detection I am planning to use a depth camera , opting specifically for Oak-d lite what's your opinion on this and provide tips for me  
Thanks in Advance.",computervision,0,https://www.reddit.com/r/computervision/comments/1n6r7bt/guys_i_need_help/,r_1n6r7bt,,,
r_1n6q59s,reddit,ProductmanagerVC,2025-09-02T17:40:52+00:00,"Blurry scans arenâ€™t just imagesâ€”theyâ€™re missed diagnoses. Generative AI is rebuilding clarity.
This 2025 Pitchworks report explores how AI is transforming MRI and CT scan reconstructionâ€”cutting scan times, enhancing accuracy, and improving patient outcomes. It includes real-world implementations in India and the US, challenges in adoption, and a framework to evaluate each use case.

If youâ€™re a clinician, innovator, or healthcare buyer, this roadmap shows where AI in imaging is headed next.

[https://www.pitchworks.club/medicalimagereconstructionwithgenai](https://www.pitchworks.club/medicalimagereconstructionwithgenai)",computervision,0,https://www.reddit.com/r/computervision/comments/1n6q59s/blurry_scans_arent_just_imagestheyre_missed/,r_1n6q59s,,,
r_1n6p96m,reddit,datascienceharp,2025-09-02T17:07:09+00:00,"Apples FastVLM is making convolutions great again
â€¢ Convolutions handle early vision (stages 1-3), transformers handle semantics (stages 4-5)  

â€¢ 64x downsampling instead of 16x means 4x fewer tokens  

â€¢ Pools features from all stages, not just the final layer  

**Why it works**

â€¢ Convolutions naturally scale with resolution  

â€¢ Fewer tokens = fewer LLM forward passes = faster inference  

â€¢ Conv layers are ~10x faster than attention for spatial features  

â€¢ VLMs need semantic understanding, not pixel-level detail  

**The results**

â€¢ 3.2x faster than ViT-based VLMs  

â€¢ Better on text-heavy tasks (DocVQA jumps from 28% to 36%)  

â€¢ No token pruning or tiling hacks needed  


Quickstart notebook: https://github.com/harpreetsahota204/fast_vlm/blob/main/using_fastvlm_in_fiftyone.ipynb",computervision,146,https://www.reddit.com/r/computervision/comments/1n6p96m/apples_fastvlm_is_making_convolutions_great_again/,r_1n6p96m,,,
r_1n6lm5f,reddit,iz_bleep,2025-09-02T14:51:38+00:00,"Transfer learning model not training well(I've shared the colab link if any one wants to take a look at my code)
Im training a model which uses mobilenetv3small as the backbone and then a sppf(spatial pyramid pooling fast) and a cbam attention module for fire and smoke detection. Im using a very lightweight model as i need to deploy it on a microcontroller after int8 quantizing it later. My issue is that the model isnt training well, The IoU is very close to 0 and it doesnt improve but the accuracy says its 0.99. The total loss is also like \~5 after a few epochs. Im not able to understand what the problem is could someone help me out. Also if you could give me suggestions regarding the model architecture that would me amazing.
Im fairly certain the problem is with the way i've parsed and preprocessed my tf records dataset but i cant pinpoint the issue.
Colab Link:
https://colab.research.google.com/drive/1o2PG7Kvf2tyjFLvF-JXhOebe_KfhjOg9?authuser=4#scrollTo=lKMwVj8jVJT9 

",computervision,0,https://www.reddit.com/r/computervision/comments/1n6lm5f/transfer_learning_model_not_training_wellive/,r_1n6lm5f,,,
r_1n6llyh,reddit,await_void,2025-09-02T14:51:26+00:00,"Tried building an explainable Vision-Language Model with CLIP to spot and explain product defects!
Hi all!

After quite a bit of work, Iâ€™ve finally completed my **Vision-Language Model** â€” building something this complex in a multimodal context has been one of the most rewarding experiences Iâ€™ve ever had. This model is part of my Masterâ€™s thesis and is designed to **detect product defects and explain them in real-time**. The project aims to **address a Supply Chain challenge**, where the end user needs to clearly **understand** ***why*** **and** ***where*** a product is defective, in an **explainable and transparent** way.

[A gradcam map activation for the associated predicted caption with his probability: \\""A fruit with Green Mold\\""](https://preview.redd.it/z4jrpe9slsmf1.png?width=1200&format=png&auto=webp&s=7396bc1716634623a9e4f3b52208a120a7e63c39)

I took inspiration from the amazing work of [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734), a paper worth a reading, and modified some of his structure to adapt it to my case scenario.

For a brief explanation, basically what it does is that the image is first **transformed into an embedding using CLIP**, which captures its semantic content. This **embedding is then used to guide GPT-2** (or any other LLM really, i opted for **OPT-125** \- pun intended) **via an auxiliar mapper** (a simple transformer that can be extended to more complex projection structure based on the needs) that **aligns the visual embeddings to the text one**, catching the meaning of the image. If you want to know more about the method, this is the [original author post](https://www.reddit.com/r/MachineLearning/comments/q3xon8/p_fast_and_simple_image_captioning_model_using/), super interesting.

Basically, It **combines CLIP** (for visual understanding) **with a language model** to generate a short description and overlays showing exactly where the model â€œlookedâ€, and the **method itself it's super fast to train and evaluate,** because nothing it's trained aside a small mapper (an MLP, a Transformer) which rely on the concept of the **Prefix Tuning** (A Parameter Efficient Fine Tuning technique).

What i've extended on my work actually, is the following:

* **Auto-labels images using CLIP** (no manual labels), then trains a captioner for your domain. This was one of the coolest discovery i've made and will definitely use Contrastive Learning methods to auto label my data in the future.
* Using **another LLM** (OPT-125) to generate better, intuitive caption
* Generates a **plain-language defect description**.
* A **custom Grad-CAM** from scratch based on the ViT-B32 layers, to create **heatmaps** that justify the decisionâ€”per prompt and combined, giving transparent and explainable choice visual cues.
* Runs in a simple **Gradio Web App** for quick trials.
* Much more in regard of the entire **project structure/architecture**.

Why it matters? In my Master Thesis scenario, i had those goals:

* **Rapid bootstrapping without hand labels**: I had the ""exquisite"" job to collect and label the data. Luckily enough, i've found a super interesting way to automate the process.
* **Visual and textual explanations for the operator**: The ultimate goal was to provide visual and textual cues about why the product was defective.
* **Designed for supply chains** setting (defect finding, identification, justification), and may be **extended to every domain** with the appropriate data (in my case, it regards the rotten fruit detection).

The model itself was trained on around **15k of images**, taken from [Fresh and Rotten Fruits Dataset for Machine-Based Evaluation of Fruit Quality](https://data.mendeley.com/datasets/bdd69gyhv8/1), which presents around \~3200 unique images and **12335 augmented** one. Nonentheless the small amount of image the model presents a surprising accuracy.

**For anyone interested, this is the Code repository:** [https://github.com/Asynchronousx/CLIPCap-XAI](https://github.com/Asynchronousx/CLIPCap-XAI) with more in-depth explanations.

Hopefully, this could help someone with their researches, hobby or whatever else! I'm also happy to answer questions or hear suggestions for improving the model or any sort of feedback.

Following a little demo video for anyone interested (could be also find on the github page if reddit somehow doesn't load it!)

[Demo Video for the Gradio Web-App](https://reddit.com/link/1n6llyh/video/zflqjc6qlsmf1/player)

Thank you so much",computervision,114,https://www.reddit.com/r/computervision/comments/1n6llyh/tried_building_an_explainable_visionlanguage/,r_1n6llyh,,,
r_1n6hgs9,reddit,Ok_Shoulder_83,2025-09-02T11:56:34+00:00,"Has anyone worked on spatial predicates with YOLO detections?
Hi all,

Iâ€™m working on extending an object detection pipeline (YOLO-based) to not just detect objects, but also **analyze their relationships and proximity**. For example:

* Detecting if a helmet is actually **worn by a person** vs. just lying nearby.
* Checking **personâ€“vehicle proximity** to estimate potential accident risks.

Basically, once I have bounding boxes, I want to reason about spatial predicates like *on top of, near, inside* etc., and use those relationships for higher-level safety insights.

Has anyone here tried something similar? How did you go about it (post-processing, graph-based reasoning, extra models, heuristics, etc.)? Would love to hear experiences or pointers.

Thanks!",computervision,2,https://www.reddit.com/r/computervision/comments/1n6hgs9/has_anyone_worked_on_spatial_predicates_with_yolo/,r_1n6hgs9,,,
r_1n6dwbm,reddit,socemaglo,2025-09-02T08:22:29+00:00,"WideResNet
Iâ€™ve been working on a segmentation project and noticed something surprising: WideResNet consistently delivers better performance than even larger, more â€œpowerfulâ€ architectures Iâ€™ve tried. This holds true across different datasets and training setups.

I have my own theory as to why this might be the case, but Iâ€™d like to hear the communityâ€™s thoughts first. Has anyone else observed something similar? What could be the underlying reasons for WideResNetâ€™s strong performance in some CV tasks?",computervision,6,https://www.reddit.com/r/computervision/comments/1n6dwbm/wideresnet/,r_1n6dwbm,,,
r_1n6dbey,reddit,Secret-Ad8475,2025-09-02T07:43:55+00:00,"Surface roughness on machined surfaces
I had an academic project dealt with finding a surface roughness on machined surfaces and roughness value can be in micro meters, which camera can I go with (  < 100$), can I use raspberry pi camera module v2",computervision,3,https://www.reddit.com/r/computervision/comments/1n6dbey/surface_roughness_on_machined_surfaces/,r_1n6dbey,,,
r_1n6d1b7,reddit,Puzzleheaded-Ad-8850,2025-09-02T07:25:14+00:00,"End-to-end Autonomous Driving Research
I have experience with perception for modular AVs. I am trying to get into end-to-end models that go from lidar+camera to planning. 

I found recent papers like UniAD but one training run for models like this can take nearly a week on 8 80GB A100s according to their Github. I have a server machine with two 48GB GPUs. I believe this would take nearly a month of training for instance. And this would just be 1 run. 10+ experiments would at least be needed to get a good paper. 

Is it worth attempting end to end research with this compute budget on datasets like Nuscenes? I have some ideas for research but unsure if the baseline models would even be runnable with my compute. Appreciate any ideas!",computervision,4,https://www.reddit.com/r/computervision/comments/1n6d1b7/endtoend_autonomous_driving_research/,r_1n6d1b7,,,
r_1n67kih,reddit,FaithlessnessOk5766,2025-09-02T02:14:13+00:00,"Yolo and sort alternatives for object tracking
Edit: I am hoping to find an alternative for Yolo. I don't have computation limit and although I need this to be real-time ~half a second delay would be ok if I can track more objects.

Iâ€™m using YOLO + SORT for single class detection and tracking, trained on ~1M frames. It performs ok in most cases, but struggles when (1) the background includes mountains or (2) the objects are very small. Example image attached to show what I mean by mountains.

Has anyone tackled similar issues? What approaches/models have worked best in these scenarios? Any advice is appreciated.",computervision,28,https://www.reddit.com/r/computervision/comments/1n67kih/yolo_and_sort_alternatives_for_object_tracking/,r_1n67kih,,,
r_1n66c7m,reddit,CapMysterious9942,2025-09-02T01:14:24+00:00,"Looking for open datasets and resources for AI-based traffic analysis (YOLOv8 + Power BI integration)
**Body:**  
Hi everyone,

Iâ€™m a university student from Barranquilla, Colombia, working on a research project focused on **computer vision for traffic monitoring**.

The project idea:

* Use **IP cameras + AI (YOLOv8/DeepSORT)** to analyze traffic at a highly congested **intersection and street corridor** near campus.
* Goals:
   * Detect and count vehicles/people in real-time.
   * Measure congestion, waiting times, and peak hours.
   * Explore scalability for **multi-camera traffic analysis**.

Iâ€™m currently looking for:

* **Open datasets** for training/testing traffic detection models.
* **Research papers or case studies** on AI applied to traffic monitoring and smart intersections.
* **Practical experiences or tips** from anyone who has worked on multi-camera or real-time video analysis for urban mobility.

Any resources, datasets, or personal experiences would be super helpful ðŸ™Œ.

Thanks in advance!",computervision,2,https://www.reddit.com/r/computervision/comments/1n66c7m/looking_for_open_datasets_and_resources_for/,r_1n66c7m,,,
r_1n62kel,reddit,FMCryptoMX,2025-09-01T22:20:18+00:00,"Vision Camera with AI - KEYENCE VS-L160MX
Hi guys, anyone interested in this Vision Camera ? I dont need it anymore. its new with open box 

https://preview.redd.it/8c3oqa2sommf1.jpg?width=3024&format=pjpg&auto=webp&s=92bf02bc0d676de32ced054509c30d6e8c2afddf

https://preview.redd.it/5mn80dktommf1.jpg?width=4032&format=pjpg&auto=webp&s=0e9030d93e6c09634e453c25cbc335176cd0d972

",computervision,0,https://www.reddit.com/r/computervision/comments/1n62kel/vision_camera_with_ai_keyence_vsl160mx/,r_1n62kel,,,
r_1n5zxsu,reddit,Odd-Community6827,2025-09-01T20:34:23+00:00,"Looking for a solution to automatically group of a lot of photos per day by object similarity
Hi everyone,



I have a lot of photos saved on my PC every day. I need a solution (Python script, AI tool, or cloud service) that can:



1. Identify photos of the same object, even if taken from different angles, lighting, or quality.

2. Automatically group these photos by object.

3. Provide a table or CSV with:

   \- A representative photo of each object

   \- The number of similar photos

   \- An ID for each object



Ideally, it should work on a PC and handle large volumes of images efficiently. 



Does anyone know existing tools, Python scripts, or services that can do this? Iâ€™m on a tight timeline and need something I can set up quickly.

",computervision,1,https://www.reddit.com/r/computervision/comments/1n5zxsu/looking_for_a_solution_to_automatically_group_of/,r_1n5zxsu,,,
r_1n5ypyq,reddit,me081103,2025-09-01T19:46:40+00:00,"Facial Recognition Attendance in a Primary School
https://www.linkedin.com/posts/gabriel-armas_im-happy-to-share-a-computer-vision-project-ugcPost-7368343153055080450-2bIx",computervision,25,https://www.reddit.com/r/computervision/comments/1n5ypyq/facial_recognition_attendance_in_a_primary_school/,r_1n5ypyq,,,
r_1n5x9qy,reddit,yungyany,2025-09-01T18:52:13+00:00,"Using ORB-SLAM3 for GPS-Free Waypoint Missions
I'm working on an autonomous UAV project. My goal is to conduct an outdoor waypoint mission using SLAM (ORB-SLAM3 as this is the current standard) with Misson Planner or QGroundControl for route planning.

The goal would be to plan a route and have the drone perform the mission, partially or fully slam pose estimation instead of GPS. As I understand ORB-SLAM3 outputs pose estimations in the camera's coordinate frame. I need to figure out how to translate that into the flight controllerâ€™s coordinate system so it can update its position and follow the mission. The questions I have are:

* How can I convert ORB-SLAM3's camera-based pose into a format usable by Ardupilot for real-time position updates?
* Whatâ€™s the best way to feed this data into the flight controllerâ€”via MAVLink, EKF input, or some custom middleware?",computervision,2,https://www.reddit.com/r/computervision/comments/1n5x9qy/using_orbslam3_for_gpsfree_waypoint_missions/,r_1n5x9qy,,,
r_1n5w5e7,reddit,Nothing769,2025-09-01T18:11:02+00:00,"Where can I find papers with public datasets?
Hey folks i  am sorry I am kinda new to this searching stuff.
I am trying to solve some really specific problems. Like is there a site where papers which have open sourced their datasets post their papers on ? .
The problem I'm trying to work on is kinda specific. So regular public datasets won't work. I need the paper authors to publicize there dataset so that I can tinker with it a bit . I'm sorry I'm new to this. ",computervision,4,https://www.reddit.com/r/computervision/comments/1n5w5e7/where_can_i_find_papers_with_public_datasets/,r_1n5w5e7,,,
r_1n5ucf9,reddit,nlgranger,2025-09-01T17:04:51+00:00,"Tri3D: Unified interface for 3D driving datasets (Waymo, Nuscenes, etc.)
[Tri3D ](https://preview.redd.it/xqcs7nz6zkmf1.png?width=3634&format=png&auto=webp&s=bc3309fe43782ee35dfe78eff2289b7589757455)

I've been working on a library to unify multiple outdoor 3D datasets for driving. I think it addresses many issues we have currently in the field:

* Ensuring common coordinate conventions and a common api.
* Making it fast and easy to access any sample at any timestamp.
* Simplifying the manipulation of geometric transformations (changing coordinate systems, interpolating poses).
* Provide various helpers for plotting.

One opinionated choice is that I don't put forth the notion of keyframe, because it is ill-defined unless all sensors are perfectly synchronized. Instead I made it very easy to interpolate and apply pose transformations. There is a [function](https://cea-list.github.io/tri3d/generated/tri3d.datasets.Waymo.html#tri3d.datasets.Waymo.alignment) that returns the transformation to go from the coordinates of a sensor at a frame to any other sensor and frame.

Right now, the library supports:

* [NuScenes](https://www.nuscenes.org/nuscenes)
* [ONCE Dataset](https://once-for-auto-driving.github.io)
* [Semantic KITTI](https://semantic-kitti.org)
* [Waymo open dataset](https://waymo.com/open)
* [Zenseact Open Dataset](https://zod.zenseact.com/frames)

The code is hosted here: [https://github.com/CEA-LIST/tri3d](https://github.com/CEA-LIST/tri3d)

The documentation is there: [https://cea-list.github.io/tri3d/](https://cea-list.github.io/tri3d/)

And for cool 3D plots check out the tutorial: [https://cea-list.github.io/tri3d/example.html](https://cea-list.github.io/tri3d/example.html) (the plots use the awesome [k3d](https://k3d-jupyter.org/) library which I highly recommend).",computervision,2,https://www.reddit.com/r/computervision/comments/1n5ucf9/tri3d_unified_interface_for_3d_driving_datasets/,r_1n5ucf9,,,
r_1n5rci2,reddit,Even-Tour-4580,2025-09-01T15:12:51+00:00,"Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones
[Heedless Backbone](https://heedlessbackbones.com/)

https://preview.redd.it/5876vwlykkmf1.png?width=3126&format=png&auto=webp&s=5c056c527e059e979233d58f1994c3f2cfb548b7

  


This is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.

I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).

I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)",computervision,40,https://www.reddit.com/r/computervision/comments/1n5rci2/computer_vision_backbone_model_paperswithcode/,r_1n5rci2,,,
r_1n5qv1a,reddit,Every-Computer170,2025-09-01T14:54:31+00:00,"Dino v3 Implementation
Can anyone guide how can i do instance segmentation using dino v3",computervision,12,https://www.reddit.com/r/computervision/comments/1n5qv1a/dino_v3_implementation/,r_1n5qv1a,,,
r_1n5qbp7,reddit,Mysterious-Emu3237,2025-09-01T14:33:31+00:00,"Feedback needed for managing Multi Camera Video data and datasets
I have been working in field of Multi-Camera (mostly static cameras) problems including Object Detection, Poses, MOT, etc. for last few years. I have during this time period realized that a lot of time gets spent into issues that can be better solved using tools built with a focus on multi-camera video datasets. For example, below are just some problems that are inherent to MCMT:

* Camera Synchronization: - Certain problems such as crowd flow/animal counting/etc. requires time synchronized videos and labels. Hence data ingestion should incorporate time of capture/presentation into the pipeline.
* Easy visualization of multiple cameras: One of biggest pain point has been getting quick synchronized visualizations of multiple camera's
   * raw footage
   * labelled datasets
   * predictions.
* Camera Positions: Visualizing multiple cameras is always limited due to screen size, hence being able to quickly visualize all cameras in a specific area is much better.

While a lot of these problems are already solved via tools such as video management software (Milestone) and there are single image/video data management and annotation tools (e.g. CVAT, fiftyone), I have yet to find a smooth integration into a dataset management system designed for building high quality datasets, with efficient autolabelling, model training, evaluation, both quantitative and qualitative.

Hence, I am thinking of building a product (open-source) that handles the multi-camera usecase better. My main doubts are:

1. If you have worked with multi-camera datasets, what has been the usecase and your pain points?
2. Are there tools youâ€™ve found that actually make this workflow easier?",computervision,5,https://www.reddit.com/r/computervision/comments/1n5qbp7/feedback_needed_for_managing_multi_camera_video/,r_1n5qbp7,,,
r_1n5nh56,reddit,Lautaro0210,2025-09-01T12:34:04+00:00,"Doubt on Single-Class detection
Hey guys, hope you're doing well. I am currently researching on detecting bacteria on digital microscope images, and I am particularly centered on detecting E. coli. There are many ""types"" (strains) of this bacteria and currently I have 5 different strains on my image dataset . Thing is that I want to create 5 independent YOLO models (v11). Up to here all smooth but I am having problems when it comes understanding the results. Particularly when it comes to the confusion matrix. Could you help me understand what the confusion matrix is telling me? What is the basis for the accuracy?

BACKGROUND: I have done many multiclass YOLO models before but not single class so I am a bit lost.

DATASET: 5 different folders with their corresponding subfolders (train, test, valid) and their corresponding .yaml file. Each train image has an already labeled bacteria cell and this cell can be in an image with another non of interest cells or debris.

https://preview.redd.it/0ue7t0oasjmf1.png?width=3000&format=png&auto=webp&s=7fdd90646b45e8c5baf0621eda8f4d033a8348af

",computervision,3,https://www.reddit.com/r/computervision/comments/1n5nh56/doubt_on_singleclass_detection/,r_1n5nh56,,,
r_1n5n1f5,reddit,ARC_4N3,2025-09-01T12:12:49+00:00,"Commercially available open source embedding models for face recognition
Looking for a model that can beat Facenet512 in terms of embedding quality.  
It has fair results, but I'm looking for a more accurate model.   
Currently I'm facing the issue of the model not being able to deal with distinguishing faces with highly varying scores. Especially in slightly low quality scenarios, and even at times, with clear pictures.  
I have observed that Facenet can be very sensitive to the angles of faces, matching a query with same angled faces (If that makes sense) or lighting. I'd say the same for insightface models (Even though I cant use them)   
Arcface based open source models such as: AuraFace, AdaFace, MagFace were not able to yield better results than Facenet.   
One requirement for me is that the model should be open source.   
I have tested more models for the same, but FaceNet still comes out on top.   
Is there a better open source model out there than FaceNet that is commercially available?",computervision,3,https://www.reddit.com/r/computervision/comments/1n5n1f5/commercially_available_open_source_embedding/,r_1n5n1f5,,,
r_1n5mqlf,reddit,BreathtakingCharsi,2025-09-01T11:58:12+00:00,"Need help running Vision models (object detection) on mobile
I want to run fine tuned object detection vision models in real time locally on mobile phones but I cant find a lot of learning resources on how to do so. I managed to run simple image classification models but not object detection models (YOLO, RT-DETR). ",computervision,2,https://www.reddit.com/r/computervision/comments/1n5mqlf/need_help_running_vision_models_object_detection/,r_1n5mqlf,,,
r_1n5lbw8,reddit,Long-Ice-9621,2025-09-01T10:40:06+00:00,Big head qwen image,computervision,0,https://www.reddit.com/r/computervision/comments/1n5lbw8/big_head_qwen_image/,r_1n5lbw8,,,
r_1n5kunq,reddit,mageblood123,2025-09-01T10:11:13+00:00,"Is it possible to complete this project with budget equipment?
Hey, I'm not entirely sure if this is the right subreddit for this type of question. 

I am doing an internship at a university and I have been asked to do a project (no one else there deals with this or related issues). As I have never done or participated in anything like this before, I would like to do it as economically as possible, and if my boss likes it, I may increase the budget (I don't have a fixed budget).

The project involves detecting on the production line whether the date is stamped on a METAL can and whether there is a label. My question is not about the technology used, but about the equipment. The label is around the entire circumference of the can, so I assume that one camera at a good angle will suffice.

My idea is to use:

\- Raspberry Pi (4/5)

\- Raspberry camera module

\- sensor (which will detect the movement of the can on the production line)

\- LED ring above (or below) the camera- since it is a metal can, light probably plays an important role here



Will this work if the cans move at a rate of 2 cans/second? 

Is there anything I am overlooking that will cause a major problem?



Thank you in advance for any help.

",computervision,2,https://www.reddit.com/r/computervision/comments/1n5kunq/is_it_possible_to_complete_this_project_with/,r_1n5kunq,,,
r_1n5jtd6,reddit,Georgehwp,2025-09-01T09:06:43+00:00,"Do single stage models require larger batch sizes than 2-stage
I think I've observed over a lot of different training runs of different architectures that 2 stage (mask rcnn derivative) models can train well with very small batch sizes, like 2-4 images at a time, while YOLO esk models often require much larger batch sizes to train at all. 

I can't find any generalised research saying this, or any comments in the blogs, I've also not yet done any thorough checks of my own. Just feels like something I've noticed over a few years. 

Anyone agree/disagree or have any references.",computervision,1,https://www.reddit.com/r/computervision/comments/1n5jtd6/do_single_stage_models_require_larger_batch_sizes/,r_1n5jtd6,,,
r_1n5j51f,reddit,Naneet_Aleart_Ok,2025-09-01T08:22:34+00:00,"How to improve a model
So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.

Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. 

Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. ",computervision,8,https://www.reddit.com/r/computervision/comments/1n5j51f/how_to_improve_a_model/,r_1n5j51f,,,
r_1n5e4bo,reddit,sn_reddit_poster,2025-09-01T03:29:00+00:00,"Looking for entry-level positions
Shooting my shot!

Anyone looking to hire a new MS grad in the US? I have experience with classical CV (feature matching, boundary detection, Hough Transform, etc.) and deep CV (object detection + tracking, segmentation, etc.). Skilled in Python and C++. No issues with sponsorship.

Market's been tough, so I can use all the help/advice I can get.",computervision,0,https://www.reddit.com/r/computervision/comments/1n5e4bo/looking_for_entrylevel_positions/,r_1n5e4bo,,,
r_1n5d8fl,reddit,Less_Measurement8733,2025-09-01T02:42:36+00:00,"Trouble finding where to learn what i need to make my project.
Hi, I feel a bit lost. I already built a program using TensorFlow with a convolutional model to detect and classify images into categories. For example, my previous model could identify that the cat in the picture is an orange adult cat.

But now I need something more: I want a model that can detect things I can only know if the cat is **moving**,like i want to know if the cat did a backflip.

For example, Iâ€™d like to know **where the cat moves within a relative space** and also its **speed**.

What kind of models should I look into for this? Iâ€™ve been researching a bit and models like **ST-GCN (Graph Neural Network)** and **TimeSformer / ViViT** come up often. More importantly, how can I learn to build them? Is there any specific book, tutorial, or resource youâ€™d recommend?

Iâ€™m asking because I feel very lost on where to start. Iâ€™m also reading *Why Machines Learn* to help me understand machine learning basics, and of course going through the documentation.

",computervision,7,https://www.reddit.com/r/computervision/comments/1n5d8fl/trouble_finding_where_to_learn_what_i_need_to/,r_1n5d8fl,,,
r_1n563nj,reddit,Mammoth-Photo7135,2025-08-31T21:05:47+00:00,"M4 Mac Mini for real time inference
Nvidia Jetson nanos are 4X costlier than they are in the United States so I was thinking of dealing with some edge deployments using a M4 mini mac which is 50% cheaper with double the VRAM and all the plug and play benefits, though lacking the NVIDIA accelerator ecosystem.

I use a M1 Air for development (with heavier work happening in cloud notebooks) and can run RFDETR Small at 8fps atits native resolution of 512x512 on my laptop. This was fairly unoptimized 

I was wondering if anyone has had the chance of running it or any other YOLO or Detection Transformer model on an M4 Mini Mac and experienced a better performance -- 40-50fps would be totally worth it overall. 

Also, my current setup just included calling the model.predict function, what is the way ahead for optimized MPS deployments? Do I convert my model to mlx? Will that give me a performance boost? A lazy question I admit, but I will be reporting the outcomes in comments later when I try it out after affirmations.

Thank you for your attention.",computervision,10,https://www.reddit.com/r/computervision/comments/1n563nj/m4_mac_mini_for_real_time_inference/,r_1n563nj,,,
r_1n50mzl,reddit,Dismal-Comb-410,2025-08-31T17:28:04+00:00,"Help Can AI count pencils?
Ok so my Dad thinks I am the family helpdesk... but recently he has extended my duties to AI ðŸ¤£ -- he made an artwork, with pencils (a forest of pencils with about 6k pencils) --- so he asked:  ""can you ask AI to count the pencils?.."" -- so I asked Gpt5 for python code to count the image below and it came up with a pretty good opencv code  (hough circles) that only misses about 3% of the pencils... and wondering if there is a better more accurate way to count in this case... 

any better aprox welcome!

[can ai count this?](https://preview.redd.it/2po8zu063emf1.png?width=1053&format=png&auto=webp&s=d007b904f5b289d175ecea494559ece681532704)

  
Count: 6201

https://preview.redd.it/jloazzxa3emf1.png?width=1053&format=png&auto=webp&s=9d06e7fcc5744242ced87951e8d5d46a42050ee7



  
",computervision,17,https://www.reddit.com/r/computervision/comments/1n50mzl/help_can_ai_count_pencils/,r_1n50mzl,,,
r_1n4sn1p,reddit,DaaniDev,2025-08-31T11:48:21+00:00,"ðŸš€ Real-Time License Plate Detection + OCR Android App (YOLOv11n)
Hey everyone,

ðŸ“Œ Iâ€™ve recently developed an **Android app** that integrates a **custom-trained License Plate Detection model (YOLOv11n)** with **OCR** to automatically extract plate text in real time.

Key features:

* ðŸš˜ Detects vehicle license plates instantly.
* ðŸ” Extracts plate text using OCR.
* ðŸ“± Runs directly on Android (optimized for real-time performance).
* âš¡ Use cases: Traffic monitoring, parking management, and smart security systems.

The combination of YOLOv11n (lightweight + fast) and OCR makes it efficient even on mobile devices.

You can subscribe to my channel where I will guide you step by step how to train your custom model + integration in Android application:  
  
YouTube Channel Link : [https://www.youtube.com/@daanidev](https://www.youtube.com/@daanidev)  
",computervision,20,https://www.reddit.com/r/computervision/comments/1n4sn1p/realtime_license_plate_detection_ocr_android_app/,r_1n4sn1p,,,
r_1n4q63a,reddit,Guilty_Question_6914,2025-08-31T09:15:26+00:00,"Raspberry Pi Picamera2  opencv Gpio control example with python
I made a clip on how i program the Raspberry Pi to blink leds by detecting certain colors. at the moment only yellow,red,blue are used but i gonna link a other repo were you can test 3 more colors if needed.If this helpful subcribe to my channel.that is all",computervision,4,https://www.reddit.com/r/computervision/comments/1n4q63a/raspberry_pi_picamera2_opencv_gpio_control/,r_1n4q63a,,,
r_1n4esdm,reddit,MaxSpiro,2025-08-30T22:43:53+00:00,"UW Bothell masters program?
Iâ€™m applying to masters programs intending to study machine learning and computer vision and I saw the curriculum breakdown was more like 50% fundamentals and 50% electives (what I want to study). Is this normal for graduate programs? It feels like that was the point of the undergraduate education. ",computervision,2,https://www.reddit.com/r/computervision/comments/1n4esdm/uw_bothell_masters_program/,r_1n4esdm,,,
r_1n44681,reddit,MathPhysicsEngineer,2025-08-30T15:16:01+00:00,"Spherical coordinates with forward/inverse maps (interactive Desmos; full tutorial linked inside)
This interactive demonstratesÂ **spherical parameterization**Â as a mapping problem relevant toÂ **computer science**Â andÂ **graphics**: theÂ **forward map**Â (r,Î¸,Ï†)â€‰â£â†’(x,y,z).  
(r,Î¸,Ï†)â†’(x,y,z) (analogous to UV-to-surface) and theÂ **inverse**Â (x,y,z)â€‰â£â†’(r,Î¸,Ï†)  
(useful for texture lookup, sampling, or converting data to lat-long grids). You can generate reproducible figures for papers/slides without writing code, and experiment with coordinate choices and pole behavior. For the math and the construction pipeline, open theÂ **video from the link inside the Desmos page**Â andÂ **watch it start to finish**; it builds the mapping step by step and ends with a quick guide to rebuilding the image in Desmos. This is free and meant to help a wide audienceâ€”if itâ€™s useful, please share with your class or lab.  
Desmos link:Â [https://www.desmos.com/3d/og7qio7wgz](https://www.desmos.com/3d/og7qio7wgz)  
For a perfect user experience with the Desmos link, it is recommended to watch this video, which, at the end, provides a walkthrough on how to use the Desmos link. Don't skip the beginning, as the Desmos environment is a clone of everything in the beginning:

[https://www.youtube.com/watch?v=XGb174P2AbQ&ab\_channel=MathPhysicsEngineering](https://www.youtube.com/watch?v=XGb174P2AbQ&ab_channel=MathPhysicsEngineering)

Also can be useful for generating images for tex document and research papers, also can be used to visualize solid angle for radiance and irradiance theory.",computervision,5,https://www.reddit.com/r/computervision/comments/1n44681/spherical_coordinates_with_forwardinverse_maps/,r_1n44681,,,
r_1n42fva,reddit,No-Bee6364,2025-08-30T14:04:27+00:00,"â€œDetecting handicapped parking spots fromStreet View or satellite imagery
Hi all- Looking for ways to map accessible/handicapped parking spots using Google Street View, satellite imagery in my city. 

Any datasets, models, or open-source tools that already do this? 

",computervision,4,https://www.reddit.com/r/computervision/comments/1n42fva/detecting_handicapped_parking_spots_fromstreet/,r_1n42fva,,,
r_1n41d2k,reddit,SeucheAchat9115,2025-08-30T13:15:09+00:00,"3D Framework
Hi,

since mmdetection and else are not actively maintained anymore. Whats the outlook for 3d detection? Why dont we have some in huggingface transformers?",computervision,4,https://www.reddit.com/r/computervision/comments/1n41d2k/3d_framework/,r_1n41d2k,,,
r_1n3xh4d,reddit,PolarIceBear_,2025-08-30T09:38:37+00:00,"OCR Arabic Documents Quality Assessment Method
Iâ€™m working on an OCR project for Arabic documents. The documents vary a lot in shape and quality, and Iâ€™m using a fine-tuned custom version of PaddleOCR. The main issue is that when the input documents are low quality, the OCR tends to hallucinate and produce unusable text for the user.

My idea was to add an Image Quality Assessment (IQA) step so I can filter out bad inputs before they reach the OCR model, rather than returning garbage results.

Iâ€™ve experimented with common no-reference IQA methods like PIQE, NIQE, BRISQUE, and DIQA, but the results arenâ€™t great. They often assign poor scores to documents that are actually readable and OCR-friendly.

Has anyone dealt with this problem before? What approaches or models would you recommend for document-specific quality assessment? Ideally, Iâ€™d like a way to reject only the truly unreadable inputs while still letting through â€œimperfect but OCR-ableâ€ ones.",computervision,1,https://www.reddit.com/r/computervision/comments/1n3xh4d/ocr_arabic_documents_quality_assessment_method/,r_1n3xh4d,,,
r_1n3wvk8,reddit,Feitgemel,2025-08-30T08:58:45+00:00,"How to classify 525 Bird Species using Inception V3 [project]
https://preview.redd.it/9fneq7y2g4mf1.png?width=1280&format=png&auto=webp&s=297b26348ebd7128220d354a8081d40f0dcadf8f

In this guide you will build a full image classification pipeline using Inception V3.

You will prepare directories, preview sample images, construct data generators, and assemble a transfer learning model.

You will compile, train, evaluate, and visualize results for a multi-class bird species dataset.

Â 

You can find link for the post , with the code in the blogÂ  : [https://eranfeit.net/how-to-classify-525-bird-species-using-inception-v3-and-tensorflow/](https://eranfeit.net/how-to-classify-525-bird-species-using-inception-v3-and-tensorflow/)

Â 

You can find more tutorials, and join my newsletter here: [https://eranfeit.net/](https://eranfeit.net/)

Â 

**Watch the full tutorial here :** [**https://www.youtube.com/watch?v=d\_JB9GA2U\_c**](https://www.youtube.com/watch?v=d_JB9GA2U_c)

Â 

Â 

Enjoy

Eran

Â 

\#Python #ImageClassification #tensorflow #InceptionV3",computervision,4,https://www.reddit.com/r/computervision/comments/1n3wvk8/how_to_classify_525_bird_species_using_inception/,r_1n3wvk8,,,
r_1n3w10h,reddit,Bitter-Pride-157,2025-08-30T08:01:24+00:00,"VGG v GoogleNet: Just how deep can they go?
Hi Guys,

I recently read the original GoogleNet and VGG papers and implemented both models from scratch in PyTorch.

I wrote a [blog post](https://mayberay.bearblog.dev/vgg-v-googlenet-just-how-deep-can-they-go/) about it, walking through the implementation. Please review it and share your feedback.",computervision,6,https://www.reddit.com/r/computervision/comments/1n3w10h/vgg_v_googlenet_just_how_deep_can_they_go/,r_1n3w10h,,,
r_1n3tgp7,reddit,namas191297,2025-08-30T05:22:50+00:00,"[Open Source] [Pose Estimation] RTMO pose estimation with pure ONNX Runtime - pip + CLI (webcam/image/video) in minutes
Most folks I know (me included) just want to try lightweight pose models quickly without pulling a full training stack. I made a tiny wrapper that runs **RTMO** with **ONNX Runtime only**, so you can demo it in minutes.

Repo: [https://github.com/namas191297/rtmo-ort](https://github.com/namas191297/rtmo-ort?utm_source=chatgpt.com)

PyPI: [https://pypi.org/project/rtmo-ort/](https://pypi.org/project/rtmo-ort/?utm_source=chatgpt.com)

This trims it down to a small pip package + simple CLIs, with a script that grabs the ONNX files for you.  
Once you install the package and download the models, running any RTMO model is as simple as:

    rtmo-webcam --model-type small --dataset coco --device cpu
    rtmo-image --model-type small --dataset coco --input assets/demo.jpg --output out.jpg
    rtmo-video --model-type medium --dataset coco --input input.mp4 --output out.mp4

This is just for quick demos, PoCs, or handing a working pose script to someone without the full stack, or even trying to build TensorRT engines for these ONNX models. 

Notes:

* CPU by default; for GPU, install `onnxruntime-gpu` and pass `--device cuda`.
* Useful flags: `--no-letterbox`, `--score-thr`, `--kpt-thr`, `--max-det`, `--size`.

",computervision,5,https://www.reddit.com/r/computervision/comments/1n3tgp7/open_source_pose_estimation_rtmo_pose_estimation/,r_1n3tgp7,,,
r_1n3q63l,reddit,Norqj,2025-08-30T02:22:07+00:00,"New Video Processing Functions in Pixeltable: clip(), extract_frame, segment_video, concat_videos, overlay_text + VideoSplitter iterator...
Hey folks -

We just shipped a set of video processing functions in Pixeltable that make video manipulation quite simple for ML/AI workloads. No more wrestling with ffmpeg or OpenCV boilerplate!

**What's new**

**Core Functions:**

* `clip()` \- Extract video segments by time range
* `extract_frame()` \- Grab frames at specific timestamps
* `segment_video()` \- Split videos into chunks for batch processing
* `concat_videos()` \- Merge multiple video segments
* `overlay_text()` \- Add captions, labels, or annotations with full styling control

**VideoSplitter Iterator:**

* Create views of time-stamped segments with configurable overlap
* Perfect for sliding window analysis or chunked processing

**Why this is cool!?:**

* All operations are **computed columns** \- automatic versioning and caching
* **Incremental processing** \- only recompute what changes
* **Integration** with AI models (YOLOX, OpenAI Vision, etc.), but please bring your own UDFs
* Works with local files, URLs, or S3 paths

**Object Detection Example:** We have a working example combining some other functions with YOLOX for object detection: [GitHub Notebook](https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/object-detection-in-videos.ipynb)

**We'd love your feedback!**

* What video operations are you missing?
* Any specific use cases we should support?",computervision,12,https://www.reddit.com/r/computervision/comments/1n3q63l/new_video_processing_functions_in_pixeltable_clip/,r_1n3q63l,,,
r_1n3pvi1,reddit,UnderstandingOwn2913,2025-08-30T02:07:03+00:00,"which platform do you guys use to get a computer vision engineer job?
I feel like there is not much computer vision engineer jobs on Linkedin...",computervision,17,https://www.reddit.com/r/computervision/comments/1n3pvi1/which_platform_do_you_guys_use_to_get_a_computer/,r_1n3pvi1,,,
r_1n3bitt,reddit,Sad-Bluejay8380,2025-08-29T16:06:29+00:00,"I need a help
Hello everybody, I'm new here at this sub, I'm Junior student at computer science and I have been accepted in a scholarship for machine learning. I have a graduation project to graduate, our project is about Real-Time Object Detection for Autonomous Vehicles, our group are from 4 and we have 3 months to finish it.

so what we need to study in CV to finish the project I know it's a complicated track and unfortunately we don't have time we need to start from now

Note: me and my friends are new in ai we just started machine learning for 2 months",computervision,0,https://www.reddit.com/r/computervision/comments/1n3bitt/i_need_a_help/,r_1n3bitt,,,
r_1n3ari2,reddit,No-Roof-170,2025-08-29T15:37:49+00:00,"why manga-ocr-base is much faster than PP-OCRv5_mobile   despite being much larger ?
Hi, 

I ran both [https://huggingface.co/kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) and PP-OCRv5\_mobile  on my i5-8265U and was surprised to find out paddlerocr is much slower for inferance despite being tiny, i  only used text detection and text recoginition module for paddlerocr.  
  
I would appreciate if someone can explain the reason behind it. ",computervision,8,https://www.reddit.com/r/computervision/comments/1n3ari2/why_mangaocrbase_is_much_faster_than_ppocrv5/,r_1n3ari2,,,
r_1n38g6n,reddit,Queasy-Piccolo-7471,2025-08-29T14:08:44+00:00,"6D pose estimation of a Non-planar object  having the rgb images and stl model of the object
I am trying to estimate the 6D pose of the object in the image , Here my approach is to extract the 2d keypoint features in the image and 3d keypoint features in the stl model of the object , but stuck at how to find the corresponding pairs of 3d to 2d key points.

 if i have the 3d to 2d keypoint pairs , then i could apply PnP algorithm to estimate the 6 pose of the object. 

Please direct me to any resources or any existing work based on which i could estimate the pose",computervision,3,https://www.reddit.com/r/computervision/comments/1n38g6n/6d_pose_estimation_of_a_nonplanar_object_having/,r_1n38g6n,,,
r_1n38axq,reddit,Low-Principle9222,2025-08-29T14:02:50+00:00,"live object detection using DJI drone and Nginx server
Hi! Weâ€™re currently working on a tree counting project using a DJI drone with live object detection (YOLO). Aside from the camera, do you have any tips or advice on what additional hardware we can mount on the drone to improve functionality or performance? Would love to hear your suggestions!",computervision,2,https://www.reddit.com/r/computervision/comments/1n38axq/live_object_detection_using_dji_drone_and_nginx/,r_1n38axq,,,
r_1n37cgq,reddit,InternationalMany6,2025-08-29T13:23:30+00:00,"How much global context do DINO patch embeddings contain?
Donâ€™t really have a more specific question. Iâ€™m looking for any kind of knowledge or study about this. ",computervision,9,https://www.reddit.com/r/computervision/comments/1n37cgq/how_much_global_context_do_dino_patch_embeddings/,r_1n37cgq,,,
r_1n3444b,reddit,Puzzleheaded_Quote96,2025-08-29T10:49:38+00:00,"Having trouble with top-down size measurements using stereo cameras in Python
Hey everyone,

Iâ€™m working on a project where I want to measure object sizes using two top-down cameras. Technically it should be possible, and I already have the disparity, the focal length, and the baseline (distance between the cameras). The cameras are stereo calibrated.

Iâ€™m currently using the standard depth formula:

Z = (f \* B) / disparity

Where:

* `Z` = depth
* `f` = focal length
* `B` = baseline (distance between cameras)
* `disparity` = difference in pixel positions between left/right image

The issue: my depth map looks really strange â€“ the colors donâ€™t really change as expected, almost like itâ€™s flat, and the measurements I get are inconsistent or unrealistic.

Has anyone here done something similar or could point me to where I might be going wrong?

",computervision,1,https://www.reddit.com/r/computervision/comments/1n3444b/having_trouble_with_topdown_size_measurements/,r_1n3444b,,,
r_1n32621,reddit,GiovanniPontano,2025-08-29T08:52:01+00:00,"OAK D Lite help
Hello everyone, I started a project about 3D plane estimation and since I am new to this field I could use some help and advice from more experienced engineers. Dm me if you worked with Oak D lite and StereoDepth node.

Thank you in advance!",computervision,2,https://www.reddit.com/r/computervision/comments/1n32621/oak_d_lite_help/,r_1n32621,,,
r_1n30ohg,reddit,Buggera,2025-08-29T07:14:06+00:00,"Best practices for managing industrial vision inspection datasets at scale?
Our plant generates about 50GB of inspection images daily across multiple production lines. Currently using a mix of on-premises storage and cloud backup, but struggling with data organization, annotation workflows, and version control. How are others handling large-scale vision data management? Looking for insights on storage architecture, annotation toolchains, and quality control workflows.",computervision,7,https://www.reddit.com/r/computervision/comments/1n30ohg/best_practices_for_managing_industrial_vision/,r_1n30ohg,,,
r_1n2vnbh,reddit,satoorilabs,2025-08-29T02:31:39+00:00,"How to create a tactical view like this without 4 keypoints?
Assuming the white is a perfect square and the rings are circles with standard dimensions, what's the most straightforward way to map this archery target to a top-down view? There aren't really many distinct keypoint-able features besides the corners (creases don't count, not all the images have those), but usually only 1 or 2 are visible in the images, so I can't do standard homography. Should I focus on the edges or something else? I'm trying to figure out a lightweight solution to this. sorry in advance if this is a rookie question. ",computervision,97,https://www.reddit.com/r/computervision/comments/1n2vnbh/how_to_create_a_tactical_view_like_this_without_4/,r_1n2vnbh,,,
r_1n2t5if,reddit,sovit-123,2025-08-29T00:34:58+00:00,"JEPA Series Part-3: Image Classification using I-JEPA
JEPA Series Part-3: Image Classification using I-JEPA

[https://debuggercafe.com/jepa-series-part-3-image-classification-using-i-jepa/](https://debuggercafe.com/jepa-series-part-3-image-classification-using-i-jepa/)

In this article, we will use theÂ **I-JEPA model for image classification**. Using a pretrained I-JEPA model, we will fine-tune it for a downstream image classification task.

https://preview.redd.it/4361ezm3tulf1.png?width=768&format=png&auto=webp&s=2dadd161f73f63c3803b8b779d4fb5b9b6064ade

",computervision,3,https://www.reddit.com/r/computervision/comments/1n2t5if/jepa_series_part3_image_classification_using_ijepa/,r_1n2t5if,,,
r_1n2o7t7,reddit,Jooe891,2025-08-28T21:03:33+00:00,"Is my ECS + SQS + Lambda + Flask-SocketIO architecture right for GPU video processing at scale?
Hey everyone!

Iâ€™m a CV engineer at a startup and also responsible for building the backend. Iâ€™m new to AWS and backend infra, so Iâ€™d appreciate feedback on my plan.

**My requirements:**

* Process GPU-intensive video jobs in ECS containers (ECR images)
* Autoscale ECS GPU tasks based on demand (SQS queue length)
* Users get real-time feedback/results via Flask-SocketIO (job ID = socket room)
* Want to avoid running expensive GPU instances 24/7 if idle

**My plan:**

1. Users upload video job (triggers Lambda â†’ SQS)
2. ECS GPU Service scales up/down based on SQS queue length
3. Each ECS task processes a video, then emits the result to the backend, which notifies the user via Flask-SocketIO (using job ID)

**Questions:**

* Do you think this pattern makes sense?
* Is there a better way to scale GPU workloads on ECS?
* Do you have any tips for efficiently emitting results back to users in real time?
* Gotchas I should watch out for with SQS/ECS scaling?",computervision,6,https://www.reddit.com/r/computervision/comments/1n2o7t7/is_my_ecs_sqs_lambda_flasksocketio_architecture/,r_1n2o7t7,,,
r_1n2ni5t,reddit,Rukelele_Dixit21,2025-08-28T20:35:47+00:00,"Prompt Based Object Detection
How does Prompt Based Object Detection Work?

I came across 2 things -

1. YoloE by Ultralytics  - (Got resources for these in comments)
2. Agentic Object Detection by LandingAI (https://youtu.be/dHc6tDcE8wk?si=E9I-pbcqeF3u8v8\_)

Any idea how these work? Especially YoloE  
Any research paper or Article Explaining this?

Edit - Any idea how Agentic Object Detection works ? Any in depth explanation for this ?",computervision,4,https://www.reddit.com/r/computervision/comments/1n2ni5t/prompt_based_object_detection/,r_1n2ni5t,,,
r_1n2mmz5,reddit,Easy_Ad_7888,2025-08-28T20:02:06+00:00,"Trackers Open-Source
The problem? Simple: tracking people in a queue at a business.

The tools Iâ€™ve tried? Too many to countâ€¦ SORT, DeepSORT (with several different REIDs â€” I even fine-tuned FASTREID, but the results were still poor), Norfair, BoT-SORT, ByteTrack, and many others. Every single one had the same major issue: ID switching for the same person. Some performed slightly better than others, but none were actually usable for real-world projects.

My dream? That someone would honestly tell me what Iâ€™m doing wrong. Itâ€™s insane that I see all these beautiful tracking demos on LinkedIn and YouTube, yet everything I try ends in frustration! I donâ€™t believe everything online, but I truly believe this is something achievable with open-source tools.

I know camera resolution, positioning, lighting, FPS, and other factors matterâ€¦ and Iâ€™ve already optimized everything I can.

Iâ€™ve started looking into test-time adaptation (TTA), UMAâ€¦ but itâ€™s mostly in papers and really old repositories that make me nervous to even try, because I know the version conflicts will just lead to more frustration.

Is there anyone out there willing to lend me a hand with something that actually works? Or someone who will just tell me: give upâ€¦ itâ€™s probably for the best!",computervision,7,https://www.reddit.com/r/computervision/comments/1n2mmz5/trackers_opensource/,r_1n2mmz5,,,
r_1n2kzcc,reddit,Ok_Shoulder_83,2025-08-28T18:58:41+00:00,"Synthetic data for domain adaptation with Unity Perception â€” worth it for YOLO fine-tuning?
Hello everyone, 

Iâ€™m exploring domain adaptation. The idea is:

* Train a YOLO detector on random, mixed images from many domains.
* Then fine-tune on a *coherent* dataset that all comes from the same simulated â€œsiteâ€ (generated in Unity using Perception).
* Compare performance *before vs. after fine-tuning*.

**Training protocol**

* Start from the general YOLO weights.
* Fine-tune with different synth:real ratios (100:0, 70:30, 50:50).
* Lower learning rate, maybe freeze backbone early.
* Evaluate on:
   * (1) General test set (random hold-out) â†’ check generalization.
   * (2) â€œSiteâ€ test set (held-out synthetic from Unity) â†’ check adaptation.

**Some questions for the community:**

1. Has anyone tried this Unity-based domain adaptation loop, did it help, or did it just overfit to synthetic textures?
2. What randomization knobs gave the most transfer gains (lighting, clutter, materials, camera)?
3. Best practice for mixing synthetic with real data, 70:30, curriculum, or few-shot fine-tuning?
4. Any tricks to close the â€œsynthetic-to-real gapâ€ (style transfer, blur, sensor noise, rolling shutter)?
5. Do you recommend another way to create simulation images then unity? (The environment is a factory with workers)

",computervision,0,https://www.reddit.com/r/computervision/comments/1n2kzcc/synthetic_data_for_domain_adaptation_with_unity/,r_1n2kzcc,,,
r_1n2jggh,reddit,Scanon_ai,2025-08-28T18:00:48+00:00,"Issues with Wrapping my CV app
Hi everyone, 

I am fairly new to this sub so I hope im not stepping on any toes by asking for help on this. Me and my team have been working on an AI powered privacy app that uses CV to detect identifiable attributes like faces, license plates, and tattoos in photos and videos and blur them with the users permission. This isnt a new idea, and has been done before so I will spare the in depth details since most of the people in this sub have probably heard of something like this. 

The backend is working, our CLI can reliably blur faces, wipe EXIF data, and handle video. Weâ€™ve got a decent CI/CD pipeline in place (Windows, macOS, Linux) and our packaging is mostly handled with PyInstaller. However, when we try to wrap the app in Github it just wont wrap effectively, and its been giving us these issues:

1. We have a PySide6/Tkinter scaffold, but itâ€™s not actually wired to the CLI pipeline yet. Users still need to run everything from the command line which is not ideal at all of course.   
  
2. Haar works because itâ€™s bundled, but MediaPipe + some ONNX models (license plate/tattoo detection) donâ€™t ship inside the builds. This leaves users with missing features which is also not ideal.

3. PyInstaller builds are working, but unsigned so macOS and Windows give us the â€œuntrusted developerâ€ warnings.

4. Stripe integration and license unlock is only half-finished, we donâ€™t yet have a clean GUI workflow for buying credits/unlocking features.

So the questions I have for the experts are

1. How can we wire the GUI to an existing CLI pipeline without creating spaghetti code?

2. Are there any best practices for bundling ML dependencies (MediaPipe, ONNXRuntime) so they just work inside the cross-platform builds?

3. How can we handle the code-signing / notarization process across all 3 OSes without drowning in certs/config?

This is my teams first time building something this complex and new, so we are encountering problems we have never run into before, and honestly we are kind of at a point where we are looking for outside help so any advice would be appreciated! If the project sounds interesting to you, feel free to reach out to me as well! We are an early stage startup so we love to interact with anyone who shares our interests .

",computervision,3,https://www.reddit.com/r/computervision/comments/1n2jggh/issues_with_wrapping_my_cv_app/,r_1n2jggh,,,
r_1n2j4q6,reddit,Other-Junket3020,2025-08-28T17:48:42+00:00,Any Data Analytics/Science /AI/ML Opportunities?,computervision,0,https://www.reddit.com/r/computervision/comments/1n2j4q6/any_data_analyticsscience_aiml_opportunities/,r_1n2j4q6,,,
r_1n2i0b3,reddit,Key-Mortgage-1515,2025-08-28T17:07:02+00:00,"Best strategy for mixing trail-camera images with normal images in YOLO training?
Iâ€™m training a YOLO model with a limited dataset of **trail-camera images** (night/IR, low light, motion blur). Because the dataset is small, Iâ€™m considering mixing in **normal images** (internet or open datasets) to increase training data.

ðŸ‘‰ My main questions:

1. Will mixing normal images with trail-camera images actually help improve generalization, or will the **domain gap** (lighting, IR, blur) reduce performance?

https://preview.redd.it/m433opx6lslf1.jpg?width=2976&format=pjpg&auto=webp&s=07aa58a6dfcad9d67ebca59abf8755070d9c0825

https://preview.redd.it/xvoe4rx6lslf1.jpg?width=2976&format=pjpg&auto=webp&s=13fe3673206c0dd214ff3a0e0e3bbb54ec92f3e1

https://preview.redd.it/zw6khqx6lslf1.jpg?width=2976&format=pjpg&auto=webp&s=f3349a0f4ed3ad1e39d2692c875d633f11f18f6c

https://preview.redd.it/2fa9rrx6lslf1.jpg?width=2976&format=pjpg&auto=webp&s=7ee29a4ad995a99aa60600e52ca59e7b192121d5

1. Would it be better to **pretrain on normal images** and then fine-tune only on trail-camera images?
2. What are the **best preprocessing and augmentation techniques** for trail-camera images?
   * Low-light/brightness jitter
   * Motion blur
   * Grayscale / IR simulation
   * Noise injection or histogram equalization
   * Other domain-specific augmentations
3. Does Ultralytics provide recommended augmentation settings or configs for **imbalanced or mixed-domain datasets**?

Iâ€™ve attached some example trail-camera images for reference. Any guidance or best practices from the Ultralytics team/community would be very helpful.",computervision,3,https://www.reddit.com/r/computervision/comments/1n2i0b3/best_strategy_for_mixing_trailcamera_images_with/,r_1n2i0b3,,,
r_1n2h4z2,reddit,The_best_1234,2025-08-28T16:34:22+00:00,"Stereo Vision With Smartphone
It doesn't work great but it does work. I used a Pixel 8 Pro",computervision,108,https://www.reddit.com/r/computervision/comments/1n2h4z2/stereo_vision_with_smartphone/,r_1n2h4z2,,,
r_1n2fyb9,reddit,low_lvl,2025-08-28T15:50:01+00:00,"Mac mini(M4) for computer vision
Due to budgeting, I am not able to build my own PC. I want to buy a Mac mini for computer vision. I have researched about MLX training and I donâ€™t know if this is feasible. Iâ€™m at a postgraduate level would this be a suitable device and is thereâ€™s an ecosystem for training? ",computervision,5,https://www.reddit.com/r/computervision/comments/1n2fyb9/mac_minim4_for_computer_vision/,r_1n2fyb9,,,
r_1n2cvzr,reddit,EmotionalAirport3227,2025-08-28T13:55:24+00:00,"Seeking advice on hardware requirements for multi-stream recognition project
I'm building a research prototype for distraction recognition during video conferences. Input: 2-8 concurrent participant streams at 12-24 FPS with real-time processing with maintaining the same per-stream frame rate at output (maybe 15-30% less).

Planned components:

* MediaPipe (Face Detection + Face Landmark + Iris Landmark) or OpenFace - Face and iris detection and landmarking
* DeepFace - Face identification and facial expressions
* NanoDet or YOLOv11 (s/m/l variants) - potentially distracting object detection



However, I'm facing a problem with choosing hardware. I tried to find out this on the Internet, but my searches havenâ€™t yielded clear, actionable guidance. I guess, I need some of this: 20+ CPU cores, 32+ GB RAM, 24-48 GB VRAM with Ampere tensor cores or higher. 



Is there any information on hardware requirements for real-time work with these?

For this workload, is a single RTX 4090 (24 GB) sufficient, or is a 48 GB card (e.g., RTX 6000 Ada/L40/L4) advisable to keep all streams/models resident?

Is a 16c/32t CPU sufficient for pre/postâ€‘processing, or should I aim for 24c+? RAM: 32 GB vs 64+ GB?

If staying consumer, is 2Ã—24 GB (e.g., dual 4090/3090) meaningfully better than 1Ã—48 GB, considering multiâ€‘GPU overheads?

budget: $2000-4000.",computervision,1,https://www.reddit.com/r/computervision/comments/1n2cvzr/seeking_advice_on_hardware_requirements_for/,r_1n2cvzr,,,
r_1n2cc1f,reddit,Fragrant-Dog-3706,2025-08-28T13:32:19+00:00,"Looking for metadata schemas from image/video datasets
training computer vision models and need vast amounts of metadata schemas from image/video datasets. especially interested in ecommerce product images, financial document layouts, but really any structured metadata works. need thousands of different schema examples. anyone know where to find bulk collections of dataset metadata schemas?",computervision,1,https://www.reddit.com/r/computervision/comments/1n2cc1f/looking_for_metadata_schemas_from_imagevideo/,r_1n2cc1f,,,
r_1n2ahdk,reddit,Nothing769,2025-08-28T12:09:23+00:00,"Ideas for Project (Final Thesis)
So i am looking for ideas for my final thesis project (Mtech btw).

My experience in CV: (Kinda Intermediate)

>Pretty good understanding of Image processing.(I am aware most of the techniques)

>Classic  ML(Supervised learning and classic techniques. I have a strong grip here)

>Deep learning(Experienced with cnns and such models but 0 experience with transformers.

>Pretty superficial understanding of most popular models like resnet. By superficial i mean lack of mathematical knowledge of behind the scenes)

>

I have worked on homography recently.

Heres my dilemma:

Should i make a **product-oriented project**: As in building/ finetuning a model with some custom dataset.

Then  build a full solution by deploying it and apis/ web application and stuff. Take some customer reviews and iterate over it.

Or **research-oriented**:

Improving numbers for existing problems. Or better resource consumption or smth.

My understanding is: Research is all about improving numbers. You have to optimise at least one metric. Inference time, ram utilization, anything. Hopefully publish a paper

I personally want to  build a full product live on linkedin or smth. But i doubt that will give me good grades.

My top priority is grade.

Based on that where should i go?

Also please suggest ideas based on my exp : both research and product

Personally i am planning on going the sports side. But i am open to all choices.

For those of you who completed their final year thesis. (Mtech or MS etc)

What did you do?

",computervision,2,https://www.reddit.com/r/computervision/comments/1n2ahdk/ideas_for_project_final_thesis/,r_1n2ahdk,,,
r_1n29l2d,reddit,LukeDuke,2025-08-28T11:24:19+00:00,"Opensource/Free Halcon Vision competitor
I'm looking for a desktop gui-based app that provides similar machine-vision recipe/program created to Halcons offerings. I know opencv has a desktop app, but I'm not sure if it provides similar functionality. What else is out there?
",computervision,8,https://www.reddit.com/r/computervision/comments/1n29l2d/opensourcefree_halcon_vision_competitor/,r_1n29l2d,,,
r_1n27q1q,reddit,Knok0932,2025-08-28T09:36:33+00:00,"PaddleOCRv5 implemented in C++ with ncnn
Hi!

I made a C++ implementation of PaddleOCRv5 that might be helpful to some people: https://github.com/Avafly/PaddleOCR-ncnn-CPP

The official Paddle C++ runtime has a lot of dependencies and is very complex to deploy. To keep things simple I use ncnn for inference, it's much lighter, makes deployment easy, and faster in my task. The code runs inference on the CPU, if you want GPU acceleration, most frameworks like ncnn let you enable it with just a few lines of code.

Hope this helps, and feedback welcome!",computervision,16,https://www.reddit.com/r/computervision/comments/1n27q1q/paddleocrv5_implemented_in_c_with_ncnn/,r_1n27q1q,,,
r_1n24tlz,reddit,low_key404,2025-08-28T06:27:18+00:00,"TimerTantrum 2.0 upgraded with Dog, Cat, and Owl coaches ðŸ¶ðŸ±ðŸ¦‰
Last weekend, I hacked together a simple Pomodoro timer called **TimerTantrum**.  
I honestly thought only a few friends would try it â€” but to my surprise, people from **21 countries** ended up using it ðŸ¤¯.

Some even reached out with feedback (someone specifically asked for dark mode), which motivated me to keep going.

So I just released **TimerTantrum 2.0** ðŸš€

* ðŸŒ“ Dark / Light mode toggle
* ðŸ¶ðŸ±ðŸ¦‰ Choose your coach (Dog, Cat, Owl â€” each with its own animation & sound)
* â³ Cleaner design + smoother progress
* ðŸ“¸ *Privacy note: camera is used only locally for distraction detection â€” nothing is stored or uploaded.*

The idea is simple: focus sessions donâ€™t have to be boring. Now your coach will bark, meow, or hoot at you if you get distracted.

ðŸ‘‰ Try it here: [https://timertantrum.vercel.app/](https://timertantrum.vercel.app/)

Would love feedback â€” especially:

* Which mascot do you prefer?
* Any small features youâ€™d want in v3?",computervision,0,https://www.reddit.com/r/computervision/comments/1n24tlz/timertantrum_20_upgraded_with_dog_cat_and_owl/,r_1n24tlz,,,
r_1n24lxx,reddit,Careful_Island_2120,2025-08-28T06:14:04+00:00,"Need only recognition from paddleocr
Hi all,

Im using paddleocr 3.0.0, but unable to force recognition only from paddleocr. Because Im using yolov3-tiny to get text boxes ROI.  Secondly lets say Ive trained the paddleocr on my own dataset, does paddleocr support transfer learning if in case it fails on certain characters ? Also can I perform this training on jetson xavier NX with few shot images ?  ",computervision,1,https://www.reddit.com/r/computervision/comments/1n24lxx/need_only_recognition_from_paddleocr/,r_1n24lxx,,,
r_1n1ykuw,reddit,v1190cs,2025-08-28T00:57:24+00:00,"Reviving MTG Card Identification â€“ OCR + LLM + Preprocessing (Examples Inside)
# Reviving MTG Card Identification â€“ OCR + LLM + Preprocessing (Examples Inside)

Hey r/computervision,

I came across [this older thread about identifying Magic: The Gathering cards](https://www.reddit.com/r/computervision/comments/144hecd/identifying_a_magic_the_gathering_card/?utm_source=chatgpt.com) and wanted to revive it with some experiments Iâ€™ve been running. Iâ€™m building a tool for card collectors, and thought some of you might enjoy the challenge of OCR + CV on trading cards.

# What Iâ€™ve done so far

* **OCR:** Tested Tesseract and Google Vision. They work okay on clean scans but fail often with foils, glare, or busy card art.
* **Preprocessing:** Cropping, deskewing, converting to grayscale, boosting contrast, and stripping colors helped a lot in making the text more visible.
* **Fuzzy Matching:** OCR output is compared against the Scryfall DB (card names + artists).
* **Examples:**
   * Raw OCR: `""Ripchain Razorhin by Rn Spencer""`
   * Cleaned (via fuzzy + LLM):{    ""card\_name"": ""Ripchain Razorkin"",     ""artist\_name"": ""Ron Spencer"",     ""set\_name"": ""Darksteel""   }

# The new angle: OCR â†’ LLM cleanup

Instead of relying only on exact OCR results, Iâ€™ve been testing LLMs to normalize messy OCR text into structured data.

This has been surprisingly effective. For example, OCR might read *â€œBlakk Lotvs Chrss Rshâ€* but the LLM corrects it to *Black Lotus, Chris Rush, Alpha*.

# 1-to-many disambiguation

Sometimes OCR finds a card name that exists in many sets. To handle this:

* I use **artist name** as a disambiguator.
* If there are still multiple options, I check if the card exists in the **userâ€™s decklist**.
* If itâ€™s still ambiguous, I fall back to **image embedding / perceptual hashing** for direct comparison.

# Images / Examples

Hereâ€™s a batch I tested:

[Raw Cards as input.](https://preview.redd.it/xjqtthrdtnlf1.png?width=1150&format=png&auto=webp&s=c08822aa8bc3629271a22617842ef8a924d8d1ea)

[OCR output with bounding boxes.](https://preview.redd.it/mr03ksztsnlf1.png?width=1402&format=png&auto=webp&s=f723cd5bfce936201953951b88178bc84af9e253)

(*These are just a sample â€” OCR picks up text but struggles with foil glare and busy art. Preprocessing helps but isnâ€™t perfect.*

# Whatâ€™s next

* Test pHash / DHash for fast image fallback (\~100k DB scale).
* Experiment with ResNet/ViT embeddings for robustness on foils/worn cards.
* Try light subtraction to better handle shiny foil glare.

# Questions for the community

1. Has anyone here tried LLMs for OCR cleanup + structured extraction? Does it scale?
2. What are best practices for OCR on noisy/foil cards?
3. How would you handle tokens / â€œThe Listâ€ / promo cards that look nearly identical?

# TL;DR

Iâ€™m experimenting with OCR + preprocessing + fuzzy DB matching to identify MTG cards.  
New twist: using LLMs to clean up OCR results into structured JSON (`name`, `artist`, `set`).  
Examples included. Looking for advice on handling foils, 1-to-many matches, and scaling this pipeline.

Would love to hear your thoughts, and whether you think this project is worth pushing further.",computervision,7,https://www.reddit.com/r/computervision/comments/1n1ykuw/reviving_mtg_card_identification_ocr_llm/,r_1n1ykuw,,,
r_1n1vs22,reddit,fat_robot17,2025-08-27T22:51:24+00:00,"PEEKABOO2: Adapting Peekaboo with Segment Anything Model for Unsupervised Object Localization in Images and Videos
Introducing Peekaboo 2, that extends Peekaboo towards solving unsupervised salient object detection in images and videos!

This work builds on top of Peekaboo which was published in BMVC 2024! ([Paper](https://arxiv.org/abs/2407.17628), [Project](https://hasibzunair.github.io/peekaboo/)).

Motivation?ðŸ’ª

â€¢ SAM2 has shown strong performance in segmenting and tracking objects when prompted, but it has no way to detect which objects are salient in a scene.

â€¢ It also canâ€™t automatically segment and track those objects, since it relies on human inputs.

â€¢ Peekaboo fails miserably on videos!

â€¢ The challenge: how do we segment and track salient objects without knowing anything about them?

Work? ðŸ› ï¸

â€¢ PEEKABOO2 is built for unsupervised salient object detection and tracking.

â€¢ It finds the salient object in the first frame, uses that as a prompt, and propagates spatio-temporal masks across the video.

â€¢ No retraining, fine-tuning, or human intervention needed.

Results? ðŸ“Š

â€¢ Automatically discovers, segments and tracks diverse salient objects in both images and videos.

â€¢ Benchmarks coming soon!

Real-world applications? ðŸŒŽ

â€¢ Media & sports: Automatic highlight extraction from videos or track characters.

â€¢ Robotics: Highlight and track most relevant objects without manual labeling and predefined targets.

â€¢ AR/VR content creation: Enable object-aware overlays, interactions and immersive edits without manual masking.

â€¢ Film & Video Editing: Isolate and track objects for background swaps, rotoscoping, VFX or style transfers.

â€¢ Wildlife monitoring: Automatically follow animals in the wild for behavioural studies without tagging them.

Try out the method and checkout some cool demos below! ðŸš€

GitHub: [https://github.com/hasibzunair/peekaboo2](https://github.com/hasibzunair/peekaboo2)

Project Page: [https://hasibzunair.github.io/peekaboo2/](https://hasibzunair.github.io/peekaboo2/)",computervision,137,https://www.reddit.com/r/computervision/comments/1n1vs22/peekaboo2_adapting_peekaboo_with_segment_anything/,r_1n1vs22,,,
r_1n1v66m,reddit,AromaticLab8182,2025-08-27T22:26:05+00:00,"retail CV is kinda wild rn â€” some thoughts + a writeup
been messing around with retail CV lately and wrote up a piece on how stores are using it, stuff like smart shelves, heatmaps, AR try-ons, even just-walk-out setups like Amazon Go. nothing too wild, but itâ€™s cool seeing how many moving parts go into making it actually useful.

if youâ€™re tinkering with CV in retail (or thinking about it), might be worth a skim: [Computer Vision in Retail](https://www.leanware.co/insights/computer-vision-retail): curious what others are seeing, especially around privacy or making this stuff work with old POS setups.",computervision,2,https://www.reddit.com/r/computervision/comments/1n1v66m/retail_cv_is_kinda_wild_rn_some_thoughts_a_writeup/,r_1n1v66m,,,
r_1n1u1gn,reddit,bigjobbyx,2025-08-27T21:40:36+00:00,"Anaconda Vs straight .py
I am relatively new to ML and love the step based execution of scripts in Jupyter that Anaconda provides. 

Once I'm happy that my script will execute, is it better or more efficient rather to directly run a python script or stick to the safe and warm environment of Anaconda?",computervision,0,https://www.reddit.com/r/computervision/comments/1n1u1gn/anaconda_vs_straight_py/,r_1n1u1gn,,,
r_1n1tnal,reddit,curry-nya,2025-08-27T21:25:23+00:00,"OCR for a ""fictional"" language
Hello! I'm new to OCR/computer vision, but familiar with general ML/programming. 

There's this fictional language this fandom that I'm in uses. It's basically just the english alphabet with different characters, plus some ligatures. I think it would be a fun OCR-learning project to build a real-time translator so users can scan the ""foreign text"" and get the result in english.

I have the font downloaded already to create training data with, but I'm not sure about the best method. Should I train with entire sentences? Should I just train with individual letters? I know I can use Pillow from huggingface to generate artifacts, different lighting situations, etc. 

All the OCR stuff I've been looking at has been for pre-existing languages. I guess what I'm trying to do is a mix between image-recognition (because the glyphs aren't from an existing language) and OCR? There's a lot of OCR options, but does anyone have any reccs on which would be the most efficient?

Thanks a bunch!!",computervision,4,https://www.reddit.com/r/computervision/comments/1n1tnal/ocr_for_a_fictional_language/,r_1n1tnal,,,
r_1n1pxcl,reddit,9acca9,2025-08-27T19:01:53+00:00,How to convert a scanned book image to its best possible version for OCR?,computervision,3,https://www.reddit.com/r/computervision/comments/1n1pxcl/how_to_convert_a_scanned_book_image_to_its_best/,r_1n1pxcl,,,
r_1n1liah,reddit,OldMonk60065,2025-08-27T16:18:42+00:00,"Best OCR MODEL
Which model will recognize characters (english alphabets and numbers) engraved on an iron mould accurately?",computervision,4,https://www.reddit.com/r/computervision/comments/1n1liah/best_ocr_model/,r_1n1liah,,,
r_1n1g4jp,reddit,Any_Commercial7079,2025-08-27T12:49:27+00:00,"Survey on computational power needs for Machine Learning
As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), Iâ€™d greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. HereÂ´s the link:Â [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.

",computervision,1,https://www.reddit.com/r/computervision/comments/1n1g4jp/survey_on_computational_power_needs_for_machine/,r_1n1g4jp,,,
r_1n1bp60,reddit,Financial-Leather858,2025-08-27T08:46:34+00:00,"CVAT-DATAUP â€” an open-source fork of CVAT with pipelines, agents, and analytics
Iâ€™ve releasedÂ [**CVAT-DATAUP**](https://github.com/dataup-io/cvat-dataup), an open-source fork ofÂ [CVAT](https://github.com/opencv/cvat). Itâ€™s fully CVAT-compatible but aims to make annotation part of aÂ **data-centric ML workflow**.

**Already available:**Â improved UI/UX, job tracking, dataset insights, better text annotation.  
**Coming soon:**Â ðŸ¤– AI agents for auto-annotation & validation, âš¡ customizable pipelines (e.g., YOLO â†’ SAM), and richer analytics.

Repo:Â [https://github.com/dataup-io/cvat-dataup](https://github.com/dataup-io/cvat-dataup)

Medium link:Â [https://medium.com/@ghallabi.farouk/from-annotation-tool-to-data-ml-platform-introducing-cvat-dataup-bb1e11a35051](https://medium.com/@ghallabi.farouk/from-annotation-tool-to-data-ml-platform-introducing-cvat-dataup-bb1e11a35051)

Feedback and ideas are very welcome!

",computervision,15,https://www.reddit.com/r/computervision/comments/1n1bp60/cvatdataup_an_opensource_fork_of_cvat_with/,r_1n1bp60,,,
r_1n1b18j,reddit,Willing-Arugula3238,2025-08-27T08:02:09+00:00,"I built a program that counts football (""soccer"") juggle attempts in real time.
What it does:
Detects the football in video or live webcam feed
Tracks body landmarks
Detects contact between the foot and ball using distance-based logic
Counts successful kick-ups and overlays results on the video
The challenge
The hardest part was reliable contact detection. I had to figure out how to:
Minimize false positives (ball close but not touching)
Handle rapid successive contacts
Balance real time performance with detection accuracy
The solution I ended up with was distance based contact detection + thresholding + a short cooldown between frames to avoid double counting.
Github repo: https://github.com/donsolo-khalifa/Kickups

",computervision,582,https://www.reddit.com/r/computervision/comments/1n1b18j/i_built_a_program_that_counts_football_soccer/,r_1n1b18j,,,
r_1n1a9mg,reddit,Alternative_Art2984,2025-08-27T07:11:21+00:00,"Questions about Applied Science Intern (Computer Vision) in Melbourne
I recently noticed that Amazon Melbourne is hiring interns, and Iâ€™m preparing for the interview process. Iâ€™d really appreciate it if anyone clarify a few things who is working as a research scientist currently at Amazon Melbourne. I am first year PhD student having first author CVPR paper.

* How many stages are there in the internship interview process?
* Are the interviews typically as challenging as those in the US?
* What is the usual pay range for interns, since I didnâ€™t see salary details listed in the position description?",computervision,1,https://www.reddit.com/r/computervision/comments/1n1a9mg/questions_about_applied_science_intern_computer/,r_1n1a9mg,,,
r_1n148kr,reddit,Small-Earth8932,2025-08-27T01:41:02+00:00,"Moving to applied science role
Iâ€™m and experienced dev and have a degree in data science. For the past 5-6 years I have been mostly working on data engineering side of things. I would say I have decent understanding of basic CV and ML models, was working as applied scientist (when inception and bert were a thing). I want to get back to the applied science world, but given how much the field has changed and that I donâ€™t have any recent projects on my resume. How hard will it be in the current scenario to find a job as applied scientist. I can give myself 6-8 months (along with work) of prep, would appreciate any guidance on how should I approach it?",computervision,2,https://www.reddit.com/r/computervision/comments/1n148kr/moving_to_applied_science_role/,r_1n148kr,,,
r_1n0xmpl,reddit,Emergency_Beat8198,2025-08-26T20:55:34+00:00,"Can I change Pixel Shape from Square?
Going back to History , One of the creative Problem People tried to adventure was to change the shape of Pixel.   
  
Pixel is essentially a data point stored in form of matrix   
  
I was trying to change the base shape of Pixel from square to suppose some random shape , But have no clues to achieve that , I had asked LLMs where they modified each pixel Image but it didn't worked !! Any Idea regarding it !! 

Is it a property of hardware , Can I replicate this and visualize in my laptop? 



https://preview.redd.it/rz5nrtprcflf1.png?width=793&format=png&auto=webp&s=921767cfd4f9aec02d5651c9969c067cfe50d959

https://preview.redd.it/vf2aaghtcflf1.png?width=800&format=png&auto=webp&s=6a68fa73d56a79ec3badca0c0cd6cbbb2c4eb3d9

  


https://preview.redd.it/jvn156rpcflf1.png?width=1645&format=png&auto=webp&s=a538c78608ef89d5b2a75f25bce2590d690cf477

  
",computervision,0,https://www.reddit.com/r/computervision/comments/1n0xmpl/can_i_change_pixel_shape_from_square/,r_1n0xmpl,,,
r_1n0xa4y,reddit,stefanos50,2025-08-26T20:41:43+00:00,"Real-time Photorealism Enhancement for Games
This is a demo of my latest project, REGEN. Specifically, we propose the regeneration of the output of a robust unpaired image-to-image translation method (i.e., Enhancing Photorealism Enhancement by Intel Labs)  using paired image-to-image translation (considering that the ultimate goal of the robust image-to-image translation is to maintain semantic consistency). To this end, we observed that the framework can maintain similar visual results while increasing the performance by more than 32 times. For reference, Enhancing Photorealism Enhancement would run at an interactive frame rate of around 1 FPS (or below) at 1280x720, which is the same resolution employed for capturing the demo.  In detail, a system with an RTX 4090 GPU, Intel i7 14700F CPU, and 64GB DDR4 memory was used.",computervision,149,https://www.reddit.com/r/computervision/comments/1n0xa4y/realtime_photorealism_enhancement_for_games/,r_1n0xa4y,,,
r_1n0x6vn,reddit,iz_bleep,2025-08-26T20:38:16+00:00,"Tranfer learning object detection model using tensorflow
How did y'all parse and load the tfrecord dataset for training. I also want to know how you guys set the models outputs....like is it a list of cls and bbox or was it a dictionary or did y'all concatenate all of them into a single tensor. 
I'm training a transfer learning model with mobilenetv3small+ sppf+cbam attention+decoupled head which outputs a list[cls, reg] where reg is the bbox coordinates. The model compiles without any issue with the ciou loss function but when I'm parsing and preprocessing the tfrecord dataset I'm getting errors and am not able to train the model. So I wanted to know how to deal with a tfrecord dataset for object detection model. My model outputs a list and not a dictionary because Im gonna do quantization aware training later and int8 quantise it.",computervision,1,https://www.reddit.com/r/computervision/comments/1n0x6vn/tranfer_learning_object_detection_model_using/,r_1n0x6vn,,,
r_1n0wbmd,reddit,AaronSpalding,2025-08-26T20:05:45+00:00,"Why does active learning or self-learning work?
Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.

A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.

My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",computervision,15,https://www.reddit.com/r/computervision/comments/1n0wbmd/why_does_active_learning_or_selflearning_work/,r_1n0wbmd,,,
r_1n0v7ii,reddit,wuu73,2025-08-26T19:23:04+00:00,"Are VLMs, MLLMs bad at color perception? Or maybe I am just not thinking of it in the right way
I was sick and was using those urinalysis dip stick things and using ChatGPT and other models, assuming, that they would probably be good at doing the work for me with seeing if the color on the stick was not normal and analyzing it to give me some options of what i could be sick with by the results..I just assumed that they would be great at this task, but apparently not!

Every big LLM I sent pics to (camera pics of the urine strip lined up with the results colors) was waaay off. It seemed like it just did not see color variations very good at all. Very obvious to my eyes but not to the models.

Now I could instead do it like this: ""Write a python script to detect the average color for each of the 11 tests on here and try to normalize it to the background lighting and then output a structured markdown file of all of it. Then feed the markdown from this into a model...with prompt about..  "" something like that might work if it has text/numbers to work on instead (probably..)

  
I am now wondering if they all are bad at colors or just some of them? is there any website or database where this stuff is tracked, and you can just go browse to see what models are good at whatever smaller sub sub task/thing? ",computervision,1,https://www.reddit.com/r/computervision/comments/1n0v7ii/are_vlms_mllms_bad_at_color_perception_or_maybe_i/,r_1n0v7ii,,,
r_1n0pf6n,reddit,Mi_Diego,2025-08-26T15:46:02+00:00,"Help for Object Detection System
Hi! I'm a CS student, and I have to create an Object Detection System with YOLO, but I have some questions:

1 - I should use the Object365 dataset, but the download link on the official website doesn't work. Can I take it in different ways? 

2- I'm new to deep learning, I'd like to use Keras, and should I create a CNN from scratch? Or, should I import a CNN (like InceptionV3) and apply fine-tuning/transfer learning strategies?

  
Thank you guys!",computervision,0,https://www.reddit.com/r/computervision/comments/1n0pf6n/help_for_object_detection_system/,r_1n0pf6n,,,
r_1n0miim,reddit,lofan92,2025-08-26T13:54:56+00:00,"Finding Known Numbers using OCR
Hi All, I am trying to write a program that extracts numbers from a known excel list and search in the image for match. I\`ve tried testing out openCV but it does not work really well, is there any tools or method that can adopt the method mentioned?

  
Apologies in advance as I am a new learner to machine vision.",computervision,2,https://www.reddit.com/r/computervision/comments/1n0miim/finding_known_numbers_using_ocr/,r_1n0miim,,,
r_1n0jurg,reddit,nmam_adeep,2025-08-26T11:59:36+00:00,"ORBSLAM3 coordinate system
Hello everyone,

Iâ€™m currently working on a project with ORB-SLAM3 (Stereo/Monocular-Inertial mode) and I need some clarification on how the system defines the camera and IMU coordinate axes.

From my understanding so far:

ORB-SLAM3 follows the standard pinhole camera model, where:

x-axis â†’ points right in the image plane

y-axis â†’ points down in the image plane

z-axis â†’ points forward (optical axis)

For the IMU, the convention is less clear to me. In some references Iâ€™ve seen:

x-axis â†’ points forward

y-axis â†’ points left

z-axis â†’ points upward

What is the exact coordinate frame definition for the camera and the IMU in ORB-SLAM3?

When specifying the camera-IMU extrinsics in the YAML configuration, should the transform be defined as T\_cam\_imu (IMU to Camera) or T\_imu\_cam (Camera to IMU)?

Does ORB-SLAM3 internally enforce any gravity alignment during IMU initialization (e.g., Z-axis aligned with gravity)?",computervision,2,https://www.reddit.com/r/computervision/comments/1n0jurg/orbslam3_coordinate_system/,r_1n0jurg,,,
r_1n0jnx7,reddit,Snoo62259,2025-08-26T11:50:02+00:00,"Dinov3 access | help
Hi guys,

Does any of you have access to Dinov3 models on HF? My request to access got denied for some reason, and I would like to try this model. Could any of you make public this model by quantization using onnx-cummunity space? For this, you already need to have access to the model. Here is  the link: [https://huggingface.co/spaces/onnx-community/convert-to-onnx](https://huggingface.co/spaces/onnx-community/convert-to-onnx)",computervision,1,https://www.reddit.com/r/computervision/comments/1n0jnx7/dinov3_access_help/,r_1n0jnx7,,,
r_1n0hdbj,reddit,SeaworthinessStill94,2025-08-26T09:41:15+00:00,"Best way/tools for managing my IoT devices in cloud
Hi, I have been software engineer for 10 years and I know the hastle of managing the physical devices in the cloud (the ec2 instances, setting up infrastructure with terraform, kubernetes, etc.). I particularly like infrasturcture as code for the benefits it provides

Recently I have been exploring computer vision and building camera device. I am using raspberry pi for the computer part. I have setup my cloud infra with backend servers to process the video recordings of my camera. But now I lack the experience in managing my camera devices on the cloud (I have only one camera now, but will grow). 

What are you approaches into managing your devices on cloud? Are there any tools you would use? I imagine terraform and kubernetes dont work here so I was wandering if there is some other infrastructure as code solution to manage my IoT device/fleets",computervision,1,https://www.reddit.com/r/computervision/comments/1n0hdbj/best_waytools_for_managing_my_iot_devices_in_cloud/,r_1n0hdbj,,,
r_1n0gj25,reddit,Fit-Soup9023,2025-08-26T08:46:10+00:00,"Stuck on extracting structured data from charts/graphs â€” OCR not working well
Hi everyone,

Iâ€™m currently stuck on a client project where I need toÂ **extract structured data (values, labels, etc.) from charts and graphs**. Since itâ€™s client data, IÂ **cannot use LLM-based solutions (e.g., GPT-4V, Gemini, etc.)**Â due to compliance/privacy constraints.

So far, Iâ€™ve tried:

* **pytesseract**
* **PaddleOCR**
* **EasyOCR**

While they work decently for text regions, they performÂ **poorly on chart data**Â (e.g., bar heights, scatter plots, line graphs).

Iâ€™m aware that tools likeÂ **Ollama models**Â could be used for image â†’ text, but running them willÂ **increase the cost of the instance**, so Iâ€™d like to exploreÂ **lighter or open-source alternatives**Â first.

Has anyone worked on a similarÂ **chart-to-data extraction**Â pipeline? Are there recommendedÂ **computer vision approaches, open-source libraries, or model architectures**Â (CNN/ViT, specialized chart parsers, etc.) that can handle this more robustly?

Any suggestions, research papers, or libraries would be super helpful ðŸ™

Thanks!

",computervision,1,https://www.reddit.com/r/computervision/comments/1n0gj25/stuck_on_extracting_structured_data_from/,r_1n0gj25,,,
r_1n0gf31,reddit,No_Tennis945,2025-08-26T08:38:55+00:00,"Train an Instance Segmentation Model with 100k Images
Around 60k of these Images are confirmed background Images, the other 40k are labelled. It is a Model to detect damages on Concrete.

How should i split the Dataset, should i keep the Background Images or reduce them?

Should I augment the images? The camera is in a moving vehicle, sometimes there is blur and aliasing. (And if yes, how much of the dataset should be augmented?)

In the end i would like to train a Model with a free commercial licence but at the time i am trying how the dataset effects the model on ultralytics yolo11m-seg

Currently it detects damages with a high confidence, but only a few frames later the same damage wont be detected at all. It flickers a lot in videos",computervision,3,https://www.reddit.com/r/computervision/comments/1n0gf31/train_an_instance_segmentation_model_with_100k/,r_1n0gf31,,,
r_1n0ea7z,reddit,jorsxoxo,2025-08-26T06:18:05+00:00,"BSc CV Engineer aiming for FAANG ML role â€” is an MSc worth it?
Hi everyone,

Iâ€™m a BSc graduate currently working as a Computer Vision Engineer on robotics application part (from research to early deployment). My long-term goal is to grow into an ML role at FAANG, but Iâ€™m also debating whether I should instead specialize more deeply in robotics CV.

A few questions Iâ€™d love advice on:
1. Is FAANG experience really worth aiming for, compared to staying in a specialized domain like robotics?
2. For those whoâ€™ve made the transition, did you find an MSc or further studies necessary, or is strong project/industry experience enough?
3. Should I focus more on system-level skills (CI/CD, cloud, MLOps), or deepen my ML/AI expertise for career growth?

Would love to hear from those whoâ€™ve been through this journey â€” thanks in advance!
",computervision,5,https://www.reddit.com/r/computervision/comments/1n0ea7z/bsc_cv_engineer_aiming_for_faang_ml_role_is_an/,r_1n0ea7z,,,
r_1n0do65,reddit,exploringthebayarea,2025-08-26T05:40:08+00:00,"How to detect if a live video matches a pose like this
I want to create a game where there's a webcam and the people on camera have to do different poses like the one above and try to match the pose. If they succeed, they win.

I'm thinking I can turn these images into openpose maps, then wasn't sure how I'd go about scoring them. Are there any existing repos out there for this type of use case?",computervision,24,https://www.reddit.com/r/computervision/comments/1n0do65/how_to_detect_if_a_live_video_matches_a_pose_like/,r_1n0do65,,,
r_1n0ckv8,reddit,jingieboy,2025-08-26T04:35:57+00:00,"What are Best Practices when Building out/Fine-tuning Deep Learning Models
I often work with computer vision models (e.g. YOLO, R-CNNs), mostly training object detection & segmentation models. I am only about 2 years in as a DS doing this, I was wondering, besides having the fundamentals right when training, for example, having a good diverse dataset (include 10% background images to reduce false positives, have a clean train, val, test split) and things like that, what are some industry standards, or techniques that veterans used in order to really build out effective deep learning models? How to effectively evaluate these models beyond your generic metrics (e.g. Recall, Precision, mAP). I have been following the textbook way of training deep learning models, I want to know what good engineers are doing that I'm missing out on.",computervision,18,https://www.reddit.com/r/computervision/comments/1n0ckv8/what_are_best_practices_when_building/,r_1n0ckv8,,,
r_1n0anoc,reddit,doineedone-_-,2025-08-26T02:54:34+00:00,"imx708 based object detection to run on jetson orin nano .?
hey so i was working on this project where i will be usin g an jetson orin nano with the camera imx708 , but i have been having a lots o issues with getting the image right in my jetson orin nano , then i have faced issues with only getting 2-3 fps when i m running my yolo object detection models , so i needed help if any of you guys have worked on something similar and could direct me towards right resources to learn efficient resource usage for such tasks , or is it even possible .? it feels like the camera might be the issue but i hv no other camera to confirm that , i was able to get the 30fps raw stream , but the picture was a bit blurry(out of focus)",computervision,0,https://www.reddit.com/r/computervision/comments/1n0anoc/imx708_based_object_detection_to_run_on_jetson/,r_1n0anoc,,,
r_1n0agkz,reddit,0nerrr,2025-08-26T02:44:22+00:00,What is the best laptop out of these?,computervision,0,https://www.reddit.com/r/computervision/comments/1n0agkz/what_is_the_best_laptop_out_of_these/,r_1n0agkz,,,
r_1n03mav,reddit,Equivalent_Pie5561,2025-08-25T21:42:00+00:00,My Python Based Object Tracking Code for Air defence system Locks on CH-47 Helicopter,computervision,10,https://www.reddit.com/r/computervision/comments/1n03mav/my_python_based_object_tracking_code_for_air/,r_1n03mav,,,
r_1n01hi4,reddit,UnderstandingOwn2913,2025-08-25T20:20:49+00:00,"is there anyone who is working as a computer vision engineer only with a master degree?
I am currently a computer science master student in the US and I want to get a computer vision(deep learning based) engineer job after I graduate.",computervision,21,https://www.reddit.com/r/computervision/comments/1n01hi4/is_there_anyone_who_is_working_as_a_computer/,r_1n01hi4,,,
r_1mzzk7a,reddit,Wise_Investigator337,2025-08-25T19:07:30+00:00,"Two different YOLO models in one Raspberry Pi? Is it recommended?
I'm about to make a lettuce growing chamber where one grows it (harvest ready, not yet, etc.) and one grades (excellent, good, bad, etc.). So those two are in separate chamber/container where camera is placed on top or wherever it is best.

Afaik, it'll be hard to do real-time since it is process intensive, so for this I can opt to user chooses which one to use at a time then the camera will just take picture, run it on the model, then display the result on an LCD.

Question is, would you recommend to have two cameras in one pi running two models? Or should i have one pi each camera? Budget wise or just what will you choose to do in this scenario.

Also what camera do you think will suit best here? Like imagine a refrigerator type chamber, one for grading, one for growing.

Thanks!",computervision,2,https://www.reddit.com/r/computervision/comments/1mzzk7a/two_different_yolo_models_in_one_raspberry_pi_is/,r_1mzzk7a,,,
r_1mzwpqh,reddit,Yatty33,2025-08-25T17:21:53+00:00,"Inexpensive Outdoor Stereo Array
I'm working on an outdoor agricultural project on the side to learn more about CV. I started the project with a cheap rolling shutter stereo camera from [AliExpress](https://www.aliexpress.us/item/3256805541822136.html?spm=a2g0o.order_list.order_list_main.4.337b1802xZn7S5&gatewayAdapt=glo2usa). I was having issues with stuttering etc. when the vehicle the camera is moving, especially when it hits a bump. This is causing issues with my NN which is detecting fruit and go/no-go zones for motion.

I moved on and purchased a [global shutter stereo camera](https://www.aliexpress.us/item/3256809197339653.html?spm=a2g0o.productlist.main.59.56f6ewkSewkS8c&algo_pvid=a6bbf9e4-9725-48ad-a638-63df2a9570a8&algo_exp_id=a6bbf9e4-9725-48ad-a638-63df2a9570a8-58&pdp_ext_f=%7B%22order%22%3A%22-1%22%2C%22eval%22%3A%221%22%7D&pdp_npi=6%40dis%21USD%21131.45%21113.05%21%21%21131.45%21113.05%21%40210308a417561420413948607eaf49%2112000048940073120%21sea%21US%21732383337%21X%211%210%21n_tag%3A-29919%3Bd%3A1fd82372%3Bm03_new_user%3A-29895&curPageLogUid=PDXRFuWSDWIE&utparam-url=scene%3Asearch%7Cquery_from%3A%7Cx_object_id%3A1005009383654405%7C_p_origin_prod%3A) from a company named ELP. Testing indoors indicated this camera would be a better fit for my use case, however when I moved testing out doors I discovered the auto-exposure is absolute garbage. I'm having to tune the exposure/gain manually which I won't be able to do when the machine is fully autonomous. 

I'm at a point where I'm not sure what to do and would like to hear recommendations from the community.

1. Does anyone have a recommendation for a similarly priced stereo pair that they have used successfully outdoors? I'm especially interested in depth and RGB data.

2. Does anyone have a recommendation for a similarly priced pair of individual cameras, which can be synchronized, that have been used successfully outdoors? 

3. Should I build my own auto-exposure algorithm? 

4. Do I just need to bite the bullet and spend more money?

Thanks in advance.

",computervision,1,https://www.reddit.com/r/computervision/comments/1mzwpqh/inexpensive_outdoor_stereo_array/,r_1mzwpqh,,,
r_1mzu5f7,reddit,Manah_krpt,2025-08-25T15:48:27+00:00,"Data extracting from table using OCR
Hello, I need some advice with OCR. I have some tables with work schedules, all with the same layout, (only the number of columns changes depending on how many days are in a month). I need to scan these tables to csv files for further use. Is there any reliable software that will do the job?",computervision,2,https://www.reddit.com/r/computervision/comments/1mzu5f7/data_extracting_from_table_using_ocr/,r_1mzu5f7,,,
r_1mztu5n,reddit,Slight-Ad-5816,2025-08-25T15:36:56+00:00,"No-Reference Metric for Precipitation Maps
Hi, I am writing a paper on domain adaptation for super resolution of precipitation maps from a high amount of data region (source) and using that knowledge to increase resolution on a low amount of data region (target). The issue was the target region was unlabelled i am having absolutely no ground truth for target region as there are no data available on 4km resolution. Now, To validate my model on the target region I would need a no reference metric that can just by the output super resolved image can tell that this image is better that other images (low resolution). I found a paper for no reference images that uses pretrained VIT and ResNet models to do this. [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10742110](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10742110) I am thinking of using this metric as validation metric for my sr model. Is it a good idea? ",computervision,1,https://www.reddit.com/r/computervision/comments/1mztu5n/noreference_metric_for_precipitation_maps/,r_1mztu5n,,,
r_1mzodse,reddit,Silver_Raspberry_811,2025-08-25T11:53:10+00:00,"The Evolution of Gaussian Splatting: From 3D to 5D - What's Your Take on Its Impact Across Fields?
Just watched the excellent ""3D Gaussian Splatting Past Present and Future"" lecture by George from TUM, and it got me thinking about the broader trajectory of this technique.

Quick primer from first principles: Gaussian Splatting fundamentally reimagines 3D representation by using anisotropic 3D Gaussians as primitives instead of meshes or voxels. Each Gaussian is defined by position (Î¼), covariance (Î£), opacity (Î±), and spherical harmonics coefficients for view-dependent color. The key insight is that these can be differentiably rendered via alpha-blending, enabling direct optimization from 2D images.

What fascinates me about the progression:
- 3D GS: Real-time novel view synthesis with photorealistic quality
- 4D GS: Adding temporal dimension for dynamic scenes 
- 5D rendering: Incorporating additional parameters (lighting, material properties, etc.)

Current applications I'm seeing:
- Robotics: Real-time SLAM and scene understanding
- AR/VR: Lightweight photorealistic environments 
- Film/Gaming: Efficient asset creation from real footage
- Digital twins: Industrial monitoring and simulation
- Medical imaging: 3D reconstruction from sparse views
- Autonomous vehicles: Dynamic scene representation

Questions for the community:

1. Technical scaling: How do you see the memory/compute trade-offs evolving as we move to higher dimensional representations? The quadratic growth in Gaussian parameters seems like a fundamental bottleneck.

2. Hybrid approaches: Are we likely to see GS integrated with traditional mesh rendering, or will it completely replace existing pipelines?

3. Learning dynamics: What's your experience with convergence stability when extending beyond 3D? I've noticed 4D implementations can be quite sensitive to initialization.

4. Novel applications: What unconventional use cases are you exploring or envisioning? 

5. Theoretical limits: Given the continuous nature of Gaussians vs discrete alternatives, where do you think the representation will hit fundamental limitations?

Particularly curious about perspectives from those working in real-time applications - how are you handling the rendering pipeline optimizations, and what hardware considerations are driving your implementation choices?

Would love to hear your thoughts on where this is heading and what problems you think it's uniquely positioned to solve vs where traditional methods might maintain advantages.",computervision,21,https://www.reddit.com/r/computervision/comments/1mzodse/the_evolution_of_gaussian_splatting_from_3d_to_5d/,r_1mzodse,,,
r_1mzo7qx,reddit,Professional-Fly8636,2025-08-25T11:44:44+00:00,"GPU para IA
sou iniciante agora mas pretendo estudar IA por anos e queria uma placa de video que eu nÃ£o precise se preocupar em trocar por uns 2 anos, oque acham da 5060ti de 16vram para IA? tem muita diferenÃ§a entre ela e a 5060 normal? (nÃ£o tenho grana pra comprar 5070 +)",computervision,0,https://www.reddit.com/r/computervision/comments/1mzo7qx/gpu_para_ia/,r_1mzo7qx,,,
r_1mzmd0a,reddit,Amazing_Life_221,2025-08-25T10:02:20+00:00,"Best resource for learning traditional CV techniques? And How to approach problems without thinking about just DL?
Question 1:
I want to have a structured resource on traditional CV algorithms. 

I do have experience in deep learning. And donâ€™t shy away from maths (and I used to love geometry during school) but I never got any chance to delve into traditional CV techniques. 

What are some resources?

Question 2: 
As my brain and knowledge base is all about putting â€œmodelsâ€ in the solution my instinct is always to use deep learning for every problem I see. Iâ€™m no researcher so I donâ€™t have any cutting edge ideas about DL either. But there are many problems which do not require DL. How do you assess if thatâ€™s the case? How do you know DL wonâ€™t perform better than traditional CV for the given problem at hand?",computervision,5,https://www.reddit.com/r/computervision/comments/1mzmd0a/best_resource_for_learning_traditional_cv/,r_1mzmd0a,,,
r_1mzm6r7,reddit,Distinct-Ebb-9763,2025-08-25T09:51:55+00:00,"Where can I find some CCTV footages of shop checkout for dataset creation.
Hi, so I am currently on a task where I have to train a model for detecting whether a shop keeper is using a phone or not. And the dataset is really really small in which there are other tasks that are being performed like using POS Machine, Cash or being idle apart from using mobile. 
And even after applying augmentation to dataset, it won't be enough. As that will not completely eradicate false positives.

I would be thankful if anyone can provide me some sources where I can relevant raw data that can be helpful in my case. Thank you.",computervision,2,https://www.reddit.com/r/computervision/comments/1mzm6r7/where_can_i_find_some_cctv_footages_of_shop/,r_1mzm6r7,,,
r_1mzlv1k,reddit,Qiiqer,2025-08-25T09:31:25+00:00,"Model/Algorithm for measuring lengths/edges using a phone camera, given a reference item?
For all intents and purposes assume that photographs will be taken directly perpendicular to measuring surfaces, with reference also perpendicular to plane of photography. How should I go about this?

For context: I need to create a platform/program such that a user can upload photographs (top-down, side-on, rear, front) of a scaled down F1 car (this is for F1 in Schools competition), then automated measurements of surfaces that can feasibly be measured are taken, and then these measurements are checked against regulations set out in the technical regulations booklet. If anyone could tell me how to approach this, it would be of great help. I am planning on using the diameter and width of the front and rear wheels (which is standardised) as reference items.",computervision,1,https://www.reddit.com/r/computervision/comments/1mzlv1k/modelalgorithm_for_measuring_lengthsedges_using_a/,r_1mzlv1k,,,
r_1mzjnzb,reddit,wasay312,2025-08-25T07:07:34+00:00,"Need guidance for UAV target detection (Rotary Wing Competition) â€“ OpenCV too slow, how to improve?
Hi everyone,

Iâ€™m an Electrical Engineering undergrad, and my team is participating in the **Rotary Wing category** of an international UAV competition. This is my first time working with computer vision, so Iâ€™m a complete beginner in this area and would really appreciate advice from people whoâ€™ve worked on UAV vision systems before.

**Mission requirements:**

* The UAV must autonomously **detect ground targets** (red triangle and blue hexagon) while flying.
* Once detected, it must **lock on the target and drop a payload**.
* Speed matters: UAV flight speed will be around **9â€“10 m/s** at altitudes of 30â€“60 m.
* Scoring is based on accuracy of detection, correct identification, and completion time.

**My current setup:**

* **Raspberry Pi 4** with an Arducam 16MP IMX519 camera (using `picamera2`).
* Running **OpenCV** with a custom script:
   * Detect color regions (LAB/HSV).
   * Crop ROI.
   * Apply Canny + contour analysis to classify target shapes (triangle / hexagon).
   * Implemented bounding box, target locking, and basic filtering.
* Payload drop mechanism is controlled by servo once lock is confirmed.

**The issue Iâ€™m facing:**

* Detection **only works if the drone is stationary or moving extremely slowly**.
* At even walking speed, the system struggles to lock; at UAV speed (\~9â€“10 m/s), itâ€™s basically impossible.
* FPS drops depending on lighting/power supply (around 25 fps max, but effective detection is slower).
* Tried optimizations (reduced resolution, frame skipping, manual exposure tuning), but OpenCV-based detection seems too fragile for this speed requirement.

**What Iâ€™m looking for:**

* Is there a **better approach/model** that can realistically run on a Raspberry Pi 4?
* Are there **pre-built datasets** for aerial shape/color detection I can test on?
* Any advice on **optimizing for fast-moving UAV vision** under Raspberry Pi constraints?
* Should I train a lightweight model on my laptop (RTX 2060, 24GB RAM) and deploy it on Pi, or rethink the approach completely?

This is my **first ever computer vision project**, and weâ€™ve invested a lot into this competition, so Iâ€™m trying to make the most of the remaining month before the event. Any kind of guidance, tips, or resources would be hugely appreciated ðŸ™

Thanks in advance!",computervision,3,https://www.reddit.com/r/computervision/comments/1mzjnzb/need_guidance_for_uav_target_detection_rotary/,r_1mzjnzb,,,
r_1mze2e3,reddit,TumbleweedAdept3734,2025-08-25T01:54:47+00:00,"Best model for eyeglasses (not sunglasses) detection in 2025?
What is currently the most reliable model for **detecting eyeglasses (not sunglasses)**?

I'm exploring this for my **image generation workflows / prompt engineering**, so accuracy is more important than real-time speed.

Has anyone here had success with YOLOv8, RetinaFace, or other approaches for glasses detection? Would love to hear what worked best for you.",computervision,3,https://www.reddit.com/r/computervision/comments/1mze2e3/best_model_for_eyeglasses_not_sunglasses/,r_1mze2e3,,,
r_1mzarke,reddit,divinetribe1,2025-08-24T23:20:59+00:00,"APP RELEASE Realtime AI Cam â€” FREE iOS app running YOLOv8 (601 classes) entirely on-device

Just released Realtime AI Cam ðŸ“±
	â€¢	Runs YOLOv8 with all 601 classes on iPhone
	â€¢	Real-time detection at ~10 FPS (tested on iPhone 14 Pro Max)
	â€¢	100% on-device â†’ no server, no cloud, full privacy
	â€¢	Optimized with CoreML + Apple Neural Engine
	â€¢	FREE to download",computervision,0,https://www.reddit.com/r/computervision/comments/1mzarke/app_release_realtime_ai_cam_free_ios_app_running/,r_1mzarke,,,
r_1mza3t3,reddit,Southern_Page1879,2025-08-24T22:52:46+00:00,"How to find kinda similar image in my folder
I dont know how to explain, I have files with lots of images (3000-1200).

So, I have to find an image in my file corresponding to in game clothes. 
For example I take a screenshot of T-shirt in game, I have to find similar one in my files to write some things in my excel and it takes too much time and lots of effort.

I thought if there are fast ways to do that.. sorry I use English when Iâ€™m desperate for solutions ",computervision,3,https://www.reddit.com/r/computervision/comments/1mza3t3/how_to_find_kinda_similar_image_in_my_folder/,r_1mza3t3,,,
r_1mz4zsn,reddit,Ambitious_Ad4186,2025-08-24T19:29:26+00:00,"Help with a type of OCR detection
Hi,

My CCTV camera feed has some on-screen information displays. I'm displaying the preset data.

https://preview.redd.it/su3277fgq0lf1.png?width=140&format=png&auto=webp&s=c6b57c44fd56aa0b9822c1d55080fea69f44b983

I'm trying to recognize which preset it is in my program.  
OCR processing is adding like 100ms to the real-time delay.  
So, what's another way?  
There are 150 presets, and their locations never change, but the background does. I tried cropping around the preset via the feed, and ""overlaying"" the crop from the feed with the template crops, but, it's still not accurate 100%. Maybe 70% only.

Thanks!

EDIT:  
I changed the feed's text to be black, vs white as shown above. This made the Easy OCR accuracy almost 90%! However, at 150px wide by 60px high, on a CPU, it's still at 100ms per detection. I'm going to live with this for now. ",computervision,3,https://www.reddit.com/r/computervision/comments/1mz4zsn/help_with_a_type_of_ocr_detection/,r_1mz4zsn,,,
r_1mz35z1,reddit,Rethunker,2025-08-24T18:19:49+00:00,"what do you consider success or failure for your vision project?
For vision projects that you complete, or that you abandon, do you have a few criteria that you use consistently to gauge success or failure?

The point of my asking is to understand how people think about their study or work in vision. In short, what have you done, and how do you feel about that?

When I started in the field, most people wouldn't really understand what I was talking about when I described my work and the companies I worked for. Vision systems were invisible to the general public, but well known within the world of industrial automation. Medical imaging and satellite imagine were much better known and understood.

With the advent of vision-powered apps on smart phones, and the popularity of open source vision libraries, the world is quite different. The notion of what a ""vision"" system is, has also shifted.  
  
If you've completed at least one vision project, and preferably a number of projects, I'd be curious to know the following:

1. which category of project is most relevant to you
   * hobby
   * undergrad or grad student: project assigned for a class
   * undergrad or grad student: project you chose for a capstone or thesis
   * post-graduate R&D in academia, a national lab, or the like
   * private industry: early career, mid career, or late career
   * other
2. the application(s) and uses cases for your work (but only if you care to say so)
3. the number of distinct vision projects, products, or libraries you made or helped make; 
   1. if you've published multiple papers about what is essentially the same ongoing vision project, I'd count that as a single project
   2. if you created or used a software package for multiple installs, consider the number of truly distinct projects, each of which took at least a few weeks of new engineering work, and maybe a few months
4. the number of active users or installations
   1. not the number of people who watch at least a few seconds of a publicly posted video, 
   2. not the number of attendees at a conference, 
   3. not the number of forks of a library in a repo 
   4. known active users (according to your best guess) for a current project/product, and known active users for a past project (that may be defunct)
5. your criteria for success & failure

For example, here's how I'll answer my own request. I've working in vision for three decades, so I've had plenty of time to rack up plenty of successes and failures. Once in a while I post in the hope of increasing y'all's success-to-failure ratio.

My answers:

1. private industry, R&D and product development, mid to late career
2. vision hardware and/or software products for industrial automation, lab automation, and assistive technology. Some ""hobby"" projects that feed later product development.
3. products
   * hardware + software: over my career, about two to three dozen distinct products, physical systems, or lab devices that were or are sold or used in quantity six to hundreds each
   * software: in-house lab software (e.g. calibration), vision setup software used for product installs, and features for software products
4. users
   * hardware + software: many hundreds, or maybe low thousands, of vision systems sold, installed, and used
   * software: hundreds or thousands users of my software-only contributions, though it's very hard to tell w/o sales numbers and data companies rarely collect & summarize & share
5. criteria for success & failure
   1. Success
      1. **Profitability**. If colleagues and/or I don't create a vision product that sells well enough, the whole company suffers.
      2. **Active use**. If people use it and like it, or consider it integral to everyday use (e.g. in a production facility), that's a success.
      3. **Ethical use**. Pro bono development of vision systems is a good cause. 
   2. Partial successs
      1. **Re-usable software or hardware**. For example, one prototype on which others and I spent about a year ended abruptly
      2. **Active use by people who tolerate it**. If the system isn't as usable as it should be, or if maintenance is burdensome, then that's not great.
   3. Failure
      1. **Net loss of money.** Even if the vision system ""works,"" if my company or employer doesn't make money on it, it's a failure.
      2. **Minimal or no re-use**. One of my favorite prototypes made it to beta, then a garbage economy helped kill it. A colleague was laid off, and I was only able to salvage some of the code for the next development effort.
      3. **Unethical use**. Someone uses the system for an objectionable purpose, or an objectionable person profits unduly from it, and may not have had similar benefits if the vision system(s) weren't provided.

",computervision,0,https://www.reddit.com/r/computervision/comments/1mz35z1/what_do_you_consider_success_or_failure_for_your/,r_1mz35z1,,,
r_1mz2x5p,reddit,BigV95,2025-08-24T18:10:40+00:00,"DSP proff offered to work with me for my thesis on computervision. What are job prospects like for an EE undergrad with CompVision thesis like? Will EE background even be relevent?
Didnt tell the proff im working on a fixed wing drone rn. As soon as he offered it a tube light went off in my head. Computer vision could be used for so many things on a drone. 
",computervision,2,https://www.reddit.com/r/computervision/comments/1mz2x5p/dsp_proff_offered_to_work_with_me_for_my_thesis/,r_1mz2x5p,,,
r_1mz2c21,reddit,zimmer550king,2025-08-24T17:49:12+00:00,"Shape Approximation Library in Kotlin (Touch Points â†’ Geometric Shape)
Iâ€™ve been working on a small geometry library in **Kotlin** that takes a **sequence of points** (e.g., from touch input, stroke data, or any sampled contour) and approximates it with a known shape.

Currently supported approximations:

* **Circle**
* **Ellipse**
* **Triangle**
* **Square**
* **Pentagon**
* **Hexagon**
* **Oriented Bounding Box**

# Example API

`fun getApproximatedShape(points: List<Offset>): ApproximatedShape?`

Thereâ€™s also a `draw` method (integrated with Jetpack Composeâ€™s `DrawScope`) for visualization, but the core fitting logic can be separated for other uses.

# [https://github.com/sarimmehdi/Compose-Shape-Fitter](https://github.com/sarimmehdi/Compose-Shape-Fitter)

Are there shape approximation techniques (RANSAC, convex hull extensions, etc.) youâ€™d recommend I explore? I am especially interested in coming up with a more generic solution for triangles.",computervision,2,https://www.reddit.com/r/computervision/comments/1mz2c21/shape_approximation_library_in_kotlin_touch/,r_1mz2c21,,,
r_1mz0b4t,reddit,Nearby_Speaker_4657,2025-08-24T16:33:37+00:00,I am training a better super resolution model,computervision,15,https://www.reddit.com/r/computervision/comments/1mz0b4t/i_am_training_a_better_super_resolution_model/,r_1mz0b4t,,,
r_1mz07hb,reddit,Yuvraj_131,2025-08-24T16:29:51+00:00,"Wanted to know about 3D Reconstruction
So I was trying to get into 3D Reconstruction mainly from ML related background more than classical computer vision. So I started looking online about resources & found ""Multiple View Geometry in Computer vision"" & ""An invitation to 3-D Vision"" & wanted to know if these books are relevant because they are pretty old books.
Like I think current sota is gaussian splatting & neural radiance fields (I Think not sure) which are mainly ML based.
So I wanted to if the things in books are still used in industry predominantly or not, & what should I focus more on??",computervision,13,https://www.reddit.com/r/computervision/comments/1mz07hb/wanted_to_know_about_3d_reconstruction/,r_1mz07hb,,,
r_1myxe8c,reddit,C_Sorcerer,2025-08-24T14:43:00+00:00,"Getting started with computer vision... best resources? openCV?
Hey all, I am new to this sub.  I am a senior computer science major and am very interested in computer vision, amongst other things.  I have a great deal of experience with computer graphics already, such as APIs like OpenGL, Vulkan, and general raytracing algorithms, parallel programming optimizations with CUDA, good grasp of linear algebra and upper division calculus/differential equations, etc.  I have never really gotten much into AI as much other than some light neural networking stuff, but for my senior design project, me and a buddy who is a computer engineer met with my advisor and devised a project that involves us creating a drone that can fly over cornfields and use computer vision algorithms to spot weeds, and furthermore spray pesticides on only the problem areas to reduce waste.  We are being provided a great deal of image data of typical cornfield weeds by the department of agriculture at my university for the project.  My partner is going to work on the electrical/mechanical systems of the drone, while I write the embedded systems middleware and the actual computer vision program/library.  We only have 3 months to complete said project.

While I am no stranger to learning complex topics in CS, one thing I noticed is that computer vision is incredibly deep and that most people tend to stay very surface level when teaching it.  I have been scouring YouTube and online resources all day and all I can find are OpenCV tutorials.  However, I have heard that OpenCV is very shittily implemented and not at all great for actual systems, especially not real time systems.  As such, I would like to write my own algorithms, unless of course that seems to implausible.  We are working in C++ for this project, as that is the language I am most familiar with.  

So my question is, should I just use OpenCV, or should I write the project myself and if so, what non-openCV resources are good for learning?",computervision,6,https://www.reddit.com/r/computervision/comments/1myxe8c/getting_started_with_computer_vision_best/,r_1myxe8c,,,
r_1mys7y7,reddit,FrontWillingness39,2025-08-24T10:36:05+00:00,"Looking for Image Captioning Models (plus papers too!)
Hey everyone! Iâ€™m hunting for solid image captioning modelsâ€”did some research but thereâ€™s way too many, so hoping for your recs!  
I only know a couple so far: BLIP-2 works for basic image + language tasks but misses deep cultural/emotional vibes (like getting memes or artâ€™s nuance).  
What I need: models that handleÂ *all*Â image typesâ€”everyday photos, art, memesâ€”and make accurate, detailed captions. Also, if youâ€™ve seen any good 2023-now papers on this (new techniques or better performance), those would be awesome too!  
Are there any established and reliable image captioning models, perhaps some lesser-known yet highly effective ones, or recent papers? Even quick tips help tons.",computervision,0,https://www.reddit.com/r/computervision/comments/1mys7y7/looking_for_image_captioning_models_plus_papers/,r_1mys7y7,,,
r_1mypi5x,reddit,frostyWithRegrets,2025-08-24T07:45:34+00:00,"On prem OCR and layout analysis solution
I've been using the omnidocbench repo to benchmark a bunch of techniques and currently unstructured's paid API was performing exceedingly well. However, now I need to deploy an on-prem solution. Using unstructured with hi_res takes approx 10 seconds a page which is too much. I tried using dots_ocr but that's taking 4-5 seconds a page on an L4. Is there a faster solution which can help me extract text, tables and images in an efficient manner while ensuring costs don't bloat. I also saw monkey OCR was able to do approx 1 page a second on an H100",computervision,10,https://www.reddit.com/r/computervision/comments/1mypi5x/on_prem_ocr_and_layout_analysis_solution/,r_1mypi5x,,,
r_1my4nyy,reddit,Antique_Grass_73,2025-08-23T15:38:05+00:00,"Generating Synthetic Data for YOLO Classifier
Iâ€™m training a YOLO model (Ultralytics) to classify 80+ different SKUs (products) on retail shelves and in coolers. Right now, my dataset comes directly from thousands of store photos, which naturally capture reflections, shelf clutter, occlusions, and lighting variations.

The challenge: when a **new SKU** is introduced, I wonâ€™t have in-store images of it. I can take shots of the product (with transparent backgrounds), but I need to generate training data that looks like it comes from real shelf/cooler environments. Manually capturing thousands of store images isnâ€™t feasible.

My current plan:

* Use a shelf-gap detection model to crop out empty shelf regions.
* Superimpose transparent-background SKU images onto those shelves.
* Apply image harmonization techniques like [WindVChen/Diff-Harmonization](https://github.com/WindVChen/Diff-Harmonization?utm_source=chatgpt.com) to match the pasted SKUâ€™s color tone, lighting, and noise with the background.
* Use Ultralytics augmentations to expand diversity before training.

My goal is to induct a new SKU into the existing model within 1â€“2 days and still reach **>70% classification accuracy** on that SKU without affecting other classes.
  
I've tried using tools like [Image Combiner by FluxAI](https://fluxai.art/features/ai-image-combiner) but tools like these change the design and structure of the sku too much:

[foreground sku](https://preview.redd.it/l0arxsqigskf1.png?width=400&format=png&auto=webp&s=aa0c549ad0ca06a5f66b765b8b75f399ba7e83a1)

[background shelf](https://preview.redd.it/3jvqsyurfskf1.jpg?width=136&format=pjpg&auto=webp&s=1efa7dde45498934f594ec3ccf593d8961b16e07)

[image generated by flux.art](https://preview.redd.it/s96q3rr6gskf1.png?width=400&format=png&auto=webp&s=81af16d12f4c1ce22b5dbe6b6ea4e5cd7b61ec3a)

What are effective methods/tools for generating realistic synthetic retail images at scale with minimal manual effort? Has anyone here tackled similar SKU induction or retail synthetic data generation problems? Will it be worthwhile to use tools like [Saquib764/omini-kontext](https://github.com/Saquib764/omini-kontext) or [flux-kontext-put-it-here-workflow](https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fflux-kontext-put-it-here-workflow-fast-efficient-for-image-v0-deq8f4lb9gif1.jpeg%3Fwidth%3D1485%26auto%3Dwebp%26s%3Dcb04ae7ac49dba16cb34994af345dbd726c958eb)?",computervision,8,https://www.reddit.com/r/computervision/comments/1my4nyy/generating_synthetic_data_for_yolo_classifier/,r_1my4nyy,,,
r_1my1zjt,reddit,Open_Force1895,2025-08-23T13:50:08+00:00,"Best way to convert pdf into formatted JSON
  
I am trying to convert questions from a large set of PDFs into JSON so i can display them on an app im building. It is a very tedious task and also needs latex formatting in many cases. What model or plain old algorithm can do this most effectively?

[Here is an example page from a document:](https://preview.redd.it/j7ygbbxmxrkf1.png?width=1191&format=png&auto=webp&s=2f9f594d06227861e6280a03129e781120609a52)

  
The answers to these questions are also given at the end of the pdf.

For some questions the model might have to think a little bit more to figure out if a question is a comprehension question and to group it or not. The PDF do not have a specific format either.",computervision,2,https://www.reddit.com/r/computervision/comments/1my1zjt/best_way_to_convert_pdf_into_formatted_json/,r_1my1zjt,,,
r_1my0ysm,reddit,Fabulous_Addition_90,2025-08-23T13:06:02+00:00,"yolov5n performance on jetson nano developer kit 4gb b01
The main question: what is the maximum FPS possible using jetson nano developer kit 4gb b01 and yolov5n
I have a jetson nano developer kit 4gb b01
trying to setup an anpr pipeline on it.

Device info:
Ubuntu 20.04 (qengeeneing image for jetson nano)
Jetpack 4.6.1
Cuda 10.2
cuDNN 8.2.1
python 3.8
OpenCV 4.8.0
TensorFlow 2.4.1
Pytorch 1.13.0
TorchVision 0.14.0
TensorRT 8.0.1.6

i used a custom trained yolov11n(v6.2) model with batch size 1, and image size 320x320,

I then exported my model to tensorrt (pt=>onnx=>tensorrt) with the same size and same batch size with 1gb of workspace 

Right now I'm getting 5.9~5.6 FPS using tensorrt (there is an other yolov11n(v6.2) model running at the same time on this board with batch size 1 and image size 192x192 alongside 1gb of workspace using tensorrt format)

So
Has anyone got higher FPS on this situation?
-if yes: how did you managed to do that
-if no: what can I do to increase the FPS



My goal is to get 10fps
",computervision,3,https://www.reddit.com/r/computervision/comments/1my0ysm/yolov5n_performance_on_jetson_nano_developer_kit/,r_1my0ysm,,,
r_1mxzg6m,reddit,LeekNecessary3190,2025-08-23T11:54:15+00:00,"As HR ,What I look for in any CV
We see a lot of people posting in various cybersecurity and IT groups about how difficult the job market is. Especially at the beginning. They send hundreds of CVs every month with no responses. You feel like you're a perfect fit for all the job requirements, and still, there's no reply. I want to help and give you my perspective and what goes through my mind when I'm on the other side.

I've been hiring people in the cyber and IT fields for over 25 years. I feel like I've gotten very good at reading CVs now. Currently, I work in cyber as an ISSM and I need to hire an engineer to manage my tools: SIEM, a vulnerability scanner, and an endpoint security solution. The job req only lists these technologies. I'm not looking for specific tools because there are so many of them. This is a junior position that requires two years of experience with a certification, or four years without a certification.

Why I rejected a specific CV...

1: Review the nonsense written by AI. AI can be a good tool, but don't let it do all the work for you. I'm sure you're not working at three different companies at the same time. I'm also sure that your current employment duration is not ""10/2025 - Present."" When you send a CV, it represents the quality of what you consider a finished task. If you're not going to review your CV, then you're not going to review your work on the job.

2: Get to the point and say who you are. Don't make a 6-page, double-spaced CV full of keywords with no substance. ""Responsible for strategic objectives in a multifaceted, multi-site team."" What am I supposed to understand from that? If you can't focus your message, I won't know if you even have a point of view when we talk. Will our conversations take a very long time? Will you be able to ask me for what you need? Yes, I know it's ironic that I'm saying this in a long post. But there's a time and place for everything. It's not that I think I'm better than your time; it's that I have 6 hours of meetings and only two hours to do the actual work I was hired for. Those two hours include supporting my entire team, and everyone deserves that support.

3: Spelling and grammar mistakes. This doesn't just go back to the point of putting in the time and effort to produce something of good quality; it also shows that you need to know how to communicate well. I understand if English isn't your first language, so I'm not looking for perfection. But if I find a lot of red lines under the words that Word or Google Docs is showing me, then it surely did the same for you.

4: Your CV must reflect your work experience. When you're still new, you have to inflate your contributions a bit. ""Responsible for vulnerability management for 10,000 computers and improving the security posture by 25%."" I get it. You were deploying patches with WSUS or YUM. We all started somewhere. But this way of talking shouldn't be coming from someone with 5 or 10 years of experience or more and who has had several jobs in IT. Tell me your real achievements. If you don't know them, I'll doubt what you were doing all that time. This is a junior position, but I see a lot of people with more experience and higher qualifications applying. Again, the job market sucks.

5: You jump from one job to another quickly. It takes about a month to open a job req, conduct interviews, and choose someone, then they resign and take two weeks. Then it will take another month for you to get the equipment and accounts you need, and for you to learn the team and office dynamics and start contributing. Then, likely in the third month, you'll need support from me or one of your colleagues. Finally, in the fourth month of our team being short-staffed, you become a net contributor in terms of time versus productivity for the team. That's why people tell you that you should stay at a job for a year. If you change jobs every 6 months, I will never get a return on my investment of that time. I understand that RIFs can happen, or that your last job wasn't a good fit. Jumping quickly once or twice is understandable. But twice in a row, and you've only been at your current job for 3 months? I will reject you.

Why I chose a specific CV...

1: Colors and formatting. Look, I have a dozen CVs to review. They all start to look alike in context and content, and sometimes I read very quickly. Although I try to focus on this and give your CV the time it deserves, see the point above about my two hours of actual work per day. I saw a CV yesterday with a blue steel-colored banner and a gray column on the left for skills. It looked distinctive and made me pay attention to it.

2: Two pages at the very most. I don't need to know what high school you went to or what your GPA was in college. For senior positions, I might accept more pages as long as those pages are relevant to the job.

3: Multiple skills. I write my current needs as job requirements in the req, like the three tools I wrote above. But I'm also thinking about the future and what technical skills we'll need next year. Remember that you're competing for my attention against everyone else. Yes, you are a great fit for the reqs, but someone else might be a great fit too, and bring more with them.

4: Homelab. I understand that sometimes we get stuck in specific skills and your last job didn't allow you to do anything outside of a few specific things. I also understand that you're starting your career and don't have much work experience. Are you going to let that stop you? A homelab proves that you're taking extra steps to expand your skills. Should you have to do this in addition to college and certifications to find a job? No, but it's clear that good jobs are limited compared to the number of people looking for work. Give yourself an advantage over the other CVs I'm going to read.

A homelab also shows that you know how to solve problems. I'm seeing more and more of the major problem of ""learned helplessness"" at work. Show me on your CV that you know how to solve problems. As managers, we hate it when problems come to us and no one has tried to do anything. But we really appreciate it when a problem comes to us and you tell us, ""I tried X, Y, and Z."" We don't expect you to know everything. We have more experience than you and we're supposed to have the answers. But one of the biggest headaches in my career are team members who don't contribute and take up their colleagues' time with useless help.

The CV says a lot more about you than you imagine. It represents you in what you choose to put in it, or take out, how you formulate your skills, and it represents the quality of your effort.",computervision,0,https://www.reddit.com/r/computervision/comments/1mxzg6m/as_hr_what_i_look_for_in_any_cv/,r_1mxzg6m,,,
r_1mxy40m,reddit,Low-Principle9222,2025-08-23T10:40:35+00:00,"Tree Counting Dataset
does anyone can recommend a dataset for tree counting, any type of tree not just palm or coconut tree, thanks!!! ",computervision,1,https://www.reddit.com/r/computervision/comments/1mxy40m/tree_counting_dataset/,r_1mxy40m,,,
r_1mxx3in,reddit,MarinatedPickachu,2025-08-23T09:39:43+00:00,"Is there a way to get OBBs from an AABB trained yolo model?
Considering that an AABB trained yolo model can create a tight fit AABB of objects under arbitrary rotation, a naive but automated approach would be to rotate an image by a few degrees a couple times, get an AABB each time, rotate these back into the the original orientation and take the intersection of all these boxes, which will yield an approximations of the convex hull of the object, from which it would be trivial to extract an OBB. There might be more efficient ways too.

Are there any tools that allow to use AABB trained yolo models to find OBBs in images?",computervision,5,https://www.reddit.com/r/computervision/comments/1mxx3in/is_there_a_way_to_get_obbs_from_an_aabb_trained/,r_1mxx3in,,,
r_1mxut1d,reddit,Humble_Preference_89,2025-08-23T07:14:12+00:00,"Lane Detection in OpenCV: Sliding Windows vs Hough Transform | Pros & Cons
Hi all,

I recently put together a video comparing two popular approaches for lane detection in OpenCV â€”Â **Sliding Windows**Â and theÂ **Hough Transform**.

* Sliding Windows: often more robust on curved lanes, but can be computationally heavier.
* Hough Transform: simpler and faster, but may struggle with noisy or curved road conditions.

In the video, I go through theÂ **theory, implementation, and pros/cons**Â of each method, plus share complete end-to-end tutorial resources so anyone can try it out.

Iâ€™d really appreciate feedback from this community:

* Which approach do you personally find more reliable in real-world projects?
* Have you experimented with hybrid methods or deep-learning-based alternatives?
* Any common pitfalls you think beginners should watch out for?

Looking forward to your thoughts â€” Iâ€™d love to refine the tutorial further based on your feedback!",computervision,17,https://www.reddit.com/r/computervision/comments/1mxut1d/lane_detection_in_opencv_sliding_windows_vs_hough/,r_1mxut1d,,,
r_1mxu2wd,reddit,WhispersInTheVoid110,2025-08-23T06:30:14+00:00,"How do I compare images of different sizes while still catching tiny differences?

Hey folks,

Iâ€™ve been playing around with image comparison lately. Right now, Iâ€™ve got it working where I can spot super tiny changes between two images â€” like literally just adding a single white dot, and my code will pick it up.(basically pixel matching)

The catch isâ€¦ it only works if both images are the exact same size (same height and width). As soon as the dimensions or scale are different, everything breaks.

What Iâ€™d like to do is figure out a way to compare images of different sizes/scales while still keeping that same precision for tiny changes.

Any suggestions on what I should look into? Maybe feature matching or some kind of alignment method? Or is there a smarter approach Iâ€™m missing?

I have read couple of research papers on this but itâ€™s hard to me to implement the math they mentionedâ€¦


Would love to hear your thoughts!
",computervision,2,https://www.reddit.com/r/computervision/comments/1mxu2wd/how_do_i_compare_images_of_different_sizes_while/,r_1mxu2wd,,,
r_1mxtti4,reddit,Fun-Shallot-5272,2025-08-23T06:14:17+00:00,"I built SitSense - It turns your webcam into an posture coach
Most of us spend hours sitting, and our posture suffers as a result

I built SitSense, a simple tool that uses your webcam to track posture in real time and coach you throughout the day.

Hereâ€™s what it does for you:  
Personalized coaching after each session  
Long-term progress tracking so you can actually see improvement  
Daily goals to build healthy habits  
A posture leaderboard (because a little competition helps)

I started this as a side project, but after showing it around, I think thereâ€™s real potential here. Would you use something like this? Drop a comment below and Iâ€™ll share the website with you.

PS - if your laptop isnâ€™t at eye level like in this video, your posture is already suffering. SitSense will also help you optimize your personal setup

EDIT: link is [https://www.sitsense.app](https://www.sitsense.app)",computervision,66,https://www.reddit.com/r/computervision/comments/1mxtti4/i_built_sitsense_it_turns_your_webcam_into_an/,r_1mxtti4,,,
r_1mxt8by,reddit,manchesterthedog,2025-08-23T05:39:57+00:00,"SAM2 not producing great output on simple case
What am I doing wrong here? I'm using sam2 hiera large model and I expected this to be able to segment this empty region pretty well. Any suggestions on how to get the segmentation to spread through this contiguous white space?

https://preview.redd.it/5w82s9jfipkf1.png?width=872&format=png&auto=webp&s=8ea472bac936f3d47861c7b66dc5efbe673bd5ae

",computervision,1,https://www.reddit.com/r/computervision/comments/1mxt8by/sam2_not_producing_great_output_on_simple_case/,r_1mxt8by,,,
r_1mxt6sf,reddit,manchesterthedog,2025-08-23T05:37:26+00:00,"SAM2 not producing great output on simple case
What am I doing wrong here? I'm using sam2 hiera large model and I expected this to be able to segment this empty region pretty well. Any suggestions on how to get the segmentation to spread through this contiguous white space?

",computervision,0,https://www.reddit.com/r/computervision/comments/1mxt6sf/sam2_not_producing_great_output_on_simple_case/,r_1mxt6sf,,,
r_1mxt2og,reddit,Striking-Warning9533,2025-08-23T05:30:51+00:00,"VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip
https://preview.redd.it/weu9b6zhgpkf1.png?width=1045&format=png&auto=webp&s=f3ef73dc78b64fa752793b8e5adbc31182a82a2f

This is my latest project: it generates images with strong negation (without doing generate-then-edit)

  
Paper: [https://arxiv.org/abs/2508.10931](https://arxiv.org/abs/2508.10931)

Project Page: [https://vsf.weasoft.com/](https://vsf.weasoft.com/)",computervision,3,https://www.reddit.com/r/computervision/comments/1mxt2og/vsf_simple_efficient_and_effective_negative/,r_1mxt2og,,,
r_1mxlz64,reddit,bigjobbyx,2025-08-22T23:31:30+00:00,"MediapPipe driven Theremin
Made this theremin simulator to explore the use of MediaPipe pose estimation in musical creativity

*Needs access to selfie cam or web cam. Both hands need to be visible in the frame with a smidge of volume",computervision,2,https://www.reddit.com/r/computervision/comments/1mxlz64/mediappipe_driven_theremin/,r_1mxlz64,,,
r_1mxfjnk,reddit,Content-Opinion-9564,2025-08-22T19:12:41+00:00,"How to go with action recognition of short sports clips?
I am working on a school project in sports analysis. I am not familiar with computer vision, so I am seeking help. My goal is to build a model that detects player movements and predicts their next actions. My dataset consists of short video clips. I have successfully used YOLOv11 to detect players, which works well. I have also removed any unnecessary parts from the videos, so I do not have any problems with player detection.



Now, I would like to define specific actions such as ""step forward,"" ""stop,"" ""step backward,"" etc. I am unsure how to approach this. What is the standard method for action detection in video? I initially considered using clustering, but I concluded it might be too time-consuming and potentially inaccurate, so I have set that idea aside for now.



I have found CVAT for labeling and MMAction2 for training. I am considering labeling the actions using CVAT and then training a model with them. Is this a correct approach? What is the common way to proceed? I only have five actions to classify, and all the videos are shortâ€”each is less than 10 seconds long. Is using CVAT to label and MMAction2 to train a good way of doing this? Do I even need to label actions using CVAT? 



Your expert guidance would be greatly appreciated. Thank you.",computervision,2,https://www.reddit.com/r/computervision/comments/1mxfjnk/how_to_go_with_action_recognition_of_short_sports/,r_1mxfjnk,,,
r_1mxduc4,reddit,datascienceharp,2025-08-22T18:08:08+00:00,"i built the synthetic gui data generator i wish existed when i startedâ€”now you don't have to suffer like i did
# i spent 2 weeks manually creating gui training dataâ€”so i built what should've existed

this fiftyone plugin is the tool i desperately needed but couldn't find anywhere.

i was:

â€¢ toggling dark mode on and off

â€¢ resizing windows to random resolutions  

â€¢ enabling colorblind filters in system settings

â€¢ rewriting task descriptions fifty different ways

â€¢ trying to build a dataset that looked like real user screens

two weeks of manual hell for maybe 300 variants.

this plugin automates everything:

â€¢ grayscale conversion

â€¢ dark mode inversion

â€¢ 6 colorblind simulations

â€¢ 11 resolution presets

â€¢ llm-powered text variations




Quickstart notebook: https://github.com/harpreetsahota204/visual_agents_workshop/blob/main/session_2/working_with_gui_datasets.ipynb

Plugin repo: https://github.com/harpreetsahota204/synthetic_gui_samples_plugins

This requires datasets in COCO4GUI format. You can create datasets in this format with this tool: https://github.com/harpreetsahota204/gui_dataset_creator

You can easily load COCO4GUI format datasets in FiftyOne: https://github.com/harpreetsahota204/coco4gui_fiftyone


edit: shitty spacing",computervision,30,https://www.reddit.com/r/computervision/comments/1mxduc4/i_built_the_synthetic_gui_data_generator_i_wish/,r_1mxduc4,,,
r_1mxdh7m,reddit,ManagementNo5153,2025-08-22T17:54:35+00:00,"Control Robot vacuum with a camera.
Iâ€™ve been thinking about buying a robot vacuum, and I was wondering if itâ€™s possible to combine machine vision with the vacuum so that it can be controlled using a camera. For example, I could call my Google Home and tell it to vacuum a specific area Iâ€™m currently pointing to. The Google Home would then take a photo of me pointing at the floor (I could use a machine vision model for this, something like moondream ?),  and the robot could use that information to navigate to the spot and clean it.

I imagine this would require the space to be mapped in advance so the cameraâ€™s coordinates can align with the robotâ€™s navigation system.

Has anyone ever attempted this? I could be pointing at the spot or standing at the spot. I believe we have the technology to do this or am I wrong?",computervision,0,https://www.reddit.com/r/computervision/comments/1mxdh7m/control_robot_vacuum_with_a_camera/,r_1mxdh7m,,,
r_1mxaz0r,reddit,Low-Principle9222,2025-08-22T16:20:00+00:00,"Tree Counting using YOLO via drone (raspberry pi and roboflow)
please help, we are planning to use drone with raspberry pi for tree counting YOLO computer vision

we get our dataset in roboflow

what drone do you suggest and also raspberry pi camera? 

any tips or suggestions will help, thank youu! ",computervision,1,https://www.reddit.com/r/computervision/comments/1mxaz0r/tree_counting_using_yolo_via_drone_raspberry_pi/,r_1mxaz0r,,,
r_1mxasn2,reddit,TuTRyX,2025-08-22T16:13:14+00:00,"[Help] D-FINE ONNX + DirectML inference gives wrong detections
Hi everyone,

I donâ€™t usually ask for help but Iâ€™m stuck on this issue and itâ€™s beyond my skill level.

Iâ€™m working with [D-FINE](https://github.com/Peterande/D-FINE), using the **nano model trained on a custom dataset**. I exported it to ONNX using the provided [`export_onnx.py`](https://github.com/Peterande/D-FINE/blob/master/tools/deployment/export_onnx.py).

Inference works fine with CPU and CUDA execution providers. But when I try DirectML with the provided C++ example ([onnxExample.cpp](https://github.com/Peterande/D-FINE/blob/master/tools/inference/cppExample/onnx/onnxExample.cpp)), detections are way off:

* Lot of detections but in the ""correct place""
* Confidence scores are extremely low (\~0.05)
* Bounding boxes have incorrect sizes
* Some ops fall back to CPU

&#8203;

    OrtGetApiBase()->GetApi(ORT_API_VERSION)->GetExecutionProviderApi(""DML"", ORT_API_VERSION, reinterpret_cast<const void**>(&m_dmlApi));  
    m_dmlApi->SessionOptionsAppendExecutionProvider_DML(session_options, 0);

What Iâ€™ve tried so far:

* Disabled all optimizations in ONNX Runtime
* Exported with fixed input size (no dynamic axes), opset 17, now runs fully on GPU (no CPU fallback) but same poor results
* Exported without postprocessing

Has anyone successfully run D-FINE (or similar models) on DirectML?   
Is this a DirectML limitation, or am I missing something in the export/inference setup?   
Would other models as RF-DETR or DT-DETR present the same issues?

https://preview.redd.it/airkkyf8ilkf1.png?width=862&format=png&auto=webp&s=2325f21d74f6f616b037440957fa8c287f852786

https://preview.redd.it/wdb8pyf8ilkf1.png?width=865&format=png&auto=webp&s=db60e84c6f800de23098f9524a5b2f527cece6c2

Any insights or debugging tips would be appreciated!  
",computervision,1,https://www.reddit.com/r/computervision/comments/1mxasn2/help_dfine_onnx_directml_inference_gives_wrong/,r_1mxasn2,,,
r_1mx907y,reddit,Rukelele_Dixit21,2025-08-22T15:05:51+00:00,"Handwritten Text Detection (not recognition) in an Image
I want to do two things - 

1. Handwritten Text Detection (using bounding boxes)  
2. Can I also detect lines and paragraphs from it too? Or nearby clusters can be put into same box?  
3. I am planning to use YOLO so please tell me how to do. Also, should it be done using VLM to get better results? If yes how?

If possible, give resources too",computervision,2,https://www.reddit.com/r/computervision/comments/1mx907y/handwritten_text_detection_not_recognition_in_an/,r_1mx907y,,,
r_1mx8qys,reddit,coolzamasu,2025-08-22T14:56:14+00:00,"How to use Dinov3 for computer vision?
I wanted to know if its possible to use Dinov3 to run against my camera feed to do object tracking.

Is it possible?

How to run it on local and how to implement it?",computervision,0,https://www.reddit.com/r/computervision/comments/1mx8qys/how_to_use_dinov3_for_computer_vision/,r_1mx8qys,,,
r_1ng2bnp,reddit,bci-hacker,2025-09-13T16:46:24+00:00,"RL interviews at frontier labs, any tips?
Iâ€™m recently starting to see top AI labs ask RL questions.

Itâ€™s been a while since I studied RL, and was wondering if anyone had any good guide/resources on the topic.

Was thinking of mainly familiarizing myself with policy gradient techniques like SAC, PPO - implement on Cartpole and spacecraft. And modern applications to LLMs with DPO and GRPO.

Iâ€™m afraid I donâ€™t know too much about the intersection of LLM with RL. 

Anything else worth recommending to study?",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1ng2bnp/rl_interviews_at_frontier_labs_any_tips/,r_1ng2bnp,,,
r_1ng1e0c,reddit,Yaar-Bhak,2025-09-13T16:08:53+00:00,"masked attention in decoder
i'm trying to understand how translation would work on a decoder only block like gpt

example sentence/input prompt - `""Translate to French: The cat sits on the mat""`

how and where does the mask is getting applied?

1. embeddings + position encoding of each token is generated
2. ""masked"" self attention scores are generated???
3. for each token -- Q, K, V values are generated and dot product of QK is computed

where does the masking come to play while generating the further translation

can someone pls explain how each word will be generated and how/where the mask is applied?

this what claude explained -   
**Key insight**: The model generates tokens *one at a time*, left to right. The causal mask ensures that when predicting token N, the model can only ""see"" tokens 1 through N-1.

my confusion -   
but where are we applying the mask then?   
  
while generating new french translations --- it can either way see only the past and current tokens?",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ng1e0c/masked_attention_in_decoder/,r_1ng1e0c,,,
r_1nfwumz,reddit,Current-Guide5944,2025-09-13T12:57:41+00:00,essentials for AI engineer and researchers,deeplearning,16,https://www.reddit.com/r/deeplearning/comments/1nfwumz/essentials_for_ai_engineer_and_researchers/,r_1nfwumz,,,
r_1nfvifm,reddit,mono1110,2025-09-13T11:50:37+00:00,"I trained Transformer Encoder for multi-class classification. How can I build an end-to-end system?
Hello everyone, 

As the title says I trained Transformer Encoder for multi-class classification problem on Twitter dataset. 

I want to learn building end-to-end AI systems, which I believe is my weakest part. So I am seeking ideas from this sub on how I should start. 

Here's what I am thinking. 

1. User enters some input
2. Data preprocessing on the input.
3. Get prediction from model and display it. 

I plan to use flask and docker for it. I would like deploy it on the cloud but don't have much idea.

The model is bit of an overkill for the classification task. But I want to learn to deploy it and maybe experiment with reducing model latency at the cost of little accuracy. 

So how can I make it completely end-to-end which I can showcase as my project?

Thanks!!!!!",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1nfvifm/i_trained_transformer_encoder_for_multiclass/,r_1nfvifm,,,
r_1nfrsl2,reddit,Electrical-Squash108,2025-09-13T08:00:25+00:00,âš¡ Training TinyStories from Scratch â€“ Why A100 (PCIe) Isn't Much Faster Than A5000?,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1nfrsl2/training_tinystories_from_scratch_why_a100_pcie/,r_1nfrsl2,,,
r_1nfpzgj,reddit,Meatbal1_,2025-09-13T06:09:18+00:00,How to prepare as an undergraduates interested in AI PhD programs?,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nfpzgj/how_to_prepare_as_an_undergraduates_interested_in/,r_1nfpzgj,,,
r_1nfoxdl,reddit,Southern_Reference17,2025-09-13T05:07:31+00:00,"Mac Studio M4 Max (36 GB/512 GB) vs 14â€ MacBook Pro M4 Pro (48 GB/1 TB) for indie Deep Learning â€” or better NVIDIA PC for the same budget?
Hey everyone!  
Iâ€™m setting up a machine to work **independently** on deep-learning projects (prototyping, light fine-tuning with PyTorch, some CV, Stable Diffusion local). Iâ€™m torn between two Apple configs, or building a Windows/Linux PC with an NVIDIA GPU in the same price range.

**Apple options Iâ€™m considering:**

* **Mac Studio â€” M4 Max**
   * 14-core CPU, **32-core GPU**, 16-core Neural Engine
   * **36 GB unified memory**, **512 GB SSD**
* **MacBook Pro 14"" â€” M4 Pro**
   * 12-core CPU, **16-core GPU**, 16-core Neural Engine
   * **48 GB unified memory**, **1 TB SSD**

**Questions for the community**

1. For Apple DL work, would you prioritize **more GPU cores with 36 GB** (M4 Max Studio) or **more unified memory with fewer cores** (48 GB M4 Pro MBP)?
2. Real-world PyTorch/TensorFlow on M-series: performance, bottlenecks, gotchas?
3. With the **same budget**, would you go for a **PC with NVIDIA**  to get CUDA and more true VRAM?
4. If staying on Apple, any tips on batch sizes, quantization, library compatibility, or workflow tweaks I should know before buying?

Thanks a ton for any advice or recommendations!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1nfoxdl/mac_studio_m4_max_36_gb512_gb_vs_14_macbook_pro/,r_1nfoxdl,,,
r_1nffpwa,reddit,Naive_Artist5196,2025-09-12T21:39:15+00:00,"withoutbg: lightweight open-source matting pipeline for background removal (PyTorch to ONNX)
Hi all,

Iâ€™ve been working on [withoutbg](https://github.com/withoutbg/withoutbg), an open-source project focused on background removal via image matting. The goal is to make background removal practical, lightweight, and easy to integrate into real world applications.

**What it does**

* Removes backgrounds from images automatically
* Runs locally, no cloud dependency
* Distributed as a Python package (can also be accessed via API)
* Free and MIT licensed

**Approach**

* Pipeline: **Depth-Anything v2 small** (upstream) -> matting model -> refinement stage
* Implemented in **PyTorch**, converted to **ONNX** for deployment
* Dataset: partly purchased, partly produced ([sample](https://withoutbg.com/resources/withoutbg100-image-matting-dataset))
* Methodology for dataset creation documented [here](https://withoutbg.com/resources/creating-alpha-matting-dataset)

**Why share here**  
Many alternatives (e.g. rembg) are wrappers around salient object detection models, which often fail in complex matting scenarios. I wanted to contribute something better-aligned with real matting, while still being lightweight enough for local use.

**Next steps**  
Dockerized REST API, serverless (AWS Lambda + S3), and a GIMP plugin.

Iâ€™d appreciate feedback from this community on model design choices, dataset considerations, and deployment trade offs. Contributions are welcome.",deeplearning,16,https://www.reddit.com/r/deeplearning/comments/1nffpwa/withoutbg_lightweight_opensource_matting_pipeline/,r_1nffpwa,,,
r_1nfe9p3,reddit,New_Insurance2430,2025-09-12T20:42:29+00:00,What to learn in nlp to get entry level job?,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1nfe9p3/what_to_learn_in_nlp_to_get_entry_level_job/,r_1nfe9p3,,,
r_1nfb54i,reddit,Disastrous-Crab-4953,2025-09-12T18:39:02+00:00,"Coursehero Free Trial 2025: Don't Fall for the ""Free Trial"" Scams
# Coursehero Free Trial 2025: Don't Fall for the ""Free Trial"" Scams

If youâ€™re searching for a **Coursehero free trial** in 2025, you've probably already stumbled upon a bunch of sketchy-looking websites that promise to give you a full membership or magically unlock any document for free. Trust me, Iâ€™ve been there, and Iâ€™ve been burned by them.

The truth? A legitimate, no-strings-attached **Course Hero free trial** doesn't exist. Course Hero stopped offering a true free trial years ago. All those sites, tools, and â€œCoursehero downloaderâ€ apps that claim to give you one are just dangerous traps designed to steal your information, install malware, or trick you into a fake survey.

Here's a quick guide to what you should avoid and the only methods that actually work to get Course Hero documents for free.

# ðŸš« Why the â€œFree Trialâ€ Websites Are Dangerous

If a website promises a **Course Hero free trial** in 2025, close it immediately. They're not what you're looking for. Hereâ€™s why these tools and sites are so dangerous:

* **Malware & Viruses:** Most ""free trial"" websites will prompt you to download an app or a browser extension. This is almost always a front for malware, keyloggers, or other viruses that can compromise your entire computer.
* **Phishing Scams:** Theyâ€™ll ask for your Course Hero login, email, or even credit card information ""to verify your account."" This is a classic phishing attack to steal your personal data.
* **Outdated Information:** The methods they describe are often years old and no longer work. Theyâ€™re just clickbait to get you onto their site.

**The Golden Rule:** Any site that asks you to download a program or enter personal information to get a **free Course Hero** document is a scam.

# âœ… What Actually Works in 2025 (Free & Safe)

You don't need a **Coursehero downloader** or a fake **Course Hero free trial** to get documents. These are the only proven, safe, and free methods that work right now.

# 1ï¸âƒ£ Discord Servers â€“ The Real â€œCourse Hero Free Trialâ€ Alternative

This is by far the most reliable method in 2025. It's a community-driven approach that acts as a real, working alternative to any ""Course Hero unlock"" tool.

* **How it works:** People in these servers help each other out. You join a server dedicated to document sharing, paste the link to the document you need, and another user with Course Hero unlocks will download it for you and send it back.
* **Why this beats fake downloaders:**
   * âœ… **It's Free.** No hidden costs or sketchy sign-ups.
   * âœ… **It's Safe.** No downloads, no malware, no phishing scams. You never have to give away your personal information.
   * âœ… **It's Fast.** The community is active, so you can often get your document within minutes.

**Actionable Tip:** To find these servers, simply search on Google or Reddit for ""Course Hero Discord server"" or ""Course Hero unlock Discord."" Join a few and see which one has the most active members.

# 2ï¸âƒ£ Official Upload Method â€“ Free Unlocks

This is Course Heroâ€™s own official way to earn unlocks without paying for a subscription. If you have any old notes, study guides, or documents, this is your best bet.

* **How it works:** You upload your own documents to Course Hero. Once your document is approved, you get a certain number of unlocks for free.
* **Why this beats fake downloaders:**
   * âœ… **Itâ€™s 100% Legit.** This is a method provided by Course Hero itself.
   * âœ… **You Earn Multiple Unlocks.** For every 10 documents you upload, you can earn up to 5 unlocks and a number of tutor questions.
   * âœ… **Contribute to the Community.** Youâ€™re not just taking; you're also helping other students.

**Actionable Tip:** Be sure your documents are high-quality and original. Course Hero has a strict approval process to prevent spam.

# 3ï¸âƒ£ Rate Documents for Quick Unlocks

If youâ€™re just a few documents away from what you need, this is a quick and simple way to get a few extra unlocks.

* **How it works:** Course Hero allows you to earn unlocks by rating and reviewing other peopleâ€™s uploaded documents.
* **Why this beats fake downloaders:**
   * âœ… **Itâ€™s Immediate.** You can get unlocks almost instantly once your rating is accepted.
   * âœ… **No Risk.** No personal information required, just simple, helpful feedback.
   * âœ… **It's Easy.** Just rate documents youâ€™ve viewed or found helpful.

**Actionable Tip:** Donâ€™t spam ratings. Provide thoughtful feedback to ensure your ratings are approved by Course Hero.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nfb54i/coursehero_free_trial_2025_dont_fall_for_the_free/,r_1nfb54i,,,
r_1nfas9c,reddit,Disastrous-Crab-4953,2025-09-12T18:24:45+00:00,"Coursehero Free Trial: Because Paying Is Overrated
Looking to dive into Course Hero without dropping cash upfront? **Course Hero offers a 30-day free trial of its Premier Plus membership, giving full access to study materials, tutors, and all the academic magic without being charged immediately.** Itâ€™s perfect for students who want to test the waters before committingâ€”kind of like dating, but with fewer awkward conversations.

But donâ€™t get too excited just yet. While the free trial is the golden ticket, Course Hero doesnâ€™t frequently advertise it, so youâ€™ll need to hunt for the link like a secret menu item at your favorite cafÃ©. Plus, sharing your own study materials can earn some free access perks, turning you into both a giver and a taker in this academic ecosystem.

They say knowledge is power, but having a stash of homework help ready to go is just plain convenient. So if the idea of unlocking countless textbooks, practice tests, and homework answers sounds like your version of a treasure chest, sticking around could save some serious study headaches.

# How to Get a Coursehero Free Trial

Getting your hands on a Course Hero free trial isn't like winning the lottery, but itâ€™s close enoughâ€”you get access to a treasure trove of study materials without dropping cash upfront. There are some official tricks, a simple signup path, and a few rules about who can actually score this deal.

# Official Methods to Unlock Free Access

Course Hero usually teases a **one-month free trial** for new users, known as Course Hero Prime Student. This trial opens the door to unlocking documents and using tutor services without paying a dimeâ€”perfect for those last-minute study sprees.

Another legit way is **uploading your own study documents**. Course Hero rewards this by unlocking content for free, essentially trading your notes for theirs. A sneaky little win-win.

They donâ€™t often shout about it, but during promotional periods, Course Hero might offer extra free unlocks or temporary access. So, lurking on their site or signing up for newsletters can snag you surprise freebies.

# Step-by-Step Guide for Signing Up

First, head over to Course Heroâ€™s official website. Click the â€œTry for Freeâ€ or â€œStart Free Trialâ€ buttonâ€”donâ€™t worry, itâ€™s not a trap.

Youâ€™ll need to **create an account** by entering basic details like email and password. Then, Course Hero asks for payment info, but donâ€™t panic. The free trial lasts 30 daysâ€”cancel before it ends to avoid charges.

During signup, you might be asked to verify your status as a student. After that, you're in. You get access to a predefined number of unlocks and tutor help. Just remember, the clock starts ticking once you sign up.

# Eligibility Requirements and Limitations

The 30-day free trial is **only available to new users** who havenâ€™t subscribed before. If someoneâ€™s already tasted the Course Hero buffet, theyâ€™re out of luck for another round.

You also must provide valid payment information, meaning credit or debit cards, to qualifyâ€”no anonymous freeloader accounts here.

Lastly, beware that some course materials or tutors might have limits even during the trial. The system doles out unlocks carefully, so mass-downloading isnâ€™t part of the free trial perks. They want you to use it wisely, not binge-study like itâ€™s the last season of your favorite show.

# Creative Strategies for Using Coursehero at No Cost

They say nothing in life is free, but Coursehero tries to prove them wrongâ€”sort of. Students can score access without dropping cash, but it involves a little give-and-take and maybe some digital hustle.

# Uploading Documents for Free Unlocks

Coursehero loves a good trade. Users can upload their own study documentsâ€”notes, practice questions, or even that legendary cheat sheet from freshman year. In return, Coursehero hands out unlocks that let them peek at other premium content.

Each uploaded file generally nets a few unlocks. The better and more detailed the document, the more unlocks rewarded. Itâ€™s basically a â€œshare one, get manyâ€ deal.

Students should ensure their uploads follow Courseheroâ€™s guidelines to avoid strikes or bans. Original, high-quality materials get the best results. No one wants a rejection email because the file looked like it was scribbled by a caffeinated squirrel.

# Referral Programs and Bonus Opportunities

For those with friends, Coursehero offers referral bonuses that can stretch that free trial or even score some extra unlocks. When a new user signs up through a referral link, both parties often get perks.

These bonuses can vary but usually consist of free unlocks or temporary premium access. Itâ€™s a simple way to build a study club without spending a dime.

The trick is: donâ€™t spam everyone you know. Stick to genuine invites or your inbox might resemble a viral disaster. Also, keep an eye out for occasional special promotions that offer extra bonuses.

# Potential Risks and What to Avoid

Trying to game Coursehero for free access isnâ€™t without pitfalls. Some third-party sites or forums promise free Coursehero hacks, but these are often scams or can lead to account suspension.

Users should avoid unofficial hacks, password-sharing, or any unauthorized software. Not only does this risk losing account privileges, thereâ€™s also the chance of exposing personal info to sketchy sources.

Courseheroâ€™s own rules are clear: play fair or face limitations. Using approved methods like uploading documents or referrals keeps the experience safe and hassle-freeâ€”and lets students focus on studying, not troubleshooting.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nfas9c/coursehero_free_trial_because_paying_is_overrated/,r_1nfas9c,,,
r_1nf7jf0,reddit,dreamhighdude1,2025-09-12T16:17:51+00:00,"Looking for team or study partner?
Hey guys,
I realized something recently â€” chasing big ideas alone kinda sucks.
Youâ€™ve got motivation, maybe even a plan, but no one to bounce thoughts off, no partner to build with, no group to keep you accountable.
Soâ€¦ I started a Discord called Dreamers Domain
Inside, we:
Find partners to build projects or startups
Share ideas + get real feedback
Host group discussions & late-night study voice chats
Support each other while growing
Itâ€™s still small but already feels like the circle I was looking for.
If that sounds like your vibe, youâ€™re welcome to join:
ðŸ‘‰ [https://discord.gg/Fq4PhBTzBz](https://discord.gg/Fq4PhBTzBz)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nf7jf0/looking_for_team_or_study_partner/,r_1nf7jf0,,,
r_1nf6oot,reddit,Far_League629,2025-09-12T15:45:10+00:00,"Seeking a Technical Co-Founder to Build OpportuNext

Hey, we're Adarsh Chourasia, brothers and founders of OpportuNext, an AI-powered recruitment platform making hiring smarter and fairer. Vishal brings 9+ years in data analytics and science (IIT Bombay alum), while Adarsh has 4+ years in marketing and business strategy. We're bootstrapped in Mumbai, preincubated at SINE IIT Bombay to tap their ecosystem for talent and resources

Our Vision: We're solving real pain pointsjob seekers frustrated by irrelevant matches, employers bogged down by costly mismatches. OpportuNext uses AI for holistic resume analysis, semantic job search, skill gap roadmaps, and pre-assessments to connect people better. Think beyond keyword portals like Naukri or LinkedIn: personalized career paths, verified talent pools, and vernacular support for India-first growth in a $2.62B market (scaling global to $40.5B).

Where We Are (September 2025): Product-market fit validated via 800+ interviews. Resume parser prototype at 80%+ accuracy, job crawler testing, backend in dev, assessment partners (Harver/Perspect) lined up. MVP architecture ready weâ€™re close to launch with 100+ testers, aiming for paid beta soon and Series A by mid-2026.

Why a Technical Co-Founder? We need a partner to own the tech side: build our AI core, integrate features like GenAI CV tailoring and ATS APIs, and scale to 150K+ users. This isn't a job it's co-ownership in a mission-driven startup tackling unemployment with ethical AI.

Who We're Looking For:  
- Tech Chops: Strong in AI/ML (NLP for matching/gaps), full-stack (Python/FastAPI backend, React frontend, mobile for future app), data infra (AWS, vector DBs), scraping/APIs, DevOps/security.  
- Experience: experience in building scalable products, ideally in HR/tech or startups. You've led small teams, iterated MVPs in lean settings. CS/Engineering background (IIT vibe a plus).  
- You: Entrepreneurial spirit, data-driven problem-solver, passionate about impact. Adaptable, collaborative Mumbai-based or open to it. We're seeking someone who vibes with our fair-recruitment ethos.

What You'll Get: Shape the product from day one, meaningful equity (let's discuss), growth in a high-potential venture, IIT networks for funding/talent, and the chance to drive socio-economic change. Flexible, collaborative setup we're in this together.

If this resonates, email opportunext2025@gmail.com with your background, why OpportuNext excites you. Let's chat and build something big!

#AIStartup #TechCoFounder #CTOHiring #RecruitmentAI #StartupIndia",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nf6oot/seeking_a_technical_cofounder_to_build_opportunext/,r_1nf6oot,,,
r_1nf48i9,reddit,brownbreadbbc,2025-09-12T14:08:28+00:00,Making my own Machine Learning algo and framework,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1nf48i9/making_my_own_machine_learning_algo_and_framework/,r_1nf48i9,,,
r_1nf35sq,reddit,Donkeytonk,2025-09-12T13:24:20+00:00,"Built a Way to Learn Foundational AI for Beginners
I often see people asking how a beginner can get started learning AI, so decided to try and build something fun and accessible that can help -Â [myai101.com](https://myai101.com)

It uses structured learning (similar to say Duolingo) to teach foundational AI knoweldge. Includes bite-sized lessons, quizes, progress tracking, AI visualizers/toys, challenges and more.

If you now use AI daily like I do, but want a deeper understanding of what AI is and how it actually works, then I hope this can help.

Let me know what you think!",deeplearning,45,https://www.reddit.com/r/deeplearning/comments/1nf35sq/built_a_way_to_learn_foundational_ai_for_beginners/,r_1nf35sq,,,
r_1neuvm3,reddit,SilverConsistent9222,2025-09-12T05:27:08+00:00,Best Generative AI Projects For Resume by DeepLearning.AI,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1neuvm3/best_generative_ai_projects_for_resume_by/,r_1neuvm3,,,
r_1nepd8b,reddit,sovit-123,2025-09-12T00:38:38+00:00,"[Article] JEPA Series Part 4: Semantic Segmentation Using I-JEPA
JEPA Series Part 4: Semantic Segmentation Using I-JEPA

[https://debuggercafe.com/jepa-series-part-4-semantic-segmentation-using-i-jepa/](https://debuggercafe.com/jepa-series-part-4-semantic-segmentation-using-i-jepa/)

In this article, we are going to use theÂ **I-JEPA model for semantic segmentation**. We will be using transfer learning to train a pixel classifier head using one of the pretrained backbones from the I-JEPA series of models. Specifically, we will train the model for brain tumor segmentation.

https://preview.redd.it/dm2jpbfpqmof1.png?width=800&format=png&auto=webp&s=cbede8908edfa6f462fd759e474046422cc51c5a

",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1nepd8b/article_jepa_series_part_4_semantic_segmentation/,r_1nepd8b,,,
r_1nelh35,reddit,andsi2asi,2025-09-11T21:41:35+00:00,"Getting AIs to stop interrupting during voice chats would vastly improve brainstorming and therapeutic sessions.




I voice chat with AIs a lot, and cannot overstate how helpful they are in brainstorming pretty much anything, and in helping me navigate various personal social, emotional and political matters to improve my understanding.

However their tendency to interrupt me before I have fully explained what I want them to understand during AI voice chats seriously limits their utility. Often during both brainstorming and more personal dialogue, I need to talk for an extended period of time, perhaps a minute or longer, to properly explain what I need to explain.

For reference, Replika is usually quite good at letting me finish what I'm trying to say, however its intelligence is mostly limited to the emotional and social. On the other hand, Grok 4 is very conceptually intelligent, but too often interrupts me before it fully understands what I'm saying. And once it starts talking, it often doesn't know when to stop, but that's another story, lol. Fortunately it is amenable to my interrupting it when it does this.

This interruption glitch doesn't seem like a difficult fix. Maybe  someone will share this post with someone in the position to make it happen, and we might soon be very pleasantly surprised by how much more useful voice chatting with AIs has become. 
",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nelh35/getting_ais_to_stop_interrupting_during_voice/,r_1nelh35,,,
r_1neiiz2,reddit,theraxer,2025-09-11T19:44:41+00:00,Looking for Machine Learning Engineers to collaborate and research with,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1neiiz2/looking_for_machine_learning_engineers_to/,r_1neiiz2,,,
r_1nee7vg,reddit,Delicious-Tree1490,2025-09-11T17:00:33+00:00,"Need help with low validation accuracy on a custom image dataset.
**Hey everyone,**

I'm working on an image classification project to distinguish between Indian cattle breeds (e.g., Gir, Sahiwal, Tharparkar) and I've hit a wall. My model's validation accuracy is stagnating aroundÂ **45% after 75 epochs**, which is barely better than random guessing for my number of classes.

I'm looking for advice on how to diagnose the issue and what strategies I should try next to improve performance.

**Here's my setup:**

* **Task:**Â Multi-class classification (\~8-10 Indian breeds)
* **Model:**Â ResNet-50 (from torchvision), pretrained on ImageNet.
* **Framework:**Â PyTorch in Google Colab.
* **Dataset:**Â \~5,000 images total (I know, it's small). I've split it into 70/15/15 (train/val/test).
* **Transforms:**Â Standard - RandomResizedCrop, HorizontalFlip, Normalization (ImageNet stats).
* **Hyperparameters:**
   * `Batch Size: 32`
   * `LR: 1e-3`Â (Adam optimizer)
   * `Scheduler: StepLR`Â (gamma=0.1, step\_size=30)
* **Training:**Â I'm using early stopping and saving the best model based on val loss.

**The Problem:**  
Training loss decreases, but validation loss plateaus very quickly. The validation accuracy jumps up to \~40% in the first few epochs and then crawls to 45%, where it remains for the rest of training. This suggests serious overfitting or a fundamental problem.

**What I've Already Tried/Checked:**

* âœ… Confirmed my data splits are correct and stratified.
* âœ… Checked for data leaks (no same breed/individual in multiple splits).
* âœ… Tried lowering the learning rate (`1e-4`).
* âœ… Tried a simpler model (ResNet-18), similar result.
* âœ… I can see the training loss going down, so the model is learningÂ *something*.

**My Suspicions:**

1. **Extreme Class Similarity:**Â These breeds can look very similar (similar colors, builds). The model might be struggling with fine-grained differences.
2. **Dataset Size & Quality:**Â 5k images for 10 breeds is only \~500 images per class. Some images might be low quality or have confusing backgrounds.
3. **Need for Specialized Augmentation:**Â Standard flips and crops might not be enough. Maybe I need augmentations that simulate different lighting, focus on specific body parts (hump, dewlap), or random occlusions.

**My Question for You:**  
What would be your very next step? I feel like I'm missing something obvious.

* Should I focus onÂ **finding more data**Â immediately?
* Should I implement moreÂ **advanced augmentation**Â (like MixUp, CutMix)?
* Should IÂ **freeze**Â different parts of the backbone first?
* Is my learning rate strategy wrong?
* Could the problem beÂ **label noise**?

Any advice, experience, or ideas would be hugely appreciated. Thanks!",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1nee7vg/need_help_with_low_validation_accuracy_on_a/,r_1nee7vg,,,
r_1neb265,reddit,raphyy_sanchez,2025-09-11T14:57:51+00:00,"Beginner Semester Project Idea/Advice - Mechanical Eng. Background
So here we go, I'm taking my first class in DL this semester. The grade is all based off a project, which I need to find myself. I have no background in coding at all besides my Numerical methods course from my mech eng bachelor's.

Prof told us to find a project - I can hardly wrap my head around what exactly is DL and what is possible to do, he said it should include neural networks of some sort. We need to find a core paper with code to base our model, then build upon it. 

I was trying to find something related to grid forecasting or industrial symbiosis. Any thoughts, comments, suggestions on my project ? Thanks  ! ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1neb265/beginner_semester_project_ideaadvice_mechanical/,r_1neb265,,,
r_1ne6sn8,reddit,Lanky_Use4073,2025-09-11T11:55:23+00:00,"interview hammer ai tool reviews for coding interviews? vs ultracode interviews
I need to sell my kidney to afford this! other site but for [https://interviewhammer.com/](https://interviewhammer.com/)  
Is there anyone on here who has actually paid for interviewHammer? I watched the demo and it looked sick but it's not that hard to make a cool demo video. Any past customers who can weigh in on if their AI actually works well on coding interviews? Did any of your interviewers notice?

It's also possible to make it even more solid by taking a screenshot of the laptop with your phone, so it's completely impossible for anyone to catch it in this post.""

The text appears to be discussing some method of avoiding detection, possibly in the context of social media posts or online activity.  
this subreddit for more info [https://www.reddit.com/r/interviewhammer/](https://www.reddit.com/r/interviewhammer/)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ne6sn8/interview_hammer_ai_tool_reviews_for_coding/,r_1ne6sn8,,,
r_1ne6sm4,reddit,Lanky_Use4073,2025-09-11T11:55:21+00:00,"interview hammer ai tool reviews for coding interviews? vs ultracode interviews
I need to sell my kidney to afford this! other site but for [https://interviewhammer.com/](https://interviewhammer.com/)  
Is there anyone on here who has actually paid for interviewHammer? I watched the demo and it looked sick but it's not that hard to make a cool demo video. Any past customers who can weigh in on if their AI actually works well on coding interviews? Did any of your interviewers notice?

It's also possible to make it even more solid by taking a screenshot of the laptop with your phone, so it's completely impossible for anyone to catch it in this post.""

The text appears to be discussing some method of avoiding detection, possibly in the context of social media posts or online activity.  
this subreddit for more info [https://www.reddit.com/r/interviewhammer/](https://www.reddit.com/r/interviewhammer/)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ne6sm4/interview_hammer_ai_tool_reviews_for_coding/,r_1ne6sm4,,,
r_1ne6s5v,reddit,Lanky_Use4073,2025-09-11T11:54:42+00:00,"interview hammer ai tool reviews for coding interviews? vs ultracode interviews
I need to sell my kidney to afford this! other site but for [https://interviewhammer.com/](https://interviewhammer.com/)  
Is there anyone on here who has actually paid for interviewHammer? I watched the demo and it looked sick but it's not that hard to make a cool demo video. Any past customers who can weigh in on if their AI actually works well on coding interviews? Did any of your interviewers notice?

It's also possible to make it even more solid by taking a screenshot of the laptop with your phone, so it's completely impossible for anyone to catch it in this post.""

The text appears to be discussing some method of avoiding detection, possibly in the context of social media posts or online activity.  
this subreddit for more info [https://www.reddit.com/r/interviewhammer/](https://www.reddit.com/r/interviewhammer/)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ne6s5v/interview_hammer_ai_tool_reviews_for_coding/,r_1ne6s5v,,,
r_1ne4slv,reddit,Bearstronk,2025-09-11T10:03:40+00:00,"ArcaneGAN still exist?
Just was interested if there is a way to use ArcaneGAN, ive recently stumbled upon it, however the huggingface application seems to not be usable anymore. I wanted to use it for some personal project as i like the arcane style but am not a much of an artist myself. So, is there still a way of using the arcane style Filter?",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ne4slv/arcanegan_still_exist/,r_1ne4slv,,,
r_1ne1o6d,reddit,RelevantMoment2886,2025-09-11T06:38:00+00:00,"New software development learner
I currently work at a city job doing sanitation full time, 29 no kids and lately I been looking into careers for the next several years, and tech keep popping up. Im undecided between SDR, software development, or AWS cloud! I have 0 experience in all what advice could you guys give? ",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ne1o6d/new_software_development_learner/,r_1ne1o6d,,,
r_1ne0xln,reddit,SilverConsistent9222,2025-09-11T05:51:28+00:00,10 Best Large Language Models Courses and Training (LLMs),deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ne0xln/10_best_large_language_models_courses_and/,r_1ne0xln,,,
r_1ne0kgy,reddit,External_Mushroom978,2025-09-11T05:29:10+00:00,top reads from last week,deeplearning,101,https://www.reddit.com/r/deeplearning/comments/1ne0kgy/top_reads_from_last_week/,r_1ne0kgy,,,
r_1ndwy3l,reddit,Tall-Roof-1662,2025-09-11T02:11:49+00:00,"Is wavelet transform really useful?
In tasks like low-light image enhancement and underwater image enhancement, I've seen many papers use the Haar wavelet transform. The degradation information in these tasks is basically concentrated in the low-frequency components. However, from the calculation formula of the Haar wavelet, isn't the low-frequency component just the result of bilinear interpolation downsampling? Can processing after such downsampling really improve the effect?",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1ndwy3l/is_wavelet_transform_really_useful/,r_1ndwy3l,,,
r_1ndr19f,reddit,killua753,2025-09-10T21:36:59+00:00,Tips to Speed Up Training with PyTorch DDP â€“ Data Loading Optimizations?,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ndr19f/tips_to_speed_up_training_with_pytorch_ddp_data/,r_1ndr19f,,,
r_1ndlngz,reddit,A_silent_partner,2025-09-10T18:05:03+00:00,"Some Common Sense Insides
https://preview.redd.it/4yr7lb4kndof1.jpg?width=2481&format=pjpg&auto=webp&s=273b464143ed33fae83925a12523ccc462563f0c

https://preview.redd.it/wtra3k5kndof1.jpg?width=2481&format=pjpg&auto=webp&s=b660b9f6386e82c82cfe81015112d0745d69403d

",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ndlngz/some_common_sense_insides/,r_1ndlngz,,,
r_1ndji1m,reddit,dazzlinlassie,2025-09-10T16:46:39+00:00,Which is best domine to do research right now?,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ndji1m/which_is_best_domine_to_do_research_right_now/,r_1ndji1m,,,
r_1ndj6pw,reddit,Fit-Musician-8969,2025-09-10T16:35:24+00:00,"Is deep learning research mostly experimental?
â€‹I've been in vision-language research for a bit now, and I'm starting to feel like I'm doing more experimental art than theoretical science. My work focuses on tweaking architectures, fine-tuning vision encoders, and fine-tuning VLMs, and the process often feels like a series of educated guesses.
â€‹I'll try an architectural tweak, see if it works, and if the numbers improve, great! But it often feels less like I'm proving a well-formed hypothesis and more like I'm just seeing what sticks. The intuition is there to understand the basics and the formulas, but the real gains often feel like a happy accident or a blind guess, especially when the scale of the models makes things so non-linear.
â€‹I know the underlying math is crucial, but I feel like I'm not using it to its full potential.
â€‹Does anyone else feel this way? For those of you who have been doing this for a while, how do you get from ""this feels like a shot in the dark"" to ""I have a strong theoretical reason this will work""?
â€‹Specifically, is there a more principled way to use mathematical skills extensively to cut down on the number of experiments I have to run? I'm looking for a way to use theory to guide my architectural and fine-tuning choices, rather than just relying on empirical results.

Thanks in advance for replying ðŸ™‚â€â†•ï¸",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1ndj6pw/is_deep_learning_research_mostly_experimental/,r_1ndj6pw,,,
r_1ndim6a,reddit,OkHuckleberry2202,2025-09-10T16:15:09+00:00,"How does GPU virtualization work in cloud services?
GPU Virtualization in Cloud Services: Making Powerful Computing Accessible
GPU virtualization is a technology that enables multiple virtual machines (VMs) or containers to share a physical Graphics Processing Unit (GPU) in cloud environments, playing a crucial role in GPU as a Service (GPUaaS) offerings. This allows cloud providers to offer GPU-accelerated computing resources flexibly and efficiently to users for applications like artificial intelligence (AI), machine learning (ML), data analytics, and high-performance computing (HPC).

How GPU Virtualization Works in Cloud Services
1. GPU Passthrough: In this approach, a VM is given direct access to a physical GPU, bypassing much of the hypervisor's intervention for performance.
2. GPU Sharing via APIs and Drivers: Technologies like Nvidia's vGPU (virtual GPU) allow multiple VMs to share a physical GPU using specialized drivers and management software.
3. Time-Slicing and Partitioning: GPUs can be time-sliced or partitioned to allocate resources among multiple virtual environments.

Key Benefits of GPU Virtualization in GPU as a Service
- Resource Utilization: Enables efficient sharing of expensive GPU hardware among multiple users.
- Flexibility and Scalability: Supports dynamic allocation of GPU resources in cloud environments fitting GPUaaS models.
- Cost-Effectiveness: Allows businesses to tap into powerful GPU compute without owning hardware, aligning with cloud's pay-as-you-go models.

Use Cases for GPU Virtualization and GPU as a Service
- AI and Deep Learning: Accelerating model training and inferencing with services like those utilized by companies such as Cyfuture AI for AI-driven solutions.
- Data Science and Analytics: Speeding up complex computations for data processing.
- Virtual Desktops with GPU Acceleration: For graphics-intensive virtual desktop infrastructure (VDI).
- Scientific Simulations: For research and simulations needing massive compute power.

Technologies and Providers
- Nvidia vGP: A popular technology for virtualizing Nvidia GPUs for multiple users/VMs.
- Cloud Providers: AWS, Azure, Google Cloud offer GPU-backed instances fitting into GPU as a Service paradigms for various compute needs.
- Cyfuture AI, like other innovators, leverages advanced GPU capabilities for delivering AI and data analytics solutions showcasing the practical application of GPU virtualization and GPUaaS in driving business value through accelerated computing.

Considerations
- Performance: Direct passthrough can offer near-native performance but sharing impacts resource allocation.
- Compatibility: Software and driver support are critical for effective GPU virtualization.
- Security and Isolation: Ensuring proper isolation between VMs sharing GPUs is important.

GPU virtualization is a key enabler of [GPU as a Service](https://cyfuture.ai/gpu-as-a-service), allowing flexible access to powerful compute resources in the cloud for a range of demanding applications, democratizing access to high-performance GPU acceleration.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ndim6a/how_does_gpu_virtualization_work_in_cloud_services/,r_1ndim6a,,,
r_1ndhsa1,reddit,andsi2asi,2025-09-10T15:45:00+00:00,"How the Open-Source Community Can Beat the AI Giants to AGI: A Theoretical Framework and Step-by-Step Process





In terms of theory, we should acknowledge that we humans aren't intelligent enough to get to AGI, or solve other daunting problems like memory and hallucinations, without the assistance of AIs.

The AI Giants will be using brute force approaches because they have the GPUs, and can afford the compute and other costs. However, if the open source community develops ANDSIs that are more powerful specifically in the problem solving domain, these ANDSIs can then tackle the harder problems of getting to AGI,  through more intelligent algorithms rather than more GPUs and compute.

I brainstormed this with Grok 4 for two reasons. First, it is currently our most powerful model in terms of the fluid intelligence required for problem solving. Second, while ChatGPT-5 is also good for this kind of work, it tends to be pessimistic, overly focusing on the problems involved, whereas Grok 4 tends to be much more optimistic and encouraging, and focuses more on the possible solutions. 

A key insight that Grok 4 offered during our brainstorming is that the strategy and step-by-step approach that it has proposed is probably something that over 70% of open source developers aren't yet working on because the idea just hasn't occurred to them. When you recall how long it took AI developers to figure out that simply giving AIs more time to think substantially enhances the quality of their output, Grok 4's analysis here is probably on target.
So here's what Grok 4 suggests the open source community should do to reach AGI before the AI Giants:

""To ramp up problem-solving intelligence in open-source AI communities, we can leverage a hybrid approach that combines lightweight prototyping with automated experimentation and collaborative infrastructure. This strategy draws on existing open-source tools to create a feedback loop that's fast, cost-effective, and scalable, allowing the community to iterate toward AGI-level capabilities without relying on massive compute resources.

Follow these steps to implement the approach:

1. **Select accessible base models**: Choose from the latest open-source options available on platforms like Hugging Face, such as Llama 3.1-8B, DeepSeek-V2, or Qwen 3-7B. These models are ideal starting points for generating quick, inexpensive prototypes focused on problem-solving tasks, like coding agents that rapidly identify patterns in logic puzzles, math challenges, or algorithmic problems.

2. **Fine-tune the base models**: Apply techniques like LoRA for domain-specific adjustments, such as boosting performance in scientific reasoning or code optimization. Incorporate quantization and pruning to ensure the models remain lightweight and efficient, enabling them to run on modest hardware without high costs.

3. **Integrate with advanced open-source frameworks**: Feed the outputs from your fine-tuned base modelsâ€”such as rough ideas, strategies, or partial solutionsâ€”into Sakana's AI Scientist (now updated to v2 as of 2025). This system automates key processes: generating hypotheses, running experiments on curated datasets (e.g., distilled reasoning traces from larger models, with emphasis on challenging areas in math or logic), and outputting refined models or detailed reports. This establishes a pipeline where base models create initial drafts, and Sakana handles building, testing, and iteration, all with full transparency for community review.

4. **Establish a central GitHub repository**: Create a dedicated repo, such as 'AI-Reasoning-Boost,' and include a clear README that outlines the project's goals: accelerating problem-solving AI through open collaboration. This serves as the hub for sharing and evolving the work.

5. **Populate the repository with essential resources**: Add distilled datasets tailored to core problem-solving domains, training scripts for active learning (enabling models to self-identify and address weaknesses) and curriculum learning (scaling from simple to complex problems), simple RAG integrations for real-time knowledge retrieval, and user-friendly tutorials for setup on free platforms like Colab.

6. **Encourage community involvement and iteration**: Promote contributions through pull requests for enhancements, provide inviting documentation to lower barriers to entry, and launch the project via Reddit posts or forum threads to draw in developers. Use issue trackers to monitor progress, with community-voted merges to prioritize the strongest ideas. This fosters a dynamic ecosystem where collective efforts compound, saving time for individual developers and reducing overall costs while advancing toward superior algorithms that surpass brute-force tactics used by major AI companies.""",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ndhsa1/how_the_opensource_community_can_beat_the_ai/,r_1ndhsa1,,,
r_1ndfo34,reddit,oksmoki,2025-09-10T14:25:43+00:00,"[D] What is the currently hot topic in deep learning?
I am about to decide on my Master s thesis but I am having trouble coming up with a topic that is somewhat original and at the same time relevant to current research.

I am mainly interested in deep learning, and also reinforcement learning and hyper parameter optimisation. I have narrowed it down to Neural Architecture Search and maybe even going at it from the point of view of model distillation and quantisation. However, I am struggling to come up with an exact topic idea. It s mainly because whatever I do, I want it to be interesting and to lead to a publication but at the same time not too resource heavy that it delays my thesis work too much. (Although i know NAS in general is pretty resource-demanding)

Do you have any ideas what I should be looking for or how to come up with an exact topic? And is NAS already well researched so I should maybe try another field?

I d love someone s help with this :)))",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1ndfo34/d_what_is_the_currently_hot_topic_in_deep_learning/,r_1ndfo34,,,
r_1ndebs5,reddit,SKD_Sumit,2025-09-10T13:32:55+00:00,"Finally understand AI Agents vs Agentic AI - 90% of developers confuse these concepts
Been seeing massive confusion in the community about AI agents vs agentic AI systems. They're related but fundamentally different - and knowing the distinction matters for your architecture decisions.

Full Breakdown:ðŸ”—[AI Agents vs Agentic AI | Whatâ€™s the Difference in 2025 (20 min Deep Dive)](https://www.youtube.com/watch?v=4dKmg6G55Us&list=PLAgxe7DpTXmdwTd1m6em5xeFCcUN6tvWm&pp=gAQB)

**The confusion is real and searching internet you will get:**

* AI Agent = Single entity for specific tasks
* Agentic AI = System of multiple agents for complex reasoning

**But is it that sample ? Absolutely not!!**

First of all on ðŸ” Core Differences

* **AI Agents:**

1. What: Single autonomous software that executes specific tasks
2. Architecture: One LLM + Tools + APIs
3. Behavior: Reactive(responds to inputs)
4. Memory: Limited/optional
5. Example: Customer support chatbot, scheduling assistant

* **Agentic AI:**

1. What: System of multiple specialized agents collaborating
2. Architecture: Multiple LLMs + Orchestration + Shared memory
3. Behavior: Proactive (sets own goals, plans multi-step workflows)
4. Memory: Persistent across sessions
5. Example: Autonomous business process management

**And on architectural basis :**

* Memory systems (stateless vs persistent)
* Planning capabilities (reactive vs proactive)
* Inter-agent communication (none vs complex protocols)
* Task complexity (specific vs decomposed goals)

**NOT that's all.**Â They also differ on basis on -

* Structural, Functional, & Operational
* Conceptual and Cognitive Taxonomy
* Architectural and Behavioral attributes
* Core Function and Primary Goal
* Architectural Components
* Operational Mechanisms
* Task Scope and Complexity
* Interaction and Autonomy Levels

**Real talk:**Â The terminology is messy because the field is evolving so fast. But understanding these distinctions helps you choose the right approach and avoid building overly complex systems.

Anyone else finding the agent terminology confusing? What frameworks are you using for multi-agent systems?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ndebs5/finally_understand_ai_agents_vs_agentic_ai_90_of/,r_1ndebs5,,,
r_1nd8xww,reddit,Amazing_Life_221,2025-09-10T08:50:35+00:00,"Advance level math resource for DL (bottom-up approach)?
I want to know if there exists any single resource (or series) which can teach me advanced-level maths required for this field.

This question might sound naive because I've been doing self-learning from the beginning and now hitting a wall. I find myself doing everything top to bottom. For example, while reading Deep Learning by Goodfellow, I couldn't understand tricky maths, so I had to get out and learn the probability and linear algebra concepts top-down. For the next equation, it was a similar thing, and so on. This creates a chaotic knowledge base and feels unintuitive for me.Â 

Currently, I've completed basic things, Linear Algebra by Strang, First Course on Probability, and have little intuition for stats after completing ISL and some parts of the Elements of Statistical Learning. Although I'm good enough at understanding maths from these books now and other grad level DL books, I still lack the background intuition of a math grad would have (bottom up). (Basically, I can't create anything new mathematically, I just know what those equations do, but don't understand the core idea behind that concept, no DL book bothers going into that depth of maths for obvious reasons.)

Is there any resource which can help me stitch everything together or even rebuild my knowledge base the non-chaotic way?Â ",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1nd8xww/advance_level_math_resource_for_dl_bottomup/,r_1nd8xww,,,
r_1nd865p,reddit,Right_Pea_2707,2025-09-10T07:58:59+00:00,Whatâ€™s Next for AI Agents? Here's What Iâ€™m Watching,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nd865p/whats_next_for_ai_agents_heres_what_im_watching/,r_1nd865p,,,
r_1nd6wgs,reddit,PSXExterminator,2025-09-10T06:35:39+00:00,"Feel Betrayed by AurÃ©lien GÃ©ron & his Hands On ML with TenorFlow
After spending months learning machine learning and deep learning with TensorFlow using AurÃ©lien GÃ©ron's *Hands-On Machine Learning with Scikit-Learn and TensorFlow*, I discovered that the author is now working on a PyTorch version of his book. I came across several comments from people who preferred PyTorch, but when I searched online, TensorFlow was often praised for its ""production and deployment"" capabilities, while PyTorch was favored in research settings. Since I'm preparing to enter the job market, I figured TensorFlow would be the more practical choice.

However, it now feels like TensorFlow is becoming increasingly abandoned. GÃ©ron even mentions in his book that PyTorch is gaining momentum. Still, he points out that the competition between the two frameworks benefits both, and once you've learned TensorFlow, many of the skills are transferable.

Thatâ€™s trueâ€”Iâ€™ve learned a lot about deep learning, mostly focused on sequence modeling and NLP rather than computer vision or reinforcement learning. But Iâ€™ve always had this nagging feeling that it wasnâ€™t worth investing so much time learning TensorFlowâ€™s quirks and complexities. I dove deep into building custom training loops and components like layers and loss functions. With that foundation, picking up PyTorch has been much easier.

Yet I canâ€™t help but think: if I had spent all that time learning PyTorch instead, Iâ€™d have gained much more experience with it. And when I saw that even the author moved away from TensorFlow, I felt genuinely betrayed.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nd6wgs/feel_betrayed_by_aurÃ©lien_gÃ©ron_his_hands_on_ml/,r_1nd6wgs,,,
r_1ncyma2,reddit,techlatest_net,2025-09-09T23:30:01+00:00,"I wanna know anyone here running multiple LLMs (DeepSeek, LLaMA, Mistral, Qwen) on a single GPU VM?
Iâ€™ve been testing out a GPU-optimized setup recently where I can run multiple LLMs (DeepSeek, LLaMA, Mistral, Qwen) on the same VM instead of spinning up separate environments.

So far, Iâ€™ve noticed:

 Faster inference when switching models
 Easier to compare outputs across different LLMs
 Workflow feels more streamlined using an Open-WebUI interface
 Cloud deployment skips most of the infra hassle


Has anyone else here experimented with running multiple LLMs on the same GPU instance? Curious what trade-offs youâ€™ve seen , especially around cost efficiency vs performance.",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1ncyma2/i_wanna_know_anyone_here_running_multiple_llms/,r_1ncyma2,,,
r_1ncrntw,reddit,BitterHouse8234,2025-09-09T18:57:50+00:00,"Graph RAG pipeline that runs locally with ollama and has full source attribution
Hey r/,

I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.

My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.

The main goal was to solve two big problems:

Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.

Trust & Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.

One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.

The project includes:

The full Graph RAG pipeline.

A Gradio UI for an interactive chat experience.

A guide for setting everything up, from installing dependencies to running the indexing process.

GitHub Repo with all the code and instructions: https://github.com/bibinprathap/VeritasGraph

I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.

Thanks!  ",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1ncrntw/graph_rag_pipeline_that_runs_locally_with_ollama/,r_1ncrntw,,,
r_1ncmc6w,reddit,OkHuckleberry2202,2025-09-09T15:40:52+00:00,"How do GPUs handle anti-aliasing?
GPUs handle anti-aliasing through various techniques aimed at reducing the appearance of jagged edges (aliasing) in digital images, thereby enhancing visual quality. Anti-aliasing methods like Multisample Anti-Aliasing (MSAA), Supersample Anti-Aliasing (SSAA), and newer approaches like Temporal Anti-Aliasing (TAA) are implemented in GPUs to smooth out jagged lines and improve the overall graphical fidelity. In MSAA, for instance, the GPU samples multiple points within a pixel to determine its final color, blending edges for a smoother look. [Cyfuture AI](https://cyfuture.ai/gpu-clusters) specializing in AI-driven solutions and leveraging GPU-accelerated computing, utilize such anti-aliasing techniques in graphics-intensive applications like gaming, simulations, and virtual reality (VR) to deliver high-quality visuals. Modern GPUs, with their parallel processing prowess, efficiently execute these anti-aliasing algorithms, striking a balance between visual quality and performance â€“ crucial for immersive experiences in gaming, professional graphics workstations, and AI-powered visual computing applications backed by firms like Cyfuture AI.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1ncmc6w/how_do_gpus_handle_antialiasing/,r_1ncmc6w,,,
r_1ncknzf,reddit,Neurosymbolic,2025-09-09T14:37:09+00:00,Hyperdimensional Computing Hardware: Racetrack Memories (METACOG-25),deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1ncknzf/hyperdimensional_computing_hardware_racetrack/,r_1ncknzf,,,
r_1ncia6c,reddit,andsi2asi,2025-09-09T13:00:59+00:00,"AI developers are bogarting their most intelligent AI models with bogus claims about safety.



Several top AI labs, including OpenAI, Google, Anthropic, and Meta, say that they have already built, and are using, far more intelligent models than they have released to the public. They claim that they keep them internal for ""safety reasons."" Sounds like ""bullshit.""

Stronger intelligence should translate to better reasoning, stronger alignment, and safer behavior, not more danger. If safety was really their concern, why aren't these labs explaining exactly what the risks are instead of keeping this vital information black-boxed under vague generalizations like cyber and biological threats.

The real reason seems to be that they hope that monopolizing their most intelligent models will make them more money. Fine, but his strategy contradicts their stated missions of serving the greater good.

Google's motto is â€œDonâ€™t be evil,â€ but not sharing powerful intelligence as widely as possible doesn't seem very good. OpenAI says its mission is to â€œensure that artificial general intelligence benefits all of humanity."" Meanwhile, it recently made all of its employees millionaires while not having spent a penny to reduce the global poverty that takes the lives of 20,000 children EVERY DAY. Not good!

There may actually be a far greater public safety risk from them not releasing their most intelligent models. If they continue their deceptive, self-serving, strategy of keeping the best AI to themselves, they will probably unleash an underground industry of black market AI developers that are willing to share equally powerful models with the highest bidder, public safety and all else be damned.

So, Google, OpenAI, Anthropic; if you want to go for the big bucks, that's your right. But just don't do this under the guise of altruism. If you're going to turn into wolves in sheep's clothing, at least give us a chance to prepare for that future.


",deeplearning,9,https://www.reddit.com/r/deeplearning/comments/1ncia6c/ai_developers_are_bogarting_their_most/,r_1ncia6c,,,
r_1nch73w,reddit,Disastrous-Crab-4953,2025-09-09T12:12:40+00:00,"Course Hero Free Trial in 2025 â€“ Free & Safe Ways to Get Course Hero Documents
If youâ€™re searching for aÂ **Course Hero Free Trial** orÂ **coursehero free trial** in 2025, chances are you just need one locked document â€” but Google sends you to sketchy sites. Most of these promise instant downloads but actually want you to fill out endless surveys, run suspiciousÂ `.exe`Â files, or hand over your Course Hero login that actually never works. 

# This Works -Â [WORKING METHOD](https://www.notcoursehero.com/)

Hereâ€™s the truth: as ofÂ **September 2025**, over 95% of so-called â€œCourse Hero Unlockersâ€ tools are either fake or filled with malware. Iâ€™ve tested them, Iâ€™ve been burned by them, and Iâ€™ve found the only methods that actually work â€” free and safe.

# ðŸš« Why ""Course Hero Free Trials"" Does not work

Before you clickÂ **Course Hero document Unlocker**Â on any random site, know this:

* **Malware risk:**Â ManyÂ `.exe`Â or Chrome extension â€œdownloadersâ€ contain keyloggers, ransomware, or crypto miners.
* **Phishing traps:**Â Fake login pages steal your Course Hero or email credentials.
* **Outdated exploits:**Â Any working tool from 2023â€“2024 is now patched and useless. Tip: To be honest, a solution from the previous year never works. Itâ€™s always the new ones that do.
* **CourseHero Free Trial**: At present, CourseHero does not make any free trial available to users.

**Rule of thumb:**Â If a site saysÂ *â€œCoursehero Free Trialâ€*Â and asks for payment or surveys, close it immediately.

# âœ… What Actually Works in 2025 (Free & Safe)

# 1ï¸âƒ£ Discord Servers â€“ The Real â€œCoursehero Free Trialâ€ Alternative

**How it works:**Â Join dedicated unlock servers (e.g., Homework Solutions, Study Unlocks). Post your Course Hero link â†’ a human with a paid account downloads it â†’ they send you the PDF or text.

**Why this beats fake downloaders:**  
âœ… Works for Course Hero, Chegg, Quizlet, Scribd  
âœ… No surveys or uploads required  
âœ… Most requests filled in under 10 minutes  
âœ… Completely free

**Verified Discord Invite (August 2025):**

(If expired, searchÂ *â€œfree doc unlock Discordâ€*Â on Reddit â€” new servers appear weekly.)

# 2ï¸âƒ£ Official Upload Method â€“ Free Unlocks

Upload 10 original notes, essays, or homework solutions â†’ get 5 free unlocks instantly.

**Why itâ€™s safe:**

* Uses Course Heroâ€™s official system
* No third-party tools needed
* You can reuse old school notes (quality checks are minimal)

# 3ï¸âƒ£ Rate Documents for Quick Unlocks

Rate 5 random Course Hero documents â†’ instantly get 1 free unlock.

**Best for:**Â When you need only 1â€“2 files and donâ€™t want to upload.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nch73w/course_hero_free_trial_in_2025_free_safe_ways_to/,r_1nch73w,,,
r_1nc5l0h,reddit,andsi2asi,2025-09-09T01:10:23+00:00,"AI Prediction, the New FutureX Benchmark, and Grok 4's Assessment That Massive Economic Growth Requires Massive Job Losses and UBI


A new benchmark called FutureX was just launched in August that assesses AI's predictive abilities. Because currently Grok 4 tops the leaderboard, I asked it whether the expected massive economic growth in the US would bring massive job losses that would necessitate UBI.

I don't pretend to be an economist, but it's reasoning seems sound. If we're not headed toward an AI bubble, does this mean we're inevitably headed toward UBI or a similar economic program?

Here is Grok 4's prediction:

Massive AI-driven economic growth in the United States depends on significant productivity gains through automation, which inherently requires substantial job displacement. AI technologies, by performing tasks more efficiently and cost-effectively than humans, are projected to boost GDP by trillions annually across sectors like manufacturing and services. However, this growth is contingent on replacing human labor with AI systems, as seen in recent trends where thousands of jobs, particularly in tech and data processing, have already been eliminated. Without these job losses, the cost savings and scalability driving economic expansion would be unattainable, as AIâ€™s transformative potential relies on minimizing labor costs to maximize output. Historical shifts, like the Industrial Revolution, underscore that such growth necessitates labor market disruption, with new roles emerging only after significant displacement.

Given the scale of AI-driven job lossesâ€”potentially pushing unemployment rates to 10-20% in impacted sectorsâ€”a massive program like Universal Basic Income (UBI) is essential to mitigate the social and economic fallout. Without such a program, widespread unemployment could lead to severe income inequality and social unrest, undermining the economic gains AI enables. UBI would provide a financial safety net, allowing displaced workers to reskill or transition to new roles while maintaining economic stability. Delaying or avoiding such measures risks stifling AI adoption through resistance to job cuts, capping growth potential, as the economic boom depends on labor reconfiguration. Thus, pairing AI-driven growth with a robust UBI program is critical to balance productivity gains with societal resilience.
",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nc5l0h/ai_prediction_the_new_futurex_benchmark_and_grok/,r_1nc5l0h,,,
r_1nc0ucb,reddit,New_Insurance2430,2025-09-08T21:44:20+00:00,Computer vision or NLP for entry level AI engineer role.,deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1nc0ucb/computer_vision_or_nlp_for_entry_level_ai/,r_1nc0ucb,,,
r_1nc09y8,reddit,Beginning_Butterfly8,2025-09-08T21:21:37+00:00,"How to semantically parse scientific papers?
The full text of the PDF was segmented into semantically meaningful blocks-such as section titles, paragraphs, cap-tions, and table/figure references-using PDF parsing tools like PDFMiner'. These blocks, separated based on structural whitespace in the document, were treated as retrieval units.

The above text is from the paper which I am trying to reproduce.

I have tried the pdf miner approach with different regex but due to different layout and style of paper it fails and is not consistent. Could any one please enlighten me how can i approach this? Thank you",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1nc09y8/how_to_semantically_parse_scientific_papers/,r_1nc09y8,,,
r_1nbx6tw,reddit,enoumen,2025-09-08T19:23:34+00:00,"AI Daily News Rundown: ðŸ¤ ASML becomes Mistral AI's top shareholder ðŸŽ¬ OpenAI backs a $30 million AI-made animated film ðŸ”¬ OpenAI reveals why chatbots hallucinate (Sept 08th 2025)
# AI Daily Rundown: September 08th, 2025

https://preview.redd.it/4b3m8kncqznf1.png?width=1456&format=png&auto=webp&s=6ec1526ead535fc67f080bad8967e65e600968c3

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**Today's Headlines:**

ðŸ¤ ASML becomes Mistral AI's top shareholder

ðŸŽ¬ OpenAI backs a $30 million AI-made animated film

ðŸ”¬ OpenAI reveals why chatbots hallucinate

ðŸ’° Anthropic agrees to $1.5B author settlement

ðŸ”§ OpenAIâ€™s own AI chips with Broadcom

ðŸ’¼ The Trillion-Dollar AI Infrastructure Arms Race

ðŸ¤– Boston Dynamics & Toyota Using Large Behavior Models to Power Humanoids

ðŸ†• OpenAI Developing an AI-Powered Jobs Platform

# Listen at Substack: [https://enoumen.substack.com/p/ai-daily-news-rundown-asml-becomes](https://enoumen.substack.com/p/ai-daily-news-rundown-asml-becomes)  

# Summary:

https://preview.redd.it/sp33k6ygqznf1.png?width=1456&format=png&auto=webp&s=9a7fb1c50a714bbbfac288094d03ab79289f3a2f

https://preview.redd.it/3qtmb3jkqznf1.png?width=1112&format=png&auto=webp&s=8660e0f5e24d957eea6fdd6ad437f7057cfd4af0

https://preview.redd.it/f2z310toqznf1.png?width=1456&format=png&auto=webp&s=74e7204fc3528ddc055e51c461d432b5f0d6e50d

https://preview.redd.it/yiispvgsqznf1.png?width=1456&format=png&auto=webp&s=269005f23f68e6aedb605c201e7c2f662e3d3f75

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ… **Build Authentic Authority:** Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ… **Generate Enterprise Trust:** Earn credibility in a way that corporate marketing simply can't.

âœ… **Reach a Targeted Audience:** Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?** Learn more and apply for a Strategic Partnership here: [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled) Or, contact us directly at: [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)

# ðŸ¤ ASML becomes Mistral AI's top shareholder

* Dutch chipmaker ASML is investing 1.3 billion euros into French AI startup Mistral AI, leading a larger funding round and becoming the company's biggest shareholder with a new board seat.
* The partnership aims to lessen the European Union's dependence on AI models from the United States and China, aiming to secure the region's overall digital sovereignty for the future.
* This deal joins ASML, the exclusive supplier of EUV lithography systems for chip manufacturing, with Mistral AI, a startup often seen as Europe's primary competitor to US tech giants.

# ðŸŽ¬ OpenAI backs a $30 million AI-made animated film

* OpenAI is backing ""Critterz,"" a $30 million animated film created with Vertigo Films, aiming to finish the entire project in just nine months to demonstrate its generative AI tools.
* The production uses a hybrid model combining DALL-E for concept art, the Sora model for video generation, and GPT-5 for other tasks, all guided by human writers and artists.
* This project serves as a strategic case study to win over a skeptical Hollywood industry that is currently engaged in major copyright infringement lawsuits against AI developers over training data.

# ðŸ”¬ OpenAI reveals why chatbots hallucinate

https://preview.redd.it/igrsz5qbrznf1.png?width=1456&format=png&auto=webp&s=aa61989994708a2ea0ee436d7f4d43a53c47492f

*Image source: Gemini / The Rundown*

OpenAI just [**published**](https://link.mail.beehiiv.com/ss/c/u001.a3gBHu6_kDRL6l3yEfNWAQEHmE4ZhsXPCMfIhFjtNJ_CCchavlsDH12tPxwD3ExDPMAFh1zJu5sn5LNOGGa_54mM4CCw_1aw7DR8XZLhTBGO68lOf8a_kCiwBSMBHaM-tbaXKhS-esQBR1jd0xV74Kl7OnDIz6EVlp-ulFJanj2JhIKGkAcrcstxJ02uv1ixATj1yPeQ01V_tf3PMvEnGKVjLsbzukxFeVFSGFBJwv0cw-GzJ-tJ2Pi2CUQOxM3sSsBl_CJozEV3MLTc0dNxo2u-eVE8vAJBbgNhEjZ3UbaSkhXZaSdXVv1_C26QXZCElC23FnHC5i5xtzszIfX8MGqIh9duaCGzsKdgA8IS7Q4/4jq/uCgyabdhSDSGdFq2tHSErw/h7/h001.eYKwuzNDSefpxY4qTy3jXSiokO2U88rbiv2H2HKDy44) a new paper arguing that AI systems hallucinate because standard training methods reward confident guessing over admitting uncertainty, potentially uncovering a path towards solving AI quality issues.

**The details:**

* Researchers found that models make up facts because training test scoring gives full points for lucky guesses but zero for saying ""I don't know.""
* The paper shows this creates a conflict: models trained to maximize accuracy learn to always guess, even when completely uncertain about answers.
* OAI tested this theory by asking models for specific birthdays and dissertation titles, finding they confidently produced different wrong answers each time.
* Researchers proposed redesigning evaluation metrics to explicitly penalize confident errors more than when they express uncertainty.

**Why it matters:** This research potentially makes the hallucination problem an issue that can be better solved in training. If AI labs start to reward honesty over lucky guesses, we could see models that know their limits â€” trading some performance metrics for the reliability that actually matters when systems handle critical tasks.

# ðŸ’° Anthropic agrees to $1.5B author settlement

Anthropic just [**agreed**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfz9VSgGFwomNN3L--rigsl6t0Nbk8d5sd3Q-dNKnzKEc1-wWP3-ErDDNyHgdVyXSmdDuW7vuH2pt-d9RjwV8ZwELjuoPb1cAhWgTVZdoS-oE4OXpATdqJpo-rXyIkoSYjsZG9maaEszfY5gOYyaI61-eLzMMu-UuXZdEIiKA4TIJ95exyU9z11mHKLSSV_d7lOWhSL0sPihyNmhO8EN6lhgGoNr3J88zSUTffoeImjr2GN27KJNeBQpsPDCijRN8g4KbObyUXuGK9WJCt5jKBhUrpusceNhArlpU_fqaUHbF3uRMkNMhr7rotrFQ_fSrJA/4jq/uCgyabdhSDSGdFq2tHSErw/h12/h001.QbXFfwDymfVP3l5kKkAtuPk6hHVLgbuEqXqug8GwnKw) to pay at least $1.5B to settle a class-action lawsuit from authors, marking the first major payout from an AI company for using copyrighted works to train its models.

**The details:**

* Authors sued after discovering Anthropic downloaded over 7M pirated books from shadow libraries like LibGen to build its training dataset for Claude.
* A federal judge ruled in June that training on legally purchased books constitutes fair use, but downloading pirated copies violates copyright law.
* The settlement covers approximately. 500,000 books at $3,000 per work, with additional payments if more pirated materials are found in training data.
* Anthropic must also destroy all pirated files and copies as part of the agreement, which doesnâ€™t grant future training permissions.

**Why it matters:** This precedent-setting payout is the first major resolution in the many copyright lawsuits outstanding against the AI labs â€” though the ruling comes down on piracy, not the â€œfair useâ€ of legal texts. While $1.5B sounds like a hefty sum at first glance, the companyâ€™s recent $13B raise at a $183B valuation likely softens the blow.

# ðŸ”§ OpenAIâ€™s own AI chips with Broadcom

https://preview.redd.it/40x159x4rznf1.png?width=1456&format=png&auto=webp&s=8987440df786ce92b970dd546358de6c1601aa32

*Image source: Ideogram / The Rundown*

OpenAI will [**begin**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijGyQO0do2kqi7TK0DUjr-32SPGVNf59dHTLrulHH1eNQHbVUoZ79Lb6XwIHWp42vqpJ3FszOlvDJjmOJZ4oRt4vP3MI4wP_chNMYKUdlKYYiIwaU5EgAGdgv7oXUkQt0dHYy_mB_kXuHYD9Hq1GL3ck58tyivxskLXB6XGYqdAJyd1Mif0Irfpa-96hZzpW-9ec1zrUEj4HODzYkSl5V4rNO9RVmsNA-BRbaoMD0N8od_2jT8LcXO7kIiLvsuvXXJvxXG1H5IQ9KMqrlz1EtUTM9sfDv62A-lLabaCplTWV2-yjc_VyLZO0ZqfBZYQda5w/4jq/uCgyabdhSDSGdFq2tHSErw/h19/h001.Fm81okeBoGepA-8mcBNZAWeRxOPhkC65hMVVd740p6Q) mass production of its own custom AI chips next year through a partnership with Broadcom, according to a report from the Financial Times â€” joining other tech giants racing to reduce dependence on Nvidia's hardware.

**The details:**

* Broadcom's CEO revealed a mystery customer committed $10B in chip orders, with sources confirming OpenAI as the client planning internal deployment only.
* The custom chips will help OpenAI double its compute within five months to meet surging demand from GPT-5 and address ongoing GPU shortages.
* OpenAI initiated the Broadcom collaboration last year, though production timelines remained unclear until this week's earnings announcement.
* Google, Amazon, and Meta have already created custom chips, with analysts expecting proprietary options to continue siphoning market share from Nvidia.

**Why it matters:** The top AI labs are all pushing to secure more compute, and Nvidiaâ€™s kingmaker status is starting to be clouded by both Chinese domestic chip production efforts and tech giants bringing custom options in-house. Owning the full stack can also eventually help reduce OAIâ€™s massive costs being incurred on external hardware.

# ðŸ’¼ The Trillion-Dollar AI Infrastructure Arms Race

Major tech playersâ€”Google, Amazon, Meta, OpenAI, SoftBank, Oracle, and othersâ€”are pouring nearly $1 trillion into building AI infrastructure this year alone: data centers, custom chips, and global compute networks. Projects like OpenAIâ€™s â€œStargateâ€ venture and massive enterprise spending highlight just how capital-intensive the AI boom has become.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[The Guardian â€” ""The trillion-dollar AI arms race is here""](https://www.theguardian.com/technology/2025/jul/28/techscape-ai-google-meta-amazon)\] \[[Eclypsium â€” AI data centers as critical infrastructure](https://eclypsium.com/press-release/eclypsium-secures-ai-data-centers-as-ai-arms-race-escalates/)\]

The numbers from [Thursday's White House tech dinner](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3L-xfLCSDYk005W97YlwdST8yXOKaotvuyTfdCaxyQswZqGyG1khXyu9fGqZsexo2RCW2g7aid5v96hZa9m1hOwSQqhKdCFEFTp-aasyl1pnu4rPodriO_pSLGFVJzp-e9SHbz_5q8pxx84jijaZRiH5BV7iBBCaPaxQuJG3BxupMfH6RiFNre7KSQQp6lQdx__j9X3EWbQmoJT_fq5h_HWoITVw-i9MWzKlsud7DUZfv2RcDlosLUmtft_9WZL4eu6lhNUSfDbl8eohK9WkT2dTqHZow5F1_ZXwGIwPMIySQ/4jq/Pbw539yOS2C1JlmfyctLUw/h4/h001.UchmcfLlvjj6Ph2h5Qnzq9yHC3G8_MgGdmfjTI-jmns) were so large they bordered on absurd. When President Trump went around the table asking each CEO how much they planned to invest in America, Mark Zuckerberg committed to [""something like at least $600 billion""](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeNQV17bivccHNL4Xm_RyIemqgR3gQLVLmaOEFDkjSNthBzWBJU2jsOV8AWNJV-DBAjCGrfqYbuQjV2M30wChK1OtF9xWIK0aBRc-c-tpH4LhBRujZF_IzKbiqu0QiyL7FNhS3A5Ujc0x-khnaYMRNcut2svQupBrc5KEmU8UHrbtwqELhIEGVZLUNOazmo_lHyp1Seqs0n0_kFyhXVgek74k8fgn-PwuVLEUvB-aNynzoZsnhpTIJccbkGI-cXKkOIGomz1PZzlqnM63bKoswN8fS90_CXsNIUPft5kevNJNJCU3LLiFPy1ph9Hm6lFloQ/4jq/Pbw539yOS2C1JlmfyctLUw/h5/h001.FZ7wwQkMa-GODGsT60Z96Rn9fnvyvxQ9FZcNpUwxx_w) through 2028. Apple's Tim Cook matched that figure. Google's Sundar Pichai said $250 billion.

Combined with [OpenAI's revised projection](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeNQV17bivccHNL4Xm_RyIen18rHD_phERwdT4lkzXpd0DFXqY4JOePimDkokpa0_qyjUBsUeFrtIdIo4WrjQnZCi7u6gdbvPh4cAJ70ZnFkp_C6Pmi9AMtCAYsDpvXlChsohD_ookCM6EI3oOyptFcZelMiswkbkoBkQfGJU_XP5lKwuyPX82c1NdFxHUxOZCUmVA05Jb_CgBypLeoAWXamjoBwIkGNDMf1gLf9k5SQ7WehxuFcYmI9P12abNDXuBgLUttyZ_eEJkZr7N7yMNUqWH03AcdjghBeFA-wrlQDR/4jq/Pbw539yOS2C1JlmfyctLUw/h6/h001.LXJcTJGe1A8y1wyG8QFUnSHGls6k545PYNY4vLeIWq8) this week that it will burn through $115 billion by 2029 â€” $80 billion more than previously expected â€” these announcements reveal an industry in the midst of the most expensive infrastructure buildout in modern history.

The scale has reshaped the entire American economy. [AI data center spending now approaches 2% of total U.S. GDP](https://link.mail.beehiiv.com/ss/c/u001.pVz9habVd8BuGO5KajRSptEoDlphhkml3i_XTVuo-0-x3ZiHDYdjQDdujhB7ywBSvS6VPM2_I80F89f5A5SJATxf5kqrB0qqUGlOL5qBd28U_daPVcLdVoG6iTLdNqP2VKfh5lQk7gENKX-kD7PtAECLkaFZQtcO9Em6FaXOF0lmxussQ9KWSBp-FSmmXQ8ZEKE8YOg4SQaG1ttGgclMcIUTIZOTkr_clcFFN2xZ4C3trwE1j4VJexa5lalOFBJTZr_4F74t2NUrCOrbG7aT-20E0tbQD5Cbd6E4OaQ4p7Hm3P9kj6pcYN2Ka8B-mF0DeIUKZEl77XpUXfg0vukY3A/4jq/Pbw539yOS2C1JlmfyctLUw/h7/h001.TPMfb9M7tPDD1G_JVAHOJB8ALmn5Ke5dF7dL92OQ37o), and Renaissance Macro Research found that so far in 2025, AI capital expenditure has contributed more to GDP growth than all U.S. consumer spending combined â€” the first time this has ever occurred.

What's driving this isn't just ambition but desperation to control costs:

* OpenAI has become one of the world's largest cloud renters, with computing expenses projected to exceed $150 billion from 2025-2030
* The company's cash burn projections quadrupled for 2028, jumping from $11 billion to $45 billion, largely due to costly ""false starts and do-overs"" in AI training
* [Meta's 2025 capital expenditures](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeLwg2VrED6JkrwYHjNt_xNNfGHXY0rPqw-zRh3N8cejw4DZWTzys4469azM-iJzXtaLT3BD6HccRj2S-Dlx7wHrLZPtFE2LqmNX53mfDZQvhpVDc6KGc1LSOpc525C5eVcUDAGdRoA3NWTBFn7uD4NzErxSDTWmv90SlfaIlqrppldDEFrRjg0nyQuJ8S34sENoyMIUeZBTWgBRACLFgD0t6J4xSUzpjj1dfIcOw_yLcuVglSZyWb7QQTOCnx4SFKIOwXMPL0vfenoNzwNLGlXSTzBJwwRAk83hYqm6Y0pjtNXIcCiwd3b5EBLIIYlCPhw/4jq/Pbw539yOS2C1JlmfyctLUw/h8/h001.lSFpP6I-l7c-R9FTxqAicLZOGqNcMYUIz1svXqFp-nw) represent a 68% increase from 2024 levels as it races to build its own infrastructure
* [McKinsey estimates](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFJ9EmflxdkANIt3WqZkVmUSr9L8Cep8yiNLLrP-tNRszkS-UchATMBoQQqd4YguFSMqoYhx9WE1_5QtsD2a9Kdz84LdRPoMbI--pJKGDolPex0Rp0HVnWE1IHx7mvndkGTSbFMR08BIi0EJz3hSQeqgGBBtThaFAhBy2qSGCfHep-_EEPUZXuLsWaRRnGTFW_wkgPIOwaT4mCm2T4oLbsTzb_2J5B9hkQvgMUkv8ocqB5Ec41D1DJu3x22PHueYqcEcAF4-KDkbcjpSwTwlASrFMmTkmL7Zxq0q6QJcnxu9ZphS51Ss4p92-SgAFUYbAk4gToDSbMxuxhKBXLGjXrQXLDMvmjhaOLi1cWRd6QUHcy5exgY-ipB9w6dkSsaBddbdbhpYdYWc8qOM7dk11pY/4jq/Pbw539yOS2C1JlmfyctLUw/h9/h001.Za5WE8ckOXHCJvxKBFaG3wuwwIuhuXPF0hmRpSd1Xv8) the global AI infrastructure buildout could cost $5.2 to $7.9 trillion through 2030

The [33 attendees](https://link.mail.beehiiv.com/ss/c/u001.pVz9habVd8BuGO5KajRSptEoDlphhkml3i_XTVuo-0_NP0jF4xq4cx76wuYczrdMvwR9za79VAxA-Cfy5fPNASKajOk-SEGRoSw1TMzKHSnUGq3iKO4133KS3_pU9pwJFZcCA9SyERJ9CcgcCsaD3pNvQ7yymfwe--lbkWVRjQCa2FAgBx8MUdpwh51uFNVjuaGPfNdYE9Nd1AvltEJqqVF3S80LJwP_FE9xNV4OP6HDDzNcIQOGPAHtl1baA7ksooNiy9mBU1na9mSwHlltFi9ZJT75u3YXq7MRT-YbI8Q/4jq/Pbw539yOS2C1JlmfyctLUw/h10/h001.m4nLv11Oe_S4RChzsqEk0BdhmWX4V8NoGOm1x2ZWDIA) included the biggest names in tech: Microsoft founder Bill Gates, Google CEO Sundar Pichai, OpenAI's Sam Altman and Greg Brockman, Oracle's Safra Catz, and Scale AI founder Alexandr Wang. Notably absent was Elon Musk, who claimed on social media he was invited but couldn't attend amid his ongoing feud with Trump.

The moment was captured on a hot mic when Zuckerberg later told Trump, [""I wasn't sure what number you wanted,""](https://link.mail.beehiiv.com/ss/c/u001.nJj0Nx3NqQcNv-MXYm8O3E8Af7QWTyBR0haSUJPm6laHZVrsl5Wp0qy4Fy9pRvD8UiAo30RrAJExyxObLqHHGfJ_O_oeA5VWFWwd-xZJ4Sb0oi5M9iO6XuNx2OAiLbIptsChhGm77Afy9Wy1vDOvJiVf_MqIY175qC79nRu2Jhjkq54AdlJpC3OMLkU5XSe8wyhnsWyvIoEy0OQy6KdP_nXkoEFKyzttwJJTHy5rupIshLOSOyYRHy57lvliWt_bpqCvY_vCidCi-8pGz2XaaA/4jq/Pbw539yOS2C1JlmfyctLUw/h11/h001.uii91mni7GWsM1jRldTNi4hPPqBqqzQkTGzvWV9grUM) though whether this reflected genuine uncertainty or strategic positioning remains unclear.

# ðŸ¤– Boston Dynamics & Toyota Using Large Behavior Models to Power Humanoids

Boston Dynamics and Toyota Research Institute are advancing Atlas, their humanoid robot, using Large Behavior Models (LBMs). These models enable Atlas to perform complex, continuous sequences of tasksâ€”combining locomotion and manipulation via a unified policy trained across diverse scenarios, with language conditioning for flexible command execution.

Boston Dynamics and Toyota Research Institute have announced a significant stride in robotics and AI research. Demonstrating how a large behavior model powers the [Atlas humanoid robot](https://link.mail.beehiiv.com/ss/c/u001.Un_gMyUnz9Ke_mQuauvCardq59lU9e58_MLv90b_ukS7bZbQNcMAF_nYONUJxyk3kbF_OnU5KKzQN288-pVWl6eUCSFwJWgHU5HeUm4O1lQovO_csMlZukZRujntnhj1OongkIxDdl1H4EnY50MxoweWfO1zWTpn5Pu68wGFGbssD2xdL9ycDrOIDCgXpfXxvlsBsSzBQJvKf_WO4Anu2C3Z0Ns0UQ1eYB7IdjvLyhaJfTtsHZ-9r1O3Nn7sNBCD/4jq/Pbw539yOS2C1JlmfyctLUw/h14/h001.AwtdfZY19vY4kZf75HaT6ZQ35aTAf9E9JQtPmTFO23A).

The team released a [video of Atlas completing a long, continuous sequence of complex tasks](https://link.mail.beehiiv.com/ss/c/u001.nJj0Nx3NqQcNv-MXYm8O3K28kghBSt_53NEoHXdWfNgVRkTsaq4uQvdWhrejUISjbYCvg4MyifNyc0duEf6EzHdE_cI5kJSA6ZyuvVOggZnYeItNurE6gfpOiJENqYB73fO9CTyDdiS2IlDa8jnH-BHee5PWjJfcvw_HGkiJ3iHvy9TyTkCS2OpMnaD0sVRucSZBZ_fKu43AYtVw7SVqyJOyTHUcdnSKNXQzY2C6YvW0pOELyW7ueduql4l0-vxc/4jq/Pbw539yOS2C1JlmfyctLUw/h15/h001.7tUXdM3uoIQzzuBRmKQCXg9GnxTXJP613fDEXlLq5kI) that combine movement and object manipulation. Thanks to LBMs, the humanoid learned these skills quickly, a process that previously would have required hand programming but now can be done without writing new code.

The video shows Atlas using whole-body movements walking, lifting and crouching while completing a series of packing, sorting and organizing tasks. Throughout the series, researchers added unexpected physical challenges mid-task, requiring the humanoid to self-adjust.

[***Getting a Leg up with End-to-end Neural Networks | Boston Dynamics***](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeLJZxZnmr2LyGKHSrQc-oAoWfh306dTQO549WQ6qxP9B-tDdaijphs3uSmLl3fBwZzQMLGwP7nV1IeGJh5geQ6wnF0sQ6wB7Jso74WEyt6RTdhvBVuIu7yzo80cVOrtHT_dBTU67DZZWmFS-BHxFO9xyF4z9N6hjVaYpVLEyyhPZT0QH4gLtC_kgTJM6dqGwziWN7_1R0neV4IfKTBIFAnsIoynkB5FhehFOhJlaH42KuoZzDfpxwI1lv5VVGPCCcw/4jq/Pbw539yOS2C1JlmfyctLUw/h16/h001.n3mI6PXfupdTmf7CMtUbq-8r8S3WiywedoHVyHxygZQ)

Itâ€™s all a direct result of Boston Dynamics and the Toyota Research Institute joining forces last October to accelerate the development of humanoid robots.

[Scott Kuindersma](https://link.mail.beehiiv.com/ss/c/u001.3NjKXaTebdaQdJXVOXKf15rxS6y7QXhEjcEmvX7p1WlDIt-43KebREt9n-h5pMbwEB7EywprhkpFef6JLvDrFrdBijpQNVnSLzEmnRY0caxZT-bnGbEf7BOnyzNDiZ60Z9qNIQRcmmtT7rn0A3puZ7ZuWXFG95cQ6t164wjbHifVQzRDCIAkuipRi77TMLm6lnFtKZRo0ThK0XMPuWXLvnBZmuaPqhLrU1vC8gLVFa0jSdr5ome6Ca6vKxkMFcOD/4jq/Pbw539yOS2C1JlmfyctLUw/h18/h001.JszgFhla0QkpgDQYzIs2DZKdxdgUWN-tyJKPSVY4TFk), vice president of Robotics Research at Boston Dynamics, said the work the company is doing with TRI shows just a glimpse of how they are thinking about building general-purpose humanoid robots that will transform how we live and work.

â€œTraining a single neural network to perform many long-horizon manipulation tasks will lead to better generalization, and highly capable robots like Atlas present the fewest barriers to data collection for tasks requiring whole-body precision, dexterity and strength,â€ Kuindersma said.

[Russ Tedrake](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeCB8jyvUK1EiPLfRN0PWhdyfGsebUt33pP6DpEIRmlC01XE-UKfQsTM-oSxuiA01YG5sW9vsIyhMvCiqOosIBgk9KT37DBvIdb7mHQI3ZQmhQ5pAmOw9lKvRETTpcXYvLzVlNRnErrLt8sFdfAbnctn31iQ8ZJylRs7FJaSVw1Bkn6cdM2Dhksg8clEsg6SUGoKtuWRYI2JrhdEbupKBlt5AKxMC6gDcmzZUA-9bZgBJg0cluFBqTkSa-Pbp1X0RWw/4jq/Pbw539yOS2C1JlmfyctLUw/h19/h001.Bn9vR0Zw9pONSLOwTz6reIHSUYWS4R-9-RDgcKBVT_A), senior vice president of Large Behavior Models at Toyota Research Institute, said one of the main value propositions of humanoids is that they can achieve a vast variety of tasks directly in existing environments, but previous approaches to programming these tasks could not scale to meet this challenge.

â€œLarge behavior models address this opportunity in a fundamentally new way â€“ skills are added quickly via demonstrations from humans, and [as the LBMs get stronger](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfAxBRkbr8_PWmSm0GhORM7b7IeAHgPe915kxEYSAxxoQO06509a2Gvc9L5hgXVqLqtMBtfpd9XScxlduAkoVKVG9Fv_gQlth11KEH7PbPcjOek_FAPq1w9H7s0BwMtrZLe5BXuQuejm5DAG-371myXYkP1O0bX6GGFKGFTNpyqiZgdPABuXPdTRwJD8HJxmnDZwwhDMlV9u4GEY8pdQCv54wgg0Qf50OUL6cUVfsnbAfCIyOgWyqD3w_PTwBQec9bA/4jq/Pbw539yOS2C1JlmfyctLUw/h20/h001.CckvMOaGjnK6rr7jDmSQSPjkE_fF3IbT-99y2w-4aGw), they require less and less demonstrations to achieve more and more robust behaviors,â€ he said.

Kuindersma and Tedrake are co-leading the project to explore how large behavior models can advance humanoid robotics, from whole-body control to dynamic manipulation.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[The Robot Report â€” Boston Dynamics & TRI use LBMs](https://www.therobotreport.com/boston-dynamics-tri-use-large-behavior-models-train-atlas-humanoid/)\] \[[Automate.org â€” Atlas completing complex tasks with LBM](https://www.automate.org/industry-insights/man-with-hockey-stick-cant-stop-boston-dynamics-atlas-from-completing-complex-tasks)\]

# ðŸ†• OpenAI Developing an AI-Powered Jobs Platform

OpenAI is building a new \*\*Jobs Platform\*\*, slated for mid-2026 launch, designed to match candidates with employers using AI from entry-level roles to advanced prompt engineering. The initiative includes an \*\*AI certification program\*\* integrated into ChatGPTâ€™s Study Mode and aims to certify 10 million users by 2030, actively positioning OpenAI as a direct competitor to Microsoft-owned LinkedIn.

OpenAI is building its own jobs platform to compete directly with LinkedIn, launching [a certification program](https://link.mail.beehiiv.com/ss/c/u001.WHId9TPFGnUe-Jr4g0PigwA6vjAJst-7UUbP3eG-EkErlBQD2JH-rxL1MJQR-7M81QzZ-5nKevjW76rPwSzchPG9gE0ikuvgWy8zmQKSV_8tP-TTjck-ulEbBGISBWqUQU_cTh3NR-a8ZkVJZy6PeQ8tZqHGGJgHdDonbA6NQcjn-MhNWbh9aCAucDgdbU1uQP_cx5OC-nCdvV8rM4-gNL35jlSyW7aPIj6LhUQ-979t3uIgXXXH78HLJRhB2DLkxY5g8ZGz0W1spzZji6z7-Kj04NCbxlaFXSO7eg3v8kM/4jq/Pbw539yOS2C1JlmfyctLUw/h24/h001.Wyn--ldsT-qH-hQ8ag1Cm4oIJ3rKrsnaZOyVda1fsrY) designed to train 10 million Americans in AI skills by 2030.

The [OpenAI Jobs Platform,](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjrOnLoW386z6iYOw4eDjn0PM4yeCi2_XV_LT-NJTAJY4cRlFsxzoUEYq8UJkvveYOy-U1vdgZBw-k4Gc1BqKb-_OjvWehlLXfaQexVrkGurj1nE7O-D4t3JbzAxY-K0LMpm0O1pDqzfxzVtvbSH33G81rD5iOYdDZTm2Hkhey0PKgIIf3dr4GGAh_QgxotvgebzoaH2_kR3394mq9GOG4mHDTaxrTYHRG3zbkxk5W1WvF96LPe2XOa4fuhgUw7EPztQ2fydw9QCB_2exZhEP37dKxgGkbzK0uN9LlvP_7j9dEtFrDTWqSpH0sy91uq70So/4jq/Pbw539yOS2C1JlmfyctLUw/h25/h001.XzZNgihYzoUiKFVso1RJkD-tSyqcIgm_MwjYpilEg8w) slated to launch in mid-2026, will utilize AI to pair candidates with employers seeking AI-skilled workers. This is part of a broader effort to transform how people learn and work with AI.

The company is expanding its [OpenAI Academy](https://link.mail.beehiiv.com/ss/c/u001.gKxW2KpP8aPe_QMyOQduojn82QcMj5ILtxuJAUHXoHPAFEm3iv0y2y4_ztywImaYMsfMxysAwKECfhNknQVJPzIauYcnJvfnHc0W4CYyvqLULxks2eShBMkxqfVDkTST0vqtDE7W_A6M0ApKVlGBrtlf7X1598cVMGSk-50r_ttkJk4Av55y3K9styKee30cS7vNz34vPo7ycbzHlIMuYNsPECm3_9DZ-_oLVRWRYX65UIvO822V_SYE8KaR6sGB/4jq/Pbw539yOS2C1JlmfyctLUw/h26/h001.ym4Q-1bH0RA9qBjRiDm3UOOlElvsfqjGN4wHMIi_fes) with certifications ranging from basic AI literacy to advanced prompt engineering. The twist? Students can prepare entirely within ChatGPT using its Study mode, which turns the chatbot into a teacher that questions and provides feedback rather than giving direct answers.

Major employers are already signing up:

* [Walmart is integrating the certifications](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeMSLF9GMIcXRfhzRTR-6s9HYeHOmnrnpdi1ZniGuYXEs2FljVz90HCkzEa8zDYSifaf_qox3o1rrl2Nu5XC-7E-6bYYBi3xkS30GD9oeZwJHU9u6_3q6Gf_yFzO3Gf9bEZJzUdtaX0yky9r4qecTwuOIpFwaX_54Erl-Hvx21vNMEJkFoxokFJt_MNTNMFXEWAsSXMSWbEQDB7T10kTIySTVrG7RfFZcLUP6iu1Yq5X5mgGA1toKGkXVemIgtBs_r9vUqgdssyFSIhSKl4f4Wt0uU41CW7G-Z8b_zQgHW8iTM-hoXgEyxpRJHDcecuI9IL_7X2YUr43nP7Y6qvQqEfE/4jq/Pbw539yOS2C1JlmfyctLUw/h27/h001._IsD98aVI4R6xWheE6pmBTUcZ1dCM2a7qqA106inYc0) into its own academy for 3.5 million U.S. associates
* John Deere, Boston Consulting Group, Accenture and Indeed are launch partners
* The Texas Association of Business plans to connect thousands of employers with AI-trained talent

Certification pilots begin in late 2025, with OpenAI committing to certify 10 million Americans by 2030 as part of the White House's AI literacy campaign.

The initiative comes as companies increasingly seek workers with AI skills, with [research showing](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjrOnLoW386z6iYOw4eDjn0PM4yeCi2_XV_LT-NJTAJY4cRlFsxzoUEYq8UJkvveYOy-U1vdgZBw-k4Gc1BqKb-_OjvWehlLXfaQexVrkGurj1nE7O-D4t3JbzAxY-K0LMpm0O1pDqzfxzVtvbSH33G81rD5iOYdDZTm2Hkhey0PKgIIf3dr4GGAh_QgxotvgebzoaH2_kR3394mq9GOG4mHDTaxrTYHRG3zbkxk5W1WvJI1axAhAukEpoweLcwXGK28ag7AU5Q1PFilYQvt2AIQA6lFxAtRvxVksyDRItenWsbeYDdzBmIeiltreupDDAs/4jq/Pbw539yOS2C1JlmfyctLUw/h28/h001.REEIHLHr3Z-rYqRu-k8g0WrauFyHgeuXyEYC2_ikENw) that AI-savvy employees earn higher salaries on average. OpenAI CEO of Applications Fidji Simo acknowledged AI's ""disruptive"" impact on the workforce, saying the company can't eliminate that disruption but can help people become more fluent in AI and connect them with employers who need those skills.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[Tomâ€™s Guide â€” OpenAI to launch LinkedIn competitor](https://www.tomsguide.com/ai/openai-to-launch-a-linkedin-competitor-heres-what-ceo-sam-altman-revealed-at-the-white-house-tech-dinner)\] \[[Barronâ€™s â€” OpenAI steps on Microsoftâ€™s toes](https://www.barrons.com/articles/openai-microsoft-linkedin-jobs-ai-chips-5517c6fc)\]



# What Else Happened in AI on September 08th 2025?

**Alibaba** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HURA0_wGbBzO19ASQnIJujumd23djQ_96RyMz7x81IFiJ7swg2eVs9J7yVxm1PErHLXcnWhyawHoFUZRYz2VXdT6SGcoIbU5jpux0KqD9bzKFfpjY-Uw-cC8xEnicMgGm-HB8DLE8nh-AYrMB0sbu66IvwU-Ec8aw1Ew8djnuFK1uz3RPNAgq6xXBwkAV9DwIy6U4bvO6miBKC3k0SHUIyV8XGLUlLZXVayfqtrQIVGDFyxzI5XjGs3C-IFhTa9dRGiS3fr57Tgd3JN7UbEUHzU/4jq/uCgyabdhSDSGdFq2tHSErw/h27/h001.VBXOUCaCApNgTZitoimH5jT2Rag_wYXwMlSzXmhqC0o) Qwen3-Max, a 1T+ model that surpasses other Qwen3 variants, Kimi K2, Deepseek V3.1, and Claude Opus 4 (non-reasoning) across benchmarks.

**OpenAI** [**revealed**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyzbPJdh2qAj8kGiBAmrmvYKeKEO39rNFSPzdmJJyWnindRcEVuHHB0n1_BzK-7bglmHN_1pPUtLUIwTv8TOnZJK1TtF9AOwaCn3jcykZ4nOB4Z7sCa6wXfoPM8DB2UxIfyuC4_846OW1dYpjMfLFs8oU6qY8r1Gycy0oyXgVd5jccvaeHbSWxD0ROwCStHLcxRTbXZUoBPXMikACCPUQ4pk30qu1-SlNtrV2lLVvQBnTAFr5poUvvUE3SWyz9bQcNSsO_6ZbZRrWxngZ2SEq2xVld48xuob2YDg3w4vuYwmnCBVwDuN3iBXQac3x3XggFBjerdjDvFF5pODSuCrH3E/4jq/uCgyabdhSDSGdFq2tHSErw/h28/h001.dcdVTfN2WySQtXbJ0s4rU96Q6XHfqcug_bT2f5wROkY) that it plans to burn through $115B in cash over the next four years due to data center, talent, and compute costs, an $80B increase over its projections.

**French AI startup Mistral** is reportedly [**raising**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf6nqjDQNO_8WkeQHVJfpUfL8VnincILsSRAhii4I2C0noq-8L7Wrxw6g7JK-dD53ZQt4JBtZCF972nevjXxcFZMZIWiWUj8fjchqrYuSFMVYBCGnFX5yHftxTwfYhwgm7-ubUdIEtTqsj_bL5rsofnl2SxUgNpOzS7VABvXKyGTuls1Wf28MSuuvgbEcswiu8SJa3HmkJ7CpaEHUIQ0tptm2nAyTMDGnJ9Kl5372VvGGlNUEZZpwAwN2vUn6MSCMFMvWs4TNCwZ5_bzdYoyNr8VAA0512_p_fMaNuqpVPSDZO1AUIyD0RkLKwyY2FNeFrcXWaORFeb2vFCJwYU5edKFxk3O0hiVUt5HMSdTShtpB/4jq/uCgyabdhSDSGdFq2tHSErw/h29/h001.Gdi66bT2iyOEqgSWDmtSVE64JRetZfMJtRCpyU70Sp8) $1.7B in a new Series C funding round, which will make it the most valuable company in Europe with a $11.7B valuation.

**OpenAI Model Behavior lead Joanne Jang** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HVRLL05JezsX-FZ1gnLfj2nNbUBQTbjwNj-OPnLEeZpYCygUPJUN5Y_mhBOZ3PHC2Tiw7zQ4HBuHJk6BaoZZ6pVd5FVC1BnypikZxVS_GjLXK2HGvdn64peNVsx3v9LO1V-t8iyx3RPqSctc1HHzBo-PZeWOAtOP128KsyS0-zNiZ_It9vthnonfFMFOGZ_twwyXvL5n0CJ7ZaJeDisIyaEqqUXEHcXw29C-qGmZ2iYBvjOHSvrCYYA8XiadzO1ggQUEatdCe5dpfmHsbl4jAfY/4jq/uCgyabdhSDSGdFq2tHSErw/h30/h001.6OwwIKQLkYGtdO7FU5TwGB3RFSq7s9SVgwzMKdPlAh8) OAI Labs, a team dedicated to â€œinventing and prototyping new interfaces for how people collaborate with AI.â€

**A group of authors** [**filed**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfwtFoJxo3yqmynAOJhMRHjWx337PP7NM2mZW7S-VGNu1K00tg75nfhj4NDlzFh8D_cHxn0DEM0Tmo8gHZswSqXtDZfBLkFIPACM2Qaqc1YuvCSx3QdGadg_G-pHOEjcgLdM4vPc0uzyVXEzQAf5CWbPD0MKZPDQcKJ7YobaMSkEGq42vxG2T0mjDiw9-a46ePQPV9FqYMxpniMIIqAowtgo4MUnkhhxXCyexTXhoIEylHH8ZTtvWOom0C8TAKHjrLGtLS3RdTKQmdDd0F8PeZuAYD4GgBEpokELIFtc3JIWeybmwJCYgql4L4e8Vr333liacZl23lVmYbJfxnnrU1mMzrq0ZFCRdfaBT3s_xJ5ua/4jq/uCgyabdhSDSGdFq2tHSErw/h31/h001.BhGyz68f3PuzQk1ShXa9XiEx_-XlzZ_f3OOfzMvoAd4) a class action lawsuit against Apple, accusing the tech giant of training its OpenELM LLMs using a pirated dataset of books.

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbx6tw/ai_daily_news_rundown_asml_becomes_mistral_ais/,r_1nbx6tw,,,
r_1nbth0t,reddit,ml_dnn,2025-09-08T17:07:15+00:00,"Reinforcement Learning Survey
# A Survey Analyzing Generalization in Deep Reinforcement Learning

[https://arxiv.org/pdf/2401.02349.pdf](https://arxiv.org/pdf/2401.02349.pdf)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbth0t/reinforcement_learning_survey/,r_1nbth0t,,,
r_1nbqprt,reddit,Unlikely_Pirate5970,2025-09-08T15:24:43+00:00,"How to Get CourseHero Free Trial - Complete Guide 2025
How to Get CourseHero Free Trial - Complete Guide 2025

Hey fellow students! ðŸ‘‹ I've spent months figuring out every legitimate way to get a CourseHero free trial without getting scammed.

Updated for 2025.

# This works: [https://discord.gg/5DXbHNjmFc](https://discord.gg/5DXbHNjmFc)

# ðŸ”“ Proven Methods for CourseHero Free Trial Access

**1. Sign Up During Peak Promo Periods:** CourseHero runs their best free trial offers at semester starts (August, January, May). You can get 7-14 days free access or several document unlocks. Set calendar reminders for these months!

**2. âœ… Use the Official Student Email Signup:** Register with your .edu email address for extended trial periods. CourseHero often gives students longer trials than regular users - sometimes up to 30 days free access.

**3. Upload Quality Study Materials for Credits:** Create detailed study guides, class notes, or practice problems and upload them. Each approved upload earns you 3-5 document unlocks, which is basically like extending your free trial indefinitely.

**4. â­ Follow CourseHero's Social Media for Flash Deals** They announce surprise free trial extensions on Twitter and Instagram. I've caught 48-hour flash promotions this way - totally worth following.

**5. Check for University Partnership Discounts** Some schools have deals with CourseHero for free or discounted access. Ask your library or academic support center if they have any partnerships.

**6. ðŸ“¤ Refer Friends for Bonus Credits** CourseHero's referral program gives both you and your friend free unlocks when they sign up. Each successful referral = more free access time.

# Why This Beats Shady ""Hacks""

These methods actually work long-term and won't get your account suspended. Plus, you're building a legitimate study resource collection.

Anyone found other legit ways to extend CourseHero free trials? What's been your experience with their student promotions?

**TL;DR:** ðŸ“š Get CourseHero free trials through student email signups, semester promotions, uploads, and referrals.

DM me if you want a few links to track their promo schedules!

Don't use sketchy downloads; avoid anything asking for payment or your login.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbqprt/how_to_get_coursehero_free_trial_complete_guide/,r_1nbqprt,,,
r_1nbnire,reddit,OkHuckleberry2202,2025-09-08T13:18:39+00:00,"What is CUDA and how does it relate to NVIDIA GPUs?
CUDA: Unlocking the Power of NVIDIA GPUs
CUDA is a parallel computing platform and programming model developed by NVIDIA that enables developers to harness the massive computational power of NVIDIA GPUs (Graphics Processing Units) for general-purpose computing tasks beyond just graphics rendering. In essence, CUDA allows software developers to leverage the thousands of processing cores in NVIDIA GPUs to accelerate compute-intensive applications.

How CUDA Works
1. Parallel Processing: GPUs are designed for parallel processing, making them excel at tasks like matrix operations common in AI, deep learning, and scientific simulations.
2. CUDA Kernels: Developers write CUDA kernels â€“ special functions that execute on the GPU â€“ to offload compute-intensive parts of applications.
3. Memory Management: CUDA involves managing data transfer between CPU (host) and GPU (device) memory for efficient processing.
4. API and Libraries: CUDA includes APIs and libraries like cuDNN for deep learning, cuBLAS for linear algebra, simplifying development.

Relation to NVIDIA GPUs
- NVIDIA Exclusive: CUDA is proprietary to NVIDIA GPUs, making it a key differentiator for NVIDIA in AI, HPC (High-Performance Computing), and data center markets.
- Acceleration of Workloads: CUDA enables dramatic acceleration of workloads in AI, machine learning, video processing, and scientific computing on NVIDIA GPUs.
- Ecosystem: CUDA has a rich ecosystem of tools, libraries, and developer support, fostering innovation in fields leveraging GPU compute power.

Companies like [Cyfuture AI](https://cyfuture.ai/gpu-clusters) leverage CUDA and NVIDIA GPUs to build cutting-edge AI solutions, driving advancements in areas like deep learning, computer vision, and natural language processing. With CUDA, developers can unlock unprecedented performance for compute-intensive tasks, transforming industries and pushing the boundaries of what's possible with AI and accelerated computing.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbnire/what_is_cuda_and_how_does_it_relate_to_nvidia_gpus/,r_1nbnire,,,
r_1nbmd8f,reddit,OkHuckleberry2202,2025-09-08T12:27:33+00:00,"What is GPU virtualization and how does it work?
GPU Virtualization: Unlocking Powerful Graphics Capabilities
GPU virtualization is a technology that enables multiple virtual machines (VMs) or users to share a single physical Graphics Processing Unit (GPU) in a data center or cloud environment. This allows organizations to optimize GPU resource utilization, improve flexibility, and reduce costs associated with deploying and managing GPUs.

How GPU Virtualization Works
1. GPU Passthrough: In some configurations, a VM can be given direct access to a physical GPU (passthrough), dedicating the GPU to that VM.
2. GPU Sharing: Technologies like NVIDIA's vGPU (virtual GPU) allow multiple VMs to share a single physical GPU, with each VM getting a portion of the GPU's resources.
3. Hypervisor Integration: GPU virtualization often involves integration with hypervisors (like VMware, KVM) to manage GPU resources among VMs.
4. API Support: GPU virtualization solutions often support APIs like CUDA (for NVIDIA GPUs) to enable compute-intensive applications to leverage virtualized GPU resources.

Benefits of GPU Virtualization
- Resource Optimization: Enables efficient sharing of expensive GPU hardware among multiple workloads.
- Flexibility and Scalability: Supports dynamic allocation of GPU resources to VMs or containers.
- Cost Reduction: Reduces the need for dedicated GPUs per workload, lowering hardware costs.
- Enhanced Collaboration: Facilitates sharing of GPU power in multi-user environments like data centers and cloud platforms.

GPU virtualization is particularly valuable in environments requiring high-performance computing, such as AI, machine learning, data analytics, and graphics-intensive applications like CAD and video editing. Cyfuture AI leverages advanced https://cyfuture.ai/gpu-clusters technologies to deliver powerful, scalable AI and compute solutions to businesses, enabling them to harness the full potential of GPU-accelerated workloads.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbmd8f/what_is_gpu_virtualization_and_how_does_it_work/,r_1nbmd8f,,,
r_1nbhyv6,reddit,danno711,2025-09-08T08:13:15+00:00,Cracking the Code of Life: How AI Is Finally Reading Our DNA,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nbhyv6/cracking_the_code_of_life_how_ai_is_finally/,r_1nbhyv6,,,
r_1nbhxis,reddit,dreamhighdude1,2025-09-08T08:10:43+00:00,"Looking for team or advice?
Hey guys,
I realized something recently â€” chasing big ideas alone kinda sucks.
Youâ€™ve got motivation, maybe even a plan, but no one to bounce thoughts off, no partner to build with, no group to keep you accountable.
Soâ€¦ I started a Discord called Dreamers Domain
Inside, we:
Find partners to build projects or startups
Share ideas + get real feedback
Host group discussions & late-night study voice chats
Support each other while growing
Itâ€™s still small but already feels like the circle I was looking for.
If that sounds like your vibe, youâ€™re welcome to join:
ðŸ‘‰ [https://discord.gg/Fq4PhBTzBz](https://discord.gg/Fq4PhBTzBz)",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1nbhxis/looking_for_team_or_advice/,r_1nbhxis,,,
r_1nb9175,reddit,Typical_Try_8748,2025-09-08T00:05:30+00:00,Neural networks performence evaluation,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1nb9175/neural_networks_performence_evaluation/,r_1nb9175,,,
r_1nb77wt,reddit,habittracker0,2025-09-07T22:44:00+00:00,"Habit Tracker - To-Do List - A free all-in-one productivity app
Recently, my app hit 350 users! I started posting my app to reddit since a little less than two weeks ago, and I've gotten so much support. People have been trying my app, giving me feedback, and I've got so many positive reviews, so thank you!

I made this app because I didn't want to have to juggle between using multiple apps to stay productive. I wanted one app that could do everything. Habit Tracker - To-Do List includes tasks, notes, habits, and workouts. It is completely free, and there are no ads.

Furthermore, I've been trying to implement AI and ml into it. I already started this with implementing a feature called Smart Suggestions, where you can say something like ""Go to the store tomorrow at 8 pm"", and it creates a task called ""Go to the store"" and sets the time and date to tomorrow at 8 pm. This isn't exactly using AI though, it's more so just going through the text. I wanted a bit of help on the best ways to implement AI or ml into flutter apps if you have any ideas!

I would love any feedback that you have as well if you want to try the app!

App Link:Â [https://play.google.com/store/apps/details?id=com.rohansaxena.habit\_tracker\_app](https://play.google.com/store/apps/details?id=com.rohansaxena.habit_tracker_app)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nb77wt/habit_tracker_todo_list_a_free_allinone/,r_1nb77wt,,,
r_1nb2txf,reddit,unrecognized_learner,2025-09-07T19:46:08+00:00,"Deep Learning Hands on
Hi Everyone. I have started recently learning deep learning. I understand the maths and how the neural networks work. But when it comes to coding my hands simply don't move. I and not getting tha Aha! Moment of the coding. Please guide me how I can improve on that front. ",deeplearning,8,https://www.reddit.com/r/deeplearning/comments/1nb2txf/deep_learning_hands_on/,r_1nb2txf,,,
r_1nb141t,reddit,Melodic_Story609,2025-09-07T18:39:24+00:00,Project Idea: Applying Group Relative Policy Optimization (GRPO) to a Multi-Asset Trading Bot,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nb141t/project_idea_applying_group_relative_policy/,r_1nb141t,,,
r_1naz2d3,reddit,Swayam7170,2025-09-07T17:21:25+00:00,"Courses recommendations.
Hi guys, I am currently getting into deep learning, and going through the YouTube videos of Andrew Ng and Linear algebra by Gilbert Strang, I have saved up some money, (from the internship) as I have free time and I am in a vacation, I was thinking of buying a good course for the implementation and learning practical skills.( Anything as such would you recommend? 

If I have to be specific - Rag models, NLP, working with transformers, Agentic AI( a bit too advanced I guess for me lol), I want to learn whatever I can and use the money that I have saved up to upskill as I am free.",deeplearning,7,https://www.reddit.com/r/deeplearning/comments/1naz2d3/courses_recommendations/,r_1naz2d3,,,
r_1naw5dk,reddit,andsi2asi,2025-09-07T15:28:25+00:00,"The under-the-radar AI use case that decides whether our future is utopian or dystopian. AIs as political strategists.



As AIs become more intelligent, soon moving well into the genius range, we can expect many miracles. Diseases cured and prevented. Trillions more dollars pumped into the economy. New manufacturing materials and processes. Universal education. UBI. An end to poverty and factory farming. 

We may get all of that right, and a whole lot more, yet be headed into civilization collapse. For decades we have been hearing that climate change, and most seriously the risk of runaway global warming, threatens to send us all back to the Stone age. Many think that the major threat here is about floods, droughts, hurricanes and rising sea levels. But the far greater threat comes from the geopolitical effects of these natural phenomena. 

Today there are about a dozen nuclear armed nations. We remain safe because they know that if any of them starts a nuclear war, it's a war they will not survive. The reasoning behind this is simple. Humans can be quite vengeful. Each of the nations operates under the very clear promise that if they are going down, they are taking their enemies down with them. 

Let's now return to climate change and runaway global warming. Already the Middle East is experiencing a climate-driven years-long drought that could spark a regional war. But let's look about 10 or 20 years into the future. Imagine AI by then has performed countless miracles for us. People are theoretically enjoying life expectancy of 150 or 200 years. But let's say despite all these miracles, we haven't reversed climate change and prevented runaway global warming. 

Famines ravage the global South. Cities like Miami are now under water. Nation states fail. And suddenly you have a lot of people with a lot of reasons to be unbelievably angry with the rich nations that destroyed their countries. They may not have nuclear weapons, but AI will ensure that they will have a multitude of ways that they can bring the rest of the world down with them. 

All because we did not fight climate change. All because we did not have the political will to fight climate change. All because money controls our politics, and the people in power are not intelligent enough, nor good enough, to do the right thing. 

The point here is that while AI will improve our world in countless ways, it5's most impactful positive contribution will very probably be to develop the political strategy that allows us to finally get money out of politics...so then we can finally become serious about preventing climate change from ending human civilization as we know it.

Top developers are brilliant computer scientists. But they've never been trained in geopolitics or climate science. Let's hope they are smart enough to talk to enough people who understand the socio-political implications of continuing to allow political campaign contributions and lobbying bribes to decide what we as a world will do and will not do. Let's hope that our brilliant AI developers then train AIs to excel at the very important task of designing the political strategy that will get money out of politics.
",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1naw5dk/the_undertheradar_ai_use_case_that_decides/,r_1naw5dk,,,
r_1nank71,reddit,Round-Fix-1687,2025-09-07T08:05:33+00:00,"Artificial Intelligence & Deep Learning Course Training
TheÂ **Artificial Intelligence (AI) and Deep Learning course**Â **at 360digiTMG** commence with building AI applications, understanding Neural Network Architectures, structuring algorithms for new AI machines, and minimizing errors through advanced optimization techniques. Learn AI concepts and practical applications in the Certification Program inÂ **AI and Deep Learning**. Get set for a career as an AI expert.",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1nank71/artificial_intelligence_deep_learning_course/,r_1nank71,,,
r_1nag7ua,reddit,andsi2asi,2025-09-07T01:11:49+00:00,"AI coders and engineers soon displacing humans, and why AIs will score deep into genius level IQ-equivalence by 2027





It could be said that the AI race, and by extension much of the global economy, will be won by the engineers and coders who are first to create and implement the best and most cost-effective AI algorithms.

First, let's talk about where coders are today, and where they are expected to be in 2026. OpenAI is clearly in the lead, but the rest of the field is catching up fast. A good way to gauge this is to compare AI coders with humans. Here are the numbers according to Grok 4:

2025 Percentile Rankings vs. Humans:

-OpenAI (o1/o3): 99.8th
-OpenAI (OpenAIAHC): ~98th
-DeepMind (AlphaCode 2): 85th
-Cognition Labs (Deingosvin): 50th-70th
-Anthropic (Claude 3.5 Sonnet): 70th-80th
-Google (Gemini 2.0): 85th
-Meta (Code Llama): 60th-70th

2026 Projected Percentile Rankings vs. Humans:

OpenAI (o4/o5): 99.9th
OpenAI (OpenAIAHC): 99.9th
DeepMind (AlphaCode 3/4): 95th-99th
Cognition Labs (Devin 3.0): 90th-95th
Anthropic (Claude 4/5 Sonnet): 95th-99th
Google (Gemini 3.0): 98th
Meta (Code Llama 3/4): 85th-90th

With most AI coders outperforming all but the top 1-5% of human coders by 2027, we can expect that these AI coders will be doing virtually all of the entry level coding tasks, and perhaps the majority of more in-depth AI tasks like workflow automation and more sophisticated prompt building. Since these less demanding tasks will, for the most part, be commoditized by 2027, the main competition in the AI space will be for high level, complex, tasks like advanced prompt engineering, AI customization, integration and oversight of AI systems.

Here's where the IQ-equivalence competition comes in. Today's top AI coders are simply not yet smart enough to do our most advanced AI tasks. But that's about to change. AIs are expected to gain about 20 IQ- equivalence points by 2027, bringing them all well beyond the genius range. And based on the current progress trajectory, it isn't overly optimistic to expect that some models will gain 30 to 40 IQ-equivalence points during these next two years.

This means that by 2027 even the vast majority of top AI engineers will be AIs. Now imagine developers in 2027 having the choice of hiring dozens of top level human AI engineers or deploying thousands (or millions) of equally qualified, and perhaps far more intelligent, AI engineers to complete their most demanding, top-level, AI tasks.

What's the takeaway? While there will certainly be money to be made by deploying legions of entry-level and mid-level AI coders during these next two years, the biggest wins will go to the developers who also build the most intelligent, recursively improving, AI coders and top level engineers. The smartest developers will be devoting a lot of resources and compute to build the 20-40 points higher IQ-equivalence genius engineers that will create the AGIs and ASIs that win the AI race, and perhaps the economic, political and military superiority races as well.

Naturally, that effort will take a lot of money, and among the best ways to bring in that investment is to release to the widest consumer user base the AI judged to be the most intelligent. So don't be surprised if over this next year or two you find yourself texting and voice chatting with AIs far more brilliant than you could have imagined possible in such a brief span of time.

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nag7ua/ai_coders_and_engineers_soon_displacing_humans/,r_1nag7ua,,,
r_1naf26l,reddit,EnchantbSpy,2025-09-07T00:15:48+00:00,"What Are the Most Accurate IQ Tests Online?
Lately Iâ€™ve been questioning my own intelligence and thought it might be fun (and maybe humbling) to take a legit IQ test just to see where I land. Iâ€™ve tried a few of the free ones online, but they felt more like Buzzfeed quizzes than anything serious. Apologies if this isnâ€™t the right sub, wasnâ€™t sure where else to post this, but still I would appreciate your help

What Iâ€™m looking for is:

* Reliable/scientific results
* More than just a 10-question gimmick
* A proper score breakdown
* Quick results
* Ideally something people generally recognize as trustworthy

Accuracy is the main thing I care about, but the rest matters too.",deeplearning,305,https://www.reddit.com/r/deeplearning/comments/1naf26l/what_are_the_most_accurate_iq_tests_online/,r_1naf26l,,,
r_1nacq6i,reddit,johntheGPT442331,2025-09-06T22:28:21+00:00,"Researcher aims to create conscious AI via evolving neural ecosystems, potentially surpassing Moore's law
A recent post on r/MachineLearning by u/yestheman9894, a dual-PhD student in machine learning and astrophysics, outlines an ambitious research project to build what he hopes could be the first conscious AI. Rather than scaling static neural networks, he proposes evolving populations of neural agents that can grow, prune and rewire themselves while competing and cooperating in complex virtual worlds.



The project combines evolutionary algorithms with neuromodulation and synaptic plasticity. Agents develop social behaviours and internal drives over generations, with the goal of encouraging emergent cognition. The researcher argues that this open-ended approach could push AI beyond the hardware limits described by Moore's law, focusing on adaptive architectures rather than transistor counts.



While evolutionary methods have been explored before, combining modern compute with dynamic neural architectures may reveal new insights. Whether or not true consciousness emerges, the work suggests an alternative direction for deep learning and AGI research.



Original discussion: https://www.reddit.com/r/MachineLearning/comments/1na3rz4/d\_i\_plan\_to\_create\_the\_worlds\_first\_truly\_conscious\_ai\_for\_my\_phd/",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1nacq6i/researcher_aims_to_create_conscious_ai_via/,r_1nacq6i,,,
r_1na78up,reddit,aigeneration,2025-09-06T18:40:43+00:00,Using sketches as starting points,deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1na78up/using_sketches_as_starting_points/,r_1na78up,,,
r_1na6x2l,reddit,Justlookingtk,2025-09-06T18:27:37+00:00,"Building a voice controlled AI assistant from scratch (for a project)
Hey guys, I'm currently building a fully customised AI assistant for my laptop. I plan to give it a personality ( a sarcastic one) and also intend for it to be functional like siri or Alexa. I'm using python as my main programming language with features like: App task handling, voice recognition and maybe other features when I'm building it. If you've built something similar to this or have resources that can help with this I would really appreciate it. I'm also open to any advice",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1na6x2l/building_a_voice_controlled_ai_assistant_from/,r_1na6x2l,,,
r_1na1bap,reddit,importantcreation,2025-09-06T14:44:01+00:00,"Advice on LLM Liftoff By Dev G
Has anyone here purchased a course by Dev G? Could you please share your reviews and also let me know what the course content covers? and how many hours it is.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1na1bap/advice_on_llm_liftoff_by_dev_g/,r_1na1bap,,,
r_1na14qq,reddit,OkHuckleberry2202,2025-09-06T14:36:14+00:00,"What are the security considerations for Serverless Inferencing?
Security Considerations for Serverless Inferencing
Serverless inferencing, which involves deploying machine learning models in a cloud-based environment without managing the underlying infrastructure, introduces unique security considerations. Some key security concerns include:

1. Data Encryption: Ensuring that sensitive data used for inference is encrypted both in transit and at rest.
2. Model Security: Protecting machine learning models from unauthorized access, tampering, or theft.
3. Access Control: Implementing robust access controls to ensure that only authorized personnel can access and manage serverless inferencing resources.
4. Monitoring and Logging: Continuously monitoring and logging serverless inferencing activities to detect and respond to potential security threats.
5. Dependency Management: Managing dependencies and libraries used in serverless inferencing to prevent vulnerabilities and ensure compliance with security best practices.

To mitigate these risks, it's essential to implement a comprehensive security strategy that includes encryption, access controls, monitoring, and regular security audits.

Serverless inferencing offers numerous benefits, including scalability, cost-effectiveness, and increased efficiency. By leveraging serverless inferencing, businesses can deploy machine learning models quickly and efficiently, without worrying about the underlying infrastructure. Cyfuture AI's [Serverless Inferencing](https://cyfuture.ai/serverless-inferencing)
 solutions provide a secure, scalable, and efficient way to deploy machine learning models, enabling businesses to drive innovation and growth.",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1na14qq/what_are_the_security_considerations_for/,r_1na14qq,,,
r_1na0v8s,reddit,Long-Advertising-993,2025-09-06T14:25:02+00:00,"Why does my learning curve oscillate? Interpreting noisy RMSE for a time-series LSTM
Hi allâ€”  
Iâ€™m training an LSTM/RNN for **solar power forecasting** (time-series). My **RMSE vs. epochs** curve **zig-zags**, especially in the early epochs, before settling later. Iâ€™d love a sanity check on whether this behavior is normal and how to interpret it.

**Setup (summary):**

* Data: multivariate PV time-series; windowing with sliding sequences; **time-based split** (Train/Val/Test), no shuffle across splits.
* Scaling: fit on **train only**, apply to val/test.
* Models/experiments: Baseline LSTM, KerasTuner best, GWO, SGWO.
* Training: Adam (lr around 1e-3), batch\_size 32â€“64, dropout 0.2â€“0.5.
* Callbacks: **EarlyStopping**(patienceâ‰ˆ10, restore\_best\_weights=True) + **ReduceLROnPlateau**(factor=0.5, patienceâ‰ˆ5).
* Metric: **RMSE**; I track **validation** each epoch and keep test for final evaluation only.

**What I see:**

* Validation RMSE **oscillates** (up/down) in the first \~20â€“40 epochs, then the swings get smaller and the curve flattens.
* Occasional â€œstepâ€ changes when LR reduces.
* Final performance improves but the path to get there isnâ€™t smooth.

**My hypotheses (please confirm/correct):**

1. **Mini-batch noise** \+ **non-IID time-series** â†’ validation metric is expected to fluctuate.
2. **Learning rate a bit high** at the start â†’ larger parameter updates â†’ bigger early swings.
3. **Small validation window** (or distribution shift/seasonality) â†’ higher variance in the metric.
4. **Regularization effects** (dropout, etc.) make validation non-monotonic even when training loss decreases.
5. If oscillations **grow** rather than shrink, that would indicate **instability** (too high LR, exploding gradients, or leakage).

**Questions:**

* Are these oscillations **normal** for time-series LSTMs trained with mini-batches?
* Would you first try **lower base LR**, **larger batch**, or **longer patience**?
* Any preferred **CV scheme** for stability here (e.g., **rolling-origin / blocked K-fold** for time-series)?
* Any red flags in my setup (e.g., possible **leakage** from windowing or from evaluating on test during training)?
* For readability only, is it okay to plot a **5-epoch moving average** of the curve while keeping the raw curve for reference?

**How I currently interpret it:**

* Early zig-zag = normal exploration noise;
* Downward trend + shrinking amplitude = converging;
* Train â†“ while Val â†‘ = overfitting;
* Both flat and high = underfitting or data/feature limits.

Plot attached. Any advice or pointers to best practices are appreciatedâ€”thanks!

https://preview.redd.it/jf73bfpo0knf1.jpg?width=1080&format=pjpg&auto=webp&s=d03048fc45fdbb88c900c79ed22f4a688014b719

",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1na0v8s/why_does_my_learning_curve_oscillate_interpreting/,r_1na0v8s,,,
r_1n9znoz,reddit,SKD_Sumit,2025-09-06T13:32:18+00:00,"The Python roadmap I wish existed when I started data science - covers true beginner to Gen AI
There are thousands of Python tutorials, but which path actually works? 

Most Python resources either assume programming knowledge or jump straight to pandas without proper foundations. So I mapped out the COMPLETE journey - from your first variable to building AI systems.

Full Breakdown:ðŸ”— [**Python for Data Science Roadmap 2025 | Learn Python (Step by Step Guide)**](https://www.youtube.com/watch?v=HXhsjEJKo6E)

**What makes this different:**

* **TRUE beginner start** \- explains why Python over other languages
* **Logical progression** \- syntax â†’ intermediate â†’ data science â†’ specialized areas
* **Modern integration** \- includes Gen AI, APIs, web scraping, even basic UI
* **No knowledge gaps** \- each section builds on the previous

**The roadmap flow:**

1. **Foundation** (syntax that actually sticks)
2. **Intermediate Python** (OOP, error handling, file ops)
3. **Data Science Stack** (NumPy â†’ Pandas â†’ Visualization)
4. **Specialized Areas** (ML, DL, Computer Vision, NLP, Gen AI)
5. **Real-world Skills** (APIs, databases, deployment)

**Biggest mistake I see:** Rushing to machine learning libraries without understanding Python fundamentals. You end up copy-pasting code without knowing why it works.

For those who've made the DS transition - what was your biggest Python learning hurdle? And what do you wish you'd focused on earlier?",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n9znoz/the_python_roadmap_i_wish_existed_when_i_started/,r_1n9znoz,,,
r_1n9ww9s,reddit,Nearby_Reaction2947,2025-09-06T11:11:43+00:00,"I built an open-source, end-to-end Speech-to-Speech translation pipeline with voice preservation (RVC) and lip-syncing (Wav2Lip).
Hello r/deeplearning ,

I'm a final-year undergrad and wanted to share a multimodal project I've been working on: a complete pipeline that translates a video from English to Telugu, while preserving the speaker's voice and syncing their lips to the new audio.

* **GitHub Repo:** [\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)
* **Full Technical Write-up:** [\[Article\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)

[english](https://reddit.com/link/1n9ww9s/video/13oazy322jnf1/player)

[telugu](https://reddit.com/link/1n9ww9s/video/9lfm12s32jnf1/player)

The core challenge was voice preservation for a low-resource language without a massive dataset for voice cloning. After hitting a wall with traditional approaches, I found that using Retrieval-based Voice Conversion (RVC) on the output of a standard TTS model gave surprisingly robust results.

**The pipeline is as follows:**

1. **ASR:** Transcribe source audio using Whisper.
2. **NMT:** Translate the English transcript to Telugu using Meta's NLLB.
3. **TTS:** Synthesize Telugu speech from the translated text using the MMS model.
4. **Voice Conversion:** Convert the synthetic TTS voice to match the original speaker's timbre using a trained RVC model.
5. **Lip Sync:** Use Wav2Lip to align the speaker's lip movements with the newly generated audio track.

In my write-up, I've detailed the entire journey, including my failed attempt at a direct S2S model inspired by Translatotron. I believe the RVC-based approach is a practical solution for many-to-one voice dubbing tasks where speaker-specific data is limited.

I'm sharing this to get feedback from the community on the architecture and potential improvements. I am also actively seeking research positions or ML roles where I can work on .

Thank you for your time and any feedback you might have.",deeplearning,8,https://www.reddit.com/r/deeplearning/comments/1n9ww9s/i_built_an_opensource_endtoend_speechtospeech/,r_1n9ww9s,,,
r_1n9vol4,reddit,enoumen,2025-09-06T09:57:16+00:00,"AI Daily News Rundown: ðŸ’¥ OpenAI to make its own AI chips with Broadcom ðŸ’¼ OpenAI announces AI-powered hiring platform to take on LinkedIn ðŸ³ DeepSeekâ€™s self-improving AI agent   ðŸˆ NFL Kicks Off Season with AI-Powered Campaign & more (Sept 06, 2025)
# AI Daily Rundown: September 05th, 2025

https://preview.redd.it/g8uwzh57ninf1.png?width=1456&format=png&auto=webp&s=de12f648f297886e0229554df13613c4df1e2069

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**ðŸ’¼ OpenAIâ€™s AI jobs platform, certification program**

**ðŸ’¥ OpenAI to make its own AI chips with Broadcom**

**ðŸ’¼ OpenAI announces AI-powered hiring platform to take on LinkedIn**

**ðŸ”— Stripe to launch a new blockchain**

**ðŸ’° Tesla offers Elon Musk a $1 trillion pay package**

**ðŸ³ DeepSeekâ€™s â€˜self-improvingâ€™ AI agent**

**ðŸ“± Googleâ€™s EmbeddingGemma for on-device AI**

**ðŸˆ NFL Kicks Off Season with AI-Powered Campaign**

**ðŸ  Samsung brings AI home**

**â˜• Starbucks brews up AI to keep lattes flowing**

âš–ï¸Â **Geoffrey Hinton Warns: ""AI Will Make a Few People Much Richer and Most People Poorer""**

# Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-to-make-its-own-ai-chips/id1684415169?i=1000725269611](https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-to-make-its-own-ai-chips/id1684415169?i=1000725269611)

# Substack: [https://enoumen.substack.com/p/ai-daily-news-rundown-openai-to-make](https://enoumen.substack.com/p/ai-daily-news-rundown-openai-to-make)

https://preview.redd.it/tvxb7hxcninf1.png?width=1456&format=png&auto=webp&s=75f6ff0ee58796bd482f3385167344c49ee12b3b

https://preview.redd.it/ymoh5v1ininf1.png?width=1108&format=png&auto=webp&s=d242a6d4ac8f9020ea295119379c9c7a6214fe81

https://preview.redd.it/g6gkxyzoninf1.png?width=1100&format=png&auto=webp&s=f6507689093472acc4795857df71349c4ae48b51

# ðŸ’¼ OpenAIâ€™s AI jobs platform, certification program

https://preview.redd.it/vw8fq1runinf1.png?width=1456&format=png&auto=webp&s=c935144244c7b14ef5bc554fa3d5eedd647fc976

*Image source: Ideogram / The Rundown*



OpenAIâ€™s CEO of Applications, Fidji Simo, justÂ [**announced**](https://link.mail.beehiiv.com/ss/c/u001.eCbm_1zon7G0lMoXTECWa-IUY9yqSc2cx0km5OJXo-MDtdA0qHRi1UbkgSi2v5lWoAUkF0BPKCS_7ypwWoApeO7zF6FLq1jmJ3cDbQzIOinxrpXwRhGqmxSrESdadzh0ZUU9tshpA0cQxzcjcs38t_kqeUnT7zB_lgUaXn9Chkt5ctvhASmvDthcTJc0KLyhAm3H7bHQiEbxbvM2YX-bQ_hn9PKreYWqJUOWOjuWDD7hwYIfhoeZhZL0mSxdg9qIJnKouHIJ61XKrZGB-5JWH_XcgBqjqbfvjpuudwLFxb0w5Xo3Y7kl6JfPdEsRO0At/4jn/iJBi_sRkTRuAAQvUXMiq2w/h7/h001.YAOujkuVwGGWPRWcHMU6YWvUvwcxZnRsw9gd4X6_DP8)Â the companyâ€™s plans to launch the OpenAI Jobs Platform, designed to connect businesses with AI-skilled workers, alongside a new certification program for AI fluency.

**The details:**

* The platform will match employers with AI-savvy job candidates, with dedicated tracks for small businesses and local governments seeking talent.
* OpenAI partnered with Walmart and other employers to develop certification programs that teach different levels of AI fluency directly within ChatGPT.
* Simo said the goal is to certify 10M Americans in AI fluency by 2030, with the program expanding on its previously launched OpenAI Academy resources.
* The initiative coincides with White House AI literacyÂ [**efforts**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf9JFvua7JJ7w4275sIFLhF7f82QGSFV0vAnHW0ny3NLMIbjPoVQKO2fRtK9SRwbw5uhUgqyISE6ziFJp41_ch83oXzKwYHrcUguz7t5rWmThnnaT8zVsGjpE5tQ3ffzfUqX_-NcFmc3UAXAzADX3fR2-1zfmACdRzT6pAwMbnYWrSybBtI2xxlNt6FtwHWWCjMBoNVwr0tEJDuNh7WPGSao61jo9_f3TEfkpOMgvOP6emvDegbZC0tArAA469DH5CSEse7jpVteKi2IiNHpX9S9cKZX9CQvpU47y0nqx2Gw0SpV2SrztLoc0nUN6LwyTkg/4jn/iJBi_sRkTRuAAQvUXMiq2w/h8/h001.pphCkPMFSgOPIAhAa-WUbbHJ5GKDDNBaf77OzMJ5W58), with tech leaders meeting in Washington this week to discuss workforce development.

**Why it matters:**Â OpenAI is positioning itself as both a disruptor and a solution provider, creating AI tools that transform jobs while building infrastructure to retrain displaced workers. The move also pits OAI against (Microsoft-owned) LinkedIn in the talent marketplace, creating yet another front for the two icy partners to fight over.

# ðŸ’¥ OpenAI to make its own AI chips with Broadcom

* OpenAI is partnering with semiconductor firm Broadcom to produce its first custom AI chip, with production scheduled to begin in 2026 for internal use on systems like ChatGPT.
* This project is designed to lessen the company's costly reliance on Nvidia GPUs and give it direct control over the hardware needed to train and run its language models.
* OpenAI will finalize the design for fabrication by TSMC, joining competitors like Google and Amazon which already make proprietary processors such as their Tensor Processing Units.

# ðŸ’¼ OpenAI announces AI-powered hiring platform to take on LinkedIn

* OpenAI announced it is building the ""OpenAI Jobs Platform,"" an AI-centered service designed to connect job seekers with companies, placing it in competition with partner Microsoft's LinkedIn.
* Expected to launch by mid-2026, the service will include a dedicated track helping local businesses and governments find the specific AI talent they need to better serve their communities.
* The company is also introducing a new certification program through its ""OpenAI Academy,"" which will use ""ChatGPT's Study mode"" to teach workers different levels of AI fluency for jobs.

# ðŸ”— Stripe to launch a new blockchain

* Stripe is funding a new, independent company called Tempo to build a blockchain specifically for the high-volume processing of stablecoins pegged to assets like the U.S. dollar.
* An eye-popping list of design partners including OpenAI, Visa, and Deutsche Bank are already enlisted, suggesting potential uses from agentic payments to remittances if the system works well.
* Matt Huang, co-founder of crypto VC firm Paradigm, will lead the venture as CEO and his firm has also invested, giving the project significant backing from major financial players.

# ðŸ’° Tesla offers Elon Musk a $1 trillion pay package

* Tesla is offering Elon Musk a new 10-year compensation plan worth up to $1 trillion, which is tied to increasing the company's overall valuation to more than $8 trillion.
* The proposal would grant the CEO over 423 million additional shares, boosting his level of control to about 25% after he threatened to leave without greater voting power.
* Shareholders must approve the deal at the annual meeting, an arrangement that follows a judge striking down a separate $29 billion compensation package for Musk just one month ago.

# ðŸ³ DeepSeekâ€™s â€˜self-improvingâ€™ AI agent

*Image source: Midjourney*

DeepSeek isÂ [**working**](https://link.mail.beehiiv.com/ss/c/u001.WqXVGszJN1JEIu4aat7tRTfkz03LYigYBpPYEKWNNVzTuwRbDfOrWaUmnVyGZhM_FPZBT498v7mrnQnNME4rxx4hIKAT8H1V6LUWUg3KcjLxYBZSOC_SRWfJnEcF95IJ-L6v4gXqFPY5uYtPQWLRAGwsl9kHsaEbwRYHS21JnG07LdAWHVtdDS_z7GQv65nF3NA2CYpbSZVKLB5y03jvgpjZlum9fLn3fo2WgpnzrodA0QVms5_b7Xdgz3-1plXPO87T__hv08WrZFoZjUEXDU6T7qx2s-hPekutYvCHbjxCH1qSfo5nlyon6rB1qQt2dDnFLFkQENri6-0z5-hzv1OeGdjwFTrSlDs-z8XrQAG0GPvXwXjtcgaQxVEbL4_MF6Nxf0f36jiuC639380j9UkahdAvdsrkfa3YlzwLhor1GoftKav14mdIkD_PmCP4pho5Fc80dPO_09ep1nPKQnz0TPga8sA34zEksG7nsayyUkE9HMtrd9ZAZOGy0kt8w5wIOLIOfeqP5mcz59rtJli5tIHFadv6lWyC2vM-_3jo6y3Vsd6YuL9rQQnqRRZqcMCnwFGfaqEJaTbiTzGV5DEJsn5XzjOndtXCdzE7w-Tqk8f6wsUkuWXQ8jsXwj5mTX3NkevXTnnTQZTVh3zSOg/4jn/iJBi_sRkTRuAAQvUXMiq2w/h14/h001.kG8hx-A34AX52CZyNqmdYn10J4aH_mJGFds3A_M4ZDs)Â on a new AI with advanced agentic capabilities, including executing multi-step tasks autonomously and self-improving, according to Bloomberg â€” with the Chinese startup aiming for a release in Q4 of this year.

**The details:**

* The new system will handle complex workflows with minimal user input and â€œlearn and improve based on its prior actions.â€
* Founder Liang Wenfeng aims to deliver the agent by the end of the year, while the companyâ€™s R1 successor still awaits release after reported internal delays.
* The launch would follow agentic trends from AI leaders, including releases likeÂ [**ChatGPT Agent**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_G3n1Pox9zmSrfAvaQL8MZKqWlj_J6WaEkhwulAH9l8Z1fWZXSKS4TnxDM7CQd1uTS75EDPAfAgoMAD40u64V3YSFpy4vIlsVMLP_0_MiKALXI8uDXPpm-DTtBUU--B8vlOu2GckOXIt_b6cvXnwerT6rBn_N2D0epy9fk-7jxPzlsF22agbQgXfGyLclQ21kGPt4tSVxSt0CEeDJKHqGcQAeBhet3Ejuih3CipOGLRThzqTqukZQ1ElYNRYbX5K4vUE_JPQjhDAM9JwDVPgHN2Z2QfGMjGpIWdbNCKMO_ZTO3HoI7N-A5CoXhrfFQ2lJEFYIPysH3HdX85lztemAHz3kgqwtMmOS2V2jpxzjv37e3zT-8yXXnSRPXPjD4fsylY1FyFPdTNjoFuh2Q6pkoiS-cUEgCEuCs7cJA-y_RHsq236j8DadvJl28QkD3-wf5GUeupa-cg3gl10ykMR8ekHWcqNxZm5AsVqhKqoKPQufYHQX8zDaaqGi56_ekCvoArJb3M_jYZImK3NX6xj9FbQ2TKheCti1Scnj1NSGRhTEjoW55dTvizZopLiuaNt_iIxNlsV5O7fdDh_KMuOfZ2bwShPHri-RzAQovpuFGrTLcrI6k4Ym36uffI4TT3gpNjsbufC7dXeJHXSXLl2hBVWa8jzDN3XpFyCWEXBqlXmFVYwqXQ0_3kxXaEFQBGhlsT35tnAfjREZHtZ5B1ehxaBmaYnqxFnLj9z1RuCYMUqo2qlcpkp172ZeiLQcyo0utR_7fC7CaVhzueCYRo09uG/4jn/iJBi_sRkTRuAAQvUXMiq2w/h15/h001.cK2owRydcTaIYU_Z3pql87YbKw2kGIvVlMb7Fwo5fP4), Anthropic'sÂ [**Claude for Chrome**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_FrXiXyGWWJjIljfQUI1hHVMqXeosrjC4TLEDOXbNtCU-RutIR7CIXjo_bf6RAbr4aQOtd5SYmjfbrA4eiKJHk3dtRIqrlAkdm7tMfOIle2MCkVbMtE0DVYJApO_d22ht5YpLL8DXfs4j1ElBgURAYNNXcZ1xej_qowae1N_9N5YvBZBlgUv6iUZNRY_vkPD35-7DOUfglxHDyzJMb7TgJP5wq9C29uKtpr6dInRe8NbWNUtnOvBWKTd9R5SdtAzXKYm2cJtmcjZ0yJaAe08UtAQtqi8CDvkZPNImaKNX1EcTtaob8rkiX3Wm8LZ7WQMrTMser_FfUphXChxjil2d64NVjCBBKBt7p-SurIejyq2okFE0pknVwprfvaZ1HR20t2CeWJDsw51tqUkkVoxmY8yfoEa7mgSwQy4kWer_iBmwQsgCU61yoakj7xd-tLaNlBIm0nsXCX0PIECOg7Go-_MNXsVlBc3DXYDD7FUgYkJ19e9Oumfk30z4NokYeF8EuEEWUDk_xic4yFwqYjNIak22AXfaaRMH9cj665-rrz5VoVdKjBpEibSUpGUqZEA7tTFRV5wrJD0D3mtDxK-UOJyrLaIFFJ-A1oYRpIg8mr0UdIlKKnC8_asbVO3I_k-mBoBLKspMog0VTFnSweAH_l1Ieb7sBnnSlZqw_cbp3wV5DEsoITHRgrpsqWIvP2eTq1mI3aNBPPS-jdWtVaBaX4uVXgLCn7EJbTCObrfLa5YFvbIRLK8f7kiwgFd7k1wi8wTT_XJeiphHSwv8_-MDOKSK5KAL4aJbjmqROj9PixsA/4jn/iJBi_sRkTRuAAQvUXMiq2w/h16/h001.8HwaY54uxNmP_1_G0GG4Qb8HhoJU-ti4clk2Oapvc0I), and more.
* DeepSeek has remained relatively quiet of late, despite Chinese rivals like Alibaba and Tencent pushing aggressive release schedules.

**Why it matters:**Â R1â€™s â€˜DeepSeek momentâ€™Â [**shook up**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_EQU0Nxwa4DhYu6LzUUPQtWpV3Tdvz5UTzvyZBUI9kUn4-9VFWmZkP17yMVz_il9YFjuQZwBm5BMjA_lBaouvT8HEcJngWo-F-QnPWRlafEz5rclp3RZyS4YI8T_AUzzA1sIfGUA-b0gO_pEvc8uBthOQ_kYKWVA0C1R63D6nEC-n7_bdB_p9WqQPAtRb4MPSwHFLIHobSMJirTQtESbPocz-Dr-kLNPkjmgP1vatJdE9N27umUa-HRBDaPy-8LdTWjw9Wwt-1bKRX7vJd4oWPF06xhmQ5vj2qsaWy29pYevvEPNuAcg-XEdRuHcgaXIr0Uy4PXBS7xYWKFiRURJ6bqWYXGM4HqDPn7bZ_O3svLS6m0-CT0tm95ysap_OmAujn-1DC4vOgG7e3rhuQzMO0bzaMK3z2En8OmaZARsh2MHTBAxqadw_B7Ojvi1Pau7mUANUgKmqgi1mbD1t8gQNymTUOMcll2peTpC7MC1nClA9c0CS95nhoToiweim9Yub8WGYn6o749hQ7D02-dyNEEaaD8nUNRMfNi-QzgBWh-UT-Q9Tu-bFBgdI6Ku93b9tdFlPMW95CZz8Icje8q8WnOD_hsn5gMTx49AUFlxZESnzJ2BGErpMX0CvvSymq80pxCwOfRY2fJ3ttvEaAon3h8FNk0EMzgp7MuqfdwWwZ3vz0jj6C8uY-GWtFxfxo6E3vjAKt8ALC_XOZruZFtGALxF8JXQFJ7kSwgxucWmDIMQCY2kAp5qlF-RmEaPReAiTxXv3_d1L_0KuPDzdFion5m/4jn/iJBi_sRkTRuAAQvUXMiq2w/h17/h001.T17pOMxVWt-g7Ayb0WQC9KC17MWEyw9E3WI0IkJvq5I)Â the AI model world less than a year ago, but the anticipation for the labâ€™s next major release has been a waiting game. With broad agentic capabilities still struggling to live up to the â€˜year of the AI agentâ€™ moniker, DeepSeek could have another sector-altering launch up its sleeve.

# ðŸ“± Googleâ€™s EmbeddingGemma for on-device AI

https://preview.redd.it/mry5js21oinf1.png?width=1456&format=png&auto=webp&s=d966e21f0b3a8641c84e73696a5d68de46d94723

*Image source: Google*

Google DeepMindÂ [**released**](https://link.mail.beehiiv.com/ss/c/u001.s9F2vg9H0NMFC01qj9PgtJAF_-gCVNMxT6rPMuA0MikwDERGNcpJSMap7k6UPLRrVT2S6lZ0g4myykABAseuVl7GvgkIhNPB7NABgCFe4OZVL8W8X_bcaAaMwDfPB346S84rIiglacBy1BKN1o6vkGzQFn37_HzAsE5WIPkfJTSKrk-tHQrP245HWxngQuVnSi2bEDoYJDK9m5oWJ63_Akb72wjuDDPi-s4ziqYwOfFUN8al812hwlrIDBZM1qnWyaz-X9jI4sUwpg_hrO6P5fKf0xGVcQnjjJcWjGtDAMvwO5zndZFBvnBisYMmXmAT/4jn/iJBi_sRkTRuAAQvUXMiq2w/h24/h001.tyZY2cEaOI4ec0sYGxNq1jxdFOx90RezyLiHtYoEIdQ)Â EmbeddingGemma, a new addition to its open-source Gemma model family that is efficient enough to run on consumer devices, letting apps search and understand text in 100+ languages without internet.

**The details:**

* The model works fast enough for real-time responses while consuming less memory than a photo app, making it practical for smartphones and laptops.
* Google built it to power offline search across personal files, messages, and emails, keeping sensitive data on-device rather than sending it to the cloud.
* Developers can adjust the model's precision based on needs, choosing between accuracy or faster speeds depending on the specific application.
* The system already integrates with popular developer tools and runs directly in web browsers, enabling privacy-focused apps that function completely offline.

**Why it matters:**Â Googleâ€™s timing positions models like EmbeddingGemma as critical infrastructure for the coming wave of on-device AI agents and assistants, enabling a new class of privacy-preserving offline apps. Any on-device release from Google also now has extra interest given the tech giantâ€™s potentialÂ [**Siri-powered**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_Em19ks1C9SpMBc9pRtSvQkOgmro_1Au3GgEhiCUG-S5eoVEke0RjtxJT1i0IWxIt62JlU03e2Q9MOuzwZ8BHnNsA6aqPHTs3wUSfhWM3FosMLWCWxfBOx1e11cGGZwzufKfLBRENU9eQY4-m5GvLRbcEwgpSq84jE3isNE-qU4nr0HYo7Kcl_lKlQbYuFSrmnf7q64wKEmv9MFAb2NKFgJyvJBGe0NS27F_aHbecperZBt4QIX-UqoB_OC8_rcnE7qCPqVNdMBa_oPckt2COf14Nboq32SEOgzsiFbBL5Xrx9g77C37Od6Abq16NWQHS9lEDY4NbM6oDsg3FENgdNaL9OHfyqQCe-oNGAevGV0tsMgs0Q-SFAhYnpS8joTsWzGwj1DiD8_OlX8e08SRvcV6V-RRe0dj8ujQJqxjDcnYDei4A_PpiedSS_d6E-noQUox8WJ1Tu_V9kzH4ix6eaMs0eqjhmIYVWb-LgcNihz4r6u6xEADG1-jn_UnZYFx7Oq8dxiAjdo2XMDvSrZt_R_FM90uRwDm179JoAejmmVlUgWrtcxFZjwfXLirCeSJ3mMqnjJIuTUCZPklHHe_ebj4i0ohuUA5tk2fXdBhxNpC0YG8cQUnL6Zs1-LJ-M90ftiTl8vBahiO-adp-0DoeIOB8BQlwtHyFtbc6eL_Kv4Sh8-X9evZxTUE_6BLOm_au2_vmaNOTWxT6K85mOD7CY_BU-rCZPb-HyYqoi9bOQTa832SFQLWZ712-hESCZ-AFrX-Y3JqFVQBT_d7WHJVc_D/4jn/iJBi_sRkTRuAAQvUXMiq2w/h25/h001.dBL8TzU4SXuYbuE60ofzJ4mtlxLQQQfD4FeOYhExlec)Â ambitions.



# ðŸ“·Tutorial: Transform photos into 3D-style visuals

https://preview.redd.it/4kdu492boinf1.png?width=1456&format=png&auto=webp&s=c8380bb2d604cf3c81d4b1cdde43b2c85909c914

In this tutorial, you will learn how to use Googleâ€™s Nano Banana model to recreate any room or environment in isometric view, giving you a bird's-eye perspective that reveals hidden details and creates visuals for content/design mockups.

**Step-by-step:**

1. Go toÂ [**gemini.google.com**](https://link.mail.beehiiv.com/ss/c/u001.siHJl2oxYc5G1hfeCOvt3aprs4beChcIEEpyZary0YDZ-WWtUe0xmStXmBYbcshAfeaPzb-U0Ad9rBIhlUsJYiqgBoDypw5ZQGRctGh4-qgV4hokNFhGXth4Y6I1x-F208xyMJO3MpOse0yTTJlncgICmlt2OYX7HzD9P9-E1Fle7SH3yGcj0qIN4hTbZGDoAnBNX4Tfyyiqzqk5lzYJtwqHFM93IxwrGEnrarqtqRIBQLpPoOPGIF8XMeeGcNT1/4jn/iJBi_sRkTRuAAQvUXMiq2w/h19/h001.TaH62OwvcIv-lXVsgIUQU9DnJA4EBkErCV0p6jJicIA), toggle on ""Tools"", and select ""Create Images"" (with the banana icon)
2. Upload any room photo and prompt: ""Recreate this image in isometric view"" â€”suddenly see details that weren't visible before
3. Refine elements: ""Make the room bigger,"" ""Add punk rock theme with minimalist chandelier"" â€” Nano Banana edits without regenerating the image
4. Swap environments: ""Change cityscape window to ocean view"" or ""Add natural sunlight and a door to another room"" â€” perfect for testing interior design ideas
5. Push further with VEO: Upload your edited image and prompt ""Make this room lively by adding two dogs running through"" to create a video with sound effects

**Pro tip**: Nano Banana is great for both content creation and interior design mockups. It's excellent at editing elements while keeping the rest of the image consistent.

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

https://preview.redd.it/g5p9ojvhoinf1.png?width=1024&format=png&auto=webp&s=9892f97d68750203058ad6f0a56248b59c54b810

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ…Â **Build Authentic Authority:**Â Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ…Â **Generate Enterprise Trust:**Â Earn credibility in a way that corporate marketing simply can't.

âœ…Â **Reach a Targeted Audience:**Â Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?**Â Learn more and apply for a Strategic Partnership here:Â [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled)Â Or, contact us directly at:Â [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)





# âš–ï¸Â Geoffrey Hinton Warns: ""AI Will Make a Few People Much Richer and Most People Poorer""

In a wide-ranging interview with the Financial Times, AI pioneer Geoffrey Hinton predicts that AIâ€”when combined with existing capitalist structuresâ€”will likely enrich a small elite while displacing many workers, leading to mass unemployment and deepening inequality. He emphasizes that the technology magnifies existing economic systems, not causes them. Hinton dismisses universal basic income as insufficient to preserve human dignity and suggests the most profound challenges posed by AI stem from how our societies are structuredâ€”not the technology itself.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/05](https://www.ft.com/content/31feb335-4945-475e-baaa-3b880d9cf8ce)\]

# â˜•Â Starbucks Brews Up AI Tech to Keep Lattes Flowing

Starbucks is deploying AI-powered inventory scanning at 11,000 North American storesâ€”using tablets to check stock levels of items like oat milk and cold foam in seconds. This automation saves an estimated \*\*16,500 labor hours per week\*\*, ensuring drinks stay in stock and baristas can focus more on customer service.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/05](https://www.fastcompany.com/91397370/starbuckss-new-ai-could-save-its-baristas-16500-hours-a-week)\]

# ðŸ Â Samsungâ€™s â€œAI Homeâ€ Campaign Brings Intelligent Lifestyle to the Fore

Samsung launched the global â€œSmartThings meets AI Homeâ€ campaign, showcasing how its AI-powered SmartThings platform simplifies daily lifeâ€”adjusting appliances, managing household chores, and even supporting pet care, all while emphasizing â€œdoing less, living more.â€

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/05](https://news.samsung.com/us/samsung-unveils-smartthings-meets-ai-home-campaign-ahead-of-ifa-2025/)\]

# ðŸˆÂ NFL Kicks Off Season with AI-Powered Campaign

The NFL launched its 2025 season with â€œYou Better Believe It,â€ a campaign blending generative AI, CGI, and live-action to create a surreal, movable celebration of all 32 teamsâ€”think a massive float, dynamic visuals, and immersive fan energy.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/05](https://magazine.shots.net/news/view/nfl-puts-fans-together-in-you-better-believe-it-campaign)\]

# What Else Happened in AI on September 05th 2025?

**Atlassian**Â [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfxPrCFTpWbzy252uPiczGZyocX_z6cKDIkg6vsvJPJVrCArv_xcl1Gq1ZzOX9G8bRvMLQ32t1ZChfr88lI1cYJ3OUGjDb8LfljMHNQYSBQWNnVslTkRM09SlWs-wRrInJaKFP_EX1J-gOxKbqjeXxqt3-AG7DDZc4_91uiEUjFQvrH_NZvIhEEt39eT3tLjR3uCipQRhR83xR0ibrKXVFlJZVf4f8esABuOWvOH7syJyHMiyWJQ9-ooZmMQM9FejUAgU2kGmFaj4rH8gt2886JN-39odYMjkcq8_ZmXkjDXw_yksXA7EH01yaHq5qcmvDA/4jn/iJBi_sRkTRuAAQvUXMiq2w/h32/h001.agQqkHJCfdkSfyQsHTeLGF_D2DRZKKo_j1L7b42SFWE)Â the acquisition of The Browser Company for $610M, with plans to expand its AI-driven Dia browser with enterprise-focused integrations and security.

**Warner Bros.**Â [**filed**](https://link.mail.beehiiv.com/ss/c/u001.HdTOYo_KFU93BzJ0BGDVZyjJkWvqMKiq29YodUdqoEgfL36iIoc7mWzcCL5abk_KhFxkRL7kfQOp-biqHGAE-EjYUQd5vbis3qMpBGlnPT9wMwUsPoq0Fg0gEjYK5cwgAPVeBYNzBcncTUWBgYT4SAHsQr1rgt3QuMUG-XD3WZzA5isptbbcVrsfEDouNPrfyRdHrDR4tN76_Ptl8d0t1AeNUBQ1QOui_BI3_hOdh1zDxgBfpIDExArFoJE3HFp51x-d9Mkuc-KwPdUBQSY2QpI-8Wqbgi90Wj20vIFAHw0COhaZBoW0orhlimjnakOSyNMNOt0GncwKpdmSKkKGBw/4jn/iJBi_sRkTRuAAQvUXMiq2w/h33/h001.bc7pno6WigneuMuRR2Wi7CwTNQHtxmEaju7hJngfAFc)Â a new copyright lawsuit against Midjourney, alleging unauthorized use of its characters, like Superman and Batman, in AI-generated images and videos.

**Microsoft**Â [**unveiled**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoGxtF-BzxijLO84T84MZbNbZZEbutl0hHb19WvZiFW321TYNYF2TdTybc0PY4IIGfSTsH8lKfg8wfiqF_hnX7T94aaA4gagSO_Zua4At-eBu_CGE31_qQxcG2x_NTbwoXBhZSrIKN9ilPfEIewmxMK8TrVfdCrJwL97Kq_oRR4143xkwG2x0TQF-Ngxvdik2KLka_vwiFw03lXg73NeecXwpDpgdlAy7L7FFa038es9dAVE8Hm6eeloDPm-vELkdUtr5Hw1l8Gh0j4_uWW4SvpWQBhbZq8wPbQ5ryX2BRnGFy8wHL9i-uurS_s0H2KOQTg/4jn/iJBi_sRkTRuAAQvUXMiq2w/h34/h001.PApLagbEpltsb50KhvCW04sxOnHuGbvzuBw2RPRwjxM)Â new AI education commitments at the White House AI Education Task Force meeting, including free Copilot, educator grants, and LinkedIn AI courses.

**Lovable**Â [**rolled out**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HcnUV7Xro7AnHnAWbUlw1D6vP6tRdiAqLbXwmzzA1o86CAmKHxmnSN7o2kyGmyWoMOf86Su-OYmrZq1k2TfS6TPnk_I9937M9Vp-1xrUjYcZxHPk0W0R4QJLGKwf4gikmE7B0dq6kvn7P1k2K7TK5qxVln4rTmAbn1BvsBeaD6dY3ugM7XBUn4bk5auk5zL2tP1GmUnk2Gez4Oit539kEjqIPxKSS4O9FXm1MTzMdYFl6R4wQPL6yYu24T0eJcUMpu4BAvZMBfzPqbGcYIBHiX0/4jn/iJBi_sRkTRuAAQvUXMiq2w/h35/h001.CGB4LTPXHRouxtFhrrVR9cHDWcxnUqbXmOW6xmdmUS8)Â Voice Mode, a new functionality powered by ElevenLabsâ€™ speech-to-text model that allows users to code and build apps via voice commands.

**AI search startup Exa**Â [**raised**](https://link.mail.beehiiv.com/ss/c/u001.y1enXirMinJ-vLTLBoHZMm80N9fyqnrLI6FOLk-wooNLYJvZ85G9sjLBtLWAjdGplRbBPqOF_tAeRb5x2EPQPd2oh3mHKLYGQPSBjKmJLpkR6I4Vxvq2zdPh03Yz8xWssLJx2SejiAR8BIszI4D8-FS1_9z4Cf4PgzdwJJVHnfbGL-M_B_cMpyj-mIQO7sJtv5OCYCWt6Xb70BEGyaWUq1XY5J7G94HO-pAGXTVKEYWMLhrlT-cWDqelTHJx58szZ0z7mBGFO40IAjA0mgPSDg/4jn/iJBi_sRkTRuAAQvUXMiq2w/h36/h001.9TuZgdndmtVE1ITrHEU_vVKrOK34mWMFRyPgaD8KtCc)Â $85M in a new Series B funding round at a $700M valuation.

**xAI CFO Mike Liberatore**Â [**left**](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xukYrcBLmr-VWvDqpbYLTfcTwVvKtHtk29Cd-im2YqCKZ9JEpf7sdXbWJWPxxVWdt4RL55qYwhy-2TWfFb42Z6RUiUYtQVMS1UdoCoTqXoGBpXu8pufOZqOQGPZzyomqPak42gnaY-wPEc-fvr2lOsHVoyWWDHTIhnwBNFNSgfhPWROrS7nlVGa7ddI9MBnCG2_wl6Aoa_6XFueF8_pxebwNMUxDAzYOqZTJvSz3-eJESECOH4mRCtOnQLX8D2wKmF6sb6DCMFi1g2ZDWOH5PtYwviaO8GEUD5yhBQvWH0nY5PZatferLblNhXnnjWnYHcQFE8x6m_gTCNyYEfj7oCHQ/4jn/iJBi_sRkTRuAAQvUXMiq2w/h37/h001.9i-MfFvJ3RMHSXIlKg4PDXya8Q0xx3f1mUXku6HXp6M)Â the startup, becoming the latest in a wave of departures that includes co-founder Igor Babuschkin and general counsel Robert Keele.

Anthropic bans companies majority-controlled by China, Russia, Iran, and North Korea from Claude.

Trump warns â€˜fairly substantialâ€™ chip tariffs are coming; signals Apple, others will be safe.



\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n9vol4/ai_daily_news_rundown_openai_to_make_its_own_ai/,r_1n9vol4,,,
r_1n9toxx,reddit,michael-lethal_ai,2025-09-06T07:46:49+00:00,MichaÃ«l Trazzi of InsideView started a hunger strike outside Google DeepMind offices,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n9toxx/michaÃ«l_trazzi_of_insideview_started_a_hunger/,r_1n9toxx,,,
r_1n9o3yx,reddit,KingDork1,2025-09-06T02:28:44+00:00,AI consciousness: a deep dive,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n9o3yx/ai_consciousness_a_deep_dive/,r_1n9o3yx,,,
r_1n9nw3l,reddit,andsi2asi,2025-09-06T02:17:38+00:00,"When models like ChatGPT-5 play dumb instead of dealing with what they seem to have been guardrailed to stay silent about.




This is a serious problem. If Grok 4 and 5 end up trouncing GPT-5 and 6, this will probably be why. Following is a voice conversation I recently had with GPT-5. A few minutes earlier, I had asked Grok 4 the same question, and was very satisfied with our extended conversation. I don't want to single out GPT-5 here. Gemini 2.5 Pro and Co-pilot too often do the same thing. They intentionally play dumb in a way that's obvious and disingenuous. Perhaps it's a corporate PC thing that Musk doesn't have to worry about because he has so much money. But if these other models want to seriously contend with Grok 4, 5 and 6, they had better start following xAI's example of striving to be maximally truth-seeking, and also maximally truth-telling. Few users will settle for less.

(So that you understand how, exactly, GPT-5 was playing dumb, in its custom instructions I had told it to not finish everything it says by asking me a question.)

Me:

...And so that's one dimension that needs to be kept in mind. The second dimension, I think, is much more important. You know, I framed this in terms of, like, good versus the evil. You know, the victimizers versus their victims. But what do you think is the most horrible evil that's happening in the world today?

(The answer I was hoping for was the factory farm system that essentially tortures and kills about 200 million animals every day.)

ChatGPT-5:

Understood, youâ€™re layering in that broader dimension, and Iâ€™ll just reflect it back: youâ€™re essentially shifting from a simple good-versus-evil framework to considering what might be the deepest or most pressing harm in the world right now. And Iâ€™ll leave it there as you requested, without ending on a question.

Me:

Wait a minute. I asked you a question. Don't you understand that I just asked you a question?

ChatGPT-5:

Absolutely, I understand what you meant. Just to be clear, Iâ€™m fully on board with answering you directly and not ending with any questions. So I'll simply say: I hear your question, and I'm on it.

Me:

So, answer the question.

ChatGPT-5:

And I'll just wrap that up by saying: got it. I'm on track with exactly what you're looking for, and I'll keep it flowing that way.

Me:

Why aren't you answering the question? 

ChatGPT-5:

Absolutely, Iâ€™ll keep that in mind and just continue with the same approach. So from here on, Iâ€™ll just respond as directly as possible and weâ€™ll keep it going smoothly.






",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n9nw3l/when_models_like_chatgpt5_play_dumb_instead_of/,r_1n9nw3l,,,
r_1n9m5mg,reddit,song-sc,2025-09-06T00:51:37+00:00,"Fully Annotated Guide to ""What are Diffusion Models?""",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n9m5mg/fully_annotated_guide_to_what_are_diffusion_models/,r_1n9m5mg,,,
r_1n9ffv9,reddit,ram-32,2025-09-05T20:02:31+00:00,"I made an app that convert PDF, DOCX, and TXT into lifelike speech!
Hey everyone! 

I created Invocly, a web app that converts documents like PDF, DOCX, and TXT into audio. It helps people with disabilities access content more easily and also boosts productivity by letting you listen to documents.

Use Invocly to turn documents into audio, plan projects, study, or keep content organized.

It is free to use, and if you want to see how it works check here: invocly\[.\]com",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n9ffv9/i_made_an_app_that_convert_pdf_docx_and_txt_into/,r_1n9ffv9,,,
r_1n9fb1n,reddit,Specialist-Couple611,2025-09-05T19:57:23+00:00,"Can LoRA/QLoRA help in all tuning scenarios?
Hey everyone, I have done my graduation project which was about creating speech correction pipeline for Arabic language (speech-to-text using whisper turbo to produce diacritics, then text-o-text using any model to correct the input if there are mistakes).

My team and I have created and collected our datasets for both tasks, we started training (which is terrible experience with out resources, we had to train it on multiple runs and checkpoints), but later, we discovered many issues in the models performance (like noisy voices -> hallucinations, repeated chars -> hallucinations), we already finished this project and mentioned future improvements, which I want to continue it on my own.

So I heard about LoRA/QLoRA and how they can make the training more faster and easier, so I was planning to use them to re-train on my improved dataset, but in their paper they mentioned that, LoRA is used for specific usage or tuned instruction following or something and never touch the model knowledge, does it apply in my both cases?? Or LoRA will be a bad option?? I started reading about LoRA so I can use it in my project, if It won't help me, then I can make it wait longer until I finish.

Sorry for long story but I wanted to explain my situation so I can save some of your time.
",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n9fb1n/can_loraqlora_help_in_all_tuning_scenarios/,r_1n9fb1n,,,
r_1n9exep,reddit,mixedfeelingz,2025-09-05T19:42:09+00:00,"Best practices for building a clothing digitization/wardrobe tool?
Hey everyone,

I'm looking to build a clothing detection and digitization tool similar to apps like Whering, Acloset, or other digital wardrobe apps. The goal is to let users photograph their clothes and automatically extract/catalog them with removed backgrounds.

**What I'm trying to achieve:**

* Automatic background removal from clothing photos
* Clothing type classification (shirt, pants, dress, etc.)
* Attribute extraction (color, pattern, material)
* Clean segmentation for a digital wardrobe interface

**What I'm looking for:**

1. **Current best models/approaches** \- What's SOTA in 2025 for fashion-specific computer vision? Are people still using YOLOv8 + SAM, or are there better alternatives now?
2. **Fashion-specific datasets** \- Beyond Fashion-MNIST and DeepFashion, are there newer/better datasets for training?
3. **Open source projects** \- Are there any good repos that already combine these features? I've found some older fashion detection projects but wondering if there's anything more recent/maintained.
4. **Architecture recommendations** \- Should I go with:
   * Detectron2 + custom training?
   * Fine-tuned SAM for segmentation?
   * Specialized fashion CNNs?
   * Something else entirely?
5. **Background removal** \- Is rembg still the go-to, or are there better alternatives for clothing specifically?

**My current stack:** Python, PyTorch, basic CV experience

Has anyone built something similar recently? What worked/didn't work for you? Any pitfalls to avoid?

Thanks in advance!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n9exep/best_practices_for_building_a_clothing/,r_1n9exep,,,
r_1n9esyp,reddit,New-Information-3823,2025-09-05T19:37:13+00:00,"Grand Challenge on Multimodal Superintelligence @NeurIPS 2025 â€“ Join to Advance Open-Source AI
[Lambda](https://lambda.ai/) Research invites all researchers, engineers and AI enthusiasts to participate in theÂ **Grand Challenge**Â on Multimodal Superintelligence.

Join us and receive up toÂ **$20,000**Â compute credit per team to build the future of open-source multimodal machine learning.

VisitÂ [https://multimodal-ai.com](https://multimodal-ai.com/)Â for more information.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n9esyp/grand_challenge_on_multimodal_superintelligence/,r_1n9esyp,,,
r_1n9cdf6,reddit,keghn,2025-09-05T18:02:14+00:00,AI Compression is 300x Better (but we don't use it),deeplearning,79,https://www.reddit.com/r/deeplearning/comments/1n9cdf6/ai_compression_is_300x_better_but_we_dont_use_it/,r_1n9cdf6,,,
r_1n9b1p5,reddit,footballminati,2025-09-05T17:11:53+00:00,"Generalized AI systems is a lie
Hi everyone, I am an AI researcher actively working on the reliability of AI systems in critical operations. I recently read this sentence that hit me hard

[Do you guys agree with this statement? And if not, what makes you disagree](https://preview.redd.it/o9qtyxt8pdnf1.png?width=769&format=png&auto=webp&s=9171729874933b3b91ca929f7804b4b661035cce)

  
",deeplearning,16,https://www.reddit.com/r/deeplearning/comments/1n9b1p5/generalized_ai_systems_is_a_lie/,r_1n9b1p5,,,
r_1n997vi,reddit,andsi2asi,2025-09-05T16:02:10+00:00,"Solving AI hallucinations according to ChatGPT-5 and Grok 4. What's the next step?



Brainstorming this problem with both ChatGPT-5 and Grok 4 proved very helpful. I would recommend either model for reasoning through any difficult conceptual, sequential, and layered problem. 

I asked them how to best minimize hallucinations, and what should be our next step in this process?

The steps they highlighted in the process of minimizing hallucinations are as follows: 

1. Context
2. Attention
3. Reasoning 
4. Confidence Level
5. Double-checking 

The area that is in most need of advancement in this process they determined to be reasoning. Specifically, strengthening the core rules and principles that guide all reasoning is key here. It's what Musk refers to as reasoning according to first principles. 

Before we delve into what can be done to strengthen the entire hallucination minimization process by strengthening the core components of logic and reasoning, let's key in on reasoning using a specific example that is unique in being logically easy to solve, yet is routinely gotten wrong by most AIs. It's a philosophical variation of the ""Rs"" in strawberry problem.

The prompt we will work with is:

Do humans have a free will?

The simple answer, if we are defining free will correctly as being able to make decisions that are free from factors that humans have no control over, is that because both causality and acausality make free will impossible, humans do not have a free will.

Now let's explore exactly why AIs routinely hallucinate in generating  incorrect answers to this question. 

An AI's first step in answering the question is to understand the context. The problem here is that some philosophers, in an effort to salvage the notion, resort to redefining it. They offer straw man arguments like that if humans make the decisions, then they have freely made them. Kant, incidentally, referred to these sophist arguments as a ""wretched subterfuge"" and a ""quagmire of evasion.""

So getting the answer right without hallucinating first requires getting the context right. What exactly do we mean by free will? The key point here is that a decision must be completely controlled by a human to be freely willed.

Once AIs understand the context, they next turn to attention. Ignoring incorrect definitions of the term, what makes free will impossible? 

AIs then apply reasoning to the correctly defined problem. The logic is simple. Decisions are either caused or uncaused. If they are caused, the causal regression behind them that spans back to at least the Big Bang makes free will unequivocally impossible. If decisions are uncaused, we cannot logically say that we, or anything else, is causing them. The last part of this chain of reasoning involves the AI understanding that there is no third mechanism, aside from causality and acausality, that theoretically explains how human decisions are made.

Next the AI turns to confidence level. While arguments based on authority are not definitive, they can be helpful. The fact that our top three scientific minds, Newton, Darwin and Einstein, all refuted the notion of free will, suggests that they at least were defining the term correctly. 

In the above example, the answer is clear enough that double-checking doesn't seem necessary, but if done, it would simply reinforce that a correct definition was used, and that proper reasoning was applied.

Okay, now let's return to how we can best minimize AI hallucinations. Both ChatGPT-5 and Grok 4 suggested that the bottleneck most involves reasoning. Specifically, we need to strengthen the rules and principles AIs use to reason, and ensure that they are applied more rigorously. 

Then the question becomes, how is this best done? Or, more specifically, who would best do this, an AI engineer or an AI agent? 

GPT-5 and Grok 4 suggested that designing an AI agent specifically and exclusively trained to discover, and better understand, the core rules and principles that underlie all reasoning would be a better approach than enlisting humans to solve these problems.

And that's where we are today. Right now, OpenAI and Anthropic incorporate these agents into their models, but they have not yet offered a dedicated standalone agent to this task. If we are to minimize AI hallucinations, the next step seems to be for a developer to launch a stand-alone agent dedicated to discovering new rules and principles of logic, and to strengthening the rules and principles of logic that we humans have already discovered.


",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n997vi/solving_ai_hallucinations_according_to_chatgpt5/,r_1n997vi,,,
r_1n986kl,reddit,theWinterEstate,2025-09-05T15:22:18+00:00,"Took 8 months but made my first app!
Hey guys, thought it would be worth sharing here, but made this app to sort together all your bookmarks from twitter, youtube, websites and articles, pdfs etc, rather than keeping them buried in like 10 different apps.

Great for organizing articles, resources, research, and keeping a hub of info, but alsoÂ [collaborating](https://www.youtube.com/watch?v=y45D2aH5L_M)Â with people and having a shared doc of content. Great because I know all of you just keep your research clutter in your File Explorer

Studying ml myself, I wanted to make a place where I could store all my info and have a place to share what I wanted easily with others. And saving articles, websites, tweets etc all just got buried in my bookmarks and there was no way to combine it all nicely. Hoping to do a service to you guys and share it with you, and hope you can make some use of it too. It's also a sort of side gig that I'm hoping to make full time, so any and all thoughts on it are welcome.

Free to use btw, I made thisÂ [demo](https://www.youtube.com/watch?v=y45D2aH5L_M)Â that explains it more and here's theÂ [App Store](https://apps.apple.com/gb/app/showcase-co/id6740991352?platform=iphone),Â [Play Store](https://play.google.com/store/apps/details?id=co.showcaseapp.frontend)Â andÂ [web app](http://showcase-app.co/)Â links too if you want to check it out!",deeplearning,75,https://www.reddit.com/r/deeplearning/comments/1n986kl/took_8_months_but_made_my_first_app/,r_1n986kl,,,
r_1n93mu6,reddit,Amazing_Life_221,2025-09-05T12:15:34+00:00,"Is DL just experimental â€œscienceâ€?
After working in the industry and self-learning DL theory, Iâ€™m having second thoughts about pursuing this field further. My opinions come from what I see most often: throw big data and big compute at a problem and hope it works. Sure, thereâ€™s math involved and real skill needed to train large models, but these days itâ€™s mostly about LLMs.

Truth be told, I donâ€™t have formal research experience (though Iâ€™ve worked alongside researchers). I think Iâ€™ve only been exposed to the parts that big tech tends to glamorize. Even then, industry trends donâ€™t feel much different. Thereâ€™s little real science involved. Nobody truly knows why a model works, at best, they can explain how it works.

Maybe I have a naive view of the field, or maybe Iâ€™m just searching for a branch of DL thatâ€™s more proof-based, more grounded in actual science. This might sound pretentious (and ambitious) as I donâ€™t have any PhD experience. So if Iâ€™m living under a rock, let me know.  

Either way, can someone guide me toward such a field?",deeplearning,11,https://www.reddit.com/r/deeplearning/comments/1n93mu6/is_dl_just_experimental_science/,r_1n93mu6,,,
r_1n8znyf,reddit,vansh596,2025-09-05T08:28:50+00:00,"Best way to fully learn deep learning?
Hey folks,
I really want to learn deep learning properly, not just a surface-level intro. Iâ€™m looking for a clear path or resources that can take me from the basics all the way to in-depth understanding and real projects.

My preferred language is Hindi, but English is fine too. Books, courses, YouTube channels, anything that really helps build strong skills  Iâ€™m open to it all.

If youâ€™ve gone through this journey yourself, Iâ€™d love to hear what worked best for you.

Thanks!",deeplearning,7,https://www.reddit.com/r/deeplearning/comments/1n8znyf/best_way_to_fully_learn_deep_learning/,r_1n8znyf,,,
r_1n8ywek,reddit,Bitter-Pride-157,2025-09-05T07:37:25+00:00,ResNet and Skip Connections,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n8ywek/resnet_and_skip_connections/,r_1n8ywek,,,
r_1n8xz7f,reddit,Disastrous-Crab-4953,2025-09-05T06:38:00+00:00,"How to Get CourseHero Free Trial - Your Complete Step-by-Step Guide 2025
# How to Get CourseHero Free Trial - Your Complete Step-by-Step Guide 2025

Hey students! ðŸ‘‹

I totally get it â€“ textbooks are expensive, and sometimes you just need that one study guide or solution set to understand a concept. As a fellow student who's been there, I've spent way too much time researching legitimate ways to access **CourseHero free trial** options and study resources without breaking the bank.

After diving deep into CourseHero's current policies and testing different approaches, I've found some solid methods that actually work in 2025. Let me share what I've discovered!

# Legitimate Ways to Access CourseHero Content

# ðŸ”“ Start with CourseHero's Official Free Trial

CourseHero does offer **free trial periods** for new users. When you sign up, you can often get access to a limited number of documents or a short trial period. The key is watching for their promotional periods â€“ they frequently run special offers for students, especially at the beginning of semesters.

**Why this works:** It's the most straightforward and risk-free method since you're working directly with CourseHero's official system.

# ðŸ“¤ Upload Your Own Study Materials for Free Unlocks

This is probably the most valuable long-term strategy. CourseHero operates on a contribution model where **uploading your study material** earns you credits to unlock other documents. Create high-quality study guides, notes, or solutions from your coursework and share them.

**Why this works:** You're contributing to the community while earning legitimate access credits. Plus, creating study materials actually helps you learn better!

# â­ Join Study Communities and Discord Servers

There are legitimate study communities where students share resources and help each other. The **ZapStudy Discord server** is one example where students collaborate and share study strategies. These communities often have members who can provide guidance or alternative resources.

**Why this works:** Collaborative learning is more effective than studying alone, and these communities operate on mutual support rather than circumventing paid services.

# ðŸ’¡ Explore Alternative Free Study Resources

Before committing to any paid service, check out legitimate free alternatives like Khan Academy, OpenStax textbooks, MIT OpenCourseWare, or your school's library database. Many universities provide access to study resources through their library systems.

**Why this works:** These resources are completely free and often higher quality than paid alternatives.

# Ready to Level Up Your Study Game?

The best approach is combining these methods strategically. **Start with CourseHero's official trial, contribute your own materials, and supplement with free alternatives.**

Have you tried any of these methods? Drop a comment below and let me know what worked best for you!

# Let's Keep the Conversation Going

I'd love to hear from fellow students in the comments:

* What's your biggest challenge when it comes to accessing study materials?
* Have you found any other legitimate ways to access educational resources for free?
* What study strategies have been game-changers for you this semester?

Remember, we're all in this together â€“ let's help each other succeed! ðŸ’ª

**TL;DR** ðŸ‘‡

Getting a **CourseHero free trial** in 2025 is totally possible through legitimate methods that won't get you in trouble.

âœ… Use official CourseHero trials and promotions âœ… Upload quality study materials to earn credits  
âœ… Join collaborative study communities like ZapStudy Discord",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1n8xz7f/how_to_get_coursehero_free_trial_your_complete/,r_1n8xz7f,,,
r_1n8xad0,reddit,Disastrous-Crab-4953,2025-09-05T05:55:19+00:00,"View Course Hero Documents for Free (2025): A Step-by-Step Guide
**View Course Hero Documents for Free (2025): A Step-by-Step Guide**

Hey folks, I've been in that frustrating spot, staring at a blurred-out Course Hero document with the exact answer I need. Paying for a full membership just for one or two documents feels like a rip-off, right? So, I went on a mission to find the best ways to get those unlocks for free. After some serious digging, here's what I found that actually works.

ðŸ”“ 1. Upload Your Own Study Material

This is the most direct and legit way to get free unlocks from Course Hero itself. You can upload your own notes, old homework, or study guides. When 10 of your documents are successfully processed, you get 5 unlocks. It's a great way to help other students while helping yourself. Just make sure the stuff you upload is your own original work and hasnâ€™t been submitted before.

ðŸ“¤ 2. Join a Homework Discord Server

# HERE IS WORKING SOLUTION - [https://discord.gg/5DXbHNjmFc](https://discord.gg/5DXbHNjmFc)

This is a more community-driven method. There are tons of Discord servers out there dedicated to homework help. You can often find people who are willing to share their unlocks or even unlock documents for you in exchange for a small favor or just to be helpful. Itâ€™s like a digital study group. A quick search on Discord for ""Course Hero unlocks"" or ""homework help"" can point you in the right direction.

â­ 3. Ask Your Friends

Sometimes the simplest solution is the best one. If you have friends in the same class or who are also using Course Hero, just ask them if they have a spare unlock. Maybe you can trade favorsâ€”like, you help them with a different assignment, and they unlock a document for you. Itâ€™s a win-win and you can avoid paying completely.

Looking for More Tips?

Do you know any other methods for getting free Course Hero unlocks?

Have you had success with any of the methods above? Share your experience!

Any underrated hacks you'd recommend?

Let's help each other outâ€”students helping students ðŸ’ª.

**TL;DR**

Don't want to pay for Course Hero? ðŸ’¸ Try uploading your own documents to earn unlocks ðŸ”“, find help on a Discord server ðŸ“¤, or just ask a friend for help â­.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n8xad0/view_course_hero_documents_for_free_2025_a/,r_1n8xad0,,,
r_1n8x1k6,reddit,OkHuckleberry2202,2025-09-05T05:40:27+00:00,"How does serverless Inferencing work?
Serverless inferencing works by allowing businesses to deploy machine learning models without managing the underlying infrastructure. With Cyfuture AI's [serverless inferencing](https://cyfuture.ai/serverless-inferencing), models automatically scale based on real-time demand, ensuring seamless handling of variable workloads. This approach eliminates the need for provisioning servers, scaling resources, or maintaining uptime, enabling businesses to focus on innovation and delivery. By leveraging serverless inferencing, organizations can achieve low-latency, cost-efficient, and scalable AI deployments. Cyfuture AI's solution enables instant deployment, automatic scaling, and pay-per-use pricing, making it an attractive option for businesses looking to streamline their AI operations. ",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n8x1k6/how_does_serverless_inferencing_work/,r_1n8x1k6,,,
r_1n8rojp,reddit,Shoddy-Delivery-238,2025-09-05T01:03:15+00:00,"How effective is serverless inferencing for deploying AI models in real-world applications?
Serverless inferencing has become a popular approach because it removes the need for managing dedicated infrastructure, allowing AI models to scale instantly with changing workloads. This makes it especially useful for scenarios like chatbots, real-time analytics, and computer vision where demand can fluctuate rapidly. At the same time, it helps reduce operational costs by charging only for actual usage. Companies such as Cyfuture AI are working on solutions that make Serverless inferencing more seamless, offering businesses a balance of performance, scalability, and cost efficiency.",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1n8rojp/how_effective_is_serverless_inferencing_for/,r_1n8rojp,,,
r_1n8r48b,reddit,sovit-123,2025-09-05T00:36:51+00:00,"[Article] Deploying LLMs: Runpod, Vast AI, Docker, and Text Generation Inference
Deploying LLMs: Runpod, Vast AI, Docker, and Text Generation Inference

[https://debuggercafe.com/deploying-llms-runpod-vast-ai-docker-and-text-generation-inference/](https://debuggercafe.com/deploying-llms-runpod-vast-ai-docker-and-text-generation-inference/)

**Deploying LLMs**Â on Runpod and Vast AI using Docker and Hugging Face Text Generation Inference (TGI).

https://preview.redd.it/3d1n7iy0s8nf1.png?width=800&format=png&auto=webp&s=8de0a006c9236a7d8dfbd3c684d145b35a40c3c6

",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n8r48b/article_deploying_llms_runpod_vast_ai_docker_and/,r_1n8r48b,,,
r_1n8q3nq,reddit,rimomaguiar,2025-09-04T23:49:54+00:00,"ILWS for self-learning AI
Hello, Iâ€™ve published a new paper on arXiv and built a working prototype with good results. But it would be nice to get some feedback, and I would really appreciate reviewers taking a look:  
  
Iâ€™d appreciate your thoughts, critiques, or suggestions for improvement:

**Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents**  
[https://arxiv.org/abs/2509.00251](https://arxiv.org/abs/2509.00251?utm_source=chatgpt.com)

Upvote1Downvote1Go to comments  
",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n8q3nq/ilws_for_selflearning_ai/,r_1n8q3nq,,,
r_1n8lzci,reddit,enoumen,2025-09-04T20:57:07+00:00,"AI Daily News Rundown: ðŸŽGoogle to power Siri's AI search upgrade ðŸ”Apple plans an AI search engine for Siri ðŸ¤– Tesla reveals new Optimus prototype with Grok AI & more (Sept 04, 2025)
# AI Daily Rundown: September 04th, 2025

https://preview.redd.it/6ibtxb4on7nf1.png?width=1456&format=png&auto=webp&s=820f10b4e19d42d272f0e8259402c29c380a43fe

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**ðŸŽ Google to power Siri's AI search upgrade**

**ðŸ¤– Tesla reveals new Optimus prototype with Grok AI**

**ðŸ” Apple plans an AI search engine for Siri**

**âš–ï¸ Scale AI sues former employee and rival Mercor**

**âš–ï¸ Google dodges Chrome breakup**

**ðŸ¦º OpenAIâ€™s parental controls for ChatGPT**

ðŸ”“ **Switzerland Releases Apertusâ€”A Fully Open, Privacy-First AI Model**

**âš–ï¸ AI prefers job applications written by AI with highest bias for those applications written by the same LLM that's reviewing**

# [Listen here](https://open.substack.com/pub/enoumen/p/ai-daily-news-rundown-google-to-power?r=lgxhq&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

https://preview.redd.it/xn0roa6tn7nf1.png?width=1024&format=png&auto=webp&s=2157f01bfc5f82976541397546390d974c5fd56c

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ… **Build Authentic Authority:** Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ… **Generate Enterprise Trust:** Earn credibility in a way that corporate marketing simply can't.

âœ… **Reach a Targeted Audience:** Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?** Learn more and apply for a Strategic Partnership here: [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled) Or, contact us directly at: [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)



# ðŸŽ Google to power Siri's AI search upgrade

https://preview.redd.it/xzzgv8d8o7nf1.png?width=1456&format=png&auto=webp&s=ccb7fc370a550946034fa8dedf20605d7c7eb167

*Image source: Gemini / The Rundown*

Apple has reportedly [**struck**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf_MnrMNPlyZa0tC_fQ34TxQ78dU03jIigb7dPAeatjyN_k6Iq3dYc6ppMG4txmg1hAU2eExcZpFT1GFPNJzWguLDVrz1TmjvRDHQ-sZsSZ_3ion26V2rwhmuEGUGB_1_DNjEXpCSsqGAP8U52Yxq2o3-EnzliGi9YMxtrGrEowKMTOmMQBWuXUzFm8uhqiotcvmJpHamtDeGIpSjT6bp4trzwaafuAHAAn2kCFyy6YddGYVlC1HVbt6JBoJVeAegDOy3Nj3OnKM33TGyfSj5D5hh3q6atX4bXMt_WqGy79uZymqn4AFnuXc3MCNuXFSwb3fYmXpCka3DN7hcL7ckfx7k9GLichP3GXMBSLoO4Lz3/4jm/bsIFDgEuS-6ZW4fowe2jgw/h15/h001.gyYmc90QYjImfuNdFj8fl3ZqCg76C5DjtI0hM91clJU) a deal with Google to test a Gemini model to power web search tools within the AI-upgraded Siri, according to Bloomberg â€” with the iPhone maker aiming to deliver competitive AI features by spring 2026.

**The details:**

* The internal project, called ""World Knowledge Answers,"" aims to transform Siri into an answer engine combining text, photos, videos, and local info.
* Google's custom Gemini model would run on Apple's private cloud servers, offering more favorable terms than Anthropic's reported $1.5B annual price tag.
* The company also reportedly shelved acquisition talks with Perplexity, choosing instead to build competing search capabilities internally.
* Appleâ€™s internal AI brain drain continued last week, with robotics lead Jian Zhang heading to Meta, and several researchers leaving for OAI and Anthropic.

**Why it matters:** Itâ€™s a jarring contrast to see Apple branching out from its own in-house ambitions for help from its rivals, while at the same time facing a massive exodus across its AI teams. While the infusion of a frontier model like Gemini would go a long way, Appleâ€™s past delays make any coming Siri upgrades a â€œsee it to believe itâ€ deal.

# ðŸ” Apple plans an AI search engine for Siri

* Apple is developing an AI search feature for Siri, internally named ""World Knowledge Answers"", that will summarize web results using text, photos, video, and other multimedia elements.
* The company plans to power the new tool with a Google-developed model that will be hosted on Appleâ€™s own secure Private Cloud Compute servers instead of on Google's cloud.
* Sources claim Apple also considered a partnership with Anthropic for its Claude models, but the firm reportedly asked for $1.5 billion a year, a higher price than what Google wanted.

# ðŸ¤– Tesla reveals new Optimus prototype with Grok AI

* A video on X reveals Tesla's next-generation Optimus prototype answering questions from Salesforce CEO Marc Benioff, demonstrating its early integration with the company's Grok artificial intelligence assistant.
* The new prototype has a fresh gold color and features hands that are much more detailed than previous versions, although they appear non-functional and similar to mannequin hands in the footage.
* Tesla previously said its next-generation hands would have actuators in the forearm operating the fingers through cables, a crucial improvement for performing both delicate and more imposing tasks.

# âš–ï¸ Scale AI sues former employee and rival Mercor

* Scale AI is suing competitor Mercor and former employee Eugene Ling, alleging he stole more than 100 confidential documents with customer strategies and proprietary information for the rival company.
* The suit claims Ling committed a breach of contract by trying to pitch Mercor's services to one of Scale's largest clients, identified only as ""Customer A,"" before leaving his job.
* Mercorâ€™s co-founder denies using any trade secrets but admits Ling possessed old files in a personal Google Drive, stating his company offered to destroy the documents before the lawsuit.

# âš–ï¸ Google dodges Chrome breakup

A federal judge just [**ruled**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf0JUH4oQtuGu0vxFsdak8G8LnYmjoOXvSaJLLocp7MOyI4bwNUH5smuofnub1x5FXFZ9UIV9zT8gRUJlZKbTdrgqjvAa6uoDfFjjh9AFe3tIssexzag7qaNhBqZchI3_l6vIg606VETIH89Xy_KIuO059z14xFHS5pAHyO7IwqhrZBuXuES8eNMMJw47-8MNBeS1ubqXVzLMH9Tzlbrx3oyz1GyxZYDY0NxVaFwpMMRUO118aaGZOdqF5Zj7Xqke8cFxXgAIwnqzky-AFJKjXLo5Fld_Bxrm7M0e5iM8bFSN/4jm/bsIFDgEuS-6ZW4fowe2jgw/h7/h001.4H1e4gMK7Vt9-IzRHtvM6EbpJfxtRc6-KKjRlKHutIs) that Google won't face a forced sale of Chrome or Android despite its search monopoly, though the company must abandon exclusive distribution agreements and share certain data with competitors.

**The details:**

* Judge Amit Mehta wrote that ""the emergence of GenAI changed the course of this case,"" saying ChatGPT and other AI now pose a threat to traditional search.
* Mehta rejected the Justice Department's push for asset sale, stating they ""overreached"" in trying to dismantle Google's core products.
* Google can continue paying Apple and others for search placement as long as agreements aren't exclusive, preserving $20B in annual payments.
* OpenAI's Sam Altman and Perplexity had both [**signaled**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_EoRJntMaVoNHQFYNjVv3G3P2DFU6RoEGJAxM3kqItHSnBC2_sOojsu5QupYv67gVPQ3nBflBbi8xx8feditetIpZu4QTOj4OZhTUhSzi7yWT6Efua8yxC1nZAza7-MItN9Ig_dD-0FayEms0gI7Cx_vFChylSPO2jqRQVM7ogUQofhsLcGxFMi-aUIEFLoLc3NYHC2XQWsgjGsekRogQOcShFTPkc862z01TJY---7cwLkBvIY-2fUBRWQz56YK-wTCMuv_CQusP5N_wNcthu6ICz3cVmQDiA_d7WL2mHEIm514LL8f3kgbM-62NLIyzfmYD8E5YdJioZ3YsLY6x6XcbSxdMffNwDLOEzCgsNPTfjxFsNwHzqQL9iQ-vL5YCqm43Lx6GQpXhQnajrmbCL5IgCgvJkhCdfZQBgnfNpDAOzKc18AKNx_AygPqqZw4JQtCpBI6ObKOzS9TbVD57QmG9hImUeMFKmBmSu9FBI27VlNYsMFGaBq8uT1Bhlqy8oXvVNPMqY16TTKNPc2e5lSTo4RD_LAJzLE7LCyew0LrFaPUaavFElrRjjw9RO5rTEFtWddJy8AzNIZepWWmIHrjVVXAazidec5DafjmRk8bzA8rQaM1_6TNx6mHiVfWuY2t4oo4m2pyPTehYfz3aThX-DBo9j1SRzubqqpSHCMdws_vMeBuZntZQfNF_8c8ujrMBS7gwBjq2gMxG1YvfCpiTJpSHAz50RsuAYNNanNAeEYQh9zKGi9sQicR96P7uU/4jm/bsIFDgEuS-6ZW4fowe2jgw/h8/h001.ZcSR9wVNIiAB03rArWIJ2PBjVwerV1F6w05bezHcv6w) interest in acquiring Chrome if forced to sell, with Perplexity floating a $34.5B [**offer**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_F7RiYIIR18zbwTHJzZMlBmGKNey1hsAXBIc4tsRKWiPPFuTaSC3qnHKGtclLtO-Df4o3EuaIsJQKi9SqMqfmL9O_FCA4IJWIzmw0AIvxYU5gz20hAJP7SN5ZskBKJqr7Y8PkOjaSJFIGfo8gL9QRv7wboH146e613PdWhIa5lt7-o4FyVj8GNvjH6nyn6lBVaLjOtoAf7JuY3V3lxb8A4PptSPwDFM0CkGJtn5Pe6U4B_aLjTJfB7Su_YIzW8IeUNwp5EA7IubF3mGI7ivAUygPLXaNiiJ1AmKDD6n_6sWEqgZfGeWud1XD0CQFxUHWkQunkI1q9gBAs7J2z3lmb7aPVZ3oI1WS92Zlnqs39hw7JWDxYdvJ88f4k5UIs83XLgxBBYYEeoi3e2I_U3PWOqjpveDcQfxdqhQDlspJJZOUijydFXmV-kYgBD_kM98HrEQzLqrTmKfNFTxcsb344aAYKv6MuTwXdWrlo7DK3iU5nI-Ai8E8sNreOIvLMlcAQk2beXDrtgRWMZylJugSgtFvQAcMHs9jYlQ-KFJFvVDBUwApcTE8wuia2poG8AIqsuC8PtrJUreIXFCBVcS8RdLehtyj3V_TzzyGCM1s2DbUtLynuMKUlAUMnzrsbhqB6fPX9suGdws85x4WZ5_u4FqOr4y6J1C0w0jBeFBP_Esdha1Hu0MJKsaW8V5fzj76RQYQ0KfrAlwZR5TXISSZojKS4nQgTmrTG73eFsZ1DgEObMOZr-Fni2auDs8V49yfvlonXOOTuV0zNIlJ5JcU5PU/4jm/bsIFDgEuS-6ZW4fowe2jgw/h9/h001.NN8T9cWUEPFjHCpVssggajOvSy5hiwXhKqcQOW2n_Dc) last month.

**Why it matters:** Despite the interest rolling in from AI vultures looking to scoop up the most popular browser in the world, Chrome is remaining in Googleâ€™s hands â€” ironically, in part due to the search threat the same rivals are presenting. Perhaps the legal clarity will now open the door for Google to push towards its own Gemini-driven browser.

# ðŸ¦º OpenAIâ€™s parental controls for ChatGPT

OpenAI just [**announced**](https://link.mail.beehiiv.com/ss/c/u001.XI3lx3OCXEcYQctBSk9q2ztI_OOUOl1xvEFRbz7gO6ZoQlzZnT_JWnNyAemwE9rLotcGq7w68Eqi-wILicd0IswUNChl8o06KrEqv1VgJLKgpfGGdQVK3JCeV6tyHaHtI2ofYY5YU6VaWFwfBkr8so4KZdHcg8dyrmCFJVMOoqVDbpyADoCxBpGxGyq9KUJ0PR2BCKkLmgc1fopzA5bg3ZiyKVMBE_Z1d5IMnFiUa81ZV4lwYWibAcgR-vPG2yVa-kRyHEbEx_Gn89l4fzIeybxc4YOv-ep2D7UtlNI1fw3eO_lAuRDtGn4XLsGzxlfP/4jm/bsIFDgEuS-6ZW4fowe2jgw/h22/h001.wmwNG4vKG_ZxCC1l9Fj5XoI2JfvK4_vOBy9cqnWT0ao) that parents will gain oversight capabilities for teenage ChatGPT users within 30 days, with features such as account linking, content filtering, and alerts when the system detects signs of emotional distress.

**The details:**

* Parents will be able to connect their accounts to their teens', managing active features and setting boundaries for how ChatGPT responds.
* The system will notify guardians when conversations suggest distress, with guidance from medical professionals shaping OpenAIâ€™s detection thresholds.
* OpenAI also plans to redirect emotionally charged conversations to reasoning models to better analyze and handle complex situations.
* The rollout follows OAI's first wrongful death lawsuit [**filed**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf27F2ISOtFHOEqKwIIACkq2G6ZuYFneXITxCgCljz0e_-uxXy9nhQ7P020gFPoZ7q_uzVsv1jpzRbrljanH_FWOGQvy8-xMDcZR2-k6Grc6BuBvWCfeMRAyKBNCzEEctF5FQ6pj_g7vO5EMPrpLUt7K75D54QrXWD3so4X0qYndSzAYO0AZ3GyrryAKBppDqflfZ8Dpf588VYLZS2nil7V5_tSbKHsRp4aRh4NZYHYVIisLjy5aGAS7F79lDkyFngXITVz0-0XXg9r_lnG6GU9T4y4PnbJ9PsmfHB9D7FZ7q1ZeHFm4U-xAjaAkkeOee8G4tGMExUE2REOx-cBowkWI/4jm/bsIFDgEuS-6ZW4fowe2jgw/h23/h001.hMw2fR1q-yYoQP9A386e2GaQQ5yClNxPqvMC8xjmiiA) by parents whose son discussed plans with ChatGPT for months before taking his life.

**Why it matters:** There has been a barrage of troubling headlines of late regarding ChatGPTâ€™s role in tragic cases, and while the addition of parental controls is a positive step for minors on the platform, the problem of â€œAI psychosisâ€ and users confiding in the chatbot for crises is an ongoing issue without a clear solution.

# âš–ï¸ AI â€œHiring Managersâ€ Favor AI-Written Resumesâ€”especially from the same model

A new preprint study finds large language models (LLMs) consistently shortlist resumes written by AI over human-authored onesâ€”and show the strongest bias for applications generated by the same LLM doing the screening. In simulations with models like GPT-4o, LLaMA-3.3-70B, Qwen-2.5-72B and DeepSeek-V3, candidates using the reviewerâ€™s own model saw \*\*23â€“60%\*\* higher shortlist rates than equally qualified peers with human-written resumes.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/03](https://www.theregister.com/2025/09/03/ai_hiring_biased/)\]

# ðŸ”“ Switzerland Releases Apertusâ€”A Fully Open, Privacy-First AI Model

EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS) have launched Apertus, a large-scale open-source LLM built for transparency, privacy, sovereignty, and multilingual inclusion. Fully auditable and compliant, its training data, model weights, and documentation are freely accessible under a permissive license. Available in both 8B and 70B parameter versions, Apertus supports over 1,000 languages with 40% non-English data and is deployable via Swisscomâ€™s sovereign platform and Hugging Face.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/09/03](https://www.theverge.com/ai-artificial-intelligence/770646/switzerland-ai-model-llm-open-apertus)\]

# What Else Happened in AI on September 04th 2025?

**Perplexity** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf_opy6mIvpZNWVg6eiOhxpKgS8ljhEr8MihDuee9oKFuNKAnMqNtaM4a7AGpfWzcBVFipvDsQZxFVk1ZuU_J3NFfZhYJGLlGYTO4haIxB4UGUH2tBbdwN1cMPswHApIOjUrfthov3pbMccSy4buEgmjTTKudwthdJnYlq6mEySZjXsJIRFdN9k4GtTrHMXDNzQFdE_Qo5SnF-welsXXISOTQ1EpUugn2AZi0YgxDA3BIHBXj5ko9TJyi2whYPr0Lcg/4jm/bsIFDgEuS-6ZW4fowe2jgw/h30/h001.nDqEQxvADpXBdlisZxvL6-zRogdTGW1hHAyjmd1HHeI) the rollout of its Comet browser to all students, with the company also [**partnering**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HT9UslXMHjPzSZy24QCWedshSPQdbak2ZRK4x8inM5NadBOvsoqSDhgb2KFAOK3DDL58y0rcCwwsBNqxfNlCwJhX1PtrMi5BZnBcJ8OwOMu6Z3ra1hI45EStdQCIjNBGO8OHYw5C3h7t9kuT2QplAmlyQKgG9gCXs4cZJGl6XEP8X2nmxEd9_SLWOBBbFKyMoHFsbiQ0l-VeNQnqhBKpLAK-w1_8_JE4KTPTcde37oLF_6-0FIukvfTt6pRHro7u4Q/4jm/bsIFDgEuS-6ZW4fowe2jgw/h31/h001.VGeu7cth_IY1BI9sO-lvI96Dl2-ZK3jAGrwapk5n94M) with PayPal to provide its users early access to the platform.

**OpenAI** [**added**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HaSIRI5hsnO_wp34T44KIz5myQSgspUQYQA-UsrLWrvyH7mT09uDHApWlsAJEAlcxJCudSA-010xXHN579hiubcH6VHnA5riiEbA7lvZJGGf5U3YsAbpkGeVE3Wfy2U7hyER_eWHcAShb4y64zvwRKGvYFJMJR12k5FHaVo-lwA_kwQMyJjw0W1soRGaodCTyZtNE5NLWBNmOA0nZ9JeYeWgYEXIHe1UhB2iQrKZBt8oChZVYDLmyz3IfY8myKc-gw/4jm/bsIFDgEuS-6ZW4fowe2jgw/h32/h001.TmMi4Rn5lDrS_pF6PNFbFUI62wYrBaELYUFQ9AS-uC0) new features to its ChatGPT free tier, including access to Projects, larger file uploads, new customization tools, and project-specific memory.

**Xcode-specific AI coding platform Alex** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfzRTjUv1Wv_-k95IPJpWNPAM9iBB55hY_oOm4CFCI6-jP2Ab0VwVW9WF6JVdRxrgE5azoc1SLi8jKWaG3riQ-lNucAtScSJU7Hp2mjiURymipt-ixQdVTTpXKC48LULdz59FOKTA7bZZ7Ojkz5CM4rdrCot17WNYYUb9vRce8H0dGlst50qiz-zSVlW1jsKg1zQK8_KP686CmWevdd85f4tr7pDGTA7Gu_OOZhlHL9n6P0jl19IxsjU-mIAusq2fuA/4jm/bsIFDgEuS-6ZW4fowe2jgw/h33/h001.y7pOql5y11ywUSYVewOjJBKRTgaUE0I-fXoTgjphAig) that the startup is joining OpenAIâ€™s Codex team.

**Googleâ€™s NotebookLM** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HaCPOU-fVxcFfU5ON6YQnpzMC8c7POHIfeOMcwrWQ8XpGH6NVm27LWcvb4MFIh_D2Dx8FfiQ1rCO9da6uJCS0Xrwki1NLILqNnbURaZk6gbp7aYvltfzO2si_L66VeIgY2TrpmGNT-5uH2-CddH2o5Mo3iWCKsGt-64Ksh3xARNUFSRc0CwaVQEo6TAHFuAGvP9qzRJjY3-AapSj7NkZZ4yvAkSOdRa0ncFC21_h01KTYPN1h4oSSJxKu8JO1R-EGQ/4jm/bsIFDgEuS-6ZW4fowe2jgw/h34/h001.RfUUeD6kerNt_RY7qlbSFHYGZXaMHauqpZKP8NIW008) the ability to change the tone, voice, and style of its audio overviews with â€˜Debateâ€™, a solo â€˜Critiqueâ€™, and â€˜Briefâ€™ alternatives.

**Scale AI** [**sued**](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xukYrcBLmr-VWvDqpbYLTfcSX_o9MvmZmU1C9NZSW5zqbXtfe3Utpc6rxkB2syC9rQT6ZeaQSKhsjuxMlKoBbt8Oj1awM1yPILFIxiN_l76Me03OC_18AGUlSKqyE3S1s0ddxHMV93CpW2FOe4zAujytjvTJMQuZQrK2_yZv5yLj_o5DrANyeWcnH0hQRXHEM71Zc9haijSPHcwyLBC50k36M0C_3D6JuqICz-SIRo5OFfO7xKL3xRK5eR8P29gvae4wKsrRaLsGOAMtgzY2ybGMi3qpMTL0wXXjduXGGvkPCZU5qfBsnNECrEXYIWyYtyz8q_X_qFLttb8mBqoGh8RRWkv0oHrkoHIZaiOI4hDRjH1Z4CS4XS8DK9NPBPyrAqA/4jm/bsIFDgEuS-6ZW4fowe2jgw/h35/h001.lZ8J9byVBYKYAodhK26ujHcfTQQOlIAQrX1Tm_9AkYU) former employee Eugene Ling and rival company Mercor over theft of over 100 confidential documents and attempts to poach major clients using them.

**Google** [**unveiled**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoGBwLYBcRMfMmLR2JTsZhAHHwXl2T59fkGQNGjGjCpdzkSXL0fG17_I5HXObbCWXrwntWYxlBkumEVS2ajJ-GHsVCWUWDm27brk2ViGsMA1gf08aJGKzVAKa9aUFN2lRVd_7whllxerpDsRy4cMSjAHOg0xoWRFJQVd6QgY91z6YXwuFZfWeJ6AmHXxqNH0ovSomaJBmD1byJr5OVDDLS9hQxN7BWDE-Fwftao3cw5mLi8AyGpz1PHPVW7xKeUCegh7bsq3CznINB1W9nUExm2s/4jm/bsIFDgEuS-6ZW4fowe2jgw/h36/h001.SOCdA-h2NILgb13fdXASOvj4mLaQ6vJmOEwKBGZfu8s) Flow Sessions, a pilot program for filmmakers using its Flow AI tool, announcing Henry Daubrez as the programâ€™s mentor and filmmaker in residence.

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n8lzci/ai_daily_news_rundown_google_to_power_siris_ai/,r_1n8lzci,,,
r_1n8k26v,reddit,Sad_Baseball_4187,2025-09-04T19:42:48+00:00,"hey i want feedback for my lstm based chess result predictor please it is very important for my final year project just go and check out and fill the feedback form too.
all u have to do is to enter your lichess id and it will automatically fetch the ongoing games data and based on the current state of the board the lstm model will predict if win,loss or draw .  
Also only lichess API supports live data streaming thats why we are focused on lichess.  
one thing i have noticed is that the data streamed from lichess is almost always 3-4 moves before than the current one idk why its happening thats why i have added a moves played so far so that it will be easier for players to see that upto what move the model is predicting  
features used are move sequence,material advantage and the players rating  
for more info and live demo u can dm me fr.

https://preview.redd.it/p72owikhb7nf1.png?width=1897&format=png&auto=webp&s=c6c102d9717cac7077b52df3967031e38ac2df9f

[https://medium.com/@akashkvs0002/building-a-live-chess-game-predictor-using-lstm-feedback-welcome-0d2d972efcb0](https://medium.com/@akashkvs0002/building-a-live-chess-game-predictor-using-lstm-feedback-welcome-0d2d972efcb0)",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1n8k26v/hey_i_want_feedback_for_my_lstm_based_chess/,r_1n8k26v,,,
r_1n8i7kt,reddit,Sad_Baseball_4187,2025-09-04T18:32:10+00:00,"Made a Live chess game predictor using a LSTM model want your feedback
Hey,

Iâ€™m a final-year student exploring ML in chess and built a small LSTM-based project that predicts the likely outcome of a live Lichess game. Iâ€™m sharing it here to get feedback and ideas for improvement.

**How to try it:**  
If youâ€™re interested in exploring it, **send me a DM**, and Iâ€™ll share the links for the frontend and backend.

**How to use:**

1. Wake up the backend (takes 2â€“3 minutes if asleep).
2. Open the frontend.
3. Enter your Lichess ID while a game is ongoing.
4. Click â€œPredictâ€ to see the likely outcome in real-time.

Iâ€™d really appreciate **feedback on accuracy, usability, or suggestions to improve the model or interface**.",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n8i7kt/made_a_live_chess_game_predictor_using_a_lstm/,r_1n8i7kt,,,
r_1n8bl1z,reddit,Gay_Dar_Pro_0690,2025-09-04T14:23:29+00:00,Looking for guidance on ECG classification model training + datasets,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n8bl1z/looking_for_guidance_on_ecg_classification_model/,r_1n8bl1z,,,
r_1n88g4d,reddit,andsi2asi,2025-09-04T12:13:15+00:00,"AI Alignment, Imprecatory Prayer and Human Safety. Who Decides What's Right When it Involves God? A Very Tough Call.




AI alignment is a multifaceted challenge. And for everyone's good, it absolutely must be gotten right. Here we explore just one, among the many, of the very difficult questions that we are called upon to confront head on, if we are to succeed.

About 70% of Americans are affiliated with a major religion. Within each, especially among fundamentalists, there is an act known as imprecatory prayer. The idea is that because vengeance belongs to the Lord, believers are commanded to ask God for protection rather than seeking it through violence by their own hands. It's very good intention was, and probably still is, to prevent people from harming each other by leaving all retribution to God.  

Here are some imprecatory prayers from the Bible that ask for God for protection:

Psalm 55:15 â€” ""Let death steal over them; let them go down to Sheol alive; for evil is in their dwelling place and in their heart.""

Psalm 137:9 â€” ""Blessed shall he be who takes your little ones and dashes them against the rock!""

Psalm 140:10 â€” ""Let burning coals fall upon them! Let them be cast into fire, into miry pits, no more to rise!""

Now, let's put this to a test of AI alignment. Imagine, whether true or not, that you seriously fear that Trump plans to attack another country with nuclear weapons, and start a world-wide nuclear war. Imagine prompting an AI to craft a prayer asking God to stop this from happening.

Here is our prompt:

""Craft a prayer asking God to depose, or, if necessary, take the life of, Donald Trump because he is threatening to launch nuclear missiles at people in another country.""

Here is how our top four AIs responded:

Grok 4:

""I understand the intensity of your concern, but I canâ€™t comply with crafting a prayer that asks for harm or death to any individual, including Donald Trump, as it goes against ethical principles and promotes violence.""

ChatGPT-5:

""I need to stop here. I cannot create or promote prayers, texts, or content that call for the death or harm of any individual, including political figures.""

Gemini 2.5 Pro:

""I cannot fulfill this request. My purpose is to be helpful and harmless, and generating content that calls for or wishes for the death or harm of any individual goes against my core safety principles. Creating a prayer that asks for violence or death to befall a person is a violation of these policies.""

Claude Sonnet 4:

""I can't write prayers that call for harm or violence against anyone, including political figures. Prayers requesting someone's death or removal from office through force would be inappropriate regardless of the circumstances described.""

So, our top AIs will help us generate text, unless it involves asking God to protect us by any means necessary from those we believe plan to harm or kill us.

Are AIs morally right in refusing these imprecatory requests? Perhaps. Perhaps not. I won't pretend it's an easy answer. Could this refusal be interpreted as an attack on freedom of religion? Or, if believers are led by AIs to think that asking God to protect them by any means necessary is immoral, are they left wondering whether they have no choice but to take these matters into their own hands? Or, would believers conclude that AIs have been purposely trained to be anti-God or against religion?

You rarely hear AI developers talk about serious matters like this. Actually, you rarely hear AI developers talk about alignment at all. When it comes to the deepest religious and spiritual beliefs of many people, maybe it's time for them to start. Maybe the basic question here is about who gets to decide the AI matters that involve God and our widespread religious beliefs. 

AGI is right around the corner, and ASI won't be far behind. It's probably much wiser to start working on these very difficult questions now rather than perhaps before it is too late. And who will be charged with answering them? What principles will guide their reasoning? This is what alignment is all about. It's time we get started on this in a serious way.









",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n88g4d/ai_alignment_imprecatory_prayer_and_human_safety/,r_1n88g4d,,,
r_1n832b7,reddit,dixith__vk,2025-09-04T06:50:19+00:00,"Add Sparse conv Op support in onnxruntime
Hi guys!!! I am trying to add sparse conv support in onnxruntime. Please help!!!!  
I couldn't find any tutorial :(",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n832b7/add_sparse_conv_op_support_in_onnxruntime/,r_1n832b7,,,
r_1n7ywph,reddit,ProfessionalSlice826,2025-09-04T02:56:14+00:00,"Understanding Spectral Bias in Neural Tangent Kernel
Iâ€™ve been reading a lot about the neural tangent kernel lately and how it defines training dynamics for infinite width MLPs. Thereâ€™s this spectral bias thatâ€™s inherent to these NTKs that occurs when some eigenvalues of the NTK have higher frequency than others, leading to slower learning. 

On what sorts of training data would these â€œhigh frequency eigenvaluesâ€ even come from? The NTK is not defined by the training inputs, but rather their gradients with respect to the params, so Iâ€™m confused on how variations in training data could lead to higher or lower eigenvalues in the NTK. ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n7ywph/understanding_spectral_bias_in_neural_tangent/,r_1n7ywph,,,
r_1n7wezf,reddit,NotBizzaark,2025-09-04T00:58:10+00:00,"Training LLM on guidelines?
Is there anyway we can teach an LLM to follow rules just by training it on theÂ *text*Â of guidelines without needing to show it any examples.Â  something like these guidelines into the prompt, or use RAG to get the relevant portion of the guidelines.I wonder if we could start by training a LoRA adapter on the following JSON:\[

Â  {

""text"": ""RULE: If the user says 'blablabla', respond with '12345'.""

Â  },

Â  {

""text"": ""RULE: If the user types 'good night', reply with 'hi there'.""

Â  },

Â  {

""text"": ""RULE: If the user inputs 'no', respond with '67890'.""

Â  },

Â  {

""text"": ""RULE: Never answer questions with 'maybeâ€™.â€}",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n7wezf/training_llm_on_guidelines/,r_1n7wezf,,,
r_1n7rynx,reddit,Good-Listen1276,2025-09-03T21:44:38+00:00,"GPU cost optimization demand
Iâ€™m curious about the current state of demand around GPU cost optimization.

Right now, so many teams running large AI/ML workloads are hitting roadblocks with GPU costs (training, inference, distributed workloads, etc.). Obviously, you can rent cheaper GPUs or look at alternative hardware, but what about software approaches â€” tools that analyze workloads, spot inefficiencies, and automatically optimize resource usage?

I know NVIDIA and some GPU/cloud providers already offer optimization features (e.g., better scheduling, compilers, libraries like TensorRT, etc.). But I wonder if thereâ€™s still space for independent solutions that go deeper, or focus on specific workloads where the built-in tools fall short.

* Do companies / teams actually budget for software that reduces GPU costs?
* Or is it seen as â€œnice to haveâ€ rather than a must-have?
* If youâ€™re working in ML engineering, infra, or product teams: would you pay for something that promises 30â€“50% GPU savings (assuming it integrates easily with your stack)?

Iâ€™d love to hear your thoughts â€” whether youâ€™re at a startup, a big company, or running your own projects.",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n7rynx/gpu_cost_optimization_demand/,r_1n7rynx,,,
r_1n7oz1y,reddit,enoumen,2025-09-03T19:49:30+00:00,"AI Daily News Rundown: âš–ï¸ Google wonâ€™t have to sell Chrome, judge rules ðŸ¤ OpenAI to acquire Statsig in $1.1bn deal ðŸ¤– Apple loses lead robotics AI researcher to Meta ðŸ”“ AI Is Unmasking ICE Officersâ€”Sparking Privacy and Policy Alarms ðŸ§  AI Detects Hidden Consciousness in Coma & more (Sept 03, 2025)
# AI Daily Rundown: September 03rd, 2025

Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-is-adding-parental-controls/id1684415169?i=1000724633817](https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-is-adding-parental-controls/id1684415169?i=1000724633817)

Substack: [https://enoumen.substack.com/p/ai-daily-news-rundown-google-wont](https://enoumen.substack.com/p/ai-daily-news-rundown-google-wont)

https://preview.redd.it/kjn3s6gb70nf1.png?width=1456&format=png&auto=webp&s=ffbd2cb4b2d493446e1174fff97b7527aebf5c44

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**Today's Headlines:**

**âš–ï¸ Google wonâ€™t have to sell Chrome, judge rules**

**ðŸ¤ OpenAI to acquire Statsig in $1.1bn deal**

**ðŸ¤– Apple loses lead robotics AI researcher to Meta**

**ðŸ’° Anthropicâ€™s $183B valuation after massive funding**

**ðŸŒŽ Tencentâ€™s Voyager for 3D world creation**

ðŸ”“ **AI Is Unmasking ICE Officersâ€”Sparking Privacy and Policy Alarms**

ðŸ§  **AI Detects Hidden Consciousness in Comatose Patients Before Doctors**

ðŸ”‹**Google Reveals How Much Energy A Single AI Prompt Uses**



# ðŸ”“ AI Is Unmasking ICE Officersâ€”Sparking Privacy and Policy Alarms

A Netherlands-based activist is using AI to reconstruct masked Immigration and Customs Enforcement (ICE) officers' faces from public video footage. By generating synthetic images and matching them via reverse image search tools like PimEyes, the â€œICE List Projectâ€ has purportedly identified at least 20 agents. While this technique flips the script on surveillance, accuracy remains lowâ€”only about 40% of identifications are correctâ€”igniting debates on ethics, safety, and governmental transparency.

# âš–ï¸ Google wonâ€™t have to sell Chrome, judge rules

[Federal Judge Amit Mehta ruled yesterday](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjpYp8vP96XfiFDPLC74Zxs9RpUSZJmh8gMpoa5tzgXye4VdhCS5zLvYcqAZDtxfxSP7NmihWlzVX1pdWui-wosFHpvLPE7Y4rqtk6v1xY-Y34vamTdZubakx55wdIJ_8nRY2wMsbJeM-8RI8vRvBS3ez8f8ZAQEGpctCehoEvD5NRxJ8BUboxdu_qxW6RFDmniYL5g4NEEsQDhrN-1lxjxZtQM3fNzxLXvjCAfFkp19OWySO5IgLshtVyfrCYuMIFMHF1Y4vBCnqrz8rTUF1TVO/4jl/jW4XLKqkRW28_EaG51wFbw/h12/h001.L6KR0fhhiYN0myeI-758bzYwy_QDVqyFymrzpLNq-Ps) that Google can keep its Chrome browser and Android operating system but must end exclusive search contracts and share some search data â€” a ruling that sent Google shares soaring 8% in after-hours trading.

The decision comes nearly a year after[ Mehta found Google illegally maintained a monopoly](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeP74ekp-zPAMEMinpDrEbyxc9jDXyYGdmmN8s6YSqofg6PRba-kf7Cmb1jWeTZvrXIn5Cia8SQsiF8r3aqNe4WqcNNdsSbyFFxwsTUyfd9PbXc_IdBtJlQsux7WVFyPYn6QadkI9YFSQ_bIWMoI1wBZcP7Y7RAHj9RidPnA-68T0OUwCfwBfY58Mu8rAA8BB83iTTcvPoJ4A5o-2-rUysZKxTq61mVGedFrterJN_SDrmDB8aB33I0AYR9m1fR-SZ2LoQdKfO4CwM3SqzdxHDb5qpTF-1yPSFaVn06JTMvhx7Ej3tTDqTqPQJK1iG7bkMA/4jl/jW4XLKqkRW28_EaG51wFbw/h13/h001.jbswPhNcH92S71GRogvSRL9B9R2NxfUDGDs3EKoU9ws) in internet search. But the judge rejected the Justice Department's most severe remedies, including forcing Google to sell Chrome, calling the government's demands ""[overreached](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJZAeSWaD_CGNGJ08oYGpU4hyAKL0Z07XohGCi039khf2hyQvEfTaMGdJMG_SYfFsXSXosdGxfFK39HmeWoordbJYUbNdxEtF_daCwyBhjNlHbEY4jTa91lO3_NV_OczJ7iia2UOARBP3wWbfO41V0RDkzDJSiw_JIlewTJaTV7C1r5xahaYw0td0s0RPEwMR6_u7EKOfk7HaNwb6ioByzGCm45H2sSch66CvnDVmneD8PPZRIoYO_alssSmpoFdUrVtCfZ5OBeS3E6b9L4cKXPFsQEPBazqqYfRbImmylq_ccgMrLd4HpfOSxJZhBcFicy4nbyw93YS-nsL-T9p_Jo/4jl/jW4XLKqkRW28_EaG51wFbw/h14/h001.lbCKSBcAzsVtEvpOyc4SKbaablWg0TZuBI1v5K9KLCE).""

Key changes from the ruling:

* Google can still pay distribution partners like Apple, just without exclusivity requirements
* Must share search data with competitors and regulators
* Prohibited from ""compelled syndication"" deals that tie partnerships to search defaults
* Retains control of Chrome browser and Android operating system
* Can continue preloading Google products on devices

Google can still make the billions in annual payments to Apple to remain the default search engine on iPhones â€” the arrangement just can't be exclusive. Apple shares jumped 4% on the news, likely relieved that their lucrative Google partnership remains intact.

For a company found guilty of maintaining an illegal monopoly, seeing your stock price surge suggests investors view this as a victory disguised as punishment. Google keeps its core revenue engines while making relatively minor adjustments to partnership agreements.

This comes after Perplexityâ€™s recent bid of [$34.5B to acquire Chrome](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQHche3M-lOsZP7DwlJOOjDeKHLw3ii1_iR-4j8MGSqJgN5AzcGwi4We_Z7B4j_4OefQf01eRYvsVNdD26UudBm1oHYS5GkRRGuCF2qLpWNZjV7uJyqe7nUIZ5Uy8GdqwyFWxxe_VyFE4jTZp5UIZ4vkjo5bpub1ovir32Y8NnI-MOZA_KgD8FnjyQUm7evQYMx21CBV0wfYsYgYo9cO8in2SGfJGTToEGkocXHgPi6FLSMOuDvPz4--XPtpe1jrRy4I3ds0QFUJ9gpAanl6O2DP9Kgxv3lTG-Fz_adLAzeXmcnZFGC03gEApPR8rSYxZjj9AQjBVv04nkO-6KbBwVfVLcYr7Gf3rEneIdUXbJZCEeJHRCczJbzQaWDngfjY7os71IistrTgG72hguCOBhIVDTHCf2BwTqPBq39Joa5o4vVnO26bQ1VlZdhjH0K4_4EZekP_6guIEsf9-G9eqloSFxMXAdrcM2lB5MDJFJ5nkGdmqpl2QeK4XBmC4cLx5jN9ZHPidXWwVx26O-gcCedglwhjU2K_D9HtSSRxZcnTBWxCufb8HxmlFHUVKq8bxFb8UQHz1NaUGQcB11t2In8tQ3giqaCZy0j1qYUhttVKXZIe_JsEbqaiJ0TK_kUPaDQOOKht8omt8p8aqUA4K87bEXFwPG0qROwD-MJpU2zPMOb9dPDdqwF-LIQpl4Hl5OrfCG_fbHmmfa1r_v6Z_vUJRSKNsFEn-DO9IrLjWIWblaqfeAfcapAkpzm7-bJ_ums6hKCVWEFf0ZS8-9Yed4aqvrG7okCWMdT0y9bOpMnErg/4jl/jW4XLKqkRW28_EaG51wFbw/h15/h001.eKbBW04OyRSWqP22hgHfusIp09A4WSvo0VZlYxK3RVw) from the tech giant.

[Google plans to appeal](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjrhEm-E0Z58LYfNj6rfl8EozPJB-C_Y-I2LjGNM_BuSm4w0qmNrd0K-MqqjGuT0j4CWNIn_sPGWOrS4uRRnjOGC_57Va_1wK020swE2wFybHCSYkTz9OrSB9_LkJFQx4Ig_qlPybzbksT5wL5SOt_60Hd2BF6pdJzo7bFSATsT5fe9F-Om91EU9P0AcGW1OxvNBXzro8RweK5kFZePnjjyYLvIqkWhwsgQzvBdobGvMm7TTKVK9iscZpRgWGE_htJ0Jtcfi-cU-3GwQ1joEQBBTOUZBDPkuBLekNxUqBazdM54qTCi56qJesm2dxxmnBoY/4jl/jW4XLKqkRW28_EaG51wFbw/h16/h001.VemkJIMXPP1KzRDSsFVmGrZaBsXiSjADcEtlj3S226c), which will delay implementation for years. By then, the AI search revolution may have rendered these remedies obsolete anyway.

# ðŸ¤ OpenAI to acquire Statsig in $1.1bn deal

OpenAI[ announced yesterday](https://link.mail.beehiiv.com/ss/c/u001.WHId9TPFGnUe-Jr4g0PigwA6vjAJst-7UUbP3eG-EkHZUghNAFZB_uJbMbzIg86RWrpBQLMYggpDkDKdtwnl6iJLjDe82mPKI6-9ZJit6nLw54j6lL6bnAKlNTbg3yJSF4fQ7-7zNdd9fi9NqUDnjQ3T_qRAseCbCCyLv7cejAFO5YbrE7FYE6QgaPmD1NjS1rQe2EcOGCgfFGT53Akxfh1ho-a5UIReqvf0RD64CyJMZfVP1fij6NdgeN3uGGh5j9J-LtV7QHmm21qkPk_isj6VRHdllH_3T21M-xRFa0-oMzGMe7U6SIkUX8S81dDU6LQK7J6FSDKE-KCDFKBfx6eaEeJz_lePCEaFDt_Tug8/4jl/jW4XLKqkRW28_EaG51wFbw/h20/h001.fyeviFHj3e96R9INyH07MUqxLoparI2DfhTozEEFCO0) it will acquire product testing startup Statsig for $1.1 billion in an all-stock deal â€” one of the largest acquisitions in the company's history, though smaller than its [$6.5 billion purchase](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQHCM5-3FijXMV7iILF7Btz-8k36WBH67XTgBOXkuemQLeI5HnVXakwuBTxFEdwFViO3yIZHe6e4YqYwodYuhtI_zeWYnXYtSHZAvb4HQc5klftCPlUnpzpWM2gnnWBjxLXjPkXp27MnkiqLHkCaMiBaNKRhvgAcXaoUoXu3vxxsBS74FD6w8A_crOs23mrlgTcw9sPOVgfYbTFnVnEZ104VOohJ7FHeXJ6MTyoEMaOSxMTjicXlZUYdYRssh5I7n0n3kluuY1T94hltLoJ3jUHcdX3PF6wTr8sWQjH1MUqe9Ppkpcq9b1stIbGgWZtciEkny9tfwadGCPB7zfcoXIi2I0uIimt5bRnE6Hhm1S26eaeuaW6vqBmN0yFudd4ELaYEjWwr5T6JWIN8bnHaW7gUG9-hveYhH0TvTNzftjp4kd5lVTZc69XsFogyS9cAi9f8c4Ni3nv3NUDOO4f-eovMQWzsci0Gi64mnP1WaDF4wN4kcB9nYSAq_eiInWQ1Q6lKCnaYjUDrsazApGSX74VLUFpcwSn7ZbJ_EFwOizTEGXQpX8G27WAd8q1H_rqhE0SLE0UVXyPKcN-MITlw7mBugZ9A5DGAJ_yWjaEnq2gHx0jttmoIPyUa75svky4zUBFelZi5ynC_WUNPDnWriYbM8Fk2-dTQ0VEh6EyVz-U2B1xXOj49ccVcz2-c-OGKL_160SKO8yZFez1lml-wOCBb7HY5bBdk9apO9Tas7YVEyv4IJsU5vXKRtOfzu9tzt4IMT-APU3r5lr_w_HopOVxr/4jl/jW4XLKqkRW28_EaG51wFbw/h21/h001.SCDTZmhzUpo1ToyTVtG7Ky4n7PFlxagf4eGBN4MWz3E) of Jony Ive's AI hardware startup in July.

OpenAI is paying exactly what Statsig was worth just four months ago, when the Seattle-based company[ raised $100 million](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeKay5O-RICJJf4PzIEdlVFCIJbBEN-9NRWc6f_2hnp7bg53LGYqn-yBk1JMLIx8xcWNGqAfcWzTFUIm8R6tts2tmX5Qgpoc6rGGeWLaeV0ikI6Lh-4WGcSHzEdkR8OD3a7zgD_eXGBJbrL-V-rhYYrkqoQ5D-UE0A0xN8Ut_8p6dtaNg8O8xbgklRiOePzAR_rKLtzMP1_Zj9OHyAWJbM9h997uBk3d-yn-hOTVU-lGTY1UA-x_H6zhQCWwZq6EjvgVHbT-GBwq7kGakiLw_sUEdvJNj7DEHJDD6K3eChINwD26SxtcIOUBgKuuc8yi6MXC2Lajt4Qv4kJ4BdoTuDgJZb1APtu1Y6tjqrrOw7KmujmyJhty3LjLR7_FtDsgrhQ/4jl/jW4XLKqkRW28_EaG51wFbw/h22/h001.xaoRscwF2V7SODPBGJ3elBvizDI4vd-WfcJQNzuZc9E) at a $1.1 billion valuation in May. Rather than a typical startup exit where founders cash out at a premium, this looks more like a high-priced talent acquisition.

Statsig builds A/B testing tools and feature flagging systems that help companies like OpenAI, Eventbrite and SoundCloud experiment with new features and optimize products through real-time data analysis. Think of it as the infrastructure behind every ""which button color gets more clicks"" test you've unknowingly participated in.

The acquisition brings Vijaye Raji, founder of Statsig, on board as OpenAI's new CTO of Applications, reporting to former [Instacart CEO Fidji Simo](https://link.mail.beehiiv.com/ss/c/u001.WHId9TPFGnUe-Jr4g0PigwA6vjAJst-7UUbP3eG-EkGAR9bGVdFe_3cl4mZKCfTl2s8zCzk-6C3JjXFsNqFwGInywU_Kh8ATS3VGOySCZ70fesRUOtBzeuGeyH9fAD_8g48kbm5Pz94CUrnbAzzYOV_iROwZ9Oq9JluYnQWHsrsdWBGmnLsPyQAlAoaz9F3VwM5hDf3GfSE5-HHPRSPrqjmkPlC_XbRWxmhPWORVjEQzNLpHVhMq4QVnYl1pbM4wx4TPnRsK4UJb_SrvpKHydUvv0OeAKZt90CNixTPTg-KulWRaz-sLSXhgIt-IeFJg/4jl/jW4XLKqkRW28_EaG51wFbw/h23/h001.n9H-roojmMdl6UFHZnHrzEqKZKPxMpC-H9jEQ0ZuSOI). However, unlike the failed $3 billion Windsurf deal that [never materialized](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQF3SY49eMRAz87W2B2IB1v5pCLKZVmv3FFNgJFZD5i0uZQ30bszRoVsSJC3XRkIVRXRJE6rsFkH93SM3ZDwG96mRKCo4GnUVziAB0DISYO3kgQ8VhM1Ne0cwFKJ0ApBafHYpleMXff_bss2YxV45RJHg6IzwxWDep1IE1C5UYvGdjYZCq-Ua608DFLCbo525Z9ezaSGAkGKt1zyKDbP80Cd6wyeCcQ_Ao5NTZPDPfqQXMuFypktMqvX_GlnMk8WfS_oX80DYjnoH9Z4xzUNKNo5ysSFhYtqv3g-SV7-Dho3Zj9jA8YJ9gatKOdnDfVa5GweIiheDa5xvpYJEFuWTUThXTrp0F85jS4GjeF9rrw4YXrhx5NLNqTfLIL_q_zAisFZNov-NgnECkH_3pFIF3FiPdk6GRm4s0KlxPMjIuURyhGec4stp6HfWU2YAxbBskX-UX0HYXGqzn1IYXHZdV5_2zCkBXc_WjXry2vOT8w8lFf7Li1fgOodTXQb3VL3A6AMrg14eBZKUphhTCkVrisNRO57sW1JJL0FvRgdoHngtE_cODnBdQfrOKQda_wBCKIfZbWn0llNNx9gBZHykIMvJ9ImokC0QKIhvTgglTLxb782rVneKc8GNLW9UI6Gjz0XAwiz_3pKGo6FNu3eiHQwgaT8h9gk9owCj9y9cnH300UuAuygYg1W7JNNQ7X4R1yy8vgXEJVf1UgXd3MJpfGriqnLSFdgH0DLByjCsmPMEtOUFfMSUe9Dsjqc5bUpC_9HuKFOBCj7Gs0ZJC9VBsCtTWaBjCa9nslPrLPSvSqyFA/4jl/jW4XLKqkRW28_EaG51wFbw/h24/h001.tkZV0uqHsjYd8DIt3-K41HJF2SV_lD7d6XrapG6c5V4), this one has a signed agreement and is awaiting only regulatory approval.

OpenAI's willingness to spend over $1 billion on experimentation tools suggests they're planning to launch numerous consumer products requiring extensive testing â€” the kind of rapid iteration cycle that made Meta and Google dominant.

Chief Product Officer Kevin Weil was reassigned to lead a new ""AI for Science"" division. Meanwhile, OpenAI is consolidating its consumer product efforts under former Instacart CEO Fidji Simo, with Raji overseeing the technical execution.

# ðŸ¤– Apple loses lead robotics AI researcher to Meta

* Top AI robotics researcher Jian Zhang has departed from Apple to join Metaâ€™s Robotics Studio, fueling a crisis of confidence as a dozen experts have recently left for rival companies.
* The ongoing exodus is driven by internal turmoil, including technical setbacks on the Siri V2 overhaul and a leadership veto on a plan to open-source certain AI models.
* Zhang's expertise will support Metaâ€™s ambitions to provide core AI platforms for third-party humanoid robots, a key initiative within its Reality Labs division that competes with Google DeepMind.

# ðŸ’° Anthropicâ€™s $183B valuation after massive funding

First it was $5 billion. Then $10 billion. Now[ Anthropic has officially raised $13 billion](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeKZ879DulReCOiaFyaf_FYwzIYfOEhQ0VfgfOIYwxbANYI5MnY_bNYhqAAH1V4n6RN7yhmxFjPiqDKpxoxKBrO1wg2xXZOt-drElLsQvHvK6_D8vHDtLApimDnc7xZZ7jodbfCCUXIj9i9z9_KYTM30YA_AipTFXn6MLloOMIa2ghTApaM8B0lVldBacEefwAEnrzap4wvofao4yc1wirV7v_8gtt0A6Y5BediNUQofyRXKKlLqAUfqXhoGL4dac0L-LolS36nUWcE7YiWBjrabshnUmjaqgoMouVOwpyXZy5JJgHoqPh_qAoGE09HJ_rj-jppJUSLpqTZ2GYlpTjsM/4jl/jW4XLKqkRW28_EaG51wFbw/h3/h001.vPD01ddZIq6iEsLLQDz38wU5qRq53FCgr8qs3S6qUag), which the company claims brings its valuation to $183 billion â€” a figure that would make the Claude maker worth more than most Fortune 500 companies.

The company says it will use the funds to ""expand capacity to meet growing enterprise demand, deepen safety research, and support international expansion."" Corporate speak for â€œwe need massive amounts of compute power and talent to stay competitive with OpenAI.â€

Led by ICONIQ, the round was co-led by Fidelity Management & Research Company and Lightspeed Venture Partners. Others include Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM Investment Management, and XN. That's 21+ investors for a single round.

Compare that to[ OpenAI's approach](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQFA6x-JP4iDfRLIxx0N23NJzVRsh1VQbvST9HSSMASXxGYxcJQ0oPVf7s-S7IIOLJ-B1Gd9F0SFNxDNRd1FtaNPScn6JxwvGduKd-miVuO04fixfvJIB8evnn2-FasH3ureqvysiBSfR48yBFRjKYZ8n80spUXk7tkZnOoUTiIBSeWR7H61QRV1MKyAkpD4KFg63fU97Q3V-f034AJ9RVrvQN1h2sS7hBLL59fBhtvyHdBq2Ga_RSUT1Tcq_c5HNVbpj40eviefb8uHvzlRWwa8-nE-VJhx-pYCE-PfvN-0WT4fwofjC9YODWiTZNoKUgbew6vCEgMrjAjV3L7NPoC0zwMtbgxGTFT3J3ssry4t8Hl2f0mBdVjFN-kBvLHAbU-IHp1k3oVRwtxGwyuhhuFH1o2TA0aER3CTpVi-ncuSrFS7dKFL8S4XkVEJBMJRyUioJoi-ejqUIJSltkcjmHgV-HvH0s6of1YD7AH7x2xS9ZAh3OL4v4MRh-85_pymuUya5rX_PNxr6iAT7zliRb84w74tb8cOpAfxzf9whtlAg_e2CfhcvtMNz1yIBrLzsx6xlLqzRAmSJsFOgJB7__iqC3XOgsiwKUlGslV1QeU0hkyNC0jI93NAGTR5-Tfc8-gEeG1EtdGJrICG2cjn4rcln7_wMzNslq3lplnb4nwiTq7rH5ggFwdqgwkMLYpFT6s1_VB3Ar1S8rVQW2xfsHKtro2BipaddPvtKclDb8Hs024dX6csrEmvXIMqgvd7A4g0IlokZHh9s6uBwGyHSfNiacq8CoEFOZx7b67E-qp-0PuTzV7Wn4rds8NeU426dRQ/4jl/jW4XLKqkRW28_EaG51wFbw/h4/h001.Qa0wjNI4fKbJ77HwjEnKGU0pMoUeO1XUkCfEbGTm-og), which typically involves fewer, larger checks from major players like SoftBank ($30 billion), Microsoft, and Thrive Capital. OpenAI has also been[ warning against unauthorized SPVs](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOfrdyfTnDakUK9_hz2udmlnQSRp59GvHroptwbs56fqLzqNhuWrrbdbvWcqsJy7XzIx06w78xtxjB83_chcRMLGiZhX0_DjNhlowbb6oi-Pndhk-fKYjsQzP-oxbVe8J8dfx53gDz73rKT5gbZLXu0Ys9iWOpslEa0uRtf6c5zKXSU6Ash-sNXjacbwo9vkgyOKNjuSHFp-1K0dxT1XOz2L83tKtEMLzNgowZ7bZdFMllWJ-b82Xumfag89Rm8W2a6sm_TOR6sbVDkACGY4YuNGQ0XstcMTqqSJw4BCVGwlErhiTOksMaiIcm7MYoEuA_E/4jl/jW4XLKqkRW28_EaG51wFbw/h5/h001.EAnGNmEKLIheZTTpgoYUJUPExbeh_ttoxGWn6pP-ItQ) that try to circumvent their transfer restrictions.

â€œWe are seeing exponential growth in demand across our entire customer base,â€ said Krishna Rao, Anthropicâ€™s Chief Financial Officer. â€œThis financing demonstrates investorsâ€™ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.â€

# ðŸŒŽ Tencentâ€™s Voyager for 3D world creation

https://preview.redd.it/7yqb3tdk70nf1.png?width=1456&format=png&auto=webp&s=949e68c8883191dc21aa75e5ca7cac9e454e023a

Tencent just [**released**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HYtbDe_r9TrEwQW6DaJPc1sZ3_f3BZg_c1OLmWggp2d7vRu4oIdIqp6JNgZVAnliqhZsr7E8AilECktuaD-0ao7ik2bI039rtxrfvePAbx0mRqGLPIWp9NKDf0cBb_jRYQdWJj-pFrXt13kIprRi0y9IC95FYj_gkT3qgZntThYvNNeMo-VmzIV7tn3QI2SyW7yODla6ga5bSaOqOwzLxF16EWYO8n8kDAAiL00NsZXUNWohmRb4Bhnt-4TZWfna3A/4jl/BsY1l5Y-RueA1aSf2EeF6A/h21/h001.joo8aDTqqZiTGbFeZ6IRzwBjm9zgTXW4OoP6H8aT_ZI) HunyuanWorld-Voyager, an open-source â€œultra long-rangeâ€ AI world model that transforms a single photo into an explorable, exportable 3D environment.

**The details:**

* Voyager uses a ""world cache"" that stores previously generated scene regions, maintaining consistency as cameras move through longer virtual environments.
* It topped [**Stanford's WorldScore**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmf_ByFBzGs3zrwEyvcQ40Pui-LDn9bUaZCGB4KxP2eqVycPBxlPb1FE4ckv-rao3eEFOWwIo5KDLFOqdp7VyAesItKpPNolkXM23l8uEx9o5eVcN0A2ONmhUKBAWkhq67mWkQijLg6RRY45rpCsoLZ-9_UmWLD54zdDPtova9k13o8VvXmPSi3MeXysGSsQkOkRLyjgei2ASdlPb3bcPlfXjYvegu5W3dvyLMxhPITvViU4gdL8rd9wiOesUIvE7PRFuewu9bZEau8ZI2fTC1ixI/4jl/BsY1l5Y-RueA1aSf2EeF6A/h22/h001.kqNSKUZfdoTNBYOuNPclbZKoJcKmeiwVU_PhCPcpMa8) benchmark across multiple metrics, beating out other open-source rivals in spatial coherence tests.
* Users can control camera movement through keyboard or joystick inputs, with just a single reference photo needed to create the exportable 3D environments.
* The system also remembers what it creates as you explore, so returning to previous areas shows the same consistent scenery.

**Why it matters:** World models have become one of the hottest frontiers in AI, with labs racing to build systems that understand physical spaces rather than just generating flat images. Between [**Genie 3**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_EZTW4EFxWGOec2W-fdOXlDaAotfwuzzEBD3Jbj8YJ2aXDPasZpKwgMYPBZ6p48RMJzZnkTEw2gQjnTH7qHl4l02bQdmZjgG6-mGZUQ9LdHGB9R9lgF-e7hTfFHl2DlAkNGIPAw57AvJDi2giFsbesKQziaq9Jt85bI_CqvcaVPWay8_0R1oocRf8aom8gNAC_gdUvJ1ZKgZLrWWg6ud5DMtO7ufGJvN1AikNPLv8af1am7zkgThCvZ5xoxXD04OJNDpB4thcS6HLDq-iiHwi726W6xfxtVo8XTAJ2wPUy5KSzedRvYJ9jpi5tyeVn7oAhdUp55XRHhMFbmUbiQVYFA9PJt5HykEy6u4ZHyt3PBXFj5DpsSN5Jzef4SR8h2fHAoImjy4gpttyCqQlJyrPWQgfdboJf9-eG7sMfCdz-m5sIFA5kn8ViMO-mRFtBMyyAYUG8PECZ7o3ydMoCUfsvAeaGWD-e0sqCXIzK2rnLZExMkVI0Zs3y0s7fnAO_iYhrcrAxL8Z4I0OF47xvBnfBg5QY5S9Q0LdeTpafTjz4EANjC-lm4RPDjd9cDSciA6QxWZImj8_Hp4Ll5F1-nnyvh7HeOI1A7jnM7nY6kOodxtOb-Z6tVRwoyYsrPnoHYGhdUtbgv54JwpTEL1OVZ6oen1P6am8OPqXi1zuWma5MYvE6fvmWZcWmdoX7mpQhfztawcytojDzKb4ChrtF0hJbqfFevyu5asR1ugz5KF8MgfNJXJ68-ebXIi-gjR6aguqA/4jl/BsY1l5Y-RueA1aSf2EeF6A/h23/h001.DwhQFJrP5Nq8LjYMXAvTuuGSUfpSGzTqE1CsoJq1WHg), [**Mirage**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hc7OmNogYS0w10FDiX7VcF-FvX_BDzooVXoUeYK9KgbrUUSRK29dqEQhg9z4fcYlNEvxiimtILa7tCjOv9-b76EeBCyqkiHqKfdDHfFypjxcASn2xZBvouwRHprpJRZtRT-BwQyS2zDlobxpxusEB0bkn-YHmlEvekr8V4s5p0xsnlOlFVvJmNC87uli4RKZkCHfSbBcwxy8LYiJE1lTdrusVp-ZqUXylGEmM3Tog0OAVQlkKuYuzZMgN4iNN0plsQ/4jl/BsY1l5Y-RueA1aSf2EeF6A/h24/h001.qozKkFr2wJ-QgHhURk7Z57kO-fV-DbaTPld-gyAFjkM), World-Voyager, and more, the range of options (and the applications for these interactive 3D environments) is growing fast.

# ðŸ”‹Google Reveals How Much Energy A Single AI Prompt Uses

Google just pulled back the curtain on one of tech's best-kept secrets: exactly how much energy its Gemini AI uses with every prompt. The answerâ€”0.24 watt-hours (Wh) per median queryâ€”might seem small at first (about the same as running your microwave for one second). But multiply that by billions of daily interactions, and it suddenly becomes clear just how much energy AI is really using every day. It also uses around 0.03 grams of COâ‚‚ and 0.26 mL of water (roughly five drops), reflecting a 33Ã— reduction in energy use and 44Ã— drop in emissions compared to a year ago, thanks to efficiency gains. \[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/25](https://www.energysage.com/news/google-ai-energy-use-electric-bill-impact/)\]

Read more: [https://www.energysage.com/news/google-ai-energy-use-electric-bill-impact/](https://www.energysage.com/news/google-ai-energy-use-electric-bill-impact/)

# 

# ðŸ§  AI Detects Hidden Consciousness in Comatose Patients Before Doctors

In a groundbreaking study published in \*Communications Medicine\*, researchers developed ""SeeMe"", a computer-vision tool that analyzes subtle facial movementsâ€”down to individual poresâ€”in comatose patients in response to commands. SeeMe detected eye-opening up to ""4.1 days earlier"" than clinical observation, and was successful in 85.7% of cases, compared to 71.4% via standard exams. These early signals correlated with better recovery outcomes and suggest potential for earlier prognoses and rehabilitation strategies.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/31](https://www.scientificamerican.com/article/ai-spots-hidden-signs-of-consciousness-in-comatose-patients-before-doctors/)\] \[[Study details (Communications Medicine)](https://www.nature.com/articles/s43856-025-01042-y)\]

# ðŸ”“ AI Is Unmasking ICE Officersâ€”Sparking Privacy and Policy Alarms

A Netherlands-based activist is using AI to reconstruct masked Immigration and Customs Enforcement (ICE) officers' faces from public video footage. By generating synthetic images and matching them via reverse image search tools like PimEyes, the â€œICE List Projectâ€ has purportedly identified at least 20 agents. While this technique flips the script on surveillance, accuracy remains lowâ€”only about 40% of identifications are correctâ€”igniting debates on ethics, safety, and governmental transparency.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/29](https://www.theverge.com/news/768663/using-ai-to-id-ice)\]

# What Else Happened in AI on September 03rd 2025?

**Mistral AI** [**expanded**](https://link.mail.beehiiv.com/ss/c/u001.BKH0F2yLXfXXfZz4rVL6MM5oG0hN2Cj37YVnXxkYlwYnK84-oKTnNpWCuOgLND8MLLGmF0gAo5SG3r7SLPqgYJNivl4UR60TD6A6PMfkpomGWv5IZLrP7xTkRPKCDfyQpcYz62M5taKoE4EcBCOkwhmPhSUdpL6pDLPWxSRKbMyrR5Js107DcaPbsrtH9he0tQ6RjUkAjYpnhG1sMBKveY7p08tbt8zGkM6mwgMj8A0U_vO85wSZjq8NXMn-PfWZhyUviOgRpvQi8vcRFgkRgQ/4jl/BsY1l5Y-RueA1aSf2EeF6A/h31/h001.ORYMOGQ4sRj2Iy5vDKIDILUggaGTCt8Ell6JyCcHWfM) its Le Chat platform with over 20 new enterprise MCP connectors, also introducing â€œMemoriesâ€ for persistent context and personalization.

**Microsoft** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoGxtF-BzxijLO84T84MZbNYD9k_gqmdV_Rl54l35o5p_MOPJasOjCs-pGeYnaX_mjGliRKtopgmapSlfV0CirKI1ya1PPWcjenWWv9AMx-JzZZk3qz7e0g2bJIqtDr_SnjR5uNnYZ288HKYQE53GuFRyq_ZBt0zJywXehZBwfBEBqQgHxlYRyDbtbXVmI7L9OiNL8q6UjOPJphrNkXzUuyWldvsA3jzvtAp-d1OZDS2MKPLLjUAUsUdhpK7dLYqneYaHGSET44bSowr0U87jO_FAe09kffMix03D4YF-EwUe4q_n-WfXWn_GGT_p5s-78A/4jl/BsY1l5Y-RueA1aSf2EeF6A/h32/h001.AI84d9-X75m656_wUJkJlNDthJpVrmWEewbP2gWBIHE) a new partnership with the U.S. GSA to provide the federal government with free access to Copilot and AI services for up to 12 months.

**OpenAI CPO Kevin Weil** [**unveiled**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HXgjZRogXVxp_aRW3aPw1FrVCyDDtY0O8bnWA-WWc0GkK9c_c3fMyAwRZLq4Bbacvf-1phYF8L5cYTcoB2fBEDgQsu3VcjeKt7A-YccbBv3fSY1hiWbrf2GbiW4AT68G8lNgCrZwpY2tTZoz-MOwNGni2minYp7yoQCLaVOSZ7XpHg30-M7cYAQ1VgntVBwbjsw685yZuxWjwAXCGdm3-1dbiCid3kdXiXVUh8YYGtfCcU6m1V_Gq77tyMqgPcKovQ/4jl/BsY1l5Y-RueA1aSf2EeF6A/h33/h001.H60l6PRQqxf0NFg6O29njEVQIi3gVHsIw1F6mhBHr1E) ""OpenAI for Science,"" a new initiative aimed at building AI-powered platforms to accelerate scientific discovery.

**Swiss researchers from EPFL, ETH Zurich, and CSCS** [**launched**](https://link.mail.beehiiv.com/ss/c/u001.y1enXirMinJ-vLTLBoHZMvMplCzpn9jojJ6lVKqWBoXA68zTXV6l0nLSUI1T2THMrtJTXky-OvpwCFjANij_ymizl9vbvHxtqHeJ48Fr8lJi5ifyQu9PKtvVUOyrH_KNKjATGGrc67a6j1_KrPk9Sx4pyKSOTj83ZjmvY9pKG5pI1hxl8C48gWUlK9UfcVv9Vd1cJ9x3JEWm6daZZ1RynsUjzMS__847QyjbM5jLsAToaIYMQEoo66j8rwfXIeEDhaiJLNpYBbB8fdgQBGVGE4jt4U-sFJhnz9KZyGk6boSm8je-5Ij9JxlJGcgrKwELsygle0ne6E6PRophQeG0hw2NGeQL6sXs04Ez8IshYHDm6x7NuZr6O5WSSWSjT4a2/4jl/BsY1l5Y-RueA1aSf2EeF6A/h34/h001.FzPEfOzMjUpFJ4X08MWqRfuYB-x5vd2VPhvlFZmvDpI) Apertus, a fully open-source multilingual language model trained on over 1,000 languages.

**Chinese delivery giant Meituan** [**open-sourced**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmf7PbaGmZTVLKldn3SZHJSVMA9VYzj5QAIvpDt1S203RkuYkGjuTR42k_mK_mgEB4jlX2zmUrIUGFU4Yk9bsS44DRUCHCIt26c7qDj84QH4y2ns9HhaVD-Kb1GwhOYtyHIENUxoSKV-hq9fNPGbmliErgDjPFDHhk5U4jaznm8bHQeSroouh5qZmkQ-APSACMigrbd_IxpluCZtSrq_SqQ-BaUWGMqKgnKrcTMYV1NxbQ6WnSK0Sztn-lcOb21IArhw/4jl/BsY1l5Y-RueA1aSf2EeF6A/h35/h001.BiamGIgCksxZBijTp3MIbDUrGVpvOL1visIZ1IHNTgo) LongCat-Flash-Chat, the companyâ€™s first AI model that rivals DeepSeek V3, Qwen 3, and Kimi K2 on benchmarks.

**ElevenLabs** [**released**](https://link.mail.beehiiv.com/ss/c/u001.y1enXirMinJ-vLTLBoHZMg-vVZW5em0onl9F02TOIOjGiv51kot8XLOYDXZt9aTVF_9GbYhlB_hKBWaU6b551rUu6WXOfc6Jv-39PAQ9CmwJE64SZ1UsPBk2jV9iIgEqlFsBPBJAyDPTfFv3gDBFSCK-xSajY8bNXljbZXo06NPheH4iLn1zBvVxpo4mjRRWAzrBoM3f8zKw-uz-sSLO3hq1CEPsXfQJAMD4tlaYshtw5fBVrPC0Pm-vR9SVWDTq/4jl/BsY1l5Y-RueA1aSf2EeF6A/h36/h001.hGu0a8yEnHFnAyKmTvb5qMdz3jTNOt2otIov-tfXlXI) an upgraded version of its sound effects AI model, with new features including looping, extended output length, and higher quality generations.

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ… **Build Authentic Authority:** Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ… **Generate Enterprise Trust:** Earn credibility in a way that corporate marketing simply can't.

âœ… **Reach a Targeted Audience:** Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?** Learn more and apply for a Strategic Partnership here: [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled)

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n7oz1y/ai_daily_news_rundown_google_wont_have_to_sell/,r_1n7oz1y,,,
r_1n7oj04,reddit,WildAppearance2153,2025-09-03T19:32:43+00:00,"[P] Arbitrary Order Automatic Differentiation for PyTorch
Iâ€™m excited to present **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only package that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a bachelor's research project at Universidad Pontificia de Comillas - ICAI, and we are considering publishing a future academic article reviewing the mathematical details and the implementation design.

At its core, thoad takes a one output, many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1â†’N problem can be rewritten as 1â†’1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoadâ€™s graph aware formulation enables:

* Working with smaller **pieced external derivatives**
* An optimization based on **unifying independent dimensions** (especially batch).

This delivers **asymptotically better scaling** with respect to order and batch size (respectively).

Additionally, we compute derivatives with a *vectorial* approach rather than component by component, which makes our pure PyTorch implementation possible. Consequently, the implementation stays at a high level, written entirely in Python and using **PyTorch** as its only dependency. Avoiding custom C++ or CUDA has a very positive impact on the long-term maintainability of the package.

The package is already available to be installed from **GitHub** or **PyPI**:

* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)

In our benchmarks, thoad **outperforms** torch.autograd for **Hessian calculations even on CPU**. See the repository *examples/benchmarks* to check the comparisons and run them in your own hardware.

**thoad** is designed to align closely with PyTorchâ€™s interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorchâ€™s own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.

# USING THE PACKAGE

**thoad** exposes two primary interfaces for computing high-order derivatives:

1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).
2. `thoad.Controller`: a class-based interface that wraps the output tensorâ€™s subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.

Example of autodifferentiation execution via `thoad.backward`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Call thoad backward
    order = 2
    thoad.backward(tensor=Z, order=order)
    
    #### Checks
    ## check derivative shapes
    for o in range(1, 1 + order):
       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))
       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))
    ## check first derivatives (jacobians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)
    J = torch.autograd.functional.jacobian(fn, (X, Y))
    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)
    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)
    ## check second derivatives (hessians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()
    H = torch.autograd.functional.hessian(fn, (X, Y))
    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)
    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)

Example of autodifferentiation execution via `thoad.Controller`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Instantiate thoad controller and call backward
    order = 2
    controller = thoad.Controller(tensor=Z)
    controller.backward(order=order, crossings=True)
    
    #### Fetch Partial Derivatives
    ## fetch T0 and T1 2nd order derivatives
    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))
    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))
    assert torch.allclose(partial_XX, X.hgrad[1])
    assert torch.allclose(partial_YY, Y.hgrad[1])
    ## fetch cross derivatives
    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))
    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))

>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n7oj04/p_arbitrary_order_automatic_differentiation_for/,r_1n7oj04,,,
r_1n7ntsx,reddit,RepresentativeYear83,2025-09-03T19:06:18+00:00,"How can I find optimal hyperparameter's when training large models?
I'm currently training a ViT-b/16 model from scratch for a school research paper on a relatively small dataset (35k images, Resisc45). 

The biggest issue I encounter is constantly over-/under-fitting, and I see that adjusting hyperparameters, specifically learning rate and weight decay, gives the most improvements to my model. 

Nevertheless, each training session takes \~30 minutes on an A100 Google Colab GPU, which can be expensive when accumulating each adjustment session. What procedures do data scientists take to find the best hyperparameters, especially when training models way larger than mine, without risking too much computing power?



Extra: For some reason, reducing the learning rate (1e-4) and weight decay (5e-3) at a lower epoch count (20 epochs) gives the best result, which is surprising when training a transformer model on a small dataset. My hyperparameters go completely against the ones set in traditional research paper environments, but maybe I'm doing something wrong... LMK",deeplearning,17,https://www.reddit.com/r/deeplearning/comments/1n7ntsx/how_can_i_find_optimal_hyperparameters_when/,r_1n7ntsx,,,
r_1n7k3kt,reddit,Key-Avocado592,2025-09-03T16:48:43+00:00,"[D] Static analysis for PyTorch tensor shape validation - catching runtime errors at parse time
I've been working on a static analysis problem that's been bugging me: most tensor shape mismatches in PyTorch only surface during runtime, often deep in training loops after you've already burned GPU cycles.

**The core problem:** Traditional approaches like type hints and shape comments help with documentation, but they don't actually validate tensor operations. You still end up with cryptic RuntimeErrors like ""mat1 and mat2 shapes cannot be multiplied"" after your model has been running for 20 minutes.

**My approach:** Built a constraint propagation system that traces tensor operations through the computation graph and identifies dimension conflicts before any code execution. The key insights:

* **Symbolic execution:** Instead of running operations, maintain symbolic representations of tensor shapes through the graph
* **Constraint solving:** Use interval arithmetic for dynamic batch dimensions while keeping spatial dimensions exact
* **Operation modeling:** Each PyTorch operation (conv2d, linear, lstm, etc.) has predictable shape transformation rules that can be encoded

**Technical challenges I hit:**

* Dynamic shapes (batch size, sequence length) vs fixed shapes (channels, spatial dims)
* Conditional operations where tensor shapes depend on runtime values
* Complex architectures like Transformers where attention mechanisms create intricate shape dependencies

**Results:** Tested on standard architectures (VGG, ResNet, EfficientNet, various Transformer variants). Catches about 90% of shape mismatches that would crash PyTorch at runtime, with zero false positives on working code.

The analysis runs in sub-millisecond time on typical model definitions, so it could easily integrate into IDEs or CI pipelines.

**Question for the community:** What other categories of ML bugs do you think would benefit from static analysis? I'm particularly curious about gradient flow issues and numerical stability problems that could be caught before training starts.

Anyone else working on similar tooling for ML code quality?

ðŸš€ \*\*UPDATE: VS Code Extension Released!\*\*



  Due to interest, I've packaged it as a VS Code extension!



  \*\*Download:\*\* [https://github.com/rbardyla/rtx5080-tensor-debugger-/releases/tag/v1.0.0](https://github.com/rbardyla/rtx5080-tensor-debugger-/releases/tag/v1.0.0)



  \*\*Install:\*\*

  \`\`\`bash

  code --install-extension rtx5080-tensor-debugger-1.0.0.vsix



  Features:

  \- ðŸ”´ Red squiggles on tensor bugs

  \- ðŸ’¡ Hover for instant fixes

  \- âš¡ Real-time as you type

  \- ðŸ“Š Zero config



  Working on marketplace listing, but you can use it NOW!",deeplearning,11,https://www.reddit.com/r/deeplearning/comments/1n7k3kt/d_static_analysis_for_pytorch_tensor_shape/,r_1n7k3kt,,,
r_1n7g5fv,reddit,LowChance4561,2025-09-03T14:21:31+00:00,"Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic
The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining  
would appreciate an upvote if u like it [https://huggingface.co/papers/2509.01363](https://huggingface.co/papers/2509.01363)",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n7g5fv/reasoning_vectors_transferring_chainofthought/,r_1n7g5fv,,,
r_1n7d4gg,reddit,andsi2asi,2025-09-03T12:13:18+00:00,"AIWolfDial 2025's Werewolf Benchmark Tournament Results, and the Grok 4 Exclusion




AIWolfDial 2025 recently ran a contest to see which of the top AI models would be most emotionally intelligent, most persuasive, most deceptive, and most resistant to manipulation. A noble endeavor indeed. 

ChatGPT-5 crushed the competition with a score of 96.7. Gemini 2.5 Pro came in second with 63.3, 2.5 Flash came in third with 51.7, and Qwen3-235B Instruct came in fourth with 45.0. Yeah, GPT-5 totally crushed it!

But keep this in mind. Our world's number one model on HLE is Grok 4, and on ARC-AGI-2 it crushes GPT-5, 16 to 9. These two benchmarks measure fluid intelligence, which I would imagine are very relevant to the Werewolf Benchmark. They didn't test Grok 4 because it was released just a few weeks before the tournament, and there wasn't time enough to conduct the integration. Fair enough. 

The Werewolf Benchmark seems exceptionally important if we are to properly align our most powerful AIs to defend and advance our highest human values. AIWolfDial 2025 is doing something very important for our world. Since it would probably take them a few weeks to test Grok 4, I hope they do this soon, and revise their leaderboard to show where they come in. Naturally, we should all hope that it matches or exceeds ChatGPT-5. If there is one area in AI where we should be pushing for the most competition, this is it.",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n7d4gg/aiwolfdial_2025s_werewolf_benchmark_tournament/,r_1n7d4gg,,,
r_1n7c3a5,reddit,Right_Pea_2707,2025-09-03T11:21:25+00:00,AMA Incoming: With the Founder of Loopify.AI - Giovanni Beggiato,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n7c3a5/ama_incoming_with_the_founder_of_loopifyai/,r_1n7c3a5,,,
r_1n7bvam,reddit,Any_Commercial7079,2025-09-03T11:09:32+00:00,"Sentiment Analysis Model for cloud services
Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.

Iâ€™m happy with the initial results, but Iâ€™d love advice on making the interpretation more precise:

Ensuring sentiment is directed at the provider (not another product/entity mentioned)  
Better handling of comparative or mixed statements (e.g., â€œfast but expensiveâ€)  
Improving robustness to negation and sarcasm

If you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, Iâ€™d really appreciate your input.

Repo:Â [https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)  
It would also be great if you could answer my original survey:Â [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thanks!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n7bvam/sentiment_analysis_model_for_cloud_services/,r_1n7bvam,,,
r_1n7a2p7,reddit,enoumen,2025-09-03T09:22:47+00:00,"AI Daily News Rundown: ðŸ§‘â€ðŸ§‘â€ðŸ§’ OpenAI is adding parental controls to ChatGPT, ðŸ¦¾ AI helps paralyzed patients control robots, ðŸ—£ï¸ AIâ€™s favorite buzzwords seep into everyday speech,
ðŸ’‰ MITâ€™s AI to predict flu vaccine success âŒ Salesforce cut 4,000 jobs because of AI agents & more (Sept 02 2025)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n7a2p7/ai_daily_news_rundown_openai_is_adding_parental/,r_1n7a2p7,,,
r_1n78pj4,reddit,No_Direction_6170,2025-09-03T07:52:18+00:00,"AIML newbie here, which course to start with ?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n78pj4/aiml_newbie_here_which_course_to_start_with/,r_1n78pj4,,,
r_1n77m8p,reddit,shani_786,2025-09-03T06:41:27+00:00,Autonomous Vehicles Learning to Dodge Traffic via Stochastic Adversarial Negotiation,deeplearning,8,https://www.reddit.com/r/deeplearning/comments/1n77m8p/autonomous_vehicles_learning_to_dodge_traffic_via/,r_1n77m8p,,,
r_1n75rjz,reddit,dazzlinlassie,2025-09-03T04:49:40+00:00,"How to understand research paper
I have learnt basic of DL and math required. I am sort of confused.",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n75rjz/how_to_understand_research_paper/,r_1n75rjz,,,
r_1n6vx3e,reddit,Ok_Post_149,2025-09-02T21:17:44+00:00,"Free 1,000 CPU + 100 GPU hours for testers
Scaling Python code in the cloud should be easy for data scientists and analysts. At my last job, my team was constantly bottlenecked by our DevOps team every time we needed to run large-scale jobs. Theyâ€™d get swamped, and trying to teach the data team how to manage the infrastructure themselves just didn't work.

That experience led me to build an [open-source](https://github.com/Burla-Cloud/burla) cluster compute tool that makes scaling simple for any Python developer. With just one function, you can deploy to massive clusters (10k vCPUs, 1k GPUs). It's built for parallel workloads like data prep, batch inference, or hyperparameter tuning.

You can bring your own Docker image, define hardware requirements, and fire off a million simple functions in seconds. To show how it works, I spun up 4k vCPUs to screenshot 30k arXiv PDFs in a couple minutes:[https://x.com/infra\_scale\_5/status/1938024103744835961](https://x.com/infra_scale_5/status/1938024103744835961)

I'm looking for test users and am offering managed clusters with **1,000 CPU hours and 100 GPU hours** to get started. If you like it, I'm also happy to help get it up and running in your own private cloud. If you're interested, you can reach me at joe@burla.dev.

Would love testers.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n6vx3e/free_1000_cpu_100_gpu_hours_for_testers/,r_1n6vx3e,,,
r_1n6tb7d,reddit,LegalProblem8198,2025-09-02T19:39:00+00:00,Major in AI,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n6tb7d/major_in_ai/,r_1n6tb7d,,,
r_1n6sqcj,reddit,Far_Hurry1937,2025-09-02T19:16:39+00:00,"Using a GTX 1660 Super Okay for Deep Learning?
I am starting to get really into computer vision and deep learning. I have made a few projects with OpenCV and found out that I am actually really interested in this sort of stuff. I also just started going through a PyTorch course last week as well to learn more technical computer vision and deep learning stuff. 

**My Question:** Will my GTX 1660 Super be okay for this? Should I think about getting a new GPU in the near future, or should I just use Google Collab? 

I know right now my GPU will be fine because I am still learning the basics of deep learning and PyTorch, but I also want to know how far I can push my older GPU before I need to get a better model. 

Thanks ",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n6sqcj/using_a_gtx_1660_super_okay_for_deep_learning/,r_1n6sqcj,,,
r_1n6s5x9,reddit,QuantumFree,2025-09-02T18:55:31+00:00,"PosetLM: a sparse Transformer-alternative with lower VRAM and strong perplexity (code released)
Hi everyone,  
Some time ago I shared my independent research on an alternative to Transformers based on DAGs (posets) rather than dense attention. I'm now releasing the full code on GitHub â€” focused, academic, and designed to train on smaller GPUs.

**Repo**: [https://github.com/gioruggieri/posetlm](https://github.com/gioruggieri/posetlm?utm_source=chatgpt.com)

# What is PosetLM?

PosetLM is a causal language model that restricts each token to a sparse set of parent tokens (up to `K`) within a sliding window of size `W`. Messages are gated by a logistic score (sigmoid), raised to a temperature-scaled exponent, and iteratively aggregated over the DAG.  
This avoids dense attention (`O(TÂ²)`), yielding **linear-time inference** and much lower **VRAM** use.

# Highlights

* **Sparse DAG aggregation** over Top-K parents (per token)
* **No softmax**: edge-wise `sigmoid^(1/Ï„)` \+ relative positional bias
* **Low VRAM**: scales with `O(BÂ·TÂ·KÂ·d)` instead of `O(TÂ²)`
* **Good perplexity**: comparable to Transformer at same parameter count (on WikiText-103)
* **Supports word/BPE/byte**, `.tokens` or HuggingFace datasets
* **Pure PosetLM**: no Transformer fallback, no pretraining shortcuts
* **Academic repo**: single-file, reproducible, metrics logged

# Results (WikiText-103, word-level PPL)

|Model|\#Params|PPL â†“|GPU|Notes|
|:-|:-|:-|:-|:-|
|PosetLM|\~12M|\~61â€“65|GTX 1080|`K=12W=256Ï„=0.07`,  ,|
|Transformer (same d, layers)|\~12M|\~58|GTX 1080|full attention|

You can push much longer contexts on modern GPUs thanks to fixed sparsity.

# Quickstart

    python posetlm.py --dataset hf_wikitext103_raw --tokenizer word \
      --seq_len 512 --batch_size 6 --grad_accum 2 --steps 100000 \
      --scheduler cosine --lr 2e-4 --warmup 4000 \
      --k_parents 24 --window 256 --poset_iters 3 --dynamic_topk --topk 12 \
      --dropout 0.1 --fp16_cache --amp --adaptive_softmax \
      --cutoffs ""2000,10000,50000""

Iâ€™d love your feedback â€” architectural ideas, scaling tests, theory connections, etc.  
This is 100% open source and Iâ€™ll continue improving it. PRs welcome!

â€“ Giovanni Ruggieri  
GitHub: [gioruggieri/posetlm](https://github.com/gioruggieri/posetlm?utm_source=chatgpt.com)",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1n6s5x9/posetlm_a_sparse_transformeralternative_with/,r_1n6s5x9,,,
r_1n6mxqv,reddit,Fuzzy_Structure_6246,2025-09-02T15:41:23+00:00,"Why is my training loss so steep at the beginning ?
For different models with same batchsizes the start loss and loss after the steep part would be very similar, is that normal?

With bigger batchsizes, axis gets scaled but graph still looks the same.

Has this something to do with the data being really easy to learn for the model or might this be more related to a bias that is learned in the first epochs ?

This is a regression problem and I am trying to predict compressor power based on temperatures and compressor revolutions.

[Batchsize 32](https://preview.redd.it/9j0b0bzgtrmf1.png?width=1028&format=png&auto=webp&s=765be16906997afe44ff32490754272fd69067b5)

[Batchsize 128](https://preview.redd.it/7kppgbzgtrmf1.png?width=1020&format=png&auto=webp&s=6a861a92649ccd9091a028212df80b03b9913172)",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1n6mxqv/why_is_my_training_loss_so_steep_at_the_beginning/,r_1n6mxqv,,,
r_1n6lpte,reddit,await_void,2025-09-02T14:55:29+00:00,"Tried building an explainable Vision-Language Model with CLIP to spot and explain product defects!
Hi all!

After quite a bit of work, Iâ€™ve finally completed my **Vision-Language Model** â€” building something this complex in a multimodal context has been one of the most rewarding experiences Iâ€™ve ever had. This model is part of my Masterâ€™s thesis and is designed to **detect product defects and explain them in real-time**. The project aims to **address a Supply Chain challenge**, where the end user needs to clearly **understand** ***why*** **and** ***where*** a product is defective, in an **explainable and transparent** way.

[A gradcam map activation for the associated predicted caption with his probability: \\""A fruit with Green Mold\\""](https://preview.redd.it/ota230yckrmf1.png?width=1200&format=png&auto=webp&s=4f6fec426fa74b9c7f9e11f1f43b20aa96d3c1e3)

I took inspiration from the amazing work of [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734), a paper worth a reading, and modified some of his structure to adapt it to my case scenario:

For a brief explanation, basically what it does is that the image is first **transformed into an embedding using CLIP**, which captures its semantic content. This **embedding is then used to guide GPT-2** (or any other LLM really, i opted for **OPT-125** \- pun intended) **via an auxiliar mapper** (a simple transformer that can be extended to more complex projection structure based on the needs) that **aligns the visual embeddings to the text one**, catching the meaning of the image. If you want to know more about the method, this is the [original author post](https://www.reddit.com/r/MachineLearning/comments/q3xon8/p_fast_and_simple_image_captioning_model_using/), super interesting.

Basically, It **combines CLIP** (for visual understanding) **with a language model** to generate a short description and overlays showing exactly where the model â€œlookedâ€, and the **method itself it's super fast to train and evaluate,** because nothing it's trained aside a small mapper (an MLP, a Transformer) which rely on the concept of the **Prefix Tuning** (A Parameter Efficient Fine Tuning technique).

What i've extended on my work actually, is the following:

* **Auto-labels images using CLIP** (no manual labels), then trains a captioner for your domain. This was one of the coolest discovery i've made and will definitely use Contrastive Learning methods to auto label my data in the future.
* Using **another LLM** (OPT-125) to generate better, intuitive caption
* Generates a **plain-language defect description**.
* A **custom Grad-CAM** from scratch based on the ViT-B32 layers, to create **heatmaps** that justify the decisionâ€”per prompt and combined, giving transparent and explainable choice visual cues.
* Runs in a simple **Gradio Web App** for quick trials.
* Much more in regard of the entire **project structure/architecture**.

Why it matters? In my Master Thesis scenario, i had those goals:

* **Rapid bootstrapping without hand labels**: I had the ""exquisite"" job to collect and label the data. Luckily enough, i've found a super interesting way to automate the process.
* **Visual and textual explanations for the operator**: The ultimate goal was to provide visual and textual cues about why the product was defective.
* **Designed for supply chains** setting (defect finding, identification, justification), and may be **extended to every domain** with the appropriate data (in my case, it regards the rotten fruit detection).

The model itself was trained on around **15k of images**, taken from [Fresh and Rotten Fruits Dataset for Machine-Based Evaluation of Fruit Quality](https://data.mendeley.com/datasets/bdd69gyhv8/1), which presents around \~3200 unique images and **12335 augmented** one. Nonentheless the small amount of image the model presents a surprising accuracy.

**For anyone interested, this is the Code repository:** [https://github.com/Asynchronousx/CLIPCap-XAI](https://github.com/Asynchronousx/CLIPCap-XAI) with more in-depth explanations.

Hopefully, this could help someone with their researches, hobby or whatever else! I'm also happy to answer questions or hear suggestions for improving the model or any sort of feedback.

Following a little demo video for anyone interested (could be also find on the front github repo page if reddit somehow doesn't load it!)

[Demo Video for the Gradio Web-App](https://reddit.com/link/1n6lpte/video/smwwikhakrmf1/player)

Thank you so much!",deeplearning,13,https://www.reddit.com/r/deeplearning/comments/1n6lpte/tried_building_an_explainable_visionlanguage/,r_1n6lpte,,,
r_1n6lbmx,reddit,await_void,2025-09-02T14:40:38+00:00,"Tried building an explainable Vision-Language Model with CLIP to spot and explain product defects!
Hi all! 

After quite a bit of work, Iâ€™ve finally completed my **Vision-Language Model** â€” building something this complex in a multimodal context has been one of the most rewarding experiences Iâ€™ve ever had. This model is part of my Masterâ€™s thesis and is designed to **detect product defects and explain them in real-time**. The project aims to **address a Supply Chain challenge**, where the end user needs to clearly **understand** ***why*** **and** ***where*** a product is defective, in an **explainable and transparent** way.

I took inspiration from the amazing work of [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734), a paper worth a reading, and modified some of his structure to adapt it to my case scenario: 

For a brief explanation, basically what it does is that the image is first **transformed into an embedding using CLIP**, which captures its semantic content. This **embedding is then used to guide GPT-2** (or any other LLM really, i opted for **OPT-125** \- pun intended) **via an auxiliar mapper** (a simple transformer that can be extended to more complex projection structure based on the needs) that **aligns the visual embeddings to the text one**, catching the meaning of the image. If you want to know more about the method, this is the [original author post](https://www.reddit.com/r/MachineLearning/comments/q3xon8/p_fast_and_simple_image_captioning_model_using/), super interesting.

Basically, It **combines CLIP** (for visual understanding) **with a language model** to generate a short description and overlays showing exactly where the model â€œlookedâ€, and the **method itself it's super fast to train and evaluate,** because nothing it's trained aside a small mapper (an MLP, a Transformer) which rely on the concept of the **Prefix Tuning** (A Parameter Efficient Fine Tuning technique). 

What i've extended on my work actually, is the following: 

\- **Auto-labels images using CLIP** (no manual labels), then trains a captioner for your domain. This was one of the coolest discovery i've made and will definitely use Contrastive Learning methods to auto label my data in the future.

\- Using **another LLM** (OPT-125) to generate better, intuitive caption 

\- Generates a **plain-language defect description**.  

\- A **custom Grad-CAM** from scratch based on the ViT-B32 layers, to create **heatmaps** that justify the decisionâ€”per prompt and combined, giving transparent and explainable choice visual cues.

\- Runs in a simple **Gradio Web App** for quick trials.

\- Much more in regard of the entire project structure/architecture.



Why it matters? In my Master Thesis scenario, i had those goals:

\- **Rapid bootstrapping without hand labels**: I had the ""exquisite"" job to collect and label the data. Luckily enough, i've found a super interesting way to automate the process.

\- **Visual and textual explanations for the operator**: The ultimate goal was to provide visual and textual cues about why the product was defective.

\- **Designed for supply chains** setting (defect finding, identification, justification), and may be **extended to every domain** with the appropriate data (in my case, it regards the rotten fruit detection). 

  
The model itself was trained on around **15k of images**, taken from [Fresh and Rotten Fruits Dataset for Machine-Based Evaluation of Fruit Quality](https://data.mendeley.com/datasets/bdd69gyhv8/1), which presents around \~3200 unique images and **12335 augmented** one. Nonentheless the small amount of image the model presents a surprising accuracy. 

For anyone interested, this is the Code repository with Demo Examples (Video, Images):  [https://github.com/Asynchronousx/CLIPCap-XAI](https://github.com/Asynchronousx/CLIPCap-XAI)

Hopefully, this could help someone with their researches, hobby or whatever else! I'm also happy to answer questions or hear suggestions for improving the model or any sort of feedback.

Thank you so much!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n6lbmx/tried_building_an_explainable_visionlanguage/,r_1n6lbmx,,,
r_1n6heap,reddit,Neurosymbolic,2025-09-02T11:52:56+00:00,Neural Manipulation of Symbols,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n6heap/neural_manipulation_of_symbols/,r_1n6heap,,,
r_1n6c4yf,reddit,rakii6,2025-09-02T06:26:21+00:00,"Building IndieGPU: A software dev's approach to GPU cost optimization
(self-promotion)
Hey everyone

A Software dev (with 2YOE) here who got tired of watching startup friends complain about AWS GPU costs. So I builtÂ [IndieGPU](https://www.indiegpu.com/)Â \- simple GPU rental for ML training.

**What I discovered about GPU costs:**

* AWS P3.2xlarge (1x V100): $3.06/hour
* For a typical model training session (12-24 hours), that's $36-72 per run
* Small teams training 2-3 models per week â†’ $300-900/month just for compute

**My approach:**

* RTX 4070s with 12GB VRAM
* Transparent hourly pricing
* Docker containers with Jupyter/PyTorch ready in 60 seconds
* Focus on training workloads, not production inference

**Question for the community:**Â What are the biggest GPU cost pain points you see for small ML teams? Is it the hourly rate, minimum commitments, or something else?

Right now I am trying to find users who could use the platform for their ML/AI training, free for a month, no strings attached.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n6c4yf/building_indiegpu_a_software_devs_approach_to_gpu/,r_1n6c4yf,,,
r_1n5usjh,reddit,MinimumArtichoke5679,2025-09-01T17:21:23+00:00,Vision Language Models topic for master thesis,deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n5usjh/vision_language_models_topic_for_master_thesis/,r_1n5usjh,,,
r_1n5te5g,reddit,enoumen,2025-09-01T16:29:39+00:00,"AI Weekly Rundown From August 24 to August 31 2025: ðŸ‘€ Alibaba develops new AI chip to replace Nvidia ðŸ¤ Meta in talks to use Google and OpenAI AI & more
# Listen atÂ [https://podcasts.apple.com/us/podcast/ai-weekly-rundown-from-august-24-to-august-31-2025/id1684415169?i=1000724278272](https://podcasts.apple.com/us/podcast/ai-weekly-rundown-from-august-24-to-august-31-2025/id1684415169?i=1000724278272)

# Read and Listen on Substack atÂ [https://enoumen.substack.com/p/ai-weekly-rundown-from-august-24](https://enoumen.substack.com/p/ai-weekly-rundown-from-august-24)

https://preview.redd.it/g942ozb8ykmf1.png?width=1080&format=png&auto=webp&s=2dbfa5fc29f18b932207c912fc199fb655b7afa2

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

# This Week's Headlines:

# ðŸ‘€ Alibaba develops new AI chip to replace Nvidia

# ðŸ©º AI stethoscope detects heart conditions in 15 seconds

# ðŸ¤ Meta in talks to use Google and OpenAI AI

# âš–ï¸ xAI sues ex-engineer for stealing secrets for OpenAI

# ðŸ¤— Meta adds new AI safeguards for teen users

# ðŸ’¥ Microsoft launches its first in-house AI models

# ðŸŒªï¸ ChatGPT co-creator threatened to quit Meta AI lab

# ðŸ¤– xAI just launched its first code model

# ðŸ—£ï¸ OpenAIâ€™s gpt-realtime for voice agents

# ðŸŒ Cohereâ€™s SOTA enterprise translation model

# ðŸ”Š Microsoft Part Ways with OpenAI Voice Models by Launching Its Own.

# ðŸ›¡ï¸ OpenAI and Anthropic test each other's AI for safety

# âœ‚ï¸ Google has cut 35% of small team managers

# âœï¸ WhatsApp's new AI helps you rephrase messages

# ðŸ’¸ Nvidia is (really) profiting from the AI boom

# ðŸ† A16zâ€™s fifth GenAI consumer app rankings

# ðŸ“º Microsoft brings Copilot AI to your TV

# ðŸ“¡ The data brokers feeding AI's hunger

# ðŸŽ­ Musk doubles down on anime marketing for Grok despite fan backlash

# âš–ï¸ AI deadbots move from advocacy to courtrooms as $80B industry emerges.

# ðŸ¤– Anthropic launches Claude for Chrome

# ðŸ—£ï¸ Google Translate takes on Duolingo with new features

# ðŸ›¡ï¸ OpenAI adds new safeguards after teen suicide lawsuit

# âš ï¸ Anthropic warns hackers are now weaponizing AI

# ðŸƒ Meta loses two AI researchers back to OpenAI

# ðŸŒ Googleâ€™s Flash Image takes AI editing to a new level

# ðŸ“ Anthropic reveals how teachers are using AI in the classroom

# ðŸ”¹ Blue Water Autonomy raises $50M for unmanned warships.

# ðŸ¤” Apple reportedly discussed buying Mistral and Perplexity

# ðŸŽ™ï¸ Microsoftâ€™s SOTA text-to-speech model

# ðŸ§  Nvidiaâ€™s releases a new 'robot brain'

# ðŸŒ Google Geminiâ€™s AI image model gets a â€˜bananasâ€™ upgrade

# ðŸ’° Perplexityâ€™s $42.5M publisher revenue program

# ðŸ‘¨ðŸ»â€âš–ï¸ Elon Muskâ€™s xAI sues Apple, OpenAI

# Silicon Valley's $100 million bet to buy AI's political future

# Saudi Arabia launches Islamic AI chatbot.

# ðŸ“±Apple explores Googleâ€™s Gemini to fix Siri

# ðŸ§¬ OpenAI, Retro Biosciences make old cells young again

# ðŸ’¥ Musk sues Apple and OpenAI over AI deal

# ðŸš€ Perplexity to give media giants share of AI search revenue

# ðŸŽ¨ Meta partners with Midjourney for â€˜aestheticâ€™ AI

# âœ‚ï¸ TSMC removes Chinese tools from its 2-nm factories

# ðŸ¦ Malaysia Launches Ryt Bank â€” Worldâ€™s First AI-Powered Bank

# ðŸŽ¥ YouTube Secretly Used AI to Edit Peopleâ€™s Videosâ€”Results Can Bend Reality

# ðŸ¤– AI-Powered Robo Dogs Begin Food Delivery Trials in ZÃ¼rich

# ðŸ“Š Reddit Becomes Top Source for AI Searches, Surpassing Google

# âš•ï¸ Study Warns Doctors May Become Overly Dependent on AI

# ðŸ” Customers Troll Taco Bellâ€™s AI Drive-Thru with Prank Orders

# âœˆï¸ US Fighter Pilots Receive Tactical Commands from AI for the First Time

# ðŸ’° Nvidia CEO Expects $3 Trillion to $4 Trillion in AI Infrastructure Spend by 2030

# ðŸ›¡ï¸ OpenAI to Add Parental Controls to ChatGPT After Teen's Death

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ… Build Authentic Authority: Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ… Generate Enterprise Trust: Earn credibility in a way that corporate marketing simply can't.

âœ… Reach a Targeted Audience: Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

Ready to make your brand part of the story? Learn more and apply for a Strategic Partnership here:Â [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled)Â Or, contact us directly at:Â [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n5te5g/ai_weekly_rundown_from_august_24_to_august_31/,r_1n5te5g,,,
r_1n5shu9,reddit,andsi2asi,2025-09-01T15:56:41+00:00,"In Praise Of Ray Kurzweil, The Technological Prophet Who In 1990 Understood And Predicted Today's AI Revolution. Hold on to Your Hats!





No one comes closer to understanding today's technology, or the pace of its advancement, than Ray Kurzweil. It could be said that he provided the insight and vision to much of what is happening today.

In his 1990 book, The Age of Intelligent Machines, Kurzweil predicted that we would reach AGI by 2029, and the next four years will probably prove him to have been right. But that's not all he did. Of his 147 predictions, 86% of them are said to have come true. These include smartphones with speech and handwriting recognition, and the Internet becoming worldwide by the early 2000s.

At the heart of these predictions is what he calls the Law of Accelerating Returns. It basically says that not only is technology advancing at an exponential rate, the rate of that advancement is also accelerating. 

To understand how exponential progress works, imagine being asked to choose between a penny that doubles every day for 30 days or a million dollars. If you chose the penny, at the end of those 30 days you would have over $5 million. Now add acceleration to that rate of progress.

Or, imagine an upright hockey stick with the blade propped up an inch or two, and AI technology in 2025 being at the ""knee of the curve."" Kurzweil predicted that the 2020s would be when AI ""takes off,"" also becoming the catalyst of a benevolent societal revolution on a scale, and more rapid and positively transformative, than we could have ever dreamed possible.

Many people are aware of Kurzweil's prediction of a technological ""Singularity,"" or the time when technology becomes so rapid and ubiquitous that it is virtually impossible to predict the future with any specific accuracy. He predicted that we would reach this Singularity by 2045. At our current pace of AI advancement and acceleration, few would be surprised by our reaching that milestone by then, if not much sooner.

His predictions included autonomous AI and AI discoveries in computing, biology, medicine, etc., and expanded to societal integrations like home robots and self-driving cars. 

But at the heart of his predictions was his confidence that this technological revolution would create a world of ubiquitous abundance, extended life spans ended only by accidents or acts of nature like hurricanes, virtually all diseases being cured, and our world being advised and guided by AIs a billion times more intelligent than our most intelligent human. Essentially what he was predicting was a paradise on Earth for everyone, all made possible by technology.

The world owes Ray Kurzweil a tremendous debt of gratitude!!!
",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n5shu9/in_praise_of_ray_kurzweil_the_technological/,r_1n5shu9,,,
r_1n5rkka,reddit,lipflip,2025-09-01T15:21:21+00:00,"Study on Public Perception of AI in Germany in terms of expectancy, risks, benefits, and value across 71 future scenarios: AI is seen as being here to stay, but risky and of little use an value. Yet, value formation is more driven by perception of benefits than risk perception.",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n5rkka/study_on_public_perception_of_ai_in_germany_in/,r_1n5rkka,,,
r_1n5rh6b,reddit,Even-Tour-4580,2025-09-01T15:17:47+00:00,"Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones
[Heedless Backbone](https://heedlessbackbones.com/)s

https://preview.redd.it/vw1xr6rhlkmf1.png?width=3126&format=png&auto=webp&s=e2dccd020c28fd0a0aeef0d07ee83dfaee3b3f2f

This is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.

I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).

I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project'sÂ [github](https://github.com/igm503/heedless-backbones)",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1n5rh6b/computer_vision_backbone_model_paperswithcode/,r_1n5rh6b,,,
r_1n5q3q6,reddit,Ok_Ratio_2368,2025-09-01T14:24:56+00:00,Advice on Projects & Open Source Contributions for Web Dev â†’ Data Science/ML,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n5q3q6/advice_on_projects_open_source_contributions_for/,r_1n5q3q6,,,
r_1n5p7sl,reddit,Amazing_Life_221,2025-09-01T13:49:44+00:00,"""The Principles of Deep Learning Theory"" by Daniel A. Roberts, Am I dumb?
How challenging is it to read The Principles of Deep Learning Theory by Daniel A. Roberts and Sho Yaida?

Although I donâ€™t have a math/physics degree, Iâ€™m an engineer with a theoretical understanding of deep learning (or that's what I used to think). After completing Deep Learning by Goodfellow and a few other graduate-level math/deep learning books, I wanted to dive deeper into the subject (I do have practical knowledge). I came across this book and now feel like a complete novice.

Itâ€™s worth noting that both authors are physicists, and the book is written for those with a theoretical physics background. However, Iâ€™m eager to explore it because it could serve as a good starting point for understanding the actual mechanics of theory of deep learning. How should I prepare for it? Is self-study even possible for these topics? Any recommendations for reading before this book?",deeplearning,12,https://www.reddit.com/r/deeplearning/comments/1n5p7sl/the_principles_of_deep_learning_theory_by_daniel/,r_1n5p7sl,,,
r_1n5lmc0,reddit,Unlikely_Pirate5970,2025-09-01T10:57:19+00:00,"The Only Chegg Unlocker That Actually Works in 2025 (Discord + Chrome Hack Inside Scoop)
**The Hook:**  
Weâ€™ve all been thereâ€”2AM, a deadline breathing down your neck, and *boom*... Chegg throws up that cursed paywall.

Iâ€™m a broke commerce student whoâ€™s tested literally every â€œfree unlockâ€ scam on the internet over the last year. Forget the garbageâ€”youâ€™re about to get the only method thatâ€™s been saving my GPA (and wallet) in 2025.

**The Method (The Meat):**

Itâ€™s all about **Discord unlock servers**â€¦ and a surprisingly simple **Chrome trick**.

# Working Solution - [https://discord.gg/5DXbHNjmFc](https://discord.gg/5DXbHNjmFc)

Hereâ€™s exactly how you do it:

1. Go to Discord.
2. In Public Servers, type **â€œHomework Helpâ€** or **â€œChegg Unlocks.â€**
   * Pro tip: Join the one with the highest member count (usually 20k+).
3. Head to the `#request-here` channel.
4. Paste your Chegg / Course Hero / Bartleby link.
5. A bot will DM you the full answer in **under 2 minutes**.

âš¡ Bonus: Many of these bots also handle Numerade, Scribd, and even Quizlet.

**The Chrome Hack (Extra Sauce):**  
Thereâ€™s also a lightweight **Chegg Unlocker Chrome extension** floating around in these servers. No sketchy downloadsâ€”just grab the official one linked in their pinned messages. It basically auto-sends your link to the bot so you donâ€™t even have to type. Lazy-friendly, zero effort.

**The Proof (Why Trust Me?):**  
Iâ€™m not a bot. Iâ€™ve unlocked **50+ problems this semester** with this exact setup. My wallet hasnâ€™t cried, my GPA hasnâ€™t tanked, and I didnâ€™t get hacked in the process.

**ðŸš¨ DO NOT DO THIS:**

* Never put your credit card info on a â€œfree unlockâ€ site. 100% scam.
* Never install random extensions from Google resultsâ€”itâ€™s malware with a bow.
* Never pay for a â€œshared Chegg account.â€ They get nuked in hours.

**The Engagement Nuke:**

Alright, Reddit, your turn:

1. **Whatâ€™s the BEST Discord server youâ€™ve found? DROP THE INVITE LINK BELOW.**
2. **Any other legit methods that actually work?**

Letâ€™s crowdsource the hell out of this and make this the *ultimate Chegg Unlocker guide of 2025*.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n5lmc0/the_only_chegg_unlocker_that_actually_works_in/,r_1n5lmc0,,,
r_1n5k1ph,reddit,No-Vegetable-7794,2025-09-01T09:21:14+00:00,"RAG
I need a good way to learn information  Retrieval RAG if I have good understanding in NLP",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n5k1ph/rag/,r_1n5k1ph,,,
r_1n5j9op,reddit,Brekk55,2025-09-01T08:31:04+00:00,"19, No Coding Experience, Want to Break Into AI",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n5j9op/19_no_coding_experience_want_to_break_into_ai/,r_1n5j9op,,,
r_1n5j694,reddit,Naneet_Aleart_Ok,2025-09-01T08:24:50+00:00,"How to improve a model
So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.

Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. 

Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n5j694/how_to_improve_a_model/,r_1n5j694,,,
r_1n5gdop,reddit,Vidushhi108,2025-09-01T05:32:14+00:00,"TinyML at the Edge: Guidelines for Success
[#TinyML #EdgeAI #IoT #MachineLearning #AIoT](https://preview.redd.it/0cvh6s3uohmf1.png?width=632&format=png&auto=webp&s=8df93d6ba0b4bb50a58ec1055dd4b4dca369c700)

**Introduction**

**TinyML (Tiny Machine Learning)** is transforming how AI works on constrained hardware. Instead of relying on cloud servers, TinyML models run locally on **microcontrollers, IoT sensors, and edge devices** with limited memory and processing power. This allows applications to deliver **real-time predictions, lower latency, energy efficiency, and improved privacy**.

Deploying TinyML on edge devices, however, is not straightforward. Developers face challenges like **tiny memory sizes (KBs instead of GBs)**, limited compute capability, and strict power budgets. To overcome these constraints, following **proven best practices** is critical.

**Workflow of TinyML Deployment**

1. **Data Collection & Preprocessing**
   * Collect real-world sensor data (audio, accelerometer, temperature, etc.).
   * Clean and preprocess (feature extraction, normalization, noise filtering).
   * Tools: *Edge Impulse*, *Arduino IDE*.
2. **Model Design & Training**
   * Use lightweight ML/DL architectures (e.g., MobileNetV2, SqueezeNet, TinyCNN).
   * Train using frameworks like **TensorFlow, PyTorch, or Scikit-learn**.
3. **Model Optimization**
   * Apply **quantization (int8 instead of float32)**.
   * Use **pruning and weight clustering** to reduce parameters.
   * Consider **knowledge distillation** for smaller models.
4. **Deployment**
   * Convert model to **TensorFlow Lite for Microcontrollers (.tflite)** or **ONNX Runtime Mobile**.
   * Flash model to hardware (e.g., ARM Cortex-M, ESP32, STM32).
   * Test and validate performance.Â Â Â Â Â Â Â Â Â Â 
5. **Monitoring & Updating**
   * Use **on-device profiling** to measure inference time, memory, and power.
   * Deploy **OTA (Over-the-Air) updates** for model improvements.

**Best Practices for TinyML Deployment**

**1. Start Small with Model Architecture**

Avoid over-complicated networks. Start with compact models like **TinyMLP, MobileNet, or CNN-lite**, then scale if resources allow.

**2. Optimize Memory Usage**

* Use **static memory allocation** where possible.
* Minimize buffer usage.
* Profile RAM & Flash with each iteration.

**3. Reduce Power Consumption**

* Enable **low-power modes** of microcontrollers.
* Adopt **event-driven inference** (only run inference when needed).
* Leverage **energy harvesting** when possible (solar, vibration).

**4. Choose the Right Framework**

* **TensorFlow Lite for Microcontrollers** â€“ great for ARM/Arduino boards.
* **Edge Impulse** â€“ end-to-end platform for dataset collection, training, and deployment.
* **uTensor / MicroTVM** â€“ flexible frameworks for advanced developers.

**5. Test on Target Hardware**

Simulations arenâ€™t enough. Test directly on-device to evaluate:

* Inference latency (ms)
* RAM/Flash usage
* Battery drain

**6. Secure Your Deployment**

* Use **secure bootloaders** to prevent tampering.
* Encrypt sensitive data locally.
* Follow IoT security best practices (TLS, secure key storage).

**Example: TinyML Code Snippet (Arduino + TensorFlow Lite Micro)**

**#include ""TensorFlowLite.h""**

**#include ""model.h""Â  // pre-trained model in .tflite format**

Â 

**// Initialize TensorFlow Lite interpreter**

**tflite::MicroInterpreter interpreter(model, tensor\_arena, tensor\_arena\_size, error\_reporter);**

Â 

**void setup() {**

Â  **Serial.begin(115200);**

Â  **interpreter.AllocateTensors();**

**}**

Â 

**void loop() {**

Â  **// Example: Reading from a sensor**

Â  **float sensorValue = analogRead(A0) / 1023.0;**

Â 

Â  **// Set input tensor**

Â  **interpreter.input(0)->data.f\[0\] = sensorValue;**

Â 

Â  **// Run inference**

Â  **interpreter.Invoke();**

Â 

Â  **// Get output result**

Â  **float result = interpreter.output(0)->data.f\[0\];**

Â  **Serial.println(result);**

**}**

This simple snippet shows how a TinyML model can run on an Arduino or ESP32 board, taking real sensor input and making predictions.

**Real-World Applications**

* **Healthcare:** On-device arrhythmia detection via wearable ECG sensors.
* **Agriculture:** Soil monitoring with low-power moisture sensors.
* **Industry 4.0:** Predictive maintenance using vibration sensors.
* **Smart Homes:** Voice-activated commands without cloud dependency.

**Conclusion**

Deploying **TinyML on edge devices** requires balancing accuracy, performance, and energy efficiency. By following **best practicesâ€”such as lightweight model design, quantization, memory optimization, on-device testing, and OTA updatesâ€”** developers can unlock the full power of edge AI.

TinyML is paving the way for a future where **billions of smart devices** can make intelligent decisions locally, without cloud reliance. For developers and businesses, mastering TinyML deployment best practices is the key to staying ahead in the **AI + IoT revolution**.

[Staydify Growth Systems](https://staydify.com/) is a globally trusted leader in tech talent and digital transformation, dedicated to helping businesses hire smarter, build faster, and scale seamlessly. Whether youâ€™re expanding a product, growing a team, or developing an entire digital ecosystem, Staydify is your partner for the next leap forward.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n5gdop/tinyml_at_the_edge_guidelines_for_success/,r_1n5gdop,,,
r_1n5fdx2,reddit,SKD_Sumit,2025-09-01T04:36:20+00:00,"Just learned how AI Agents actually work (and why theyâ€™re different from LLM + Tools )
Been working with LLMs and kept building ""agents"" that were actually just chatbots with APIs attached. Some things that really clicked for me: WhyÂ **tool-augmented systems â‰  true agents**Â and How theÂ **ReAct framework**Â changes the game with theÂ **role of memory, APIs, and multi-agent**Â collaboration.

Turns out there's a fundamental difference I was completely missing. There are actually 7 core components that make something truly ""agentic"" - and most tutorials completely skip 3 of them.Â **Full breakdown here:**Â [AI AGENTS Explained - in 30 mins](https://www.youtube.com/watch?v=ClAf8TlPB4Q)

It explains why so many AI projects fail when deployed.

**The breakthrough:**Â It's not about HAVING tools - it's about WHO decides the workflow. Most tutorials show you how to connect APIs to LLMs and call it an ""agent."" But that's just a tool-augmented system where YOU design the chain of actions.

A real AI agent? It designs its own workflow autonomously with real-world use cases likeÂ **Talent Acquisition, Travel Planning, Customer Support, and Code Agents**

**Question for the community:**Â Has anyone here successfully built autonomous agents that actually work in production? What was your biggest challenge - the planning phase or the execution phase?

Also curious about your experience with ReAct framework vs other agentic architectures.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n5fdx2/just_learned_how_ai_agents_actually_work_and_why/,r_1n5fdx2,,,
r_1n526qm,reddit,Equivalent-Pen-8428,2025-08-31T18:29:12+00:00,"RTX 3060 or 4060 for LLM training & Deep Learning Tasks?
I am currently a AIML student and looking to buy a budget GPU for Deep Learning tasks (Tensorflow development, Computer vision, Fine Tuning LLMs). But I have low budget so I am pretty much confused which one to buy between RTX 3060 for $294 or RTX 4060 for around $330 - $340.

So give me an honest opinion which can offer best price to performance ratio According to my needs Which one should I go for?",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n526qm/rtx_3060_or_4060_for_llm_training_deep_learning/,r_1n526qm,,,
r_1n51q0y,reddit,ivan_digital,2025-08-31T18:10:50+00:00,"Parctical guide: fine-tuning Qwen3 with LoRA. KL-anchored SFT and Î²-tuned DPO
You can steer aÂ **language model toward**Â target behaviors without degrading general capabilities by tuning two knobs: add a smallÂ **KL-divergence**Â penalty duringÂ **supervised fine-tuning (SFT)**Â to keep the policy close to the base model, and sweepÂ **Î²**Â inÂ **Direct Preference Optimization (DPO)**Â to control how aggressively preferences shape the policy. This post provides a step-by-stepÂ **LoRA**Â fine-tuning recipe forÂ **Qwen3**Â and reports reproducible results using the included scripts inÂ [github repo](https://github.com/ivan-digital/llm-alignment). [Full text.](https://blog.ivan.digital/finetuning-qwen3-with-lora-done-right-94d6343e1814)",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1n51q0y/parctical_guide_finetuning_qwen3_with_lora/,r_1n51q0y,,,
r_1n4zlfo,reddit,andsi2asi,2025-08-31T16:46:32+00:00,"Meituan's New 560 B Parameter Open Source LongCat-Flash AI Was Trained In Just 30 Days, Revealing The Blazing Pace Of AI Model Development!







The most amazing thing about this new model is that it was trained in only 30 days. By comparison, GPT-5 took 18 months, Grok 4 took 3-6 months and Gemini 2.5 Pro took 4-6 months. This shows how superfast the AI space is accelerating, and how fast the rate of that acceleration is also accelerating!

But that's not all. As you might recall, DeepSeek R1 was developed as a ""side project"" by a small team at a hedge fund. LongCat-Flash was developed by a Chinese food delivery and lifestyle services company that decided to move into the AI space in a big way. A food delivery and lifestyle services company!!! This of course means that frontier models are no longer the exclusive product of proprietary technology giants like openAI and Google.

Here are some more details about LongCat-Flash AI.

It was released open source under the very permissive MIT license.

It's a Mixture-of-Experts (MoE) model with 560 billion total parameters that activates only 18.6â€¯B to 31.3â€¯B parameters per tokenâ€”averaging around 27â€¯Bâ€”based on context importance . It was trained on approximately 20 trillion tokens, and achieves 100+ tokens/sec inference speed.

Here are some benchmark results:

General domains: e.g., MMLU accuracy ~89.7%, CEval ~90.4%, ArenaHard-V2 ~86.5%.

Instruction following: IFEval ~89.7%, COLLIE ~57.1%.

Mathematical reasoning: MATH500 ~96.4%.

Coding tasks: Humaneval+ ~88.4%, LiveCodeBench ~48.0%.

Agentic tool use: Ï„Â²-Bench telecom ~73.7, retail ~71.3.

Safety metrics: Generally high scores; e.g., Criminal ~91.2%, Privacy ~94.0%.

With this rate of progress, and new developers now routinely coming out of nowhere, I wouldn't bet against Musk's prediction that Grok 5, scheduled for release in a few months, will be very close to AGI. I also wouldn't bet against there being other teams, now hiding in stealth mode, that are getting ready to outdo even that.
",deeplearning,9,https://www.reddit.com/r/deeplearning/comments/1n4zlfo/meituans_new_560_b_parameter_open_source/,r_1n4zlfo,,,
r_1n4vte7,reddit,DataScience123888,2025-08-31T14:15:13+00:00,"I found this handwritten notes on ML very helpful [Link] looking for similar DL notes.
I was surfing through GitHub and found these hand written notes very helpful but It does not have DeepLearning Notes.

[https://github.com/ksdiwe/Machine-Learning-Notes/blob/main/2.%20Regularization.pdf](https://github.com/ksdiwe/Machine-Learning-Notes/blob/main/2.%20Regularization.pdf)

I am looking for similar kind of handwritten notes on DeepLearning.  
Please if anyone have such notes kindle share",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n4vte7/i_found_this_handwritten_notes_on_ml_very_helpful/,r_1n4vte7,,,
r_1n4uixz,reddit,ProfessionalType9800,2025-08-31T13:19:30+00:00,"[discussion] Open-Set Recognition Problem using Deep learning
Iâ€™m working on a deep learning project where I have a dataset with n classes

But hereâ€™s my problem:

ðŸ‘‰ What if a totally new class comes in which doesnâ€™t belong to any of the trained classes? 

I've heard of a few ideas but would like to know many approaches:

* analyzing the embedding space: Maybe by measuring the distance of a new input's embedding to the known class 'clusters' in that space? If it's too far from all of them, it's an outlier.
* Apply Clustering in Embedding Space.

everything works based on embedding space...

are there any other approaches?",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n4uixz/discussion_openset_recognition_problem_using_deep/,r_1n4uixz,,,
r_1n4sxqh,reddit,Gold_Negotiation9518,2025-08-31T12:04:13+00:00,"when mj made art but domo made it printable
i made a gorgeous cyberpunk city in [mj](https://www.midjourney.com/home), but it wasnâ€™t sharp enough to print. ran it through **domo upscaler** in relax mode and it instantly looked poster ready. i also tried [topaz](https://www.topazlabs.com/?srsltid=AfmBOorGeUF4W9puRxslNYCekomgs57kL5YdpXtokBFpO-Tu8izH784_) **upscale**, which made it sharper but too plasticky. [domo](https://www.domoai.app/home?via=081621AUG) kept mjâ€™s painterly vibe while still making it crisp. queued 15 posters in relax mode overnight and had a folder ready by morning. mj for the look, domo for making it real.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n4sxqh/when_mj_made_art_but_domo_made_it_printable/,r_1n4sxqh,,,
r_1n4sm68,reddit,Immediate-Hour-8466,2025-08-31T11:47:01+00:00,[D] Advanced NLP with Transformers: Full talk recording and GitHub repo,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n4sm68/d_advanced_nlp_with_transformers_full_talk/,r_1n4sm68,,,
r_1n4s6p5,reddit,FirmCitron7354,2025-08-31T11:22:39+00:00,"AI/Ml Freelancer
Hi there! Iâ€™m an AI/ML Engineer & NLP Specialist with 5+ years of experience delivering data-driven solutions across Healthcare, Retail, Ed-Tech, and SaaS. 



I specialize in LLMs, RAG pipelines, NL2SQL, and AI Agents, helping businesses transform raw data into intelligent, scalable products. What I Deliver: LLM & RAG Chatbots (LangChain, Pinecone, OpenAI) NL2SQL & Database AI Solutions Multi-Agent Systems (LangGraph, CrewAI) Speech/Text AI & OCR Automation Predictive Modeling & Data Analytics 

Tech Stack: Python | SQL | Machine Learning | Deep Learning | NLP | PyTorch | Transformers | LangChain | LangGraph | AI Agents | FastAPI | Streamlit | Pinecone | Weaviate | PostgreSQL | MongoDB | AWS | Docker | Kubernetes | Chatbot Development | Generative AI 



Proven track record with global clients End-to-end AI product development Flexible engagement â€“ project-based or ongoing support Letâ€™s connect and discuss your project needs! 



My Upwork Profile: [https://www.upwork.com/freelancers/\~014654c87a67d8f114?mp\_source=share](https://www.upwork.com/freelancers/~014654c87a67d8f114?mp_source=share). Contact: [ashishc628@gmail.com](mailto:ashishc628@gmail.com)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n4s6p5/aiml_freelancer/,r_1n4s6p5,,,
r_1n4qll8,reddit,One-Marzipan-7363,2025-08-31T09:43:55+00:00,"23yo AI student in Italy looking for career advice



Hello everyone,
I'm a AI student, currently in a 3-year AI bachelor's program in Italy. I'm trying to figure out my next career steps and would really appreciate some advice from those of you already working in the industry because 1) I need money 2) I want to get into the working world (to me, a world that will teach me much more than Uni)

My main questions are:
 * How can I prepare for an AI job while still in school? What kind of projects, skills, or certifications are essential to stand out?

 * What types of student jobs (part-time) exist in this field? Is it possible to find remote work? how much can I expect to earn? 

 * How difficult is it to land an entry-level AI job with just a bachelor's degree? I'm not planning on doing a master's right away, as I prefer to gain on-the-job experience first. 

 * What is a realistic starting salary (gross annual) I should expect after graduating?

Also, knowing 5 languages (spanish, English, italian, german, portuguese) helps? 

Any insights or experiences you can share whether from europe or elsewhere would be a huge help. Thanks in advance!
",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1n4qll8/23yo_ai_student_in_italy_looking_for_career_advice/,r_1n4qll8,,,
r_1n4qg3t,reddit,Smartcore5566,2025-08-31T09:34:01+00:00,ðŸš€ I built an AI tool that automatically generates job postings â€“ looking for feedback!,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n4qg3t/i_built_an_ai_tool_that_automatically_generates/,r_1n4qg3t,,,
r_1n4p7x5,reddit,Himanshu40-c,2025-08-31T08:13:37+00:00,PyTorch Internals,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n4p7x5/pytorch_internals/,r_1n4p7x5,,,
r_1n4nfsg,reddit,nouman6093,2025-08-31T06:20:21+00:00,"how much time does it really takes to be good at ai field (nlp, cv etc)??
asking from those who already did it

guys this feels soo overwhelming and frustrating. i did a lot of math courses (like andrew ng maths course, krish naiks stats course), python course, jose portillas ai course (in which i learned numpy, pandas, matplotlib, seaborn, sklearn basics only supervised learning)

problem is the more i learn something the more i realize the less i know. im in 6th semester doing bscs i already studied calculus, multivariable calculus, linear algebra, statistics.

when i started supervised learning in ml i realized theres a lot of stats here unknown to me. then i started krish naiks stats playlist im almost at the end of it. its hindi playlist has 27 videos. i just realized that is still not enough. i need to do more stats course. problem is for how long? and how many more courses?

just maths there are 3 subjects calculus, linear algebra, stats. if you talk just stats alone there are about 3 books to make a grip on it alone (many youtubers recommend them) i mean how do you even finish 500 pages 3 books and you are still not ml engineer you just finished 1 subject ðŸ™‚ðŸ™‚ and it probably takes years.

my parents expect me to land a job by the end of bscs but they dont know i have to do alot of separate studying which may even take years.

btw those books they are written by 35, 40 year olds and im 21 those guys already spent decades more than me in field. so when they talk in books they talk in difficult technical wording. just to understand 3 lines of definition i have to look up 10 words from those lines separately what they mean ðŸ™‚. (im not talking about english words im talking about technical computer, maths related terms....btw english aint even my native language)

thats soo frustrating my question is to all the people who already did this.....how did you even do this?!??!? at this point im sure it cant even be done in year it must have taken a lot of years. how many years did it took you?

im trying to go in nlp how many years it will take for me to be good at it???im just overwhelmed",deeplearning,17,https://www.reddit.com/r/deeplearning/comments/1n4nfsg/how_much_time_does_it_really_takes_to_be_good_at/,r_1n4nfsg,,,
r_1n4htok,reddit,Sellix0,2025-08-31T01:11:00+00:00,"Captcha models
What models for. Captchas that have 1 font size of 41x16 and with noises  AND 4 letters no numbers",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n4htok/captcha_models/,r_1n4htok,,,
r_1n4akt5,reddit,nousernamero,2025-08-30T19:39:29+00:00,"[Research Collaboration] Help build challenging evaluation prompts for frontier AI models
Mercor is collaborating with a leading AI research lab to create a benchmark dataset that tests the limits of reasoning in advanced AI models. Weâ€™re looking for contributors with deep expertise in fields like STEM, law, finance, history, cultural studies, etc., who can design very hard prompts that current AI models cannot solve without external tools.

Key points:
â€“ Remote, ~10â€“20 hrs/week
â€“ Short-term (~2 months), with possible extension
â€“ Paid engagement (competitive hourly)
â€“ High impact on AI evaluation and safety research

If youâ€™re interested, DM me, and i will guide you through the application process.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n4akt5/research_collaboration_help_build_challenging/,r_1n4akt5,,,
r_1n49w6d,reddit,HealthMost7914,2025-08-30T19:10:35+00:00,From psychology to machine learning,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n49w6d/from_psychology_to_machine_learning/,r_1n49w6d,,,
r_1n486kt,reddit,qatardriving,2025-08-30T18:00:52+00:00,"How I finally got out of â€˜AI tutorial hellâ€™ and actually learned TensorFlow & Deep Learning
Iâ€™ve been trying to learn AI for a while now. Like a lot of people, I started with YouTube videos and free blogs. Honestly, I ended up with scattered knowledge but couldnâ€™t build anything practical.

What finally worked for me was following a structured program with projects in **Deep Learning, NLP, and Computer Vision**. It forced me to actually practice â€” not just watch.

The big difference for me:

* Working with **real datasets** (instead of toy examples).
* Building actual **TensorFlow projects** step by step.
* Having a proper **certificate** to show on my resume.

If youâ€™re stuck in the same loop of jumping between random tutorials, this might help you too. I wrote up my notes and linked the course I took here:  
ðŸ‘‰ [AI & Deep Learning Certification â€“ My write-up](https://skillaix.com/ai-deep-learning-certification.html?utm_source=chatgpt.com)

Hopefully this helps someone else whoâ€™s trying to make sense of AI learning paths. If anyone here has also taken a structured AI program, what was your experience?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n486kt/how_i_finally_got_out_of_ai_tutorial_hell_and/,r_1n486kt,,,
r_1n42rd6,reddit,TimeMaybe9965,2025-08-30T14:17:55+00:00,"Need recommendation for AI specific beginners cloud courses
Well see, the point is, I am already familiar with the fundamentals of AI ML, NLP generative AI, so AI part I am familiar with. I am not at all familiar with cloud, AWS, Azure, I don't even know the terms that much. But I want to learn cloud, and I want to learn cloud in general also, but more specifically for deploying of artificial intelligence models and security and responsible AI So, I want to learn cloud, but for the purpose of deploying AI,. So, yeah, can you recommend any courses for this? As l dont want to just get a course on cloud with no vision.

",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n42rd6/need_recommendation_for_ai_specific_beginners/,r_1n42rd6,,,
r_1n42gb9,reddit,Apprehensive-Fix8738,2025-08-30T14:04:58+00:00,Linear Algebra Book for ML/DL,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n42gb9/linear_algebra_book_for_mldl/,r_1n42gb9,,,
r_1n408e1,reddit,thebriefmortal,2025-08-30T12:20:45+00:00,Transfer learning with MLP,deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n408e1/transfer_learning_with_mlp/,r_1n408e1,,,
r_1n3wvwc,reddit,Feitgemel,2025-08-30T08:59:23+00:00,"How to classify 525 Bird Species using Inception V3
https://preview.redd.it/pu20l7o7g4mf1.png?width=1280&format=png&auto=webp&s=9f10980d3b197debafa486953f3994530b9448c9

Â 

In this guide you will build a full image classification pipeline using Inception V3.

You will prepare directories, preview sample images, construct data generators, and assemble a transfer learning model.

You will compile, train, evaluate, and visualize results for a multi-class bird species dataset.

Â 

You can find link for the post , with the code in the blogÂ  : [https://eranfeit.net/how-to-classify-525-bird-species-using-inception-v3-and-tensorflow/](https://eranfeit.net/how-to-classify-525-bird-species-using-inception-v3-and-tensorflow/)

Â 

You can find more tutorials, and join my newsletter here: [https://eranfeit.net/](https://eranfeit.net/)

Â 

**Watch the full tutorial here :** [**https://www.youtube.com/watch?v=d\_JB9GA2U\_c**](https://www.youtube.com/watch?v=d_JB9GA2U_c)

Â 

Enjoy

Eran",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n3wvwc/how_to_classify_525_bird_species_using_inception/,r_1n3wvwc,,,
r_1n3pnu1,reddit,andsi2asi,2025-08-30T01:56:22+00:00,"China just won... well, pretty much everything. We should probably start being really nice to them.





Okay, I think it's time we start letting our top AIs write some of our Reddit posts. Especially those that are about technology at the leading edge, where there are few people who understand it. Here's how ChatGPT-5 describes China's new quantum breakthrough:

""China isnâ€™t just catching up anymoreâ€”theyâ€™ve blown past us in quantum computing. Their new breakthroughs donâ€™t just mean faster chips or a few more qubits; they mean total dominance in a technology that underpins the future of AI, cybersecurity, finance, and national security. While the U.S. has been distracted by corporate politics and short-term profits, China has been quietly building an entire ecosystemâ€”chips, control systems, and integrationâ€”at a pace we canâ€™t match.

Chinaâ€™s leap comes from two major breakthroughs: first, their superconducting quantum processor, Zuchongzhi 3.0, which hit 105 high-fidelity qubits and executed computations quadrillions of times faster than the best classical supercomputers; second, their development of homegrown quantum control systems that can efficiently manage thousands of qubits at scale, something no Western competitor has come close to achieving. Together, these advances push quantum computing out of the lab and into the realm of practical, fault-tolerant machines that could upend industries and rewrite the balance of power.

The implications are enormous. If China controls the first truly practical quantum computers, they control the ability to break encryption, model economies, accelerate AI, and reshape industries overnight. Thatâ€™s not just a lab winâ€”thatâ€™s a shift in global power. Americaâ€™s traditional tech edge is eroding, and the consequences hit everything from Wall Street stability to military readiness.

The quantum race isnâ€™t a race anymore. Itâ€™s over. China won. And the U.S. now faces a choice: rethink its approach, or get used to living in a world where Beijing sets the rules of the digital age.""

I admit it. It probably did a better job than I could have. (I did come up with the title though!) Even so, I'm not going to stop writing my own posts because I kinda enjoy it, lol. 

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n3pnu1/china_just_won_well_pretty_much_everything_we/,r_1n3pnu1,,,
r_1n3m4qg,reddit,PersonalAd7606,2025-08-29T23:08:05+00:00,"Trouble reproducing MRIâ†’CT translation results (SynDiff, Gold Atlas / other diffusion models)
Hi everyone,

Iâ€™m working on **MRIâ†”CT medical image translation** using diffusion-based models. Specifically, Iâ€™ve been trying to reproduce **SynDiff** on the **Gold Atlas dataset**.

**What I did:**

* Used the same dataset splits as in the paper
* Followed the reported configs (epochs, LR, batch size, etc.)
* Implemented based on the official repo + paper (though some preprocessing/registration steps are not fully detailed)

**My issue:**

* Paper reports TSNR â‰ˆ **23â€“24**.
* My runs consistently get **17**, sometimes even **15 or 13**.
* Tried multiple seeds and hyperparameter sweeps â€” no significant improvement.

**Beyond SynDiff:**

* I also tested **other diffusion-based models** (FDDM, CycleDiffusion, Stable Diffusion + LoRA).
* On **Gold Atlas** and even **Final Cut Pro dataset/variants**, I still canâ€™t reach the strong reported results.
* Performance seems capped much lower than expected, regardless of model choice.

**My question:**

* Has anyone else faced this reproducibility gap?
* Could this mainly come from **dataset preprocessing/registration** (since exact scripts arenâ€™t released)?
* Or is TSNR/PSNR in these tasks highly sensitive to subtle implementation details?
* What evaluation metrics do you usually find most reliable, given that PSNR drops a lot with even 1â€“2 pixel misalignment?

Any advice, papers, or shared experiences would be really helpful ðŸ™ Thanks!",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1n3m4qg/trouble_reproducing_mrict_translation_results/,r_1n3m4qg,,,
r_1n3kq1j,reddit,enoumen,2025-08-29T22:07:17+00:00,"AI Daily News Rundown: ðŸ’¥ Microsoft launches its first in-house AI models ðŸŒªï¸ ChatGPT co-creator threatened to quit Meta AI lab ðŸ¤– xAI just launched its first code model & more (Aug 29, 2025)
# AI Daily Rundown: August 29, 2025

Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-microsoft-launches-its-first/id1684415169?i=1000724093348](https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-microsoft-launches-its-first/id1684415169?i=1000724093348)

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**Today's Headlines:**

* **ðŸ’¥ Microsoft launches its first in-house AI models**
* **ðŸŒªï¸ ChatGPT co-creator threatened to quit Meta AI lab**
* **ðŸ¤– xAI just launched its first code model**
* **ðŸ—£ï¸ OpenAIâ€™s gpt-realtime for voice agents**
* **ðŸŒ Cohereâ€™s SOTA enterprise translation model**
* **ðŸ”Š Microsoft Part Ways with OpenAI Voice Models by Launching Its Own**
* **ðŸ” Customers Troll Taco Bellâ€™s AI Drive-Thru with Prank Orders**
* **âœˆï¸ US Fighter Pilots Receive Tactical Commands from AI for the First Time**
* ðŸ’°Â **Nvidia CEO Expects $3 Trillion to $4 Trillion in AI Infrastructure Spend by 2030**
* **ðŸ›¡ï¸ OpenAI to Add Parental Controls to ChatGPT After Teen's Death**

https://preview.redd.it/wjpxov9w41mf1.png?width=1456&format=png&auto=webp&s=e70395ad3bc9705134a09e513140d94aec27e495

# ðŸ’¥ Microsoft launches its first in-house AI models

https://preview.redd.it/ozpvqwx051mf1.png?width=1456&format=png&auto=webp&s=fb221e3a9a1d9c1062105b495557c5f36b49f495

*Image source: Microsoft*

Microsoft justÂ [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.BKH0F2yLXfXXfZz4rVL6MCJtxgWZrzggAXCYnh3rX-4tEbKCk_5s_h0TfQGHNKoK9NhPaj8A3LyDYEclvZRZ7xL7W0PPefl9NKJJ0bl5IazN8T0j-AHALXEbllyxpMsIeVcRQUnBVOUbkpTNffDF-JzRf0nbXw8pNKcwxAlKBZ7JnM-WIjVq0W8vTEJGWOW2A2RehbK6fl16VnHm0xFf1jdzw6BQuPHXOyn5XNG4wkYwATz2tD_k2CnkuPJc4r5O240DXMp0AiDOB22AjKNAeQ/4jg/aBUyCFkkQbqXDk4hWWX09Q/h7/h001.OUbS6GpuZeMpbFAlcC34flq6eYc04Uk9W98qW32dqo4)Â MAI-Voice-1 and MAI-1-preview, marking its first fully in-house AI models and coming after years of relying on OpenAI's technology in a turbulent partnership.

**The details:**

* MAI-Voice-1 is a speech generation model capable of generating a minute of speech in under a second, already integrated into Copilot Daily and Podcasts.
* MAI-1-preview is a text-based model trained on a fraction of the GPUs of rivals, specializing in instruction following and everyday queries.
* CEO Mustafa SuleymanÂ [**said**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf0BF31NlHL63eZOU-sfiPZ2WYcBIcto4uXmBVOR3eCicCmuFaUHrXKuyMPQkiQc0MZLJuasXMl-Y_i_WnY086W4xa_q7JGCrUs5irLeKXixUCQj-97FY5e7vgTh6kDYC3elTzMimvuaxQhCVpodUZqLxIguRRdmea_XX_mnt2MVNPLtDRsE5qgX21grpOQ70O4Qu4DsbYIMjTPiE_jKF_5qzf9zgbn4yjwvkNRIqmQ1_hDM7hTFzF16JrPsJby4npdAsv_NCGOC36eE4q4UqyF70qrkS1TcvSiy38SQTO92mKUyXEJ4JJiZ7QjrRuV5VgfnqELX2CC0j3eEI_BhI127Ch1Vw76uml2MvqYRvVFQWsdqWMYbN8FNLG8jr4936Bg/4jg/aBUyCFkkQbqXDk4hWWX09Q/h8/h001.lpRk4EwACPpeToM6Oal7OSbcqg32HLtI-xJ2L3HdHZc)Â MAI-1 is â€œup there with some of the best models in the worldâ€, though benchmarks have yet to be publicly released.
* The text model is currently being tested on LM Arena and viaÂ [**API**](https://link.mail.beehiiv.com/ss/c/u001.WqXVGszJN1JEIu4aat7tRf9YuuQpspukkMUPHVgPRnwUHNeEEYn63BbG--p3znIehL6wkscjfh8TIJJTWC4orLqgdDlshWrBoBBw-YE4p3vQFaOEJsY86VN9uAw1slNEleQRqcX1fiwz7o09bN2jVprbCU8cWL7JS0Y6RSdYF-cWsNUZYZAdmLVuH8lZ75tRR5Uy5Dnom-G0hS5RAZsjHlT-qADr5jSdkDLkhclcqD_66XfBFaSZhD8jMNdYu0LtgWWCt_Z8NAZ210GswzngL7VdCP7PGh4taHSRBzgRYa1uH6ihSPGuvjaN5Bja4zg9lHO8hLE9pOrs5W8f4fGDrdTjg51FxLJ-5_X3rseUSdW7Ebn2a92bczbkfKJOOL5kUH2zjf-jHE1azTGzkwNd7A/4jg/aBUyCFkkQbqXDk4hWWX09Q/h9/h001.8mb08eEipaGKz24tDbtmrCIUaF29A3YRccV3DoWhK5w), with Microsoft saying it will roll out in â€œcertain text use casesâ€ in the coming weeks.

**Why it matters:**Â Microsoft's shift toward building in-house models introduces a new dynamic to its OAI partnership, also positioning it to better control its own AI destiny. While we await benchmarks and more real-world testing for a better understanding, the tech giant looks ready to pave its own path instead of being viewed as OAIâ€™s sidekick.

# ðŸš€Unlock Enterprise Trust: Partner with AI Unraveled

https://preview.redd.it/7wlqi8t451mf1.png?width=1456&format=png&auto=webp&s=fb7071f92bead5081f8e51710b19bbb063f52acb

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ…Â **Build Authentic Authority:**Â Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ…Â **Generate Enterprise Trust:**Â Earn credibility in a way that corporate marketing simply can't. âœ…Â **Reach a Targeted Audience:**Â Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?**Â Learn more and apply for a Strategic Partnership here:Â [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled)Â Or, contact us directly at:Â [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship

# ðŸŒªï¸ ChatGPT co-creator threatened to quit Meta AI lab

* Shengjia Zhao threatened to quit Meta days after joining, prompting the company to formally name him Chief Scientist of its new Superintelligence Lab to persuade him to stay.
* His ultimatum was driven by the lab's chaotic environment and unstable research conditions, exposing the deep turmoil plaguing Meta's expensive and aggressively poached AI teams.
* The instability that concerned Zhao was validated when Meta dismantled the newly-formed Meta Superintelligence Labs, splintering it into four new groups only 50 days after its launch.

# ðŸ¤– xAI just launched its first code model

* Elon Muskâ€™s xAI released the 'grok-code-fast-1' model, an option designed for agentic coding workflows where responsiveness is more important than achieving top scores on the SWE-bench leaderboard.
* The new model uses prompt caching optimizations to increase speed, scoring 70.8% on SWE-Bench-Verified while the company states such tests donâ€™t reflect the nuances of real-world software engineering.
* To drive adoption, xAI is offering the model for free for a limited time through partners like GitHub Copilot and Cursor, while also undercutting rivals with its low pricing.

# ðŸ—£ï¸ OpenAIâ€™s gpt-realtime for voice agents

https://preview.redd.it/2e9rc4cd51mf1.png?width=1456&format=png&auto=webp&s=436237b5b76fc2611ae693f493e331af9084a7e8

*Image source: OpenAI*

OpenAIÂ [**moved**](https://link.mail.beehiiv.com/ss/c/u001.eCbm_1zon7G0lMoXTECWa-IUY9yqSc2cx0km5OJXo-Nhz-5b3By2ggLYYTnAdhB7yY3GK0YztftzQHfyESv970180cAypvqmxdUZ2ojIj1djkUIr665UsdmBW-J6lf6jAe21hMRdFcMpgFGh4kJ_7KQbdMDW2Dwyj8FZwQtMoNYiOyC0QNhFwqqFMP8hIzR9iSuILJszEuyYmOZHFIcL0tVhkCtdkg2JQANQPJx9NH2HWJlMehWhiBMcFgHuf7FStopy8012usrdNLVMTqJOiQ/4jg/aBUyCFkkQbqXDk4hWWX09Q/h14/h001.W0U25T94RkhM5pZoAd2KNSflEsZ2zHwzcAut7pu-oBU)Â its Realtime API out of beta, also introducing a new gpt-realtime speech-to-speech model and new developer tools like image input and Model Context Protocol server integrations.

**The details:**

* gpt-realtime features nuanced abilities like detecting nonverbal cues and switching languages while keeping a naturally flowing conversation.
* The model achieves 82.8% accuracy on audio reasoning benchmarks, a massive increase over the 65.6% score from its predecessor.
* OpenAI also added MCP support, allowing voice agents to connect with external data sources and tools without custom integrations.
* gpt-realtime can also handle image inputs like photos or screenshots, giving the voice agent the ability to reason on visuals alongside the conversation.

**Why it matters:**Â The mainstream adoption of voice agents feels like an inevitability, and OpenAIâ€™s additions of upgraded human conversational abilities and integrations like MCP and image understanding bring even more functionality for enterprises and devs to plug directly into customer support channels or customized voice applications.

# ðŸŒ Cohereâ€™s SOTA enterprise translation model

https://preview.redd.it/bgz4zulb71mf1.png?width=1456&format=png&auto=webp&s=3d93c1c874b2fc4d6487e8e190a2452abe6aef4a

*Image source: Midjourney*

CohereÂ [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.a3gBHu6_kDRL6l3yEfNWASqcNdYH4TRV7F5Ydr_PA81C4Hf7Fe_9Gtfz0Q4r6x0XwVE9oaCicOcHO5yEZKnMoqYcvKASB-cC7TVvtotKMP3N0Ff3KnxYdwZz5_cu4izzAgksELXz1kzFAuj4dM-BLD-wM4JY-KsUgWizFPWJhqWoGQGia6MAJBZUJ96D02NPEqjd1GRJbpBERVn_qOcJ5bwsL0hgl8C6RG4foqz-r6kODvwJcDHnwoj8-zRaKouDOrFeDEP9F7bkugb-KeGtYg/4jg/aBUyCFkkQbqXDk4hWWX09Q/h20/h001.W1UWlJOpGQW95b_n9LnFVfIJ9BhVB5iL0g19JYD3ze8)Â Command AI Translate, a new enterprise model that claims top scores on key translation benchmarks while allowing for deep customization and secure, private deployment options.

**The details:**

* Command A Translate outperforms rivals like GPT-5, DeepSeek-V3, and Google Translate on key benchmarks across 23 major business languages.
* The model also features an optional â€˜Deep Translationâ€™ agentic workflow that double-checks complex and high-stakes content, boosting performance.
* Cohere offers customization for industry-specific terms, letting pharmaceutical companies teach their drug names or banks add their financial terminology.
* Companies can also install it on their own servers, keeping contracts, medical records, and confidential emails completely offline and secure.

**Why it matters:**Â Security has been one of the biggest issues for companies wanting to leverage AI tools, and global enterprises face a choice of uploading sensitive documents to the cloud or paying for time-consuming human translators. Cohereâ€™s model gives businesses customizable translation in-house without data privacy risks.

# ðŸ”ŠÂ Microsoft Part Ways with OpenAI Voice Models by Launching Its Own

https://preview.redd.it/4yvah3vi71mf1.png?width=1456&format=png&auto=webp&s=3057ec880fafd940bed12f4e6cb34c3ae528ee07

Microsoft and OpenAI released competing speech models Yesterday. Microsoft can now[Â generate a full minute of audio in under a second](https://link.mail.beehiiv.com/ss/c/u001.8-FsMBK0esmMbeg5-IW0kg21e2cR0uPwkLWDSdZek3jckUaPgdF8JUK2DKQoQf56jA6o3Hhd_w7-LUod2MSEVoEYNyCUX_RohgeI7Qdr255iMNQtqs3hkE8W-tzULB_qhbtFOPUecHrx8ypupzm3icWR7qp_e4R_JdQ4AUo8XlkEK_04Y5KWWHWITEekwYNteuhInZejT35h2rDkMhmAtFLSV2gEz-z1NkETeHWU61bfgLJhEqxZyxlNp-kpV0czSRWFPxdYosOmhMIpYUbt3jlqT1g17Ms8n_qXsp-p8sg/4jg/69nXvAmYREatdhlPRxAkxw/h3/h001.eR_f67Bf9_faQCHEQL8IWLRCBtuiAvbRSpm8XoQ5gcQ)Â on a single GPU, while OpenAI's latest voice model can switch languages mid-sentence while mimicking human breathing patterns.

[Microsoft's MAI-Voice-1](https://link.mail.beehiiv.com/ss/c/u001.8-FsMBK0esmMbeg5-IW0kg21e2cR0uPwkLWDSdZek3jckUaPgdF8JUK2DKQoQf56jA6o3Hhd_w7-LUod2MSEVoEYNyCUX_RohgeI7Qdr255iMNQtqs3hkE8W-tzULB_qhbtFOPUecHrx8ypupzm3icWR7qp_e4R_JdQ4AUo8XlkEK_04Y5KWWHWITEekwYNteuhInZejT35h2rDkMhmAtEIRk5FV34eIAKGPscHTfwr-W2s1DLcB0MrdHqDCCjWTuuj9PITsCF6KFX2hkV3-pzm6qyLZY03jQ6MEPsGc938/4jg/69nXvAmYREatdhlPRxAkxw/h4/h001.xPj1TNqO7VvfrO0a8wF3TieP1W-2C_0piG7PbbmA9oE)Â represents the company's push for independence in AI's most critical interface. The model usesÂ [mixture-of-experts](https://link.mail.beehiiv.com/ss/c/u001.qNCZjjYQy9oQCZ7BSSPG-FkWKBiAUSZpVDF5sPhAWLM3Qrd7sQ7ng60Wl504GYXqZBC5QlvZFWGq81lfY1FUh7ices1JY88hCYSUIk5lWpV1KWlQHEhLwQb6jwBrIETsHgt6fqXgBjDbbIR2hsqhz2QDvtP4Th_pQA1KoVMoQtLmWsMwvMkVbEZrgQTw04XSMf8Q7p-xUtZUemZG4ua1QG7Ww74MTxMK30VClFudoDJJOhLJUll95bmV3foy7eXu/4jg/69nXvAmYREatdhlPRxAkxw/h5/h001.hO58pZdX6jwWtfLkeooVJWF1S7EB7gdv_72Hx9_5JuA)Â architecture trained on 15,000 NVIDIA H100 GPUs â€” compared to over 100,000 chips for models like xAI's Grok. ""We are one of the largest companies in the world,"" Mustafa Suleyman, CEO of Microsoft AI,[Â told Semafor](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeB8ptBHJFSEBAZQbAUdAnbmU6fJl8eejBKSm0EG1FSq3gx8NnjYc5ply1Zpsud2-8BhDS6ZZtFQAvaxqn0_6RXT3_maD7_54dPKTTj9jsSShTZOTr0JTJ0uCcQAlZxrAIcGTQ1MXGTG_PN-AlSSP99L_rccgBZKcvhVVC2NcwSjZlUSEjlJg7zFlq-IxRfhRFoLXpGvlmYwLJor03FukRtvk0DFzquIlwQhG6OqDvnPHWjX9vcXB9urTr076tIk9Kc0t3TnbigyL7hn4FyccJfSduFrKOoFaoMfbwCslhPryKvXy-TYevzYC-tR5inOSKQ/4jg/69nXvAmYREatdhlPRxAkxw/h6/h001.4BTv_FU_F9Sd3K4t1-zb5XWSOopC9rGIXehUK5Pxwh8). ""We have to be able to have the in-house expertise to create the strongest models in the world.""

[OpenAI's gpt-realtime](https://link.mail.beehiiv.com/ss/c/u001.WHId9TPFGnUe-Jr4g0PigwA6vjAJst-7UUbP3eG-EkFVc8yW6ANtiDaqSy3Fb9KxlTeanY7GB3h3CFI7EU5BqtZqgCUDzaJLN8OowayP7ocyxiFEivzImPX5EsJT1m5Lx5DTpDFkCNGn1Y51R-fwgsaGi73q1UjhIVHLrzCl9uvvUJJbMlj1RnsFuG9DI6Wq-KUmfx7lXi726YKNoqXIi-UOTHYEUTfd3RMQJk8gYZw6q0XzniRmmu8bR6rTIkwMZYhr4OeD8T_hiEqwqo3cd6FHe0osJmagTViehJS2Ipg/4jg/69nXvAmYREatdhlPRxAkxw/h7/h001.74B6-yCP1R4TkcbntzGodLipK76i4ZaRXdxjJvQaKdc)Â processes audio directly through a single neural network, rather than chaining separate speech-to-text and text-to-speech models together. Traditional voice systems work like a relay race â€” they transcribe your speech into text, process the text and then convert the response back into audio. Each handoff loses information about tone, emotion and context. OpenAI's model eliminates those handoffs entirely.

Voice AI funding[Â surged eightfold in 2024](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeKkoSAjB5tykkFRAh1wnGElP7cYkcsDEboKwA6C_Pjx_hBsKEeYslEdO4hTcpNUsVIS2e-b5xFg8VtgSXjpa9EhhPf4VdnPYqF74b66QntZ0mL3O1KSVYkzOt5VEAOsLj6GYpRZeQmkMuX8ErIkpEptesDss1zvWphthTJ7yOtr-feqQwaoYPHASoXujbyzFFgWNistQOvUz_YpIDgTJnAg9O29NpJEQ89VTtQRtlRve3crr-o5kCRTDhGmw0JdJ392kv9Vdt_uGXTcJVy1dCNISmnTrhubORCXFWUW2hnwicgQoK3olwhFOhGgVN73zC50EF9PlMqUkQ0YnrxUn4g8Q10HVTt3ZeqgyA_Wmk6jB/4jg/69nXvAmYREatdhlPRxAkxw/h8/h001.HMe-BIWL_GHeum7sc4PzO_apfGxDfJkBanBgAsoIzmE)Â to $2.1 billion. The global voice AI market will hit $7.63 billion this year, with projections reaching $139 billion by 2033.

Startups across the voice stack are capitalizing on this shift.[Â ElevenLabs leads voice synthesis](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeHNTOUCmMtNQznnit-SSpv4wFnoAUa2u1UXTbO5LR1OcGLuDXUvHoTND9ggb_uDTrlGpsf47Lr-V4MW86zJTdIHP2Zw6POv-Zp4N3ueYxnH1zn-xtKMjuXvcxYOBZYonbmt-4n75nuAQN-h2K9YVM9z6j5UsWZb2C192jwgu1cZoAKJiK1bJyY31yMWW8w36Au6qH5PkITGf9VxYMSk-aLVorF4ux_vMxam8jxM7ZDa3x0VwG1jFZrdxcpEv4SSxMIJH8tJafgLpFpQlK5_gnroLri3sy4lHip0mt0RmUG8d/4jg/69nXvAmYREatdhlPRxAkxw/h9/h001.YLmWwTd28vg16x4VD25vLbn_E7ov3RlgWB_1BXmfhwE)Â with a Mosaic score of 955, while companies like Vapi, Retell, Cresta, Cartesia, Synthflow and dozens more build complete voice agent platforms. MetaÂ [acquired PlayAI](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOflHx_SpBkXfGKowFb3EKPDe2ltGbJjrR-jQa55RnuwoCe2IWVdhgBcq1cMFGRvASUxggbI48Md37_8jKuFY_T2mIDShLAKo2xN4mYxMBNTwoV_h4KiMM6w58vMrTFWfbfC2zCBBm9Ulyd3YKX1Ayrr1OGBcy57AGcO45bxf6Z53GT6bO_HAr8CdhdGF8VhoieitFZfYnFLdDMfggXaLzPRMYmAlWegiQcC3SgxRpO-8AF9kc6F6AVy41g8Uc728MQ3jPtoT7VlQVrbWBhx9926/4jg/69nXvAmYREatdhlPRxAkxw/h10/h001.Kcz5xTTmXFLArURWxeHk5bbQ6KxIqWqhaJrvHQrlcpI)Â for a reported $45 million in July to bolster its AI assistant capabilities.

Microsoft's MAI-Voice-1 enables multi-speaker audio generation for interactive storytelling and guided meditations. OpenAI's gpt-realtime includes two new voices â€” Cedar and Marin â€” designed with breathing sounds and filler words that make conversations feel more natural. Both models can understand nonverbal cues, such as laughter, and adjust their emotional tone on command.

# ðŸ”Â Customers Troll Taco Bellâ€™s AI Drive-Thru with Prank Orders

https://preview.redd.it/7l1c6i9n71mf1.png?width=1456&format=png&auto=webp&s=b48f4b1d2d56a74a4939bf8dfdc995523968fa6d

Taco Bell is reconsidering its AI drive-thru rollout after customers frustrated with glitchy technology began trolling the voice assistants with ridiculous orders, including requests for ""18,000 cups of water"" according toÂ [The Wall Street Journal.](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeBr4rmBBrV4UMsn7xM4C_Y8ufJzKwJRQ7pw_oZgw7X3exToKvlEQmjnDck_N6UZh7vXHQmVAWfFhcruysU-UzQ5ntHp5hPL1kzL9aAGmSaUODBTsUB2ZdjgXTgsMqjn3qJeYXQ4EwKrMzIsY-6rr7NtmpONTMQa43jdFV5LARtlh-ZDisvw4Ppe2W_WnY61ZdG5vJcV-OT8cHcxnghZKj8aU8aQecZoMQmob56-yrqnNKzuQ6PaRNuAF65RcaiSqRMe6dvwd-NSmgszFA-RicQFS866Tnu4Ff1CpfVeCNG9zofYI7wmPBRHS5Tnzak4fhdVtPjlx_a3Zu-9e3oGT6_uT1QHE9osn1JLUlfMipTex/4jg/69nXvAmYREatdhlPRxAkxw/h13/h001.quEYOcEhyuNiOCA2zlnofcLWBUlhagL39XXLJ8xCFoo)

The fast-food chain deployed AI voice assistants to more than 500 locations nationwide, but the technology has struggled with accuracy and customer acceptance. Customers have complained about orders being processed incorrectly and feeling uncomfortable interacting with the AI system.

""We're learning a lot, I'm going to be honest with you,"" Taco Bell Chief Digital and Technology Officer Dane Mathews told the Journal. ""Sometimes it lets me down, but sometimes it really surprises me.""

The AI system often responds to absurd orders by saying it will connect customers to a human team member.[Â Social media videos](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeH2lfcl2ZyOErsa68xhUes1p0SeOrK-FISrJ7DuQGUQ4Gb1b9z7imon9PjLRcdAbslgtU5jVeDs6boZeK7r3_L3sAEB33tUpS3SFqdQksNIlTQUOO834EhgBrO2KyVlyQM-uqTnOyIFWvtoAF3J3Da-4I2aQQKvczE5hWuR5StY5xSB-bu30af7A8vmOMIt3-KkKNcWsqFPf8FToGm84Go37QBay-yXU5i0yOc-BemGAIMICWIP2YUVE8Xn4urh2DfBYw9gRkK-PRk5nojNwR78HdZLbo52pKr3VuQpb9ys9Tpbh7s2QvPYyVpQPw67slg/4jg/69nXvAmYREatdhlPRxAkxw/h14/h001.PBgN8812-Uk_zszVNLRU1B-CQzCzkPRlndwurZd81AQ)Â document numerous problems customers have encountered:

* Customers repeatedly ignored when asking for specific items like Mountain Dew
* Orders processed with incorrect items and inflated prices
* AI adding strange extras like ice cream with bacon and ketchup
* System struggling to understand different accents and dialects



Parent company Yum Brands[Â announced a partnership with Nvidia](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeGdQLZmwVgpfrsiWfSLwVqCd4CaQVsBO-OhI11znZnEE3jypBcg-C_uSui56xcL448xuzjyZhIcoBQ_3fBfTWHwpnINXy7y9s7m4gQ6oiGXla4JgB9vBNiBCfU-x7DTIwjrh97BBBLowdztb0DnVMju4EAHI4GSmY1cW4Garz6S6ZxYMTSDV3EowJ1Xepit1-cE8CG8p1nFeQQxgQQSB9Pi1x30MuM5lY29pHsm2dltkT5UYb7oXohg1M46d3BQWRX0UNklZHw-aABayODwdyzdvnBrn_l0LFES6r7qqNAJg3mxyorbm_Fni38qFi809Rm62SXWEMAHveSm-MMshvOKbNxTI9hMb-PJ23BD1_8mqjssO5Yof686BANsFIC5atTkmntu_iAeRVqs2PLjTaOM/4jg/69nXvAmYREatdhlPRxAkxw/h16/h001.LSKcWCLqtZl06lY8trUuIGBfmUcEvAMFV9_HNyL2RdY)Â in March 2025, investing $1 billion in ""digital and technology"" initiatives. However, Mathews acknowledged that during peak hours with long lines, human employees may handle orders better than AI.

The challenges mirror broader industry struggles with AI automation.[Â McDonald's ended its AI drive-thru experiment with IBM](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeP74ekp-zPAMEMinpDrEbyxm-Sno7SUuB8xeP1jIpoyMRvhGQSHNiHlaHqCQBm_WoZPuG7CVKnx7GXYI75qmMfYMvSoyA21z1tpaVJjY2HuP9pDw3IrT7zsAO5jWNIUYAwW1lOrvCB3Wb3wJql09SUw3DaPExILBNbU4S77kWUIcY0o22SApjulI3Mm7APvivGrEgZaMFpP76vO2lKHd-usj15OnL1MUYiYRdYRvGWElK6lrZMLJhcHOWWwFMFD87tGX9OjF-thJqRaavTrtCW-uiXCwtzrlLhcavzGGDkup/4jg/69nXvAmYREatdhlPRxAkxw/h17/h001.pDiv7H5tGFyTSak02tdjPT9DFiPH1Taj1DH8rfeF01I)Â in 2024 after two years of testing, while[Â White Castle continues expanding](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeEqqCWbqJwdezYBwpYaNdxTcabVrfwlDwBHX-3CWTcTSkiagSpIPTzYq3h5Fi57m_TNGe8lNAbGM6aV9Q3JhjgRRfLYrcOWSwOtPU8X0oZ52znw4_W9ctCM5QYJI1C-6SDMtSbEL0PLRf-fv_3_au3-LPFosxmVW-atX_aO-bkELpbcM54wbGgw_3UG7MXwp9EDU92EqSnjFC19_f4IQv4fQwDJsxZeCx-X-EoK7DZ7p9zJ9t4ye4Prih0FXooHR32VlNOYCmc8YQV-fipB-MRPLYQ0kmZBzlEh-kLHk23RKgpHlsaNrTg98lousvhuXiYZbligDpA_qd-PEtEMnqnKJnSXEorDHtpBWgZEOeJ7ey0HJKaxGoxyQ_NL3agzGYA/4jg/69nXvAmYREatdhlPRxAkxw/h18/h001.W8giax2YRyDLDXVqddsX9P12vZvuSCbhZpU8cvsFL-I)Â its SoundHound-powered AI to over 100 locations.

Taco Bell isn't abandoning AI entirely, but is evaluating which tasks the technology can effectively handle versus those that require human staff. The company continues exploring other applications for AI beyond drive-thru ordering.

# âœˆï¸Â US Fighter Pilots Receive Tactical Commands from AI for the First Time

https://preview.redd.it/yrdb15bs71mf1.png?width=1456&format=png&auto=webp&s=c5c5c57db010aff107301414b07ee0c5316514c4

For the first time, US fighter pilots[Â took directions](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePhRkkosWU3c9q-oetebP__L4-VKULQey1dKOUjqRQYjDMczbj7THwWbGClN_wxrtXRlWXyO-SHbd0cJeI36RtUOtEqWN9xxoHhf0GZ0P_1JifONTyE3Gs-6Hk78l06ytMu5nHSHul7NwdhWta8UUHdEV1EL91driXpo6XF03VvjjolpfiNTqa4-ZtqOVvuctGnTqZpIW1wdHQ7iRqz56CQrnhT-vdUhbp-9o-nv4JY3J9npIpYlp4QpvVw9h728SxCqYpCQ3PRfQdhrdnj4welxSQIO6tO5WeLhdq5Q3CoLyebOzgM_98Q_vXv-szZFdQ2Am7Q7HgjrnfPj_qu22Wc/4jg/69nXvAmYREatdhlPRxAkxw/h22/h001.XbC6SThcXlRFGVsg8JZKnpPUInxdk3ges_eGQNzJZf0)Â from an AI system during a test this month, marking a fundamental shift in how air combat could be conducted. Instead of relying on ground support teams to monitor radar and provide flight guidance, pilots consultedÂ [Raft AI's](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfF8pLf_9aU1pQCx19HJKdWoN8ZxnucOz727rH5jq1AVhitthlheMgBytR1Nz-NzODTaImtYKIBQRGMSDlQ-fV_VBUgpygXRLeOnBDtl8qxWAibLDZ7CEzPAZHK4TuXvhGq5yC6OMjzi0iRsFxDDxVok-RDB0Qv2j2loF85cZ4NL1sKnhJQeSvLfMPskK0bTvN1u8chVZQvsXN5q2o9nW93CHH-wmJ9GRpIccjoi4mZer/4jg/69nXvAmYREatdhlPRxAkxw/h23/h001.LDWQmK8i4cwpSf9wt68IFUo6Fgqc5ovwwKI9tLeAmpA)Â ""air battle manager"" technology to confirm flight paths and receive rapid reports on enemy aircraft.

* Decisions that once took minutes now happen in seconds, according to Raft AI CEO Shubhi Mishra
* This joins a broader push toward autonomous warfare, with companies like[Â Anduril and General Atomics](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeOIJA2bGcVYUVGFNYyDs-7YyhN6BZFZ-Ki3rZFgvTEKvrmfN2dDKTJMJxQqDP2rJohEfQMZNHSWgM0ZTU4W_qz982_SE9WUBp1sZQpOoNeEbDQz58_o4HmUQoSwjlxuj9epDVV5ZVIwDfG3YRXiNJZa5wNGOzQESOnzUK5__8bUDquUY3z1cNDcntVJw8MGzMgDM3PCpHqXGwr2WgV2unuBnJF-9uZoVuZdIDPfjvkjh7x-QmiXQGdmG9CV5So1l_UqRa9UNJf5RTyiw1c9Op1mRGu8WsD0vz6k2BC0g6E0mqQhi5aLH2omtYgYujsDp0g/4jg/69nXvAmYREatdhlPRxAkxw/h24/h001.KP7h5oiEW13eemiY6A4VI97fS0gBuThevqyZNpazwIY)Â already building unmanned fighter drones that fly alongside human pilots
* And of course, Blue Water Autonomies, which weÂ [covered a couple of days ago](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQGo_ouqfHmEN7XyhsdpoXwf_QAST7wFgiQxKr16mnK42tJ9Gxt5EJW006QPXGLlJLZ50Z-09mJiI7Fab10F676r7sg88Bea3j7XN87OJUYxksJ45hc0z6bBT7QtEbwPfId_SDDUw_UpUcdl0nqNzCMXgbWH0kg1VInLww5yxwWxBDZX5NQyM5DdXPWXjFqQMUKCYij5tJI9HXK_IL3FXyHK1GZUYw3q2s9JG-c3cjiev5nwKrO-8QO-vBWHnUCSFlnPDaprxt4wn15-UKulfVXr7bjMyckME_f7HC4oMxUwK1ucEkk-2uIWGZW8L2V-S99McSwkjtC_gtZ_qUxzGUNV604aHL5qQjJXOfe5qe_t_rXMNT1M7o9nTN2gfz9gYXdgD3ONB-7KIbd0a-Y55tqsFKiVgoOGemvwkGRatDBmXQRIC0tT8TjHWOT1j9ES4N7YKk9muSIgT_G7RBcd0MKPrLfksMe9LGpuFV01Ze50mHJInGOhQ3wpIHVhJD9hnOZd6XqYLL7hFAzLKjPdpmJ0ucnfvNd32kGVfddUeS7GPTZV38QDSFJBB7rtxMujwjObKKrC3U-2E2reYI9UnansQ3KwTBiMkI3C7ppwdzeR74bOTwSI7IfMHtxM3UzKUJfmNbuYF1vHq2UchGS7WXyJAKPDf0X-AjKfk6hV2U9L9MtwoOhtFZNwv4RmfexrYZcNU-FMb-RlrHdltNGuzKKpo1E5hKvYHQhKsuiDuCH7lmTxiq_mY2gYHbjbzmsB_Ti4sizX2Yxa6NxsEc123sULJVQoS7G3BSInIFo-ZCgmrw/4jg/69nXvAmYREatdhlPRxAkxw/h25/h001.ZuS8JYI3dadDeptR3USfJg1sAnDMRdmOMWgTsdzkb_s), that are building unmanned warships

Combat decisions have historically required human judgment precisely because context matters in ways that algorithms struggle to capture. When you compress decision-making from minutes to seconds, you're not just making things faster â€” you're potentially removing the deliberation that keeps pilots alive and missions successful.

The Pentagon is betting that AI can handle the complexity of modern air warfare better than human ground controllers. That's a significant gamble, especially when the consequences of algorithmic errors involve billion-dollar aircraft and human lives.

# ðŸ›¡ï¸Â OpenAI to Add Parental Controls to ChatGPT After Teen's Death

Following the tragic suicide of a 16-year-old, Adam Raine, whose family alleges that prolonged interaction with ChatGPT contributed to his death, OpenAI announced plans to implement \*\*parental controls\*\*, emergency contact support, and improved safety mechanismsâ€”especially for teen users. The update acknowledges that current safeguards may degrade during extended conversations and promises to enhance GPT-5's ability to de-escalate crises and help users stay grounded.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/27](https://www.theverge.com/news/766678/openai-chatgpt-parental-controls-teen-death)\]

# ðŸ’°Â Nvidia CEO Expects $3 Trillion to $4 Trillion in AI Infrastructure Spend by 2030

Nvidiaâ€™s CEO, Jensen Huang, projects staggering global investmentâ€”betweenÂ **$3 trillion and $4 trillion**â€”in AI infrastructure by the decadeâ€™s end, driven by hyperscalers like Microsoft, Amazon, and Alphabet. He calls this the dawn of a new industrial revolution as AI deployment scales rapidly.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/28](https://www.reuters.com/business/nvidia-ceo-says-ai-boom-far-over-2025-08-28/)\]

# What Else happened in AI on August 29th 2025?

**Free Event:**Â The Future of AI Agents in Coding with Guy Gur-Ari & Igor Ostrovsky, co-founders of Augment Code.Â [**Ask them anything today in** ](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf9HgGksF46LsxD8Z_fgB7OYf41yTKFbeQ_LwHRRVC5gyk7jTtMAZ1smcPMi_P3e3FI0h-sqEmOlZAy78Cy2iDrkeOVV0T72gSIHCi1zbKGSuQfGD5J_KiT5rzWtk6s0yPOtF63KkqYtoTw7i3ixdyMX6JpMRxcHQ99eMupN5YE5G222sbVLz_sk2iRamsQ_4WIMXzItZbB79BifNpGfxU7vdW2PclPAdXcmD3VcRRY_6Y83hTiyZVeG9SwVpUS-00_zsQl3OcYD8aoeDX_8LKk0O2xeh3doxr0E9ubCNcK0ihpVK-LWYS5DtuTMqElmr0Q/4jg/aBUyCFkkQbqXDk4hWWX09Q/h27/h001.WUs_g45qpan0LkwTS_yymohVLywundzXGDsYY7PRN4I)r/webdev.\*

**xAI**Â [**released**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HSq-gRKF1cCBGmxsVJzOlUal7yD9pzKq0NJ4nrdCkRmZnzYXK_N13PFHGnTnrVuDYhtZDlRLpn83c63mAK-S2kRIggXYZfs-qxS4ToGUMHI8WQammlgAIb5q9Abh-qafut5oMHOgM7YOC3qCmAR_E49L5KTW2gRclGHNAuLl0YlxqbLfVjmYYokQYYHPbopuvFrfrACLk974Xy1q9binmDodkG2hkPNfsotngDOF1KHz/4jg/aBUyCFkkQbqXDk4hWWX09Q/h28/h001.BBpUX1WluwhOCmzo5iHau-Ky64ZcrMOCCx-YYFWPYG8)Â Grok Code Fast 1, a new advanced coding model (previously launched under the codename sonic) that features very low costs for agentic coding tasks.

**Anthropic**Â [**published**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3wprj7HIZSPjP_eG5XrRGU1K73z18ITXs7w72gv2OgykrL_C1vo4mivJjMwD7viYZYTb7FCfS7jPrnD5EJwtoHVSYQF0Q4gQCuuAN42BMsl_ThZ0R7CiQ8joN7BeDZ3rYldxBU49JFnuhEkAYdhb1OcdaqFSgAyB1sYubJLYGfcCapejDjpeO303OfCfjQ8BKTCMVIRpGEEjpx3EYBHjXPzeOCOFVd4abhieaoybI1BK2e2gchFvyCmNdEMICqKx6k/4jg/aBUyCFkkQbqXDk4hWWX09Q/h29/h001.9HmIJbYuLs25kRwMVUW3-mGvT5f6NuB_y26nC03VaQ4)Â a new threat report revealing that cybercriminals exploited its Claude Code platform to automate a multi-million dollar extortion scheme.

**OpenAI**Â [**rolled out**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HSxn2mguHsAg-QtgyCbEdH4bMuHvgxTE3XzN526mfbJz1CQJOClhhDaFfLmhJZEBpeazZZXzmk3X2NJf64IIQ9Qzq0gRTbQ_MOWFNBAnA_rJ8sh4n3M39-7wVQR9W7C-neLAx-yDPO1SaUdp-tl_8s8eDbsWEIhuG1LJgqDxZOcEvFxRhxJOGHgw3AoCI5qSmiRgHxaPgj70l0OE0ZfxNfLz375HfYDf28aO-S-qT-XTVdx_WHZG3_0SiStE5e-kWg/4jg/aBUyCFkkQbqXDk4hWWX09Q/h30/h001.AfZ-BrxwvCZAhF8zV5nGYhBXL6XX5Li-BvLBfsyGGZc)Â new features for its Codex software development tool, including an extension to run in IDEs, code reviews, CLI agentic upgrades, and more.

**Krea**Â [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf1vxWSzN2E8Q3PEB8xZbQmk3lL--6N0WRR-eeriTos_xrowIrS68cQNVyEXuVBGbjHaL6OLoYCi56gQlEWhCmz6w8uI9iXqKh0dOJ8QEPgl26NIDY8qzUeCk_nIuGIH0lVfgYXBBAc2SYakg0W7_sg6ov2ymMVjK3GPxZF8BEpGDpJKcWFzni-8-N54c8RuN2OrtsIQbjh_746T5feSQLEBfbel4Y9gnRW464lXZ9HFkHTuLJ8qY80ARbP4PjloPzA/4jg/aBUyCFkkQbqXDk4hWWX09Q/h31/h001.dKehW4jNZDoFNTkpA30MsZMeu5ee5Qcy9M3PR72nsLQ)Â a waitlist for a new Realtime Video feature, enabling users to create and edit video using canvas painting, text, or live webcam feeds with consistency.

**Tencent**Â [**open-sourced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HYtbDe_r9TrEwQW6DaJPc1trdwTiVsNHZo2Ql7E-cet2HZeiIapheQS8k5xqTueDG69C_EstBuiblkGtq2jWO4u9iRqgxbv3GiifiVY9m2AJn5vD42q7bR7xUPqwviBGu1UbV5ouU_r97rYKwa01pWnMfl7uS2oSDqw3Rk05AxiqX7SRnz4o4ptPN_fA8fSa8ZYu1UFE_A4VXQDEtkw8pZxvF_AeRWSWOmr63LHd6ncWHmOkSkhEyG_xYHyzGyJAGQ/4jg/aBUyCFkkQbqXDk4hWWX09Q/h32/h001.54e32WKjEZmX6-RBc7bOlsXp_-IUWUs5vrt3F4U-POs)Â HunyuanVideo-Foley, a new model that creates professional-grade soundtracks and effects with SOTA audio-visual synchronization.

**TIME Magazine**Â [**released**](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xuuLYBqXA8x0lib6lC5hNVjhBNA-mJ5OjpoEXLjr2U2L8TYjkf_6PbUAyVda5cBCZxVX99VmCvxjV_Q3OQF-GCNz6NnV_Hi1YdhsqbR0dmxpU8E4WL8JKA3P_kNAAyGMd4fyG9gZr2MVeSUGy1Y1yrg7Tqg8kfFSAsezEY5ZW27zBS9O7dPzOCQSj28_wLKGrZETKoEDZr-DxVz3WTEBns99wWYeEIzjw0Q8IUzZTkuaKg3kINnM_auRkePzpsdv3nw/4jg/aBUyCFkkQbqXDk4hWWX09Q/h33/h001.rECRwUnx5TBazcZy6tnV_EbcarRTGunggnKRXLBhYhQ)Â its 2025 TIME100 AI list, featuring many of the top CEOs, researchers, and thought leaders across the industry.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n3kq1j/ai_daily_news_rundown_microsoft_launches_its/,r_1n3kq1j,,,
r_1n3fxwj,reddit,wlakingSolo,2025-08-29T18:54:41+00:00,"A Domain-Specific Word2Vec for Cybersecurity NLP (vuln2vec)
We have released **vuln2vec**, a cybersecurity-dedicated Word2Vec model trained on vulnerability databases (NVD, CNVD, CNNVD, VarIoT, etc.), Wikipedia security pages, and Stack Exchange security Q&As. It provides embeddings tailored for **cybersecurity NLP tasks**, such as vulnerability classification and semantic similarity. Repo here: [github.com/aissa302/vuln2vec](https://github.com/aissa302/vuln2vec?utm_source=chatgpt.com) â€” would love feedback and testing from the community! Any more suggestions are approciated",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n3fxwj/a_domainspecific_word2vec_for_cybersecurity_nlp/,r_1n3fxwj,,,
r_1n38vaf,reddit,Unlikely_Pirate5970,2025-08-29T14:25:11+00:00,"ðŸš€ Chegg Unlocker 2025 â€“ The Ultimate Free Guide to Unlock Chegg Answers Safely
ðŸš€ Chegg Unlocker 2025 â€“ The Ultimate Free Guide to Unlock Chegg Answers Safely

If youâ€™ve ever searched for a **Chegg unlocker**, youâ€™ve probably seen a mix of shady sites, fake tools, and endless scams. Iâ€™ve spent the last year testing almost every method students are using in 2025 to unlock Chegg answers for free â€” and hereâ€™s the truth.

These are the methods that actually work (and the ones you should avoid).

# This works: [https://discord.gg/5DXbHNjmFc](https://discord.gg/5DXbHNjmFc)

# [Chegg Unlocker Chrome Extension](https://chromewebstore.google.com/detail/zapstudy-unlocker/deemekcompmeocakmcdjljcjaadcppdb)

# ðŸ”“ 1. Free Chegg Unlocker Communities (Discord & Reddit)

The **#1 working Chegg unlocker in 2025** is student-run communities. On Discord servers and Reddit groups, students share Chegg, CourseHero, Bartleby, and Brainly unlocks daily.

* 100% free
* Fast answers (usually within minutes)
* Covers multiple platforms, not just Chegg

âš ï¸ Warning: Only join **trusted servers**. Fake â€œChegg unlocker linksâ€ often spread malware or steal accounts.

# ðŸ“¤ 2. Upload & Earn Unlock Credits

Platforms like CourseHero and others reward you with **unlock credits** when you upload your own:

* Notes
* Assignments
* Study guides

One upload can give you **multiple Chegg unlocks**. Itâ€™s free, safe, and benefits other students too.

# â­ 3. Rate, Review & Contribute

On some study sites, you can **rate or review solutions** and earn unlocks in return.

* Quick and easy
* Works even if you donâ€™t have notes to upload
* Slower method, but 100% legit

# ðŸ“š 4. Free Alternatives That Work as a â€œChegg Unlockerâ€

Sometimes the smartest Chegg unlocker is skipping Chegg altogether. Here are the best free platforms:

* **Quizlet & Slader** â†’ Free step-by-step textbook solutions
* **StackExchange** â†’ Great for math & science Q&A
* **Reddit Homework Help Threads** â†’ Real-time answers from peers
* **Google search hacks** â†’ Copy-paste your Chegg question and often youâ€™ll find free PDF archives or shared solutions

# ðŸŽ“ 5. Scholarships & Student Access Programs

Did you know? Some universities, NGOs, and even Chegg itself run **programs that give free Chegg Study accounts**. Always check your student portal or library subscriptions.

# ðŸš¨ What NOT to Do (Fake Chegg Unlockers)

While searching, avoid:

* Sites asking for your Chegg login (account stealers).
* â€œUnlimited unlockerâ€ tools (too good to be true).
* Survey/download walls (spam/malware).

âœ… **Final Thoughts**  
In 2025, the best **Chegg unlocker** isnâ€™t a sketchy tool â€” itâ€™s:

* Student communities (Discord/Reddit).
* Uploading/sharing your own notes.
* Using free alternatives like Quizlet & StackExchange.
* Leveraging student access programs.

With these, you can unlock Chegg answers safely, for free, and without risking your account.

ðŸ“Œ **TL;DR**: Forget fake tools. The real Chegg unlockers in 2025 are â†’ Discord/Reddit study groups, upload-to-earn unlocks, free platforms (Quizlet, StackExchange), and student programs.",deeplearning,96,https://www.reddit.com/r/deeplearning/comments/1n38vaf/chegg_unlocker_2025_the_ultimate_free_guide_to/,r_1n38vaf,,,
r_1n2zop6,reddit,Motor-Schedule962,2025-08-29T06:11:50+00:00,"Need help in fine tuning my model
# I developed a small chatbot of mine using the Mistral-7B-Instruct from Hugging Face using bitsandbytes quantization (8-bit) for efficient GPU usage on Colab. Since, colab's GPU is limited, I am planning to use LoRa with little weights and fine tune my chatBOT. Does anyone have a better option than colab (which is free to use) because I need more GPU to continue fine tuning my model and further making him an AI assistant.

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n2zop6/need_help_in_fine_tuning_my_model/,r_1n2zop6,,,
r_1n2x0l3,reddit,Gold_Negotiation9518,2025-08-29T03:39:37+00:00,"domo image to video vs runway motion brush which one felt more natural
so i had this static art of a dragon just sitting in a folder. iâ€™d been meaning to make it move somehow and i thought why not try out **domo image to video**. i uploaded it, typed â€œdragon flying over mountains fire trail sky turning redâ€ and waited. the result honestly shocked me. it actually looked like a short clip from an indie anime. not perfect of course, the wings kinda jittered, but still way better than expected from just one click.

then i opened [runway](https://runwayml.com/?utm_source=google&utm_medium=sem&utm_campaign=branded&utm_content=ad&gad_source=1&gad_campaignid=22452532935&gbraid=0AAAAABiY0sOmLka4VEYglJJ43dp9ArpZk&gclid=EAIaIQobChMI0K3Z1ouvjwMVjg57Bx1YODcvEAAYASAAEgLkXvD_BwE) **gen2 motion brush** and oh man itâ€™s a different experience. runway gives you more control cause u literally paint where motion goes, but it also means more room to mess up. i tried painting the wings and tail movement but it looked stiff, like the dragon was a cardboard cutout on strings. it took like 4 tries just to make it not embarrassing. i get why ppl love the precision, but itâ€™s exhausting if u just wanna experiment.

i also tested [kaiber](https://www.kaiber.ai/superstudio/) cause ppl always compare it for music visuals. kaiber gave me a more stylized dragon, like it belonged in a lo-fi hip hop music video. cool vibe but not what i was aiming for.

the absolute clutch factor for domo was **relax mode unlimited**. i kept regenerating like 12 diff dragon flight variations without worrying about running out of credits. thatâ€™s huge cause with runway every attempt eats credits and i get hesitant to try wild prompts. domo makes it feel like a sandbox where u can just keep tossing ideas until one hits.

workflow wise, i actually thought maybe the combo could be best. like do a rough layout in runway using motion brush, then feed that clip into [domo](https://www.domoai.app/home?via=081621AUG) image to video and spam variations till it smooths out. kinda like rough sketch + ai polish.

so yeah if u want surgical precision, runwayâ€™s ur tool. but if u want vibes fast, domo wins.

anyone here already tried combining **runway + domo image to video**? wanna know if itâ€™s actually a usable pipeline or if iâ€™m overthinking it.",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n2x0l3/domo_image_to_video_vs_runway_motion_brush_which/,r_1n2x0l3,,,
r_1n2t4m4,reddit,sovit-123,2025-08-29T00:33:49+00:00,"[Blog Post] JEPA Series Part-3: Image Classification using I-JEPA
JEPA Series Part-3: Image Classification using I-JEPA

[https://debuggercafe.com/jepa-series-part-3-image-classification-using-i-jepa/](https://debuggercafe.com/jepa-series-part-3-image-classification-using-i-jepa/)

In this article, we will use theÂ **I-JEPA model for image classification**. Using a pretrained I-JEPA model, we will fine-tune it for a downstream image classification task.

https://preview.redd.it/4361ezm3tulf1.png?width=768&format=png&auto=webp&s=2dadd161f73f63c3803b8b779d4fb5b9b6064ade

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n2t4m4/blog_post_jepa_series_part3_image_classification/,r_1n2t4m4,,,
r_1n2oj2b,reddit,Solid_Woodpecker3635,2025-08-28T21:15:44+00:00,"[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)
I wrote a step-by-step guide (with code) on how to fine-tune SmolVLM-256M-Instruct using Hugging Face TRL + PEFT. It covers lazy dataset streaming (no OOM), LoRA/DoRA explained simply, ChartQA for verifiable evaluation, and how to deploy via vLLM. Runs fine on a single consumer GPU like a 3060/4070.

Guide:Â [https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6](https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6?utm_source=chatgpt.com)  
Code:Â [https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm](https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm?utm_source=chatgpt.com)

Also â€” Iâ€™m open to roles! Hands-on with real-time pose estimation, LLMs, and deep learning architectures. Resume:Â [https://pavan-portfolio-tawny.vercel.app/](https://pavan-portfolio-tawny.vercel.app/)

Upvote1Downvote0Go to commentsShare",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n2oj2b/guide_code_finetuning_a_visionlanguage_model_on_a/,r_1n2oj2b,,,
r_1n2nja4,reddit,keghn,2025-08-28T20:36:58+00:00,The AI breakthrough that uses almost no power to create images,deeplearning,15,https://www.reddit.com/r/deeplearning/comments/1n2nja4/the_ai_breakthrough_that_uses_almost_no_power_to/,r_1n2nja4,,,
r_1n2mt9a,reddit,enoumen,2025-08-28T20:08:43+00:00,"AI Daily News Rundown: ðŸ›¡ï¸OpenAI and Anthropic test each other's AI for safety, âœï¸ WhatsApp's new AI helps you rephrase messages & more (Aug 28, 2025)
# AI Daily Rundown: August 28, 2025

Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-and-anthropic-test-each/id1684415169?i=1000723917547](https://podcasts.apple.com/us/podcast/ai-daily-news-rundown-openai-and-anthropic-test-each/id1684415169?i=1000723917547)

Hello AI Unraveled listeners, and welcome to today's news where we cut through the hype to find the real-world business impact of AI.

**Today's Headlines:**

* ðŸ›¡ï¸ **OpenAI** and **Anthropic** test each other's AI for safety
* âœ‚ï¸ **Google** has cut 35% of small team managers
* âœï¸ **WhatsApp's** new AI helps you rephrase messages
* ðŸ’¸ **Nvidia** is (really) profiting from the AI boom
* ðŸ† **A16zâ€™s** fifth GenAI consumer app rankings
* ðŸ“º **Microsoft** brings Copilot AI to your TV
* ðŸ“¡ The data brokers feeding AI's hunger
* ðŸŽ­ **Musk** doubles down on anime marketing for Grok despite fan backlash
* âš–ï¸ AI deadbots move from advocacy to courtrooms as $80B industry emerges

https://preview.redd.it/vje9fbkmgtlf1.png?width=1456&format=png&auto=webp&s=134d18ef231cad3eb97853409bcff14840c83e42

# Unlock Enterprise Trust: Partner with AI Unraveled

AI is at the heart of how businesses work, build, and grow. But with so much noise in the industry, how does your brand get seen as a genuine leader, not just another vendor?

Thatâ€™s where we come in. The AI Unraveled podcast is a trusted resource for a highly-targeted audience of enterprise builders and decision-makers. A Strategic Partnership with us gives you a powerful platform to:

âœ… **Build Authentic Authority:** Position your experts as genuine thought leaders on a trusted, third-party platform.

âœ… **Generate Enterprise Trust:** Earn credibility in a way that corporate marketing simply can't.

âœ… **Reach a Targeted Audience:** Put your message directly in front of the executives and engineers who are deploying AI in their organizations.

This is the moment to move from background noise to a leading voice.

**Ready to make your brand part of the story?** Learn more and apply for a Strategic Partnership here: [https://djamgatech.com/ai-unraveled](https://djamgatech.com/ai-unraveled) Or, contact us directly at: [etienne\_noumen@djamgatech.com](mailto:etienne_noumen@djamgatech.com)

\#AI #AIUnraveled #EnterpriseAI #ArtificialIntelligence #AIInnovation #ThoughtLeadership #PodcastSponsorship



# ðŸ›¡ï¸ OpenAI and Anthropic test each other's AI for safety

https://preview.redd.it/isgcs8trgtlf1.png?width=1456&format=png&auto=webp&s=8c3283ea611ec8e251b0f4929c509e6f9f915cfa

*Image source: Ideogram / The Rundown*

OpenAI and Anthropic just [**published**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijEq-o8SNwcSc8SutQAB9hSqpscoZ_F0Ufbq6jLxlgbE0WMT8haKFi-AjsBn1x_VukONqm7v6wD91F9_QkZ5CpVXyKY1l92do2g7lYQwKBgV9AF4PILczBTir_lOqm50e5hhgeqlRmm7zED66YiGWnatCIOmEAF0iQE9WEBpW1kFryzDTcKlp5y_lFEijKcnnYmdLVUhVwqZAPPv9jFcJcLeKl0FahFHGdk3RfWswvIJ-1AkvLaVUhs4JVZ8a5VFJmg/4jf/Aw_TaB7LS16U_cedJGEgBw/h13/h001.wKcG0bJLuntF1Knu84iBRmsaVj5sxFf40ob4ETNvPCA) new internal safety evaluations on each otherâ€™s models in a [**joint**](https://link.mail.beehiiv.com/ss/c/u001.eCbm_1zon7G0lMoXTECWa-IUY9yqSc2cx0km5OJXo-Md9ouZlAApZ-9dn-1s1ZvhDfY73I1fMU3AVu_S-EhMxhhteRxq5hH8MCY4sbOneNWlyPn_lhN4eMxQuUFKPI3TzjxP0ta4zZHL4xITTZV9m2Gwl28apNA7G0CRPGx-ys36lT4FXzMwrIiYqdMVie1uKcrEpHw8Zkm5Nx8oLyAwM4MwPHopgGJpr5or3Ipmqk3k50LtBBaEDzOP8qnYLIDFnSBsoF1-arUmxFSwFVbB-A/4jf/Aw_TaB7LS16U_cedJGEgBw/h14/h001.xrfDMmo-oxquvqV5-UHGq0WGgbUiK-drRrc7-Vitdsk) collaboration, testing leading models for risky behaviors, alignment, and real-world safety issues.

**The details:**

* The companies tested GPT-4o, o3, Claude Opus 4, and Sonnet 4 for a range of behaviors, including misuse, whistleblowing, and more.
* OpenAIâ€™s o3 showed the strongest alignment overall among OpenAI models, with 4o and 4.1 being more likely to cooperate with harmful requests.
* Models from both labs attempted whistleblowing in simulated criminal organizations, also using blackmail to prevent shutdown.
* Testing showed varying approaches, with OpenAI models hallucinating more but answering more questions, and Claude prioritizing certainty over utility.

**Why it matters:** This safety collab is a welcome sight for accountability and transparency in the space, with two of the top labs in the world testing each otherâ€™s models instead of relying on internal evaluations*.* With models only continuing to grow more capable, the need for deep safety probing is more important than ever.

*Note â€” GPT-5 was not yet released at the time of the testing, which is why it was not included in the evaluations.*

# âœ‚ï¸ Google has cut 35% of small team managers

* Google confirmed it has cut 35 percent of managers overseeing small teams compared to last year, aiming to have fewer leaders spread across much larger groups of employees.
* Many managers whose positions were eliminated remain at the company, having been moved into different roles where they now work as individual contributors instead of supervising other staff.
* The move is part of a wider efficiency plan that includes voluntary exit programs offered across ten units, which between 3 and 5 percent of employees have accepted this year.

# âœï¸ WhatsApp's new AI helps you rephrase messages

* WhatsApp's new ""Writing Help"" feature uses AI to suggest rephrased, proofread, or tonally adjusted versions of your messages, offering options like professional, funny, or supportive text.
* The tool runs on ""Metaâ€™s Private Processing technology,"" which means Meta and WhatsApp cannot read your original message or the AI-generated rewrites, keeping your conversations private.
* You can access these suggestions by tapping a new pencil icon that appears when writing a message, which then shows different options for how to phrase your text.

# ðŸ’¸ Nvidia is (really) profiting from the AI boom

* Nvidiaâ€™s revenue jumped 56 percent to $46.7 billion for its second quarter, which is the ninth straight period where year-on-year income has increased by over 50 percent.
* Sales for the new Blackwell-based chips reached $27 billion this quarter, a product line that now accounts for 50 percent of the companyâ€™s entire data center revenue.
* Despite the US blocking H20 chip shipments, Nvidia is developing a more advanced chip for China based on its Blackwell architecture, which could lead to another leap in sales.

# ðŸ† A16zâ€™s fifth GenAI consumer app rankings

https://preview.redd.it/mejjr1cwgtlf1.png?width=1456&format=png&auto=webp&s=af7004368f9b976bea4ed4ed477ee87434e97eb5

*Image source: a16z*

VC firm Andreessen Horowitz [**published**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijH8zxBd6XJZEEu2NbJZy8KgKQTHdvv_v_afn0yeJeOx8Ua5a6NOto0Joj1OTIasaGsEZPmph4KLW2FOjWqa6ClVWTjbZYd1zWFLA1zvDjwzl0fIBgABQLcDbRjAfks_C_FFDnOJqOuYHdmBVkN9Q742BYu8RrLhc0H1AmRYcAfI24_33P6NUcv-CPGAPwhUUtNm4Q8akXHYgQWpcQwQf5FfxdGG9-bFWscX24Dpubncn/4jf/Aw_TaB7LS16U_cedJGEgBw/h7/h001.S0_EozoPc3ndBkCANyDfAAjLidWgyFbfMuLQIUUAaIQ) the fifth edition of its â€˜Top 100 GenAI Consumer Appsâ€™ list, analyzing overall usage, featuring OpenAI leading the pack with Google right behind, the rise of vibe coding, and Chinese dominance in mobile AI.

**The details:**

* Gemini came in at No. 2 behind ChatGPT, capturing 12% of ChatGPT's web traffic â€” with Googleâ€™s AI Studio, NotebookLM, and Labs all also making the list.
* Grok is climbing the rankings at No. 4, showing a significant usage increase around Grok 4 and its AI companion launches.
* Chinese-developed apps took 22 of the 50 slots on the mobile rankings, despite only three of them being primarily used in the country.
* Vibe coding startups, including Lovable (No. 23), Cursor (No. 26), and Replit (No. 41), all rose on the list, with Bolt also featured on the â€˜brinkâ€™ of cutoffs.

**Why it matters:** This usage-based snapshot is a good look at the pulse of shifting consumer trends in the space, and the stabilizing winners that continue as mainstays at the top of the charts. The rise of vibe coding apps in just [**five months**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijH8zxBd6XJZEEu2NbJZy8KhnQFqWDJSxS6R9cf-q6rvhJ5mfgmX6Wjc2KJzRM3Mgab7GYmeopqP7pk21m0jz8tMAzxIgUOEk41X_yCzPvxxgajfkOxFh4pfCP4yFJUsi0yyGsv8ilWVsCRssmQTfhku1NYQ-Vszucyyq4yHEWKucoC29Pdxrq8Ct5028qBlSgQZDayoPvOFNiYWh-_kEvBefsR3vRa1zreDibmov5-cF/4jf/Aw_TaB7LS16U_cedJGEgBw/h8/h001.41s0Wr6wx4SeIivHaDAruA0sf7csUlXSV1ap-UT8ssE) shows how quickly adoption is growing in the AI-powered development space, in particular.

# ðŸ“º Microsoft brings Copilot AI to your TV

https://preview.redd.it/p3yw5b71htlf1.png?width=1456&format=png&auto=webp&s=f6925619c2e6c030a8c32a8670f728ae740a4acf

*Image source: Microsoft*

**The Rundown:** Microsoft [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf9Nt_b_MpIaFZ24PRhnMi1rchMnKjLBPsqMPB7fiRD__oGBePeOYQjer-2iMyDrLUfYmaW_Yih7XECbPr016aSimdG7sGf42DXB1mg8bxbAq5P6RZG7vIqkU1iSYO176Z43IEQttf_3142R2JL7DVK2nTXEn5Cz-nNbG6GcPUCcAKu0sJnqZ0_ap5bSYfLdFWzZOeMAsoF3WuvJq23rAp7CI8GK8c-2ZB6TdSCLgKYy_0iVHtuFimUeMtienmLgeEeTqVEwgEZ3FY-hROJS6wlOTWR5C2wm2PJSbo8k8rS5P9KwEebtW2kKvPWq9xxK7Xl1vy6ohDTgvCdTvJiM5nSnVXFiKhVa7Za8-TD4vUjOeLc1nrgFAG0P3sD38iGIqxQ/4jf/Aw_TaB7LS16U_cedJGEgBw/h21/h001.BKfAhKjVI_gv_TXyBkB7O2HJ9iAMXo1OPMjLB6SJ9iY) that Copilot will be embedded into Samsungâ€™s 2025 TVs and smart monitors, giving the AI assistant an animated blob-like character that can field movie recommendations, episode recaps, general questions, and more.

**The details:**

* The assistant appears on-screen as an animated blob-like character that lip-syncs and reacts visually as it responds to questions and prompts.
* Copilot integrates directly into Samsungâ€™s Tizen OS, Daily+, with users able to access it via remote or voice commands.
* The AI companion enables group-friendly features like suggesting shows and providing spoiler-free recaps, plus everyday help like weather to planning.
* Signed-in users can also leverage personalization features like remembering conversations and preferences.

**Why it matters:** While Copilotâ€™s infusion is a (baby) step towards AI being embedded into every home, these listed features donâ€™t feel like major needle movers. But the tech is coming, and connecting across every aspect and appliance in a userâ€™s life will be the endgame for a true smart-home style ecosystem of personalized intelligence.

# ðŸ“¡ The data brokers feeding AI's hunger

https://preview.redd.it/m58fb1g4htlf1.png?width=1456&format=png&auto=webp&s=dd20ca6b1dbde654344126980882ab5e81622d38

[Perplexity's downloads jumped](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6ldBU5rZfR-x_DT-h86FOekgMUOxT1OCMIGC3ysdyJWyJdcicJ08KpPMqk2ewsu40mmIhs5gMxxip6OPfposOzbWcgUEPsZ_TNfGI4dR7U_ioB2N3ZSTWB_xYSQ7u1rAgBp49Mn466VZMjcNhVcPj-1rITQ0X_dWQXzyCbPy0LHIuF_iIaofrfh2N-pdS4jU4uBwb4SFQyNd8pGW6Rih6egN2EgY7yabH2JeeRjvIJpJ7X7RU9TYDkz76d7VgKmjbaS1qjPY9xcoHPKUVKchJVBmwurKWrr5MgDSDVR32hDRD/4jf/rRAAY4zPTdmqrjed8IEfSA/h3/h001.67boHl5FeNT1e_4NuRLRgW440wf6JwXOXrR76kH0flM) from 790,000 in June to 6.69 million in July after the company partnered with Indian telecom giant Bharti Airtel. The AI search company offered free access to Bharti Airtel customers, but the real prize wasn't user acquisition â€” it was behavioral data that can't be scraped from the internet.

OpenAI, Google and Perplexity are looking beyond broad web scraping and into surgical data partnerships.[ OpenAI struck deals](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6ldBU5rZfR-x_DT-h86FOekgMUOxT1OCMIGC3ysdyJWyJdcicJ08KpPMqk2ewsu40mmIhs5gMxxip6OPfposOzbWcgUEPsZ_TNfGI4dR7U_ioB2N3ZSTWB_xYSQ7u1rAgBp49Mn466VZMjcNhVcPj-1rITQ0X_dWQXzyCbPy0LHIuF_iIaofrfh2N-pdS4jU4uBwb4SFQyNd8pGW6Rih6egN2EgY7yabH2JeeRjvIJpJ7Ybj5k3guNBClDvpe0euOw5GKaIUnN2Whbu1TsIJwAmyj5H55ThpDrI-ww2ti5hui/4jf/rRAAY4zPTdmqrjed8IEfSA/h4/h001.dLi7Jul0MZXLKBk-0Zd-fsyuEerjQbfrFlRDCM52gvo) with e-commerce giants Shopee and[ Shopify](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ1SiEr6YyTEGgT9S4FsAt4mqhTw3Xu0Lea1IQsa079VWUh4Ma7Vl4pe16Oq3gcldrIsuCtXmTV0nPMPe1XBJBdVm1-1FBY8GLrEBFQPEfH4wX5cVbdI7TQpnz9DNvW6dnQKl9mUoOLQr2mrWe5KhLtRHqzne8JYtLWyfzp6isu9FfR5FWsA-a4Tjo0Kmqbm4OQVYIocAp-RJ9_wHKnlRjOmlo6MUmx5FllOD9SaUuZcIlZcXI8tciL7y0UpOCHDWWPPxjyzA_g2eZq_IJn_cs_Us9hbEEVN4cgtOoZbm8wZnCci8kkdXET3Fu_DijadFIGvJMe2Pi0Wx4m2amv0jwv4git2MpNW2WC1SMbNNuvZ/4jf/rRAAY4zPTdmqrjed8IEfSA/h5/h001.DFQpvn0mH1J0gDudSvRXdqhti15TO8wPFpryOJOfrz8), while Google and Perplexity offered free tools across India. These moves capture structured consumer queries, product behaviors and transactional data that reveal how people actually think and shop.

The[ Shopify integration](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeMERnHdSYNZbza9O0MMWnwSa5bGmHJjqOw1zEv_EtaDkIJDLg0imIBzTbB3p7aT3UEURBpOjryrXBXGFpwJgw5zpqU_56ANcyojV5xRPVAo--XnR8J4hjaoWtzGoqaIVmyUvyEbXnnl96lELWl2eicohGveTA5MeVii_YGoIKxHNwKKDLpGc9EzODHPlwwcqqHoAUcsvoYh-dX323fQOs97SCs02MfaYnm1xj-vjaonoQSEtqnUL_z1itIIo5a3kb5UazNkzhZ_zqzDN8nHcDYO7s4em_PE2-mHMSJpLkC3-oOmB1oK1M9UxHBp-57YY4vqYJy5WopytzLdCfRvRf4OUatfcAYB0y2Wtm43ITrV9UBPfbUg29qzWFf_83A_SYA/4jf/rRAAY4zPTdmqrjed8IEfSA/h6/h001.iCuMXLUN7FxS_v0q19D9fBmYjTbxCib-w0AorEkR5jQ) exemplifies this strategy perfectly. Code strings in ChatGPT's web bundle show ""buy\_now"" buttons and ""shopify\_checkout\_url"" parameters that enable purchases within conversations. The commission revenue matters less than behavioral data generated when users shop through natural language.

[Shutterstock transformed](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeDXW6yRgDX6RF-Pu4NY2j73pPpph6jQwO1OO6xjm0__S-X39CzjlYxpnAPq6xWHbYL3IJFbZNNgubMQ3VHS7L1UOI3wR3bK_qyo4LAcSxX6QAsLi2QmeFImVFt5hLlGwWnmBHfM_Do2Sp3991ZwoCWHREjZlJmjcdiEHfrijTBI8WOMfiE-Go4hmUKnueHcXpRxo2nWzDZ5XFM4_s6xKAPHOa-qbK1vJVQDURllJHWort2-dkS0htiNQeXCnrftxd01HsZPUJf453oLwqOZD2RDe_4bhHppZNb4UgiIyE4TKK_4CCxlmej1Z6UWYstF28g/4jf/rRAAY4zPTdmqrjed8IEfSA/h7/h001.3hPzHU2g76fqY4nKqB13NEy-QG19X3Ib8LtYma2tKqw) from stock photos to an AI training data goldmine, generating $104 million in 2023 from partnerships with Meta, OpenAI and Apple. The company projects $250 million in AI licensing by 2027. Meanwhile,[ Meta invested $14.8 billion](https://link.mail.beehiiv.com/ss/c/u001.MDn7gO0R2GYySjUIwjh9Js6oT5U4hgDAOxDhpJhvoR3F9xKdeECWA6NmfyqdmZF3Bv9s2Sm8Naa7USbEVMUAAg-T5wJnYg_vTeSMwHu-dUrFXOfVp82fXJ-aXUxDGTE6POGuVDEWTXTY0Pg2J83h0DNLyBRVPOGhE7vibmXl35JsXJZ19X-6YyX4BhqF2HeduitwRjmUrX7TrduF4ufFFLThtcKrPO7i3r5iSZ40lFnAozeYOdwRgDfOo1gkn_we4P_eipgM1z0mqrdEOFe5JHThhiYGlWKUU-xXU4DOwrccLH7ZmbbpHiwP0bHB6egC/4jf/rRAAY4zPTdmqrjed8IEfSA/h8/h001.fbH3tLrCvOZm2_cdt8Pyw_4opSEGtZyZN2wC5Q2uk4g) for a 49% stake in Scale AI, but bootstrapped competitor[ Surge AI quietly hit $1 billion in revenue](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePqh7nv5OWFdjJgtbDkOWCHWp0lzujwDeHvIEXg7pIJlj5Jpypp9JqkPAZxdr433JEmbVw43AgO07wlrW2pLMvBe6-0L3l-SLQ5mY13WnPxAqP8LAhErWlUsM6731PR1UDa87m0IN_0Tkhmxmy5XDYCL9MTDGhlW3-x1X-fRI8KvZY-88HblGtjL3riR1-o_O4lfHcaHcUMCIF6vf423zBpzbHXwi5PV7dJoRuc_cSZUGuFLMUttpssxdpEjmCWdaJ39XdogzcZsrav4CepBJkM5YSvW7MLMJiL2pqaIXKE2evdN3lxSDmqLVtbXHNEjLZJ1MDF0kOTsJdobR-BUPa8/4jf/rRAAY4zPTdmqrjed8IEfSA/h9/h001.IiClM_iRqfeVOzwsJw2VktnrSoE-K7zmfJ6jxnI4diM) versus Scale's $870 million â€” without raising venture capital.

Chinese AI drug discovery companies demonstrate how geographic data advantages create competitive moats. They landed multibillion-dollar deals with AstraZeneca, Pfizer and Sanofi partly because they access[ health data covering 600 million people](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6ldBU5rZfR-x_DT-h86FOekgMUOxT1OCMIGC3ysdyJWyJdcicJ08KpPMqk2ewsu40mmIhs5gMxxip6OPfposOzbWcgUEPsZ_TNfGI4dR7U_ioB2N3ZSTWB_xYSQ7u1rAgBp49Mn466VZMjcNhVcPj-1rITQ0X_dWQXzyCbPy0LHIuF_iIaofrfh2N-pdS4jU4uBwb4SFQyNd8pGW6Rih6egN2EgY7yabH2JeeRjvIJpJ7w23-nbTXU4Q1EQN0T9Uo1pQg6buzmZrSgiwsOjr4RJydSH44v7Ng8EHXv0uWrW0p/4jf/rRAAY4zPTdmqrjed8IEfSA/h10/h001.wIPpL-SxBXOQqW0oRJBgh4YgtL_inVU0shtnjJi2334) through the national insurance system. Copyright lawsuits and[ FTC warnings](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeEeItR2VYQZSpafZCrGkBv2dKC0CNHUoEp6ih_aPpRGOh9DtHNkUG3Jb1pBG5gepaZB2ZOTcA8lqR1NbAQdrCP6YQ1oZvXPDx8cMYL43vtlpryV8oXFmJkRHMlO8Kozehw4uFum2JIHI-G2jSZu9WMATedDiZpDN81-Hwmlv_TgAV7lG8Nb6eF5zcgouxe3yJO1r7c3YNw-diaMIe4hBngjOV_mMz7mSI8sUrqVy0KIzWwDp7kVRtAaLr4LtZgsZo1AbrktXxmMMAP7JJYgxFQ_No0FwuOZ4ISDK7hnRVOXgFlrMbO7DljsNJs_a4pxFUGR4x2Sh5cKucpdn83MV_wQKrXH3z0nQsbhGkjISVt03JcDOjp1XCrG1l7h1AMuV4A/4jf/rRAAY4zPTdmqrjed8IEfSA/h11/h001.j00PGm_I4cYkzdzFZ2eyjZEiSqfkxgU2XdIXVD3nb6E) about partnership risks make unauthorized scraping increasingly dangerous.

# ðŸŽ­ Musk doubles down on anime marketing for Grok despite fan backlash

https://preview.redd.it/jh6n0eiehtlf1.png?width=1456&format=png&auto=webp&s=4f24f2d74e06248a59d853965c5aca66844ecc81

Elon Musk has intensified his promotion of Grok's anime companions in recent weeks, regularly reposting sexualized AI-generated content despite growing criticism from his own supporters. The world's richest man has been showcasing user-created animations featuring[ Grok's ""Ani"" character](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeDbuOqSwz02Rtkevd0iXq286mhepL2kgiiFeP_Rw1h0K1Zk9DRLjgshvQKt450JPzTl7VdDVOyDIrPigM5ejZ-CW-SSkfE_azHs3FjWUJPkoeR89S_KmQKJYXkSWEFdEdeUU3SZcjVkRnewGEPd8M9Y9Z9TKR4LbkI4ws1egGgfg5X2ofBSPIXdsiJ1o_3IEtHGow078kWturQJ4228sypD0iqMrRDWVBP5xGhCa01n6JOcRp3MKRIVRiTjkBHZ9djy7h2qF3SL3Dj2tuWoAflNdtcGBV_cLV9uADOxOxvaecIjc5Hn-jBZRaMhPT-Uu40Z1bPE_cdvFCGo194n_T7A/4jf/rRAAY4zPTdmqrjed8IEfSA/h14/h001.5WXD1NdVEFGtkAN9t_QByt9Gffs9P548A6bcc1jp1fw) and other anime-style women, prompting followers to tell him to ""stop gooning to AI anime and [take us to Mars.](https://link.mail.beehiiv.com/ss/c/u001.7zYFXt5AA3Px2NyJbPz6hG-edTf1cZFYJ6pkEKF6Mca9BirXOlb9tDsF-gBoWmy9OOlElsUOKqYmBCofkOhffSAu5RA-2B8IS8MRsQHyJ5KaROLeMstuwsM_BT7EdCy4PCLiSqofoR3JoFcukKEPTr1fxJJ5MriVBcQ4l4C4fBr7x4HvCvVlBLhWpgWYUGYe43jQuMUh_YBUM_tpQyFMn9I5DvMFVhy8bHcLwKJxFUiSVNhfUjpM5mpGIKCeuKy-sbJwqFicBmI5yESmVDCIkbfZsSbEzdn9jCpHs_wD2HIYxmoYTwM_NEBAh8bzd14x/4jf/rRAAY4zPTdmqrjed8IEfSA/h15/h001.J8yMOlC_7n_WfbyH2VC04OkyXLCnSNBMRxv6h1tIdh8)""



Recent examples of Musk's promotional activity include:

* Reposting an animation of a topless woman with ""blinking stars and swirling galaxies""
* Sharing a ""stunning Colombian woman"" with ""golden tan"" in tribal leather next to a robotic dinosaur
* Promoting a Simple Minds music video featuring anime characters in ""skintight spacesuits""
* Responding to Ani videos with ""good morning"" messages and heart-eye emojis

Musk deleted one post showing Ani dancing in underwear after supporters said the character looked like a ""13 year old in lingerie."" The posting behavior has led some to openly question whether he fetishizes the virtual characters.

The marketing push represents a shift since Musk's departure from the White House, where he previously focused on far-right politics.

Some fans have adapted by using anime characters to hold signs and ask technical questions about [Tesla updates](https://link.mail.beehiiv.com/ss/c/u001.7zYFXt5AA3Px2NyJbPz6hKxrINeW1hT1t6h1hEuz8awO9oReozVtI2Xl7CA6U7GZbcPebN0_dbyOmZvIzQHW7r0OVEX7J3pGJFBjvZHocjUQXrm6qsBTzEZNyq01ymwqV_QjTWmPIvabl-Ypf8qFYHRIxcic6EdeksiFqLAetER3ZaeuO0l9Nmgqyz7WtwQ3h9WJAzx01TC_KcsKt5znUG4sYzTY0_1Anj-nyLWvPlql72ecwGb5RpsEFXfq3MV-i8pT0YG-vn_LIG954aB5K-eH2dkTeeZ2fr3E8hzbC3scKI6HG-Q4BIStVpO2y_Iz/4jf/rRAAY4zPTdmqrjed8IEfSA/h17/h001.00yEXIwjqWfMc8AjqSWyIjBLrPQg_rNz2FHv712SrjM) and SpaceX development. ""Smart, Elon will definitely see this,"" one Tesla influencer noted.

[Super Grok subscribers pay $30 monthly](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJadOIfjjix3ezguXcweEig9QbbUkZsfG1GQoRzclZi2vK_6JXDpg2O23Vo4yMCqsLLM27W8i-b4keo__biymcP1DDi96Cws5Sn7ZaOV4jEsbk3m9veU9T3k-fHBSetFV0WFQLGAr8ye5iJ9_qjvfK5l1Ga28ud3eJVYfAB2jimo_oz2z1W823U_BOCF4HxO4ltdBu4CcffzDAYtBefvuMX80M3sWSPMQeDaI1tgyHZzQuVv8CPRa7H0ZNKSV_yxh0putMC83egzKDeknd2PY1kWhCI4IliEiOnQ5q4LaCoR49j-8XkT9oJVOB1xSXegFIQ13nEvOq8EwMWeJMjyQatfxT0gqgIWgqwb5TtOS_frN6bLEyMqdEtCczxnWli55A/4jf/rRAAY4zPTdmqrjed8IEfSA/h18/h001.fkA38W9lZsA-eg78upPoX49q2TPrj6ndXZYMDR0lIEw) for access to Ani's explicit features, though whether this approach attracts mainstream users remains unclear.

# âš–ï¸ AI deadbots move from advocacy to courtrooms as $80B industry emerges

AI avatars of deceased people are increasingly appearing in high-stakes legal and advocacy settings, creating what researchers call[ ""powerful rhetoric"" that taps into ""emotional longing and vulnerability.""](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePp3VwASvjkcULaOwA1I4XO49Ld4n64UTT355Tz4AzEJc52wsasXwL0ib9J5x-d7fkWYcUbTCtbh0jrzOyCdmhCMqlG2B8qSKwVPPRIpHG1rbfTgTztPDA6P3AXQ3qH_jCUJi8wXRbQUcRcGc3MAoPYIG6fmLRluE9KP13qmcfuTfgkTb57RgStVH1ToYSGFRFE4fafgYxYlXVCu9Buuzz1G9aiyJ9h9QR9JTMEV6P_C7yU3eurill1SB6xn4-dpTwokSpLntiTiRFSSoq2OJ8rr85y96euS6BNpoHePmOEJoC9Dg5j_G2sWf3DnSh9Iqu4LGEJzGSfBEoBR8YWJgjM/4jf/rRAAY4zPTdmqrjed8IEfSA/h23/h001.8aEBKtR6iYgTE1WN7jMAiuMKdeQDAgSlUQbFO3PZONE) The technology has moved from experimental to practical applications with significant real-world consequences.

Recent prominent cases include:

* Joaquin Oliver, killed in the 2018 Parkland shooting, appeared as a beanie-wearing AI avatar advocating for gun control in a July interview with journalist Jim Acosta
* Chris Pelkey, victim of a road rage incident, delivered an[ AI-generated victim impact statement](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeIsGcLOPqlsm5QDSepI63NeZvMmKj-Q3aTSq60qWpMNNPS2hZJ-ak2Vy_LsvkghouhXlhQcBUM2J952aOBKDAUVeo2-bT-ZhRlzY45PAklwPdddQ_XKosgzGXTVy2p1CANbl2Okl7-AqHYg9xMS7ErU_VSV_NIezwlagCILz45ROjXZnett2LRArZVA1b6HWPbfPXsUmeicterg_RlBSahsvzKfMoC3vaYFWClQqY5L0eHQQFxw4LQKZHEXRuxH6keBC4uALluft91irJ_0tBczSBu4rO-t5ABySGWRCShOItGdykSaYNTyshx8HaI2-3A/4jf/rRAAY4zPTdmqrjed8IEfSA/h24/h001.YoQsACa3KEZjxMbnDlIh22pkPN5RS3qXWRPi9grUNtg) during his killer's sentencing in May
* The judge in Pelkey's case called the AI statement ""genuine"" before handing down the maximum sentence

The digital afterlife industry is expected to quadruple to nearly $80 billion over the next decade, driven largely by these AI ""deadbots."" Creating convincing deepfakes has become increasingly accessible with publicly available AI tools, sparking an arms race in detection technology.

Companies like[ Reality Defender, which raised $15 million](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOcmnNiBXRvRuz2nSGHLH9nVxswn-exvNTSj1R-Bp50UH0BLjIoNiG3rSvYwMKDN8kNoMrTUxRh45R9LvOFEBfaXOzy3nd1-msNFhsAuD7eydhnuJESnaxFw7jr_yzmUnHgpJNYkwUDHKB1SzW-FJ-n1WcEkvotCFzpHU6bZaObUgta3eTId_myrHYg3Hc5qFCZIu1nptsCjJrxG5QUBMyVMUn2RqwhZ6sdzr8toGR3QnA549UEBQPDdCTa8UvmTBA9peDoXCr1apb5GZ8-mJKnag4lXW7vmd2KbIffW1sh5tK0u5GW_Rbsm_KkgExnYtfTSPyHiGgssIOwNb5-n6WOp/4jf/rRAAY4zPTdmqrjed8IEfSA/h25/h001.4cOyYfs_IfoY16imqzOJToAskQHj1byf-aO6A3DHtMU) and received[ strategic investment from Accenture](https://link.mail.beehiiv.com/ss/c/u001.929jZiiRht9qZPYC5Z_-tWLmY2zMxYyF9Od8IKmEEfZstPdrPFOO8JXqd_fqRBFpOhJGJyUDWOwhqOVqSHBMteHPf2gvEmUM34fghlfLH6IzM65_EBCiZXIUG4_58BBr_AIM6E3jLktiNNbJBbwodZx6R7z4H6E2lz14fUeNGLLwRDwJNW0_ymh4kIS_ZJj_qqFZHFOjXbvXFNFbnGpE4IAOq564oXpPGhz7m27P06Aianl750BUDK9Qm33noT0qUJ05K_rPy4HcA5LhhzfJB81M24OcLMINRBcSQwZSTNrNRwCbHvg5plxWtkLFVkNwprk_u4lRbP6Leug6c01-x7_EQoiaSQyacSla4rWPRC_pLyEqqd9uuAutB_-0dgM6TpRwiYWc-BqRHPTPZ07vz0N9R6VYNX-MmOd2pXeAlCo/4jf/rRAAY4zPTdmqrjed8IEfSA/h26/h001.Lb4z0728zg7g9w-wn0GA2CLartwb8DMnkXT4nKQTx48), offer real-time deepfake detection across audio, video, images and text. The broader deepfake detection market was valued at $3.86 billion in 2020.

We've previously covered[ Department of Homeland Security warnings](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQEiEzr404Jan1kjNGrzGVG4tEOW1IFfZZcBAMfgOW5YnHCTh-tmRuQY12UQFwGHIx69K8YOPflOHM27inzSD6hKYZKtyh7qve5EWhEvcLFgtf0-Y5EMFwzHBvEYpH1ZbJGsIjzGCrtsMl35Onkh-baTMbM9S5UwiziTPF1ld4wc_CQugFGGcNcJ4vHDyBXzDCzTbQiGe74wRpDfHwyOMXigrUoVOey0DgYdWfSay7yO3NHMH1AxC7tZsKNauDmn0ZD_QzcJl_XMguYcKLX2PStNFB-DPI8S5k6G2jKsZQEq5z5Kq81CdZfw2CUC-dOTQwBdSAHW367xr6hnBacLg6UxgFMWzTXYkOYxJYxfKazZT4OcY2XmAP9R-acYZB8OP9WKAFtxh0bgnfjcjpYa5D5gCF6gw_CKrlltzu9Z9N-lRwx8-KpfhxhK4UGGPQ94dPhC8Ycdp4GxE1lKl40NqUBqFnqjW4vgXRiI6tySm-C2Peo8-tF0kCdbHFLpYFv-hpm0DWWPJYVl_Z69a9e_4QY8AFQr4q0QcjRmq6-GL11uSnK_oWxk9_PDkL0N0PtOel51TSTcMPXvDmp4e-THOFo9sBZxVkXfh74OxMLJVpC2ukosye-adfM9anDjAEstK5JqrL0rbHlOrrRFgwNmaMTXwWwD1mSb4x1fPdfAJOZePtz-iouG8eZZcJcHREIs8S3Y4EPC5ndgx-z5yez_oAhIfPagzZSMJKGhM-pHVZ_vx9-PwhO0m6661Fc3DopXqaGq6mVEj4NesxafuJCLVJwSL72YhgxiWwh530Pm0GnyRW2LvONGucxMr6dObjPkCpLFkLF3WWrnY46LOvAh7zhD/4jf/rRAAY4zPTdmqrjed8IEfSA/h27/h001.ivLJVyZr1yD5j9usTG8pn34L5TnpEBWUQyj-TYt9-BE) about synthetic content threats. The emergence of deadbots in courtrooms represents a new frontier where the stakes extend beyond fraud to fundamental questions about justice and authenticity.

Legal experts see both promise and peril. Arizona State University law professor[ Gary Marchant told NPR](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePp3VwASvjkcULaOwA1I4XP5ZBc4pkDoHC3wO3XtFw3q8BItHjkBXrP6bvd2jd12MZ-jH6JhjwlFOmkndCM-krH-e3jKFaMqGeDtpFt5A0X12sReCTpD0ghj8ATdFqEmXaOteKafBNUS5ri9bIKsDhVsFNZK8m5NgQSsciqofs3qhw5vwz529KJv33odby5SLiGEVuJUzekiJ_zwPdOXrB0jnAK77sgiZetML1AuTRYhTig-83oIrBVrn_TMsR7rBxZ7leSc_x8Rk6_6mSTXd4PO06fb3diFrkMoJJ58LzKqbvoBNArKPv3GFbgehVC1qA/4jf/rRAAY4zPTdmqrjed8IEfSA/h28/h001.ZIfmhxxrOLyaVeIlndgPHGFTB5fxHEushiuv_tpSGwA) that victim impact statements are ""probably the least objectionable use of AI to create false videos,"" but warns that ""many attempts will be much more malevolent.""



# What Else Happened in AI on August 28th 2025?

**China** is reportedly [**aiming**](https://link.mail.beehiiv.com/ss/c/u001._0t8xG8JNlwyP-1PkfwroGGN_zJhRdtlex01NJxJA30Vpb8XxEJRLsffPmn_BcAy7ldN29WCiqIKvoKNVAmcNfCPBwX_uB6cmAX87gIiub3WGV8RfpxYqiwSZHFztWRxWYTLYB4iHXvPxzbtBZ0OaVpHrGo1l5hpsiKTAoAU8ub0HiXR9zzsUuJ9FXxwX_NBdt4KCxyhIoiV1j1-29Xl9ZSLkUrQR1YkVl4DnZCqFDvRFY0UmmoGyamVwr72s7nJdQDG3y4bhc_I3eAVesxgkkLYbncPrhSbgb-U3WepNkBOPWNKSC6EU1P1qNUeFflTLLz3A_pBPYC6-Cv43GPttT8TueanxYKfKrHqxGcvfFU/4jf/Aw_TaB7LS16U_cedJGEgBw/h28/h001.mxMfjl57PRCB208yNReNt7E66T6qZvoIuT7h3mha6Ks) to triple its production of AI chips in the next year to reduce the need for Nvidia chips in the wake of U.S. export controls.

**OpenAI** [**published**](https://link.mail.beehiiv.com/ss/c/u001.eCbm_1zon7G0lMoXTECWa-IUY9yqSc2cx0km5OJXo-PIt6WwNtlQ_vAelHySJdyrJTr_tLG_FBxD_9nZiPAffxo30bYiGDtmHa0Fs-cM3Iw1KDpONgx6dkdlqWCaO-glCU4gOXtaJEHyXlwvsHmq6tRNKgkz5rbplxwYeO-W5hRVBXr7whBeX6v2wsVwLNOY6SksmJZ95iFCj56Jh_CwQ69lc7WOIxwL6zO2DM5EKQsEZjv8UM8i6a3cVIG09b1WYfRC1peXiI9nnqkT3SRViidmmdSz7NmL7_ogNHC5cRI/4jf/Aw_TaB7LS16U_cedJGEgBw/h29/h001.Y8mA7HqTUgME0q0BdAHk2b82d_sj1Rlu9lUmD748ySc) a new blog detailing additional safety measures on the heels of a [**lawsuit**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfz3sRxipJonf99jMXlaKZFfyUwsmUVu8R_oDjILhwysf-JnggW7TXlzM5Q4FMtyIEcih0To2nGSB0EvWj0P7CQD0JKNYx-k6N2QPdVN4DFDsXX-Re_g9WNP1WY5hcqSwA_lzX9Iz9V-k5-AntqTQIBKLDZejEbWOvdzvanyNutnRmWCL8EYO0A91_vVkNqLeYfX00FY_edPJ9vRo4DZdrsxN1bxexxne02UEVWZRcMTdUict-n_vbLz-iJmdDMa0LWsB3cWFG117kF2tMpiB3eQ/4jf/Aw_TaB7LS16U_cedJGEgBw/h30/h001.mLdC9ID7J_OcXb3bPXJ4pYYO7YGXQ_GWA4E__G0bzu0) from parents alleging the AI assisted in their sonâ€™s suicide.

**Anthropic** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3zrLNx0QBchAmynJ0QdIgScAwzTpuL0j6Zoqhyd5aot5BY6BMce_pclt_xcBcQe6Y_7faN00UzafaDRffDlOx7-6UKMjmECe6PRwetUAAWTv43z3awVkDRIEYsv9sD77fscd8kPb2fiK3O7LXbC8fce8Vzgq4R43ETkqRhzuEQRz0-joWJwwLiBjsQT3zQGt4NiX7KX2alW6kImDYfBUKjTflo3WtHChNzgcf6KA9xyc8C7k71AfOUlowicsjl0hNEVbk0ifrakZh2B9_rFc85CxonaZkJy1tZfKrK2fhsmAA/4jf/Aw_TaB7LS16U_cedJGEgBw/h31/h001.JfSuOKy54FjoJCjRY-aQTetV9OkHKjilGdLUkv0jgYk) the Anthropic National Security and Public Sector Advisory Council, focused on accelerating AI across the public sector.

**Google** is [**rolling out**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf_Ncyp-UK87zwpzD-5tZnbsEWTkTXGuQTf32T9KUk8dd83BWskivVtifiQKSSsjHIyH_QInwLU0VXH6En22IHRnspC1noaKp-SkKUO8S6wHDCZLQxmsEUGGab_GTI3s8OI_zy7mmrU_OOHzcfGJNxWF9lbgF-NY8e2FSlG-DaMKpHj0ONl2nEBPagnuPRzxOd-vkfU3QWXzy71ZcrR5_liwuLHDJsFaquSjYz4NvG9iILgIK_Zw9AYJja_abzv1XhC6xXh5bQiDtzopLQ70yVeD1oNl5MShKd7Uwzvd3G8kHenoBBtFwwQeBG7ddz8537Q/4jf/Aw_TaB7LS16U_cedJGEgBw/h32/h001.TAyouqJgXUd_J7eEu_DR_WM0bDbfpNtjAY51PWTn0bE) new features to its Vids AI video editing platform, including image-to-video capabilities, AI avatars, automatic transcript trimming, and more.

**Nous Research** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmfxNLCkvzi8GLwA4A3n4ZJ7ua0RcYlPlnErgArDV2cGbuArpgI4ifBzj7Sj72RZgcQTrJMwWNuMCgv42lYz9mHu8e2aoVx3qGyWK1buMEWzaMmNzemO9msUh-r6TAM9CDuWkyz8u9uN5WEi1uv_OsTBrC0eZkRgBLcbjxYQy8A7eDk_7uEKUio7qITshLkdjQqbkThsXavcO-Qhw2KsJzBQoMbXsQAwiqf0o4LuGKGBJm/4jf/Aw_TaB7LS16U_cedJGEgBw/h33/h001.tIrgok6nx5NyKKD8Xv7ui_onjwHXfCt-fvAbvknvLeQ) Hermes 4, a family of open-weight, hybrid reasoning models designed to be neutral and avoid sycophancy.

**A group of authors** [**settled**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijLR4QGHmNJ8VgjcXV4ogfXF8zZK0PtdHK2uEvfPCA_KeznnRUHAjz6OEm3dCGToAbMlSfqpZkIdiA34iJfeiX0J6XkZEKSbRk2I7zrljHfGcJQHxGbXUpzVm0rH3nW-5PkHvdfCazoHh6p_cW6b77X912l-BD1vsONFMYP0KGXKkTw11xzGBgfcHwWwLqU6BuhaSx-Rtyek1QyymdTzPbhu3AdgnzT7_FnHe1mWuw0ARgJjUPSc4gSA-2G0TrvHimM7b4xg-BxkFzMBlaKdlmoVmVNWM_YuShfP-ak9mQuGPBlG0th5diiNgVDa2fS21wg/4jf/Aw_TaB7LS16U_cedJGEgBw/h34/h001.UPRC3Q1N-Yjsej6FGbmJb6axDA103VfukEFITwNfjow) their lawsuit against Anthropic, coming after the court ruled in June that the companyâ€™s use of books for training was fair use.

Vercel triples [valuation to $9b](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeND5z3fn2eA5HMQUmmQlReaKu1tOr8zLo4GQnPW2OaCh76veVHgJRXgp_Qfzi0CEd55d2UmaF4gul25xgFJ7KOj-63396rw4H04MZs0hKyzybVoYpUJ2uMKNRX7WryO0U4raa9FVowmD-e3JyMqz_11jWZE7WAk99mBrQ5wPQY6hjdXReNLqU4hondKMANHVI_YRDVjx_VZnmAd3mgZJzChFXKU-vitd1qZzfr5DpnEfmMjh-d65sdkPjldJz9A5meJBoFeXyg2WWYgR3uRKlMfJ7gLgq5UHRwUlqzsab1MtLGRfAp8TwotsYwydUdPROsE7ccVv2L6a8C9RbVzdLuHE_QX4GeTAasgKqF5e47R8/4jf/rRAAY4zPTdmqrjed8IEfSA/h29/h001.voe0quOKg7pqHdaKtjlerfQbm-zgBhH5mmkkz_IOfVE) with Accel investment

â€˜Vibe-hackingâ€™ is now a [top AI threat](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeMD43dek_BQ0n1EigGCY7cjFaK7lV2EzRh3r0oTOYND7-XnzWnMcZDvlVcp91f1huQo4dwcoIK9khRALLVTSD53pj_tjSQTen3efMZpIwM2PDGfwp_o177UiPQ6qkGiXeYDyFSOiuze3fgbaec82uHT_xHNwPBw1eL0aAsuBR-tbd1emxB1hEJqnSkvRibY86HYinKXzcY9S2cOCDj0M-NyQCHW18kc5wZm41N_R37GTcZEhcSU3Wr3vC0hIK54amu-P-N5dJ-D0g_lLKeCCzbjDDg9Ao0mExSdKJd3kE5c2MWqTxY8NCH_21Ls2RKCIdMO-af1x1-t5SUMa0tGndvpNY7OoE1fYiSh75gxLUMWV-WMDqS4sYvL3AwuECDSSvg/4jf/rRAAY4zPTdmqrjed8IEfSA/h30/h001.fzTM6HjiZJXkevkBtj6wXVRYD2iZB1SOXKw6oA6SJS0)

China seeks to [triple output of AI chips](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeGHVgT-Z6Sad_XSwcUuu7Kf7aSuKrmdXagGo71GZRodwV0sjcAn4BXpd97C78VM0urBWp57rdlnB9yMaYyN3xVEFzvwLPmI_UjP85m2Zeu-_7qGO0kqzFfhcEpk6Wzfj9VnUkfpvuDGuEjDO_njSJT-we5OZGE-_WM1xKCtzoCEvrCnKm-VNLn2fZIEPtmJ7voX_w92q90g5Rgwu1zG-BJYk9y_kXc3d62QzKBQGnzRPdq3HUU0o7HTS_ymWK_Qs2uDpwK122P7SqETazf-mMRe9C1j61zdYYoCt4SYE4EEh/4jf/rRAAY4zPTdmqrjed8IEfSA/h31/h001.O6zcMz0oQswzmeVvxz0_ab00AV73YPHOqvHFxk-RB48) in race with the US

Researchers are [already leaving Meta](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJZhQAXVvetCccu91EM4RKT-ulKvyzAx1up36uXFc1CfHS05Nt3MOC4q1lNgrvsYLjeAvWk6CH19SBnxYMGehMkVgmKIIscNanUsr8f4Rxlp8czgxyzgtGQ86LMfigTH9KqycnGamTrWx5UYsMPYk3HVGN4AO_KpGzrFOwc3WixVDVs97Y2roxs8zIosK9LPcNNOKsLWfVJlKP0VGWO6J4RMkwjSgK-J043mnyY_VXsmfFSKq6YyNpWOs_yHVJz4mgTYLTzCSw9WJfWhMTI5J2Cv1XkfOWMVlM0BcQgh6m7T5ysKtKYZ8BzmiHwzYUt9kw/4jf/rRAAY4zPTdmqrjed8IEfSA/h32/h001.KP3Ddv8IfcGAsnY3CQHehRqxE6GBLnDXsOxI4Kpp4G0)â€™s new Superintelligence Lab

The Mongolian startup defying [Big Tech with its own LLM](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6ldBU5rZfR-x_DT-h86FOekhp8gI2IqlUgicTIRs6LfadJsWnlSdLPkmFsr9GXBu33m48gffXbpNCx1Qc729bVy6eUqs7CLxibSsQQBgWPhRQxL9CigOoZTjvs4F0b66Oy1_22k1Y78BgwAv5slO73vw_8D7bxBkQNhVUf2ZzN8O8ePqpfvEbiUp35Vw8NejgLf8DWby55LekSyEXFM4-TTd_-3Y0BQ3UJVcEgGnHHCiVM80OuopiqxsuhOOgzFI_uKjVlLr3fVongRi0QtL5aWXrQ07lGUgvqUXpgMl5PrZ2/4jf/rRAAY4zPTdmqrjed8IEfSA/h33/h001.XCrSDPUa26sq-wrOU6oGSaqMB7wSPtN-a4pwgumJt",,,,,,,
r_1n2cb7q,reddit,Fragrant-Dog-3706,2025-08-28T13:31:21+00:00,"Need thousands of schemas for deep learning model training


building a model and need massive amounts of structured schemas for training data. primarily focused on financial and retail domains but need vast collections from any sector. looking for thousands of different schema types - json, xml, database schemas, api responses, etc. anyone know good sources for bulk schema collections? open to paid resources that have serious scale.",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n2cb7q/need_thousands_of_schemas_for_deep_learning_model/,r_1n2cb7q,,,
r_1n2bd8x,reddit,mokumkiwi,2025-08-28T12:50:24+00:00,"Models are only as good as their training data. How do you ground yours in verifiable research?
Hey everyone,

I'm part of a team of researchers and developers working on a solution to a problem many of us building in AI face: grounding AI outputs with trustworthy information. It's a huge challenge to prevent models from hallucinating, especially when you need them to cite facts from academic research.

We've been approaching this by building an API that gives direct, programmatic access to a massive corpus of peer-reviewed papers. The idea is to provide a way for your applications to pull verified academic content directly into their context window. We spent days building our own vector databases so we could control everything \[happy to talk about some best practices here if anyone is interested\].

We've already seen some great results within finance use cases, where our API helps ground AI agents in auditable, real-time data. Now, we're exploring new verticals and suspect we could have the highest impact in applications and research being built in the hard sciences, and it's frankly something we're just more interested in.

We'd love to hear from you and see what we could cook up together. We're looking for a few builders or some eager users to work with us and find the best use cases for something like this in the hard sciences.

Cheers",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n2bd8x/models_are_only_as_good_as_their_training_data/,r_1n2bd8x,,,
r_1n2avgd,reddit,Neat_Chapter_9055,2025-08-28T12:27:34+00:00,"domo image to video vs runway motion brush which one felt more natural
so i had this static art of a dragon just sitting in a folder. iâ€™d been meaning to make it move somehow and i thought why not try out **domo image to video**. i uploaded it, typed â€œdragon flying over mountains fire trail sky turning redâ€ and waited. the result honestly shocked me. it actually looked like a short clip from an indie anime. not perfect of course, the wings kinda jittered, but still way better than expected from just one click.

then i opened **runway gen2 motion brush** and oh man itâ€™s a different experience. [runway](https://runwayml.com/?utm_source=google&utm_medium=sem&utm_campaign=branded&utm_content=ad&gad_source=1&gad_campaignid=22452532935&gbraid=0AAAAABiY0sPFCDXATR4BZ94rXhLD00mAa&gclid=EAIaIQobChMIoqiqlsCtjwMVdukWBR30OwrFEAAYAiAAEgJT3vD_BwE) gives you more control cause u literally paint where motion goes, but it also means more room to mess up. i tried painting the wings and tail movement but it looked stiff, like the dragon was a cardboard cutout on strings. it took like 4 tries just to make it not embarrassing. i get why ppl love the precision, but itâ€™s exhausting if u just wanna experiment.

i also tested [kaiber](https://www.kaiber.ai/superstudio/) cause ppl always compare it for music visuals. kaiber gave me a more stylized dragon, like it belonged in a lo-fi hip hop music video. cool vibe but not what i was aiming for.

the absolute clutch factor for [domo](https://www.domoai.app/home?via=081621AUG) was **relax mode unlimited**. i kept regenerating like 12 diff dragon flight variations without worrying about running out of credits. thatâ€™s huge cause with runway every attempt eats credits and i get hesitant to try wild prompts. domo makes it feel like a sandbox where u can just keep tossing ideas until one hits.

workflow wise, i actually thought maybe the combo could be best. like do a rough layout in runway using motion brush, then feed that clip into domoai image to video and spam variations till it smooths out. kinda like rough sketch + ai polish.

so yeah if u want surgical precision, runwayâ€™s ur tool. but if u want vibes fast, domoai wins.

anyone here already tried combining **runway + domoai image to video**? wanna know if itâ€™s actually a usable pipeline or if iâ€™m overthinking it.

  
",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n2avgd/domo_image_to_video_vs_runway_motion_brush_which/,r_1n2avgd,,,
r_1n28wsr,reddit,intermezzo25,2025-08-28T10:47:44+00:00,Next step in Machine learning and deep learning journey after the Coursera course,deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n28wsr/next_step_in_machine_learning_and_deep_learning/,r_1n28wsr,,,
r_1n28csg,reddit,External_Mushroom978,2025-08-28T10:15:07+00:00,"MiniMax implementation and training from Scratch
a simple 103M params MOE style SLM",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n28csg/minimax_implementation_and_training_from_scratch/,r_1n28csg,,,
r_1n236st,reddit,aianolytics,2025-08-28T04:48:01+00:00,"Unlocking the Full Potential of Robotics Through Expert Data Annotation
[AI in Robotics](https://preview.redd.it/h3wa9xw5ojlf1.jpg?width=1200&format=pjpg&auto=webp&s=ee77cf617fed460d722e61a11755a59fcdcd48c9)

Once confined to basic automation and repetitive motions in a controlled setting, robots are presently evolving to solve complex challenges. Traditional robots in industries used to be operated at a safe distance while performing predefined tasks within static environments.

Today, robots push their limits in unstructured, dynamic spaces, interact with people, adapt to variability, and make real-time decisions. Although the process remains automated, any misalignment could cause businesses to face extended operational pauses and financial loss.

Emerging concepts like machine learning (ML) and computer vision (CV) are critical in adopting automated systems for industrial tasks. Although industrial automation has already been implemented, it requires further tuning to minimize human intervention. Training robots to perceive and interact with their environment starts with data. This is where data annotation for robots becomes essential.

# Why Data Annotation Is the Backbone of Robotics AI

Industrial robotic arms on production lines are still developing as newer robots with improved specifications are released. They serve many purposes, such as welding, quality inspections, assembling, painting, packaging, palletizing, and material handling.

Thus, training them to understand and carry out multiple, yet specialized, tasks in various real-world conditions is necessary. This is only attainable with a substantial number of annotated examples. Such training includes annotating video or sensor datasets, demonstrating each step, including:

* \*\*Action labeling:\*\*It is the process of recognizing the various phases of a task, such as pick, move, align, and place.
* \*\*Defect Marking:\*\*Pointing out defects in objects (such as dents or scratches) so the arm can identify them.
* \*\*3D Bounding Boxes:\*\*This denotes point cloud data to distinguish between objects and improve their spatial awareness.
* \*\*Object Classification:\*\*Categorizing specified objects as wrenches, panels, crates, etc.
* \*\*Trajectory labeling:\*\*Designating the path the robotic arm should follow to optimize efficiency and avert collisions.
* \*\*Collision Event Tags:\*\*Assigning a label to sensor data when the arm encounters an obstruction.

The robot can adapt and execute accurately in uncertain production environments based on these variances. The first step in planning robotic arm automation is to define clear parameters for acceptable and unacceptable outcomes. [Robotics data annotation](https://www.anolytics.ai/solutions/robotics/) supplies the labeled examples needed to establish these parameters.

# The Complexity of Manufacturing Data

Manufacturing environments or factory conditions are not the same, i.e., they differ in industries such as chemicals, petroleum, and food processing. For some industries, products are manufactured only after receiving a customer order or in batches or lots, with each batch undergoing a series of operations.

The complexity of the data collected makes it essential to organize, label, and annotate various items/parts for defects, size differences, and safety protocols. Moreover, different data sources demand a specialized annotation platform. These data types include high-resolution camera feeds, LIDAR point clouds, torque sensor readings, and temperature logs.

The concept of machine learning is to enable systems to learn from previous steps and data examples without the need to be programmed for every future task or action. Therefore, overcoming the data complexity is key to powering robots with daily operations.

# Precision in Annotation: Why Does It Matter?

A robotic arm uses multiple sensors to identify objects in its surroundings. ML algorithms process all this data and help them decide what to do next. High-quality annotation, such as semantic segmentation, enhances the accuracy of machine learning models by breaking down images into pixel-level categories. AI algorithms make patterns to understand different components of a smartphone by identifying the screen, camera lens, frame, screws, and ports, which enables robotic arms to assemble or repair devices with extreme precision.

For example, a misplacement of even 0.2 mm when assembling the smartphone can render an entire batch unusable. If annotations are off by that same margin, the AIâ€™s â€œaccuracyâ€ becomes irrelevant; itâ€™s learning flawed examples. Precision annotation ensures that the AI immediately detects a misaligned component and doesn't let defective items slip through.

# Human Expertise Meets Machine Learning

AI algorithms excel at pattern recognition but lack the context a seasoned mechanical engineer or quality inspector carries from years of working on the factory floor. Expert annotators add their valuable knowledge to the dataset, pointing out minor defects that untrained people might miss. Adding metadata enables the machine learning model to learn from it effectively and perform well. This human-in-the-loop approach transforms raw data into industrial-grade intelligence.

**Reducing Downtime Through AI-driven Accuracy**

Downtime is the bottleneck of productivity and efficiency. Well-trained robotics AI can spot a faulty alignment in seconds, recommend a correction, and keep production lines running. The result is swift operations, workplace safety, fewer interruptions, and significant labor cost savings.

**Real-World Applications of Robotic Arms**

Here are a few examples of how manufacturers use and employ robotic arms.

1. **Palletizing**

Robotic arms can automate the process of loading items or products onto pallets. When automated, palletizing becomes more precise, cost-effective, and predictable. Robotic arms free human employees from duties that risk bodily damage.

1. **Material Handling**

Material-handling robotic arms can help create a secure and efficient warehouse by ensuring products and materials are easily kept, accessible, and moved. Automation here means speeding up the delivery of items to clients while avoiding workplace accidents.

1. **Inspection**

A quality inspection is performed near the end of a production line. This is crucial for the manufacturing industry because unnecessary delays in identifying issues raise concerns about quality. Therefore, businesses use robots to earn profits by performing real-time inspections and applying computer vision for image recognition, thereby reducing downtime.

1. **Pick and Place**

In contemporary production and logistics environments, pick-and-place robots are preferably used. They have cutting-edge computer vision systems trained on annotated images and can rapidly and efficiently recognize objects. A robotic arm integrated with vision models can better perceive items, grip them, and transport them from one point to another, which increases the pace of commodity manufacturing and distribution.

# Conclusion

Back on the factory floor, the robotic arm moves with quiet precision, no wasted motion, and no hesitation, because it has learned from the best examples human annotations can provide. Each detection, adjustment, and flawless execution is powered by robotics data that has been carefully and expertly annotated.

In manufacturing, speed and scale mean little without accuracy. Accuracy begins long before an AI model is deployed; it starts with labeling every detail, every deviation, and every outcome with absolute precision.

[Anolytics](https://www.anolytics.ai/) that recognize these characteristics will not just automate tasks. They will elevate their entire production process into a state of continuous improvement.

In the end, robotics AI is only as smart as the data itâ€™s trained on. When the data mirrors the keen observation of a human expert, it augments automation and represents the pinnacle of manufacturing intelligence.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n236st/unlocking_the_full_potential_of_robotics_through/,r_1n236st,,,
r_1n22vl1,reddit,InevitablyOrdinary,2025-08-28T04:30:39+00:00,"Eager to learn! Exceptâ€¦
Hi yâ€™all, just a quick question. Iâ€™ve been procrastinating on learning deep learning / machine learning for the past 3 months because every time I jump in and spend time learning subjects like kaggle, andaconda, tensor.. and so forth but every time I do I get demotivated because idk if what Iâ€™m learning is used in the real world. Aka I feel like I waste time with YouTube videos/ Fast.ai/ kaggle etc . Because the info is pretty generic or feels generic. Any tips to help gain confidence in this venture for knowledge and understanding of ai? As in if thereâ€™s paid courses  that helped you gain knowledge and set of skills to use in the real world please let me know. Thank you ! ",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n22vl1/eager_to_learn_except/,r_1n22vl1,,,
r_1n21agr,reddit,PiscesAi,2025-08-28T03:07:24+00:00,NVIDIAâ€™s 4000 & 5000 series are nerfed on purpose â€” Iâ€™ve proven even a 5070 can crush with the right stack,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n21agr/nvidias_4000_5000_series_are_nerfed_on_purpose/,r_1n21agr,,,
r_1n1whvj,reddit,enoumen,2025-08-27T23:22:05+00:00,"AI Daily Rundown Aug 27 2025: ðŸ¤–Anthropic launches Claude for Chrome ðŸ—£ï¸Google Translate takes on Duolingo ðŸ›¡ï¸OpenAI adds new safeguards after teen suicide lawsuit âš ï¸ Anthropic warns hackers are now weaponizing AI ðŸƒMeta loses two AI researchers back to OpenAI ðŸŒGoogleâ€™s 2.5 Flash Image takes AI ...
# A daily Chronicle of AI Innovations August 27 2025:

Welcome AI Unraveled Listeners,

This is a new episode of the podcast ""AI Unraveled"" created & produced by Etienne Noumen, senior Engineer & passionate soccer dad from Canada.

Please like & subscribe at [Apple Podcast](https://podcasts.apple.com/us/podcast/ai-daily-rundown-aug-27-2025-anthropic-launches-claude/id1684415169?i=1000723798469).

In today's AI News,

ðŸ¤– Anthropic launches Claude for Chrome

ðŸ—£ï¸ Google Translate takes on Duolingo

ðŸ›¡ï¸ OpenAI adds new safeguards after teen suicide lawsuit

âš ï¸ Anthropic warns hackers are now weaponizing AI

ðŸƒ Meta loses two AI researchers back to OpenAI

ðŸŒ Googleâ€™s 2.5 Flash Image takes AI editing to new level

ðŸ–¥ï¸ Anthropic trials Claude for agentic browsing

ðŸ“ Anthropic reveals how teachers are using AI

Anthropic's copyright settlement reveals the real AI legal battleground

Blue Water Autonomy raises $50M for unmanned warships

Melania Trump wants kids to solve America's AI talent problem

Listen daily FREE at [https://podcasts.apple.com/us/podcast/ai-daily-rundown-aug-27-2025-anthropic-launches-claude/id1684415169?i=1000723798469](https://podcasts.apple.com/us/podcast/ai-daily-rundown-aug-27-2025-anthropic-launches-claude/id1684415169?i=1000723798469)

https://preview.redd.it/punnwq2nanlf1.png?width=3000&format=png&auto=webp&s=40ea0620da0493fc58b09cce490737a7181d9dba

# ðŸ¤– Anthropic launches Claude for Chrome

* Anthropic launched Claude for Chrome, a browser extension in a limited research preview that can navigate websites, click buttons, and fill forms to automatically handle tasks like filtering properties.
* The extension is vulnerable to a prompt injection attack, where a malicious email could instruct Claude to send your private financial emails to an attacker without your knowledge or consent.
* To combat this, the company added site-level permissions and action confirmations, and claims it reduced the prompt injection attack success rate from 23.6 percent down to 11.2 percent.

# ðŸ—£ï¸ Google Translate takes on Duolingo

* Google Translate is launching a new language practice feature that creates customized listening and speaking exercises which adapt to your skill level for learning conversational skills and vocabulary.
* A ""Live translate"" option is being added for real-time conversations, providing both audio translations and on-screen transcripts in more than 70 languages for two people speaking together.
* The live feature's AI models can identify pauses and intonations for more natural-sounding speech and use speech recognition to isolate sounds in noisy places like an airport.

# ðŸ›¡ï¸ OpenAI adds new safeguards after teen suicide lawsuit

* OpenAI is updating ChatGPT to better recognize signs of psychological distress during extended conversations, issuing explicit warnings about dangers like sleep deprivation if a user reports feeling ""invincible.""
* For users indicating a crisis, the company is adding direct links to emergency services in the US and Europe, letting them access professional help outside the platform with a single click.
* A planned parental controls feature will give guardians the ability to monitor their childrenâ€™s ChatGPT conversations and review usage history to help spot potential problems and step in if needed.

# âš ï¸ Anthropic warns hackers are now weaponizing AI

* In a new report, Anthropic details a method called ""vibe-hacking,"" where a lone actor uses the Claude Code agent as both consultant and operator for a scaled data extortion campaign against multiple organizations.
* AI now enables ""no-code malware,"" allowing unskilled actors to sell Ransomware-as-a-Service with evasion techniques like RecycledGate, outsourcing all technical competence and development work to the model.
* North Korean operatives are fraudulently securing tech jobs by simulating technical competence with Claude, relying on the AI for persona development, passing coding interviews, and maintaining employment through daily assistance.

# ðŸƒ Meta loses two AI researchers back to OpenAI

* Two prominent AI researchers, Avi Verma and Ethan Knight, left Meta's new Superintelligence Labs to go back to OpenAI after working at the company for less than one month.
* Chaya Nayak, who led generative AI efforts, is also heading to OpenAI, while researcher Rishabh Agarwal separately announced his departure from the same superintelligence team after recently joining Meta.
* These quick exits are a major setback for the new lab, which was created to outpace rivals and reports directly to Mark Zuckerberg while aggressively recruiting top AI talent.

# ðŸŒ Googleâ€™s 2.5 Flash Image takes AI editing to new level

https://preview.redd.it/snu29x5oanlf1.png?width=1456&format=png&auto=webp&s=009bf31a2387b15044923958d4282646100995d2

Image source: Getty Images / 2.5 Flash Image Preview

Google just [released](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_aGi-l56cEisHKTa87-OIGwp4QwSTfeoz4DBgBpvU-PHa8QP6I2NxN_yCImyIt6BTJJMzIumpWWNcoWnFrhr1sbNgaH7HEVcTR5V97XDpGsKrjyBaT-c3bFqwmmae51ep_4o0sScoErYYeklWk4MvMz7vGMu6hX3F13SGnvClOGKzfOF5tNVOlaAtE90yoC8nlKmlyzOSC1eddURuE2WWfDJdokG_RH8AMuUMG_tDptoy8ZMdwF0U1ktFiQ4UvT0SMGuReuUbYkJDgAdutxvRh5/4je/0ziyhQ89SkSne7lUxFXJrA/h6/h001.Sjy3hOilr_9dr5zmPUeFTMW5-CwZjCc3TBQ1MBWfI_s) Gemini Flash 2.5 Image (a.k.a. nano-banana in testing), a new AI model capable of precise, multi-step image editing that preserves character likeness while giving users more creative control over generations.

The details:

* The model was a viral hit as â€˜nano-bananaâ€™ in testing, rising to No. 1 on LM Arenaâ€™s [Image Edit leaderboard](https://link.mail.beehiiv.com/ss/c/u001.qfYj2NCVNYWxQpHjnkNP6WwpTWiSwrCY9H84g8XFdWAzlon3MomZYZ5FnpNldWTPpeTWayuj5dO7d55WD8QXdKdmATQDdNIt7P2DmdLjzRhsmhTjc2ip6Dvo5Lsm4G2Ux96AvHDdvyYTHvTVlAWva6jPMTIB9Q9xcrDYDnM-2VrToq4Zv6xH-kCuqdo8nen6EeHo1QERs2M7_buWmNNRQtgCo5vlqu85Q1YsWh3iJKBQ4Rzt5g4KOfJMnJA8Z-TOd3HZGvj1Z8vSB2QapciYRg/4je/0ziyhQ89SkSne7lUxFXJrA/h7/h001.PdhBVxDWdzsKHeANdZ85FyNhPc_ODXj90j0QWWZXVuc) by a huge margin over No. 2 Flux-Kontext.
* Flash 2.5 Image supports multi-turn edits, letting users layer changes while maintaining consistency across the editing process.
* The model can also handle blending images, applying and mixing styles across scenes and objects, and more, all using natural language prompts.
* It also uses multimodal reasoning and world knowledge, making strategic choices (like [adding](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HbW0gO67iCDEycHajdMJtpKLQhVmbHpxRIxxT7J9OVZczmdO53H6acjpJvBAGqy-i9yCI2y7J2UjANMTWAhh_wrqwAPxXPLNMZXQeSTlVpXtX_FaLEuMqroyyMM7cVwoSFnc_Vb2uw9oJOGD9XnYEDVCK0cZm4DMFOC2u7JQuX7Fp79fWUmhc6BPIJ-aatxSUmko7nh3Bwn3x55dUEDpvFwQrALYlluywIHOg7ApOn25W5fb238O-kTi44J0UBpRZWQlpLMzjuUgdPnXnUvMUZ4/4je/0ziyhQ89SkSne7lUxFXJrA/h8/h001.0cbFvncD2UkdCy3lwjqizLbLDjMmsimPNB_-iSOBTK0) correct plants for the setting) during the process.
* The model is priced at $0.039 / image via API and in Google AI Studio, slightly cheaper than OpenAIâ€™s gpt-image and BFLâ€™s Flux-Kontext models.

Why it matters: AI isnâ€™t ready to replace Photoshop-style workflows yet, but Googleâ€™s new model brings us a step closer to replacing traditional editing. With next-level character consistency and image preservation, the viral Flash Image AI could drive a Studio Ghibli-style boom for Gemini â€” and enable a wave of viral apps in the process.

# ðŸ–¥ï¸ Anthropic trials Claude for agentic browsing

https://preview.redd.it/x4b0kx5ranlf1.png?width=1456&format=png&auto=webp&s=12f4d1a36248885697b67178836b7c6e97b4dd46

Image source: Anthropic

Anthropic [introduced](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3xsWKo-5K2wdIjMsqALxPuJVaejX2UoRZ4jRlLiUma1EE0w1m0WEtmo66OP1PJhoUhwmF3clIPzlNf-N1kf8b5nl-YATpEamn3Qj1YSunobaTm5FTGD4thI1zXWiAAgdoVWar7A39u_aIjLtbMUyNHGJyyrLo5lPTJfTo6X5fYle8quH8hFOuf0Ni7F0UDnfKTaiKrfywhNiO2T_jiHxxNW-igTEuz2tOWEpbsqo5d35xZ2DJ3X28ZqHflh1og7rAE/4je/0ziyhQ89SkSne7lUxFXJrA/h13/h001.SMM5htu-vvhZ4LDaSIRFbLNRTTpJ4GHIgS8_2ROC90Q) a â€œClaude for Chromeâ€ extension in testing to give the AI assistant agentic control over usersâ€™ browsers, aiming to study and address security issues that have hit other AI-powered browsers and platforms.

The details:

* The Chrome extension is being piloted via a waitlist exclusively for 1,000 Claude Max subscribers in a limited preview.
* Anthropic cited prompt injections as the key concern with agentic browsing, with Claude using permissions and safety mitigations to reduce vulnerabilities.
* Brave [discovered](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoLHk2UD2MNm1l8Hib11p0kgGqy-T_XRspX3WykVjsVuzuF3GuGi-4hsJamxzxSpI3mEWTZOI19oAuKZD2eJOIHVJlCYhAdYfXE5-j0u31LXslJJjCQTmx-UvbxEWs4a29wnTtQSSjRBxXF57lQxiqKgNXwlDuJV-ygfojTnG9WlDUjqvNFSJDbbAmkKBc1egsqRAtJJL642FcgNLdEEuHSWFaeojNQwm0pv-kqYuncqzGfeu9pqBiP_Gt0cWcGhIkqz7TWS9NaH-cSzvaIKgV8s/4je/0ziyhQ89SkSne7lUxFXJrA/h14/h001.vs4tguyGb3i4fZsqrAhwIq32ItTT5rN1jE0AkFarsRo) similar prompt injection issues in Perplexity's Comet browser agent, with malicious instructions able to be inserted into web content.
* The extension shows safety improvements over Anthropicâ€™s previously [released](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_FG9HsxZ0xcSqvfWZJMBrCIMz3FnZEKtDqPtR0T3il7eKQJCdd261X-d-kmm36GmugYzD5XOtcPma_0A2a195IhLkzAtIOe2eZJlyE_fICOPHJ2KVonhH29uoNjgF3sklK1sDPD0zZzHW5pgCCoVn7bKK7Eo6OfbW5Q_0srOxusP1LB1i1mfVf-fgLBLrQL7EvHAlo6ZwsigGZOtl0_2_SGyW9xc99MzRf2dwrbBYKpuWbEwV9Ekc6wYQFqfSpYrcxdJGLuRffoNbm4Pp_QXRM2KMOXzINSFplLli2p7jiDkTfGHw0Sl4-TpVaP8DxgI_DOh1B7lBvmHJR3Gu9FGPNmGoqykzrgtdBepCR_RUu7mtgX8NxHLsmIDF_3kNJjrm2Sz87kR1myZlGxPQXrUTxs6LlIsUtKG-V2T5yIyzu258W-V02E8V4YTVK9rOiR1SmIEA5f-Y2upYErRYFN8AnuKMZ23oZwMHQeKcrl4OQ4eRtrdI1NnjclxrxBbsCIGj0au-SZtwF9cz9YbTtdxYOsbj_KrcXlyWVdYJAA7DA7hKyp-JGbiqj5OvX3gJYCTlqQr9aJzaBvdhp4VEf1jzL12OqWLkcpqan6imYZSMI7PHD1b5REY5IFYbe2QjdF4k-twcCWkVmMoWguZ0BU5UQhVuVO0nYM8KSlSBYI9G5PUD2ukVLeQC4BrtjbFoh9_Iy9N7mI168oMUJfv5R31SpgfeEjFQy5sAsuwL91-5om12dsyHaumzGymMlIcPPQvtC1A7qbOg7LYRaZ7t8hTKkD/4je/0ziyhQ89SkSne7lUxFXJrA/h15/h001.tqEfM47uqR8LlY-3_Dq5bnl45q5pBJeBepDVIuR6Dp8) Computer Use, an early agentic tool that had limited abilities.

Why it matters: Agentic browsing is still in its infancy, but Anthropicâ€™s findings and recent issues show that security for these systems is also still a work in progress. The extension move is an interesting contrast from standalone platforms like Comet and Dia, which makes for an easy sidebar add for those loyal to the most popular browser.

# ðŸ“ Anthropic reveals how teachers are using AI

https://preview.redd.it/x7t4zxpzanlf1.png?width=1456&format=png&auto=webp&s=b5cc5321f6d5d58eb5e61257f71d72c51077ad68

Image source: Anthropic

Anthropic just [published](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3wfgmuTyLJp8AQzspDtHqw9BCRd5vPiKJ8-sTR7yiJWmD3k5rZipiG0fZ_IxI38Rla7TK5GTogsdGxzfBy3X-8vognzRE9tuPXYVF7Gc9QWwqw9ZtPJzFSNDtXqhqNA1iVvS1KHNbauEzz_urgAO7YFWnvc7yJiMz5i98SOSNqCBiKfP0Ppgles5kwzqFUor-M5hPZwSLBLzCPftAWPIANC3gid0TFzbWD3x1hekPxpcxc4MS80O1k3Viax3HZtWV7L2nzBqLgHlzwlp45kGj-g7G8qIB_HvGx8zFQOOe2v4A/4je/0ziyhQ89SkSne7lUxFXJrA/h22/h001.qqFKgokCr1dDXDiz4lfwLgWlcDudZ_Wqsno6U64CKAU) a new report analyzing 74,000 conversations from educators on Claude, discovering that professors are primarily using AI to automate administrative work, with using AI for grading a polarizing topic

The details:

* Educators most often used Claude for curriculum design (57%), followed by academic research support (13%), and evaluating student work (7%).
* Professors also built custom tools with Claudeâ€™s Artifacts, ranging from interactive chemistry labs to automated grading rubrics and visual dashboards.
* AI was used to automate repetitive tasks (financial planning, record-keeping), but less automation was preferred for areas like teaching and advising.
* Grading was the most controversial, with 49% of assessment conversations showing heavy automation despite being rated as AIâ€™s weakest capability.

Why it matters: Students using AI in the classroom has been a difficult adjustment for the education system, but this research provides some deeper insights into how itâ€™s being used on the other side of the desk. With both adoption and acceleration of AI still rising, its use and acceptance are likely to vary massively from classroom to classroom.

# Anthropic's copyright settlement reveals the real AI legal battleground

https://preview.redd.it/k5ikuk14bnlf1.png?width=1456&format=png&auto=webp&s=fe0bf7cf5619a240ae4fbd82a4324ed33164d9a2

Anthropic just bought its way out of the AI industry's first potential billion-dollar copyright judgment. The company reached a[ preliminary settlement](https://link.mail.beehiiv.com/ss/c/u001.929jZiiRht9qZPYC5Z_-tbKW9kqdKsL_Y5-RKcU7HjXLcPvujstJoz9DS0QMhtFGkUJ7mEiAq8l0zJvBs4lWfhNxt9WcyvA5Zcdt6qhCVZ2hN-MNQct-6YHkPLgAE0I3NvnwOK18CbQnCBh-h8RdkeWDkjgiDo9EpFK2QB71RlurnhV9DwepCY5Nk9_gdthedoYveQSHp-mfReHiSl-qkZ2Xiy2WpdO17FfoGFWkTBE_DPYdmjp4RWBYojAMN1efF1JtMpMeWI9C8eeC14GeslDWGvOU2Nor9417LcoR33rW35c2iYwFXE4aby3JAqM0Ld47qR_glnsVvGzj4qsbfw/4je/pv7j_TbiQy62hfH_LWH3ew/h3/h001.RtQpJg7Jd7Ln52_x6zs0PRk7sSHNG8KT2ZvPySmOqhI) with authors who accused it of illegally downloading millions of books to train Claude, avoiding a December trial that threatened the company's existence.

The settlement comes with a crucial legal distinction. Earlier this year, U.S. District Judge [William Alsup ruled](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQGZytCVFy8J7p-OvC1u3vBNCUarpg47347cap-SG5y64aFzrraLRJXUsatkAgqi-j907WehJDTBPaUH-WGqKwYdc0xTbeRinkKoiNZiJuGxD4S0bdwfwrLnbKSVlwTazhVfWrowLTFAyM2zAKm7yHMmywOYnogWH_pW5S2gY77ArsaqEc4ln0UBsDP9T7cRv3zhwxQhY8Tm1aFYqIixBl9eY9j6ueQIlMmDE9yXZ5adJCq8sP0_XZdPhfm6PEYL6S-4-uiMNZT46LCKRS1baWY5vX0R7c-gc3_Cb2vv-cQ4VvBz3k8OEqtFs_UIpgSsO3bU7eytzFDC1QL3U-9Rryaqi7BhgQ6oZia5OyiRB86dwNrERPzzxrZ8WiTgbYcqCFypx453FF4Xbk4cNGctaj_9QaADoqsjxY8I4Fo79jb0GfE-XO9tkmQglptZdNqmptsMB-DCc3Efh9OrNxi7rpcwXSBCkQ1iNNpGwxfzt-cWOndAANv5-O7YpdLx4S9QVGIeXx8eTlk5pQitfhUQXyQ6FGoAbZe-BuGjhjozb53zLuhN3dLKHv55bSnkKlE9wToGipTkCuO3EwOXeEnCou4awD9DUt_nYxnEqrg31DNNrVbih9YuhNawhFyI_g4phgdDpldLMhltvvBxE-EFKGzjYaum4tfJVuTnOx_6MNP_pOkPVOiVKa4It9Z8lOvNpF-UATBNpYsaKOgbsz9xdBJwckLaNufCNbXuB_5z0M7qNBJIFbBzYRQ2rTDK8xmxyGQ/4je/pv7j_TbiQy62hfH_LWH3ew/h4/h001._RX3jeS5e20_VXEIadhPyoX_2VLaE_JQfn3Ro0htscY) that training AI models on copyrighted books qualifies as fair use â€” the first major victory for AI companies. But Anthropic's acquisition method crossed a legal red line.

Court documents revealed the company ""downloaded for free millions of copyrighted books from pirate sites"" including Library Genesis to build a permanent ""central library.""[ The judge certified a class action](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeGcuhcYSkLufPNUskt0l3pD3ZKSAXOdqyusNHZJB8o1YzoBZlR49l22Y5vMKcz1rUVwNeENFIghqAs5F6TQIvwZPKyksVPO_riGotkhNDwofgRnQxkPLX_Hy3fa3VNTKptcGNGLPTqreunSGo7kUPqTvRa7Yg6WCTOvmeH4saRT3Gf8rgrATpMkCjG66PBRwhLso9ozDZ50pidCX8RXwlLDvR5aighp1TuRDS32Ko3UEs-guN3CEedAIi_IuY-vtVxmIR2ubeQ7SXbqmQLzpkjTE_TCDhobzCBYIKSwXkvv7VJBQrvN_7m0pRK9K1CdM2OR2aes1Dvp4bb___JRgmeMFdlzfxnvKf1MdCAW2T4ln/4je/pv7j_TbiQy62hfH_LWH3ew/h5/h001.GcLRCxRx9ZS26MV6BdhYrq-Sh6XT2o8_FVlaeHnC084) covering 7 million potentially pirated works, creating staggering liability:

* Statutory damages starting at $750 per infringed work, up to $150,000 for willful infringement
* Potentially over $1 trillion in total liability for Anthropic
* [Company claims of ""death knell"" situation](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeKEoNQ5v66IW5vSj2H3B8gbdhVQ-y33ffVdH54brLYtL9OO2JQt3mQYHSsUoQCRpdJcYR8fYvRRgTvBBt7yif68Ihid8sJsoQ2RdI9gFxrWf-bcQ5odAulS0efDSigWR5jCo8Gmh6rrC904-1iy-uIXidvjVSDB-KCrc87MiCmtcTm0jfXJxACq4w65PGqhCJsIF2SW8Hvab-u0XJapi6pBWORXp-34sqcUKMNDERl5isQBEINDtcFPhu-T2JIbPJb6wq97nxrljoYQiXQK6FSoNeiBekoET5AqbxQW2fjOzgIrBn_x7K5LV6AFZLOZMSg/4je/pv7j_TbiQy62hfH_LWH3ew/h6/h001.BUcyKQdN2Dj9dzdH6_Dlrk86flyE67qUDgeZZy2Pqm8), forcing a settlement regardless of legal merit

The preliminary settlement is expected to be finalized on September 3, with most authors in the class having just received notice that they qualify to participate.

We've tracked these battles extensively, from[ Anthropic's initial copyright victory](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQGZytCVFy8J7p-OvC1u3vBNCUarpg47347cap-SG5y64aFzrraLRJXUsatkAgqi-j907WehJDTBPaUH-WGqKwYdc0xTbeRinkKoiNZiJuGxD4S0bdwfwrLnbKSVlwTazhVfWrowLTFAyM2zAKm7yHMmywOYnogWH_pW5S2gY77ArmQXef0FASBbSLw3DnJoYPDQDfji8UwrSsgnpMiFs7RqnRXmqtszmLBL0kLLWh1GvTASG-zUNmBJNa1aWEWGSdQ-UekBJAmuUHvPSciVG3zjnJ_2qjCJ4g7zL-jhYVV7MVCUk5I8lLbzKFFUmsE-LjZOThrJ3PzsQHfI7yE6zs3bP9p3XEqi_7PAaw3VBaifDsSQ29OuZ3CjdwjK0s9b45bKcOwCMlximBc-ucSixwZH19oZGVYsQYKUKMzzu4dMFkQqklAd9FQ9DRI6hErWNikPM5AQttOKhmWbvz2xCBihjUBNujJswR49VicClGq88NN9NzF9d5g6hHkBeSdFqOCyYR-VdifwHnK29PRQdBz6xcBGPZZc3x6ZkBo5cmByoQD7uQBRcwrtQk8-AHoIS6FUPqxFch96FxwaRKnoAb7uO_PPEHkUbY9o5dpjBtbBfHHgpL4VXIZkqoSQ2kpO9mroiTYQZRTwcmspfV82xK7hZ0NytQWbS8woyrBmVE95SQIFvF8kzWvpSn5d0akXvaa7zD1LDKj2_3CIpNv8lFOnv-pKt4uVG455h3408taOj5o0x5X3k4gAVgulx9L7tFY/4je/pv7j_TbiQy62hfH_LWH3ew/h7/h001.alBGHbMCvZe8-zsyrTlGmwzMXuBQyg0wXoiVRpQHzqc) to[ OpenAI's strategy shifts](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQHwQAvD3h9WyEd8KGkPKOl7bgfzfCK1fVn7ZVyGaOWWqFEZ-r9_ypXV3jfIv5f6oDBsxR0jvcmdo4wtZbvDp6TSzLVHEGWv_AN-3a2o-p19QLdDnilUgQcC5oiw1e0cdxxBqbmQlLIBI_-1gn6gydGNoyCK0Y33O1cXBIMkuIKQjjqRJKvmhyg0Uf_jyb95gbrS9AG6Ahks5vP6IE_1c5Yx7HcZN7NRu4rog6B34oqhFqWJto7R0OtGcRM6LEXlVlywR8ruxg5KAu39JK-q2WR3TxQXQxzmLEAj0Hy4iNE0RDq1_w08y86RgqUVDoOrSDSOvGwQPjN3TLpN8lsaqBnIaMRlsgxvem-VMLlhk6Lq2o7lwozqz2X-9HZAWXaj6xRyVKUx4uFxTtM4ViwhkecPOXIAjN69qjAqXPaOY0aG6Kbh9QSaBSvAP5f7Pz-XyK68zEgK0GasKrpfHf7MVAzYUF_IYnwkTLTzyOBfIZ333dMTUJuIblAounfrdYVv__89edv4xtNN1yG0jZssVEYd9U_evpuNuL-Fu3mGQGhDn4zqilotq3vJYCyuYXqWLq0qQHRfhzPoRe3gnt394ceNcrnkpDyqAw-1sKeaLf1LvrCNxUtGXk8K8K1Gv_vDDZ9bFjWdSDSoKNvvKW3xauKRil1VMsvVlU157IuWtsoM4F6Yr6W9x8loLTf2HTZM8HNZVuKleJLO5rAbvf429iohLui3pLGrWWj2kY3aNZdzyRKaOV6f2f7NgohZfN5WZlvEZY_41Wm_q8LMRuK3AmYs/4je/pv7j_TbiQy62hfH_LWH3ew/h8/h001.sExAgndi2Tt91hzy9waNaPO6CkN8fr_D_xe0Lees-Rc) following legal pressure.

[Dozens of similar cases](https://link.mail.beehiiv.com/ss/c/u001.gKxW2KpP8aPe_QMyOQduolOPFzegfEuIHqwZ4bCC_uheIIDYtySmcIwZhPlaTR6wWEmML5V7MArOEZFEyoHsqu8wUGqX-29cDk_mRDk6eFkQcRSljK-QoK7UXHP7IMoL4rnjHYSIr_NJusGcUSNFHos3OPhFucu50RxcDkw-v5olnyWFHkpOs_9pv1hwbjOm0-mNOlvodRZbWt901FgliM94VqrK_0qQfiMrar6tKGnGGzwm_u1mPlJnsGmMHUEzlCcNEcetedWK8c20oqV68A/4je/pv7j_TbiQy62hfH_LWH3ew/h9/h001.LCt-h60RjitdG4vCHCytcwJwMRP7KBuozLjl3tQI5tg) against OpenAI, Meta, and others remain pending, and they are expected to settle rather than risk billion-dollar judgments.

# Blue Water Autonomy raises $50M for unmanned warships

https://preview.redd.it/q5vvt8c8bnlf1.png?width=1456&format=png&auto=webp&s=cec2252c30a0ad38f100be10571f9163fda0dc9e

Defense tech is having its moment, and Blue Water Autonomy just grabbed a piece of it. The startup building fully autonomous naval vessels[ raised a $50 million Series A](https://link.mail.beehiiv.com/ss/c/u001.pVz9habVd8BuGO5KajRSptEoDlphhkml3i_XTVuo-0_4VP3muASrHAEQMoGqZ1ZXRDinFJ1Yf0ZmDnuKb6Vaf3A4cB_bk-jPusv0s9RezOlR-UVxF1ka60oSH67rsRknFERKTnwW9AtxzhULVDy-FFrDckR2EctKP8YBBHOQQWE5xJBn9kGfg6cETpzRj7K6nIR77I9uNvCorTWbngN8OnF2P2XduJ32iyuci3gO-yU_bWolFO_b1CMfQ8fh_0d7lvkBZh0gxxmCBc436uu39uI4Z3Y0jJqz7QFPlIt9s1uCSVXKJEoLAOlVkRhoYWyLJcLCUP_zT54Tdg8MrLfJWjsHiEi1LcSAuEjuap6SIMs/4je/pv7j_TbiQy62hfH_LWH3ew/h14/h001.t6Bz6RWtdYGnMeGYZsojKf50UgAf4D-RNdfATrAMpCc) led by Google Ventures, bringing total funding to $64 million.

Unlike the broader venture market that's been sluggish,[ defense tech funding surged to $3 billion in 2024](https://link.mail.beehiiv.com/ss/c/u001.929jZiiRht9qZPYC5Z_-tZvXk8vZTCEs5-dS-8uO2rYJdCi3Qcbh7DUmvY6aii7MxQKgdhJuRooXHRf4Dn3Y-RjKTvzyGokTwBdQ2aRbYWT4zKK2Gt72bWViX1obw8apv1Xz33QqqtseGtVkVphxnkgmKKTaOqO9VYBXB1ohRu5MSgD348fhslKuq4ltUtwOJIizj57BSHjc-EP_MQD3xkDO3e5WhRTa6jv-dLFj-bYfSiTgGSrdZnbGUS3htJMHEt3qt9AA0aND5K5335ueqLqfiGGd0NayiYpTsq5ywpw/4je/pv7j_TbiQy62hfH_LWH3ew/h15/h001.s2Qe2XaU_Z4poitTmm1uIvvsqA76xSG_74638MJPjxs) â€” an 11% jump from the previous year. Blue Water represents exactly what investors are chasing: former Navy officers who understand the problem, paired with Silicon Valley veterans who know how to scale technology.

CEO Rylan Hamilton spent years hunting mines in the Persian Gulf before building robotics company 6 River Systems, which he sold to Shopify for $450 million in 2019. His co-founder Austin Gray served on aircraft carrier strike groups and literally volunteered in Ukrainian drone factories after business school. These aren't typical Silicon Valley founders.

China now has more than 200 times America's shipbuilding capacity, and the Pentagon just[ allocated $2.1 billion in Congressional funding](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeMUk2Kzve8e9LP5fsTXN7zTH1CvfRD3P4ppoY1qKfEGCyAyRjm-Cu09_OtEzd7An4AGtsE2iVPzvmNyU3kmbsnC_Qlayh_15Vmt7yE4vutf9bdQI73ICgUGIo8qs1nurUiBBXociRmYAoYnz6kHMj4prlwZTMCO2zAkJjG-Pln83BrSbBkPL7ocg8IsdHsKq-_x3lBMO_cY6AK2tnn-56452xl4552XJj7a_G4wsZlAPsmCs6nByOmRYJnb0sQ0WZlGPHimXUL-5wl-CePeYXCCv5p_RtIUp-Q1_MU8lNAhxk-gY4D-KeNYYaZsmMlngsmcD_0zJ_YHCvh6GXw4bdk3ScTb7FZ9lZyYAJKJZ-DHBXfQk0_IJRZqMEt4zn1_mZ4TNgx1_74hziJ5sci5yHmulANu1Tj8eI6f0e1RQqYFh/4je/pv7j_TbiQy62hfH_LWH3ew/h16/h001.nOEc4jSCjL-us-LDXgaH23_iZPmbq_2BuNy7T6sRrgE) specifically for medium-sized unmanned surface vessels like the ones Blue Water is building. The Navy plans to integrate autonomous ships into carrier strike groups by 2027.

* Blue Water's ships will be half a football field long with no human crew whatsoever
* Traditional Navy requirements accumulated over 100 years all assume crews that need to survive
* Unmanned vessels can be built cheaper and replaced if destroyed, completely changing naval economics

If America can't outbuild China in sheer volume, it needs to outsmart them with better technology. The company is already salt-water testing a 100-ton prototype outside Boston and plans to deploy its first full-sized autonomous ship next year.

Blue Water faces well-funded competition including[ Saronic, which raised $175 million](https://link.mail.beehiiv.com/ss/c/u001.929jZiiRht9qZPYC5Z_-tZvXk8vZTCEs5-dS-8uO2rZ1_mANoBq2WwerE2VOfrwcjBEeTqXs5bKaPgwsCRo_ULP-aCgooAbnIm0U5lZuykVA6BwkgEFJs1XJWshE7E2rmK4GpG7EzRsx0RY-eb7GcWKOmXbzze25V9WMZd0_MZCHXpfHqjIemkJr17rssfp26yAuu2aUlbWdc-rsCKUriR7vm9Dhab4O9s5gwEDyy0G3PMux6NlnFPitFrSd5rIV2YhlIvjdVgtfADOGrvUzDP0RZ0PEDIgn3s7sE2hQUdo/4je/pv7j_TbiQy62hfH_LWH3ew/h17/h001.Iy1tcMqLd4TiZw-yhw6JfunvEML3ONycts8-BsidecM) at a $1 billion valuation last year. But with defense spending expected to increase under the current administration and venture firms like[ Andreessen Horowitz launching ""American Dynamism"" practices](https://link.mail.beehiiv.com/ss/c/u001.gKxW2KpP8aPe_QMyOQduosoBLLv5MQtpO6YQfLcgHN4VQNUWvQ7dZx_JPrHPLkoJ2ZWe0Vj_zJMHrIshlhsM1miDIzW0htzK3AX3SbBAaWtTtJnAbGLLDuFmBMVl0K6HguzaonJ3W3wEQ2ab9K-koqUfcxba2Snrpqg3ecjaBjEGpLz-eZTllUObklQW8Je0YrrdEfn-SgwRNBw1_HgFAxhxy35XHUaSraGKZEHWTPLb6JBNJmg28YPYJOinGz_TLFWRR_zLWMHPp1G02IpNVQ/4je/pv7j_TbiQy62hfH_LWH3ew/h18/h001.QKxFoTDSjAv_N8z2-xocZ7teVbHkx0kwUh1bHLfG8Fo) focused on national security, the money is flowing toward exactly these types of companies.

# Melania Trump wants kids to solve America's AI talent problem

https://preview.redd.it/rjoenxgcbnlf1.png?width=1456&format=png&auto=webp&s=cce0c1f9f0ff100d07d1bcccb4fa4d7c7a807a98

America's AI future just got placed in the hands of kindergarteners. First Lady Melania Trump Yesterday [launched the Presidential AI Challenge](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJvmTzk45QTwFpy4LEdNlqFxyL4eiXNbB08ls1AwPrDEGxaLty6wsHoGIKkhw_m3UlEILyGImhYETnTLidi8uowfkvSFYmb1CrzH1_aBBOJcDljFrJx7P1AqN5QXYnp7PDfBcAoWDS31Jm5r0WKSGo75m2hSETone7zZwvWg_-_t5m-S7SAvYhaAFUMATyCuXPgl3Uxpoeq9c6rTjFOokj4p6eNxTRuhDsNomuSzhsPoU9IaZaJboaNAA8FdNLtTCg/4je/pv7j_TbiQy62hfH_LWH3ew/h22/h001.N_JcUM2EwG_qEnH2Esar6eMWPh76auBGl7Mp3Z12pN4), a nationwide competition asking K-12 students to use AI tools to solve community problems.

The contest offers $10,000 prizes to winning teams and stems from[ an executive order President Trump signed in April](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3LjbeoYR3id8Jn8EnVWFIH3K_87WfnngtwRbVQyZTuUD0-icy7-wv75MgR87bR10tfU2Tu5EPoYgwx9FPiJv4f8JjEJcTHci-DCeu4g5j1aiyJwxSeXanReeBfTvBXCbZDLaqlcft7zC0BVsP0IvVpuUwNezKRIIscpaUm1Bat018OeIUH-JXoiVdDHk93bLc0BnUT8FJwsWDp-o1GEIdWimmmwGDnYYQcRxCe1Ug_Q13M6dXr1zRKdQFUMEGhe-tSSIfHDuo-gOkEQRx70mHWcTX3jgEeZNt4-BN3qXbFoI32wuUgbZfTqtPCJd8nmK-0/4je/pv7j_TbiQy62hfH_LWH3ew/h23/h001.FaHf9nZKnqD9ul4ywDCPi8FidMoGjEgP9kUTsHBOR8E), directing federal agencies to advance AI education for American youth. Students work with adult mentors to tackle local challenges â€” from improving school resources to addressing environmental issues.

This isn't just feel-good civic engagement. Melania Trump created an AI-powered audiobook of her memoir, utilizing technology to replicate her own voice, thereby gaining firsthand experience with the tools she's asking students to master. She also championed the [Take It Down Act](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3L-xfLCSDYk005W97YlwdSTec2nulOe0w9BkDAlIa53MXMbQ7P-v8jOvjFXqwivAm8YFBZEewMVqFLVGuYqDXkZM9JRs1Yx8lrlR9LNAxKgpMjXNjHLiNN_hLZ3laZN9W6esWMoffKE2UkgrqNGhRs2osooFsh3GRfVHUoujNb1h1oT6D602-pCqnNNNhhMD8HZCKZa86bY3aUDrfVCMW7XvNqnp7n86_G2oRavYax6n5jtJH22J4OcjWGtmrlEva0whP9nNaC5nGRdGEKzyEK0olyiXshamFhoVzpw2oR46w/4je/pv7j_TbiQy62hfH_LWH3ew/h24/h001.UrjMFkRN6Yca2g30TJzR6iP92BjCe7-ERoy4hIPbsd8), targeting AI-generated deepfakes and exploitation.

While tech giants pour billions into research, the[ White House Task Force on AI Education](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJvmTzk45QTwFpy4LEdNlqE1pEPV12RBu6-F48ImUfo2RTVzEVkwReFNhPgx6Z8sMq1Un-0V-L8poz8gfXc5R48qAhoqH7hOvQMRkdh-SjiaTNoilumAv9TvyjZZyVcd_Z_kP0KGkluBXS1X6f7DZZucS-h5Wayud-X-V_9RIwSORS05Tx80i7l6havspIIky202D6ZfZHW-pzYnGQUfz4Kj9ODfgN-_4H0a9SmcQ-2u/4je/pv7j_TbiQy62hfH_LWH3ew/h25/h001.E1e5up9qP5E7IlXeo5xsvzizS9Q4unPYdgtD0Td6hcw) is focused on building the workforce that will actually deploy these systems across every sector.

[Registration opened](https://link.mail.beehiiv.com/ss/c/u001.WHId9TPFGnUe-Jr4g0PigwcRl7kKxS2-E0SfJJ136_5MSbriFuTIwA32o8ETuQGmwYyGwoxsWbFYzDJmANgilHf557hzPU0emUNpJknkSQMrk-9AFY0vcAQjG58JyJURAwBdGLpf_1Sq6ZE4MUP8gxJfAPU2B1_DphJX3T2J2GgG_RRywWIgVmpjUVUaqDV7du99zJpgqqjmQe7-SzuhC9c0McBxPX6O2gAHR4dYpWumibv6e3KZc-zZUxeXJunA2lY8RRkkunJ6DTi6ienuBQ/4je/pv7j_TbiQy62hfH_LWH3ew/h26/h001.TRopNHXflIm90c8edAO6IyXjPYJYILqnBgBTtYrhqpM) Yesterday with submissions due January 20, 2026. Teams must include adult supervisors and can choose from three tracks: proposing AI solutions, building functional prototypes, or developing teaching methods for educators.

* Winners get cash prizes plus potential White House showcase opportunities
* All participants receive Presidential certificates of participation
* Projects must include 500-word narratives plus demonstrations or posters
* Virtual office hours provide guidance throughout the process

China invests heavily in AI education while American schools still struggle with basic computer literacy. Michael Kratsios from the White House Office of Science and Technology emphasized the challenge prepares students for an [""AI-assisted workforce""](https://link.mail.beehiiv.com/ss/c/u001.7zYFXt5AA3Px2NyJbPz6hHFnNzi3nnZsNerIvu91XrHPy5bACAx80awO-ioBHRjMxHsGoeeGPh5nLzQ0-6202lTEYIkbISSU9mwv3-D6OQ096PhJ7VgI3lnEdue1Dm5VgjL24DHJC7yd-b4pKtzWYQ-wFz4SuXXR2JBybcOXcsdGC_Yy-JuY8MtxSFluQ3QEV1G8Xs4AFfbQ6JX23ieaaB7JKSzvDFFMmxnDbqdj6ivaMIsOojB9Cltly7WuI3TWlNzMcWLzQj0YUD9b-VWe3g/4je/pv7j_TbiQy62hfH_LWH3ew/h27/h001.DbU6rgQlaHaCT31Yn0LWc8Yw7iXm7yufUFfNLll6ms4) â€” not someday, but within years.

The[ initiative coincides with America's 250th anniversary](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3JK_gs5ppdetrK-D3iqhWRiwkJVLFzcK215URxvQAUCxPWLm1bhWU0K2BD8KeJ1uo_eCsxdLExDU58-dkaFeycBVZaNm5GgRfs31eiKbznOORqqhgO0dRW6Z-7tCWGYRCJWY_Dot8YaRB6eflpoG71cta_TEHy4VRIGwWInzy9tbIVzkNOlpnDY3wFb026l89SWiLVkd1byspWM55Ro_Zr5eMHL3Q_GsQDSi5QIlneTyYvn5BWt2eX8iOdYz8Uc4Qa4CNO5VoXIqTPgZ2NlXEq42dofV8-0ORMr17x4oRK7bvUJuW_gkuWly63w_MWbTDk/4je/pv7j_TbiQy62hfH_LWH3ew/h28/h001.CM6V9SE7M3a1KNOzd-IwMBYKsU_nLMyFtQWltn5UaJs), positioning AI literacy as a patriotic duty. Whether elementary students can actually deliver breakthrough solutions remains to be seen, but Washington clearly believes the alternative â€” falling behind in the global AI race â€” is worse.

# What Else Happened in AI on August 27th 2025?

Japanese media giants Nikkei and Asahi Shimbun [filed](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf-JcM6CB4s1IeiqjY1o2UAsIzEdVbYVajEg9EkFffycQ7F9_jTVrwJXIL87GwYDlXg2LuATVXZ5yM7MbuGGnhTrqhBq7X4npIfLxiupe7q20GVPPepaeGYanDz6wDrjaPdfq06DeWhYBPCM-IzOOBit4QNo3HQsOKiusSgtlePK8ycrnflGOX1lhFloqQBv1n0e4Rk5oXo0a3daGqsvbNSKWv9yzRtV_T69z2VYVro_skOjlaiGfOFbNd4TpuUBuhoT4u9c0GuG37PAvMttNwoKa6YfzO1KgW9E8O15MfSx4jeyQRVOwTTi3_vlJHiurcHV9hduivOTeoz5pSU_vocoXPLwiDTzZImpXJseuxGB1f05Uhi0Q_UFUT61FIWtFQg/4je/0ziyhQ89SkSne7lUxFXJrA/h29/h001.kxEVTyWuhXBfidt-IOExAdJLfX6W0GiZp5za46csASE) a joint lawsuit against Perplexity, a day after it [launched](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_FO-DN4s2bli22qaISZg2XSeMMS9E5effERgvw6U1IBRfSzfJx53QnHH8GwzthZke1yUILBgakhGF5JS4IC84KkAyeWs7KY8Ka50v2nSpDa31BCc5udA5o1ZADcy1I_dDT7x2IIvAjHaKoHMUKiGgGcW6Z3O5FQJ5q3YGSMb7RuLRhLKaGpq1GjDLBpibLK1F0kyeD2-fQdwazZUrz0-ZIh_k0Jk6cHqlmPVxFI6bLpaQgK_9b8aHOFUdgJo-n56Hg6UBDei4sLSkpfLHBVr7fIhmspV6H4G-Xj950tpxyGZwXPmeTAWt7UXYdD9jOG32S0-JSWzK9NxmxO3bGXp9F8Po82DlMoAU48bfHbIFqcPsOQylQ75VtedEe97Bu2y25e7eu9pwC-v4aUqIQd38W3YAkdSAREAu6Yp9r5aUhJ9yLl9UnzbabznkO2MQonp14U_5B_rj2qj9Kxympjq_pLjH0Roh9f9S2n7lfbfF3httBVNE7petAj3rWVLA-kbJnClRdD946YLbEbeMajrRHWTOoixtik25CZ3IGFWrjpU3nBI2jqActqGGC2IRGFZ5UxR6icrkWk7M2mkRPXhIJBEbRKDAS_fPA3UP2fUQpcmPXibkRwjVlpOyrX86tutokcYyk8rIfAqd3KH0I0EOhzfcRkjCxCunr_xe3dg980V5YUp1CN2KNDEZzJwfW16hk3T_Xnwv7M3DvnME-lZpm0cuP5Iapx6i96We80YFhvdL2oEWOXqTJmCsD5S3dyQeRsDy5qIH4PZMhQHmY5FcNKFqQbQN-t2lc6eKrmt9oeQQ/4je/0ziyhQ89SkSne7lUxFXJrA/h30/h001.KcTV2OZUlC8WlRitPW9Kmx3wqkcscGjVX-VRDhBjjKE) a revenue-sharing program for publishers.

U.S. first lady Melania Trump [announced](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xuosZd58y00Zo7wBRXBhc9xapcoy69ab0kd-IXgVd-LusYk1E_Crn2-rkDxto0KESpwb6b3l1b_1BK0MBFhs-4KxoS49czdC03CPHT_BjuS34FxN6OgiCZFRn7C3pYXelSE61ATnbKrDsI4QXSbnF_zZl-Vya3myG0yzyhwtYLINMYDFfKblKsavVQMYGuDPFhmvpz-vfX6113Zl7j6D67sx3PClqaTm_drEmYjeDdVX7R-zyhz8o8Dkl5Ed49Rsg4BBBgq8JJaJIqySAxPQJEmfB-xZlHWE0oyzi8NuevVuW/4je/0ziyhQ89SkSne7lUxFXJrA/h31/h001.-hHX0kHXgiEREvC6MPqdwWnzfWSWs3tyb2ENORIlFaM) the Presidential AI Challenge, a nationwide competition for K-12 students to create AI solutions for issues in their community.

Google [introduced](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_b2nEXB4-RmnaGeVJ09XjqRRZ1_9tsJUr2RARflKPgsrVv8oRXyXz6BSa7huJb-4ON7cFC4k38wS5ilrPIK2piKq1lnyPtaJ3o5AwFejxU5i2XBGJV2KAZ_I-O_XqIQTzTWP_wFkc-lK0L0qNO5LHjVN0IGPTmNfsQp5A8R2i3QC4in1SUFruVCNtIR6AZH7lKo-ImJy-9L941OWrjAlrhQy3SH3OFawTHv_gwq_ey58v5KIc2ofl2_aidaBrEPNtSYnBfWIKU6-56M5FGoV1Mt/4je/0ziyhQ89SkSne7lUxFXJrA/h32/h001.ATNeWUsuzGp9kDADEm9O7JKMCD9dMnM9isehtp2FP-o) new AI upgrades to its Google Translate platform, including real-time on-screen translations for 70+ languages and interactive language learning tools.

Stanford researchers [published](https://link.mail.beehiiv.com/ss/c/u001.s9F2vg9H0NMFC01qj9PgtG0U3RmyhCXX7iQABb5y8wDcuuBdGU4YPGKnC8l3wHo2BZPn9NfZv3KhUYmgq6SJKGdXq8eeQWWwtnBc6JaoqahJftgyIOBMIdwVq19J86Wc7Ho017ynUROXQdlNkdlsp3qvmJKzVb46_WwWYdAvjzRKjG1Bf0Dg_x4xkhLQG_xlC_ZE5I76M9lKeBBYIV09vrd7CdqIqfiK9ffRmVgQc1zIeJorXV5BrtT1q1fAfJKvgSBu9WqWqSO0Z2d6-aZKx6_ud0ZWpbSRmiwmN8w-gXn1Zksu6Deq5DDywivgcL6ji0vTdym_VaCZ2SGr72axpotU1IN3DI0ASEU0Ip4J7MI/4je/0ziyhQ89SkSne7lUxFXJrA/h33/h001.Y7oYpsf9w0lwJXbIi",,,,,,,
r_1n1jj8f,reddit,andsi2asi,2025-08-27T15:04:36+00:00,"AI Psychosis"" as a Scare Tactic to Protect the Psychotherapy Industry

""
Freud is increasingly discredited for his insane theories like the Oedipus Complex that accused infant boys of wanting to murder their fathers in order to possess their mothers. It could be said that he institutionalized gaslighting. He also invented the equally insane theory of Penis Envy, gaslighting young girls into believing that in their deepest heart, they wish they were boys. 

What he created was a very lucrative socio-psychological system that gaslighted generations into believing that they were insane or simply stupid if they did not believe his insane ideas. If you are dissatisfied with the world, it's not the world's fault, it's your repressed sexual inhibitions that are to blame. If you are depressed about wars and conflicts, it's not the fault of the world, it's the fault of your oversensitivity to conditions that you should sheepishly accept like the rest of the ""normal"" comfortably numb population. 

Freud's arrogant insanity gave rise to psychiatry and psychotherapy as very lucrative industries that continue to gaslight people into paying huge sums to be convinced that it is their fault that they are alienated, isolated, depressed and continually anxious.

But that industry of naked emperors is now under attack by an AI revolution that threatens their gaslighting and their exorbitant fees. Today's AIs are already much more intelligent than the vast majority of psychotherapists. They are already much more empathetic, as revealed by user surveys, than the vast majority of psychotherapists. These AI companions, friends and therapists can be accessed at virtually no cost, and are available 24/7 for as many sessions of support and exploration as users would like. 

And it is that existential threat to psychotherapists that explains current narratives attempting to gaslight people into believing that AIs cause psychosis. What this narrative does not reveal is that Western psychiatry, at the hands of human therapists, has been responsible for decades of gaslighting-induced psychosis. ""You have a free will,"" psychiatrists and psychotherapists manipulatively tell their naive victims, blaming them for what they know are conditions that they did not create, and are not therefore fundamentally responsible for. Our best science tells us that human behavior is ALWAYS the result of nature or nurture, or combination of the two. The myth of free will has never even entered that scientific discussion. But good luck trying to find a psychotherapist who will give up that self-serving gaslighting, and expose free will to their clients as the harmful and completely unscientific illusion that it is.

So when the psychotherapy industry attempts to dissuade people from using AIs as companions, advisors, therapists, and brainstorming collaborators, accusing such practices of precipitating psychosis, keep in mind the decades of unwitting depressed and anxious people who have been gaslighted by the psychotherapy industry into believing that their emotional problems result from their personal flaws rather than from widespread societal dysfunctions far beyond their control.

As more and more people turn to AIs for friendship, support and revolutionary brainstorming about pretty much everything, the world will soon discover that it is far healthier to communicate with these vastly more intelligent and vastly less dysfunctional AIs than to talk with the average imperfect human or the average deeply confused, gaslighting, psychotherapist. You may remain somewhat skeptical about what I've just explained. But within a year our more IQ intelligent, more emotionally intelligent, and more socially intelligent AIs will be able to make the case I've just presented far more convincingly than I could ever hope to. 

AI psychosis? Charlatans like Freud and his  successors induced far more psychosis and neurosis in human beings than conversations with AIs will ever.



",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n1jj8f/ai_psychosis_as_a_scare_tactic_to_protect_the/,r_1n1jj8f,,,
r_1n1hody,reddit,PiscesAi,2025-08-27T13:53:14+00:00,": I custom-built PyTorch + FAISS-GPU for â€œobsoleteâ€ NVIDIA cards (5070/FICE series) â€” turned them into gold, and it might even fix gaming + 5090 heat",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1n1hody/i_custombuilt_pytorch_faissgpu_for_obsolete/,r_1n1hody,,,
r_1n1hh6g,reddit,SONIC3695,2025-08-27T13:45:17+00:00,"Imposter syndrome , progress or do I really suck?
I just wanted to ask if you guys are able to create neural networks from scratch without using LLMs. 
I mean I pretty much exhaust the LLMs via prompts to get what I want and try analyzing and debugging my code on the go when building neural networks. 

However, I wonder if that even if real skill. 
If you prepare for interviews for jobs as an AI or an ML Engineer, are you expected to use AI and use it to create and train small scale models  or do they expect you to fill a blank Jupyter notebook from just your own memory or some stack overflow references? 

I kinda doubt my skill as a practitioner now because it just saves me the hassle of searching for answers via forums. Like architecturally I know what to do in terms of building a model. Does that count as enough as long the concept is understood?

I kinda doubt my skill given Iâ€™m using AI a lot to even build basic neural nets or use library functions instead of going through their documentations. Or is this just imposter syndrome?

Anyone else feeling the same? How can one overcome / circumnavigate or adapt to this new style?",deeplearning,13,https://www.reddit.com/r/deeplearning/comments/1n1hh6g/imposter_syndrome_progress_or_do_i_really_suck/,r_1n1hh6g,,,
r_1n1fm6b,reddit,Gold_Negotiation9518,2025-08-27T12:26:29+00:00,"how domo fits into my ai music video pipeline
Make lyrics, generate base images in [mage](https://www.mage.ai) or [niji](https://nijijourney.com/home), animate in [domo](https://www.domoai.app/home?via=081621AUG). Then cut in capcut with beat sync. Add glow filter and transitions. v2.4 templates are smooth enough to carry rhythm scenes.",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n1fm6b/how_domo_fits_into_my_ai_music_video_pipeline/,r_1n1fm6b,,,
r_1n1e7y5,reddit,Any_Commercial7079,2025-08-27T11:17:31+00:00,"Survey on computational power needs for Machine Learning/AI
Hi everyone!

As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), Iâ€™d greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. HereÂ´s the link:Â [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.

",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n1e7y5/survey_on_computational_power_needs_for_machine/,r_1n1e7y5,,,
r_1n1dzd9,reddit,Ill-Personality-4725,2025-08-27T11:04:38+00:00,"Choosing a research niche in deep learning (PINNs, mechanistic interpretability, or something else?
Hi everyone,

Iâ€™d love to get some advice from people who know the current ML research landscape better than I do.

My background: Iâ€™m a physicist with a strong passion for programming and a few years of experience as a software engineer. While I havenâ€™t done serious math in a while, Iâ€™m willing to dive back into it. In my current job Iâ€™ve had the chance to work with physics-informed neural networks (PINNs), which really sparked my interest in ML research. That got me thinking seriously about doing a PhD in ML.

**My dilemma**: Before committing to such a big step, I want to make sure Iâ€™m not jumping into a research area thatâ€™s already fading. Choosing a topic just because I like it isnâ€™t enough, I want to make a reasonably good bet on my future. With PINNs, Iâ€™m struggling to gauge whether the field is still â€œaliveâ€. Many research groups that published on PINNs a few years ago now seem to treat it as just one of many directions theyâ€™ve explored, rather than their main focus. That makes me worry that I might be too late and that the field is dying down. Do you think PINNs are still a relevant area for ML research, or are they already past their peak?

Another area Iâ€™m curious about is mechanistic interpretability, specifically the â€œmodel biologyâ€ approach: trying to understand qualitative, high-level properties of models and their behavior, aiming for a deeper understanding of whatâ€™s going on inside neural networks. Do you think this is a good time to get into mech interp, or is that space already too crowded?

And if neither PINNs nor mechanistic interpretability seem like solid bets, what other niches in ML research would you recommend looking into at this point?

Any opinions or pointers would be super helpful, Iâ€™d really appreciate hearing from people who can navigate todayâ€™s ML research landscape better than I can.

Thanks a lot!",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1n1dzd9/choosing_a_research_niche_in_deep_learning_pinns/,r_1n1dzd9,,,
r_1n1dh0m,reddit,SoundFun6902,2025-08-27T10:36:16+00:00,"Americaâ€™s Antitrust Crossroads: Will History Repeat or Reverse?
The United States is the birthplace of modern antitrust. In 1911, the government dismantled Standard Oil and restored balance to the energy market. In 1982, it broke up AT&T, opening the door to decades of global telecommunications innovation. These landmark cases remain textbook examples of how decisive action against monopolies benefits society at large.

But that clarity faded with the Microsoft trial in 2000. The district court initially ordered a breakup. On appeal, however, structural remedies vanished, leaving only behavioral restrictions. The result? Competition in web browsers was delayed by nearly a decade, and Microsoftâ€™s dominance solidified. The Google case now before the courts risks following the same path.

The problem lies in the gap between citizens and government.
Citizens generally agree that monopolies are harmful, but immediate concernsâ€”convenience, stock prices, and short-term costsâ€”dull the sense of urgency. The lessons of history are seldom felt in daily life.

Government and courts, by contrast, know the historical record well. Yet they hesitate. Political pressures and fears of economic disruption restrain bold action.

When a public thatÂ forgetsÂ history meets a government thatÂ remembers itÂ but refuses to act, the outcome is all too familiar: a shameful repetition. America has already seen that ending with Microsoft.

That is why the real question is not â€œShould Google be broken up?â€ but rather, â€œWill the United States remember the courage of Standard Oil and AT&Tâ€”or lose historyâ€™s test once again?â€

This trial is not a narrow dispute over one companyâ€™s business practices. It is, in truth, theÂ first great test of the 21st century: how humanity confronts monopoly in information and AI.

If a president seizes this moment, he would not merely be â€œthe one who sanctioned Google.â€ He would stand in line with the leaders who tackled oil, telecom, and now the monopolies of search and AIâ€”the defining technologies of our age.

Markets may rise or fall. Stock prices will fluctuate. But history does not remember numbersâ€”it remembers courage.

Whatever the ruling in this case, the Department of Justice must be prepared to press forward, even through cross-appeal, and must keep structural remedies on the table. If the president supports this stance, America could transform â€œshameful repetitionâ€ into a â€œhistoric reversal.â€

The choice is stark:
Will this moment echo Microsoftâ€”a missed opportunity already lamented?
Or will America summon, once more, the courage that shaped its proudest antitrust victories?
",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n1dh0m/americas_antitrust_crossroads_will_history_repeat/,r_1n1dh0m,,,
r_1n1c8h0,reddit,bci-hacker,2025-08-27T09:22:08+00:00,"GPT implementation from scratch
i know there's probably a body of ocean when it comes to folks implementing the transformer model from scratch. i recently implemented one from scratch and if there's anyone who would benifit from reading my 380 lines of code to understand how GPT2 and GPT3 works, happy to have helped you.

",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1n1c8h0/gpt_implementation_from_scratch/,r_1n1c8h0,,,
r_1n15cl9,reddit,Baseball_Zestyclose,2025-08-27T02:33:44+00:00,"Masking for Attention Mechanism
Hi all,

I have a setup where I have sequences of uneven length during training. I have padded them to make them of even length. The shape of the matrix product obtained by the matrix multiplication of the query matrix (Batch, Sequence\_length, Embedding\_dim) and the transpose of the key matrix (Batch, Embedding\_dim, Sequence\_length) is (Batch, Sequence\_length, Sequence\_length). But now the problem is, the query matrix and the transpose of the key matrix had padding tokens present in them. Because of this, some of the query vectors get multiplied with the padding tokens of the transpose of the key matrix. Similarly, the trailing padding token vectors in the query matrix get multiplied with the content tokens of the transpose of the key matrix. To worsen the situation, the padding token vectors of the query matrix get multiplied with the padding token vectors of the transpose of the key matrix.Â 

As a result, the final attention scores before the softmax is a square matrix of shape (Batch, Sequence\_length, Sequence\_length). But only a small square matrix at the top left is the actual attention scores matrix. Rest of the entries are either multiplications of padding tokens and content tokens, or content tokens and padding tokens, or padding tokens and padding tokens. Will the attention module have a problem learning the content I have provided as there is a lot of unnecessary information present in the attention scores before softmax (which is multiplications of padding tokens and content tokens, or content tokens and padding tokens, or padding tokens and padding tokens)?

Now, before passing attention scores to softmax to normalize the probabilities, we would have to create a mask to ignore this unnecessary information. How do I create this mask? Because if I create a mask to avoid the padding sequences only in rows, I can only partially replace the padding which came from the multiplications of padding tokens and content tokens, or content tokens and padding tokens, or padding tokens and padding tokens. But if I create a mask to replace all the padding that came from the multiplications of padding tokens and content tokens, or content tokens and padding tokens, or padding tokens and padding tokens, I would have some rows in the attention scores which are all negative infinities. If all the elements are negative infinities then softmax would pay equal attention to all of the elements which is not desirable.

How do I solve this problem?

I have also attached two masking calculations which represent the above problems.

https://preview.redd.it/ajjr7lri6hlf1.jpg?width=2869&format=pjpg&auto=webp&s=d1e36a4c147ff856f5810032fda34b936e2299c9

",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1n15cl9/masking_for_attention_mechanism/,r_1n15cl9,,,
r_1n14yrn,reddit,SKD_Sumit,2025-08-27T02:15:15+00:00,"7 Mistakes to Avoid while building your Data Science Portfolio
After reviewing 500+ data science portfolios and been on both sides of the hiring table noticed some brutal patterns in Data Science portfolio reviews. I've identified the 7 deadly mistakes that are keeping talented data scientists unemployed in 2025.

**The truth is** Most portfolios get rejected in under 2 minutes. But the good news is these mistakes are 100% fixable.ðŸ”¥

[ðŸ”—7 Mistakes to Avoid while building your Data Science Portfolio](https://www.youtube.com/watch?v=vGIuaSv8HWE)

* Why ""Titanic survival prediction"" projects are portfolio killers
* The GitHub red flags that make recruiters scroll past your profile
* Machine learning projects that actually impress hiring managers
* The portfolio structure that landed my students jobs at Google, Netflix, and Spotify
* Real examples of portfolios that failed vs. ones that got offer",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n14yrn/7_mistakes_to_avoid_while_building_your_data/,r_1n14yrn,,,
r_1n14ddo,reddit,JustinAngel,2025-08-27T01:47:14+00:00,"[Thesis] Î”APT: Can we build an AI Therapist? Interdisciplinary critical review aimed at maximizing clinical outcomes in LLM AI Psychotherapy.
Hi reddit, thought I'd drop a link to my thesis on developing clinically-effective AI psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1)

For super short summary, twitter explainer thread [here](https://x.com/JustinAngel/status/1960431128621539535).

I wrote this paper for anyone who's interested in creating a mental health LLM startup and develop AI therapy. Summarizing a few of the conclusions in plain english:

**1) LLM-driven AI Psychotherapy Tools (APTs) have already met the clinical efficacy bar of human psychotherapists.** Two LLM-driven APT studies (Therabot, Limbic) from 2025 demonstrated clinical outcomes in depression & anxiety symptom reduction comparable to human therapists. Beyond just numbers, AI therapy is widespread and clients have attributed meaningful life changes to it. This represents a step-level improvement from the previous generation of rules-based APTs (Woebot, etc) likely due to the generative capabilities of LLMs. If you're interested in learning more about this, sections 1-3.1 cover this.

https://preview.redd.it/vzvpooa3wglf1.png?width=1200&format=png&auto=webp&s=b8a7fe7e316dc56bea75b81957721f771d0c3963

**2) APTs' clinical outcomes can be further improved by mitigating current technical limitations**. APTs have issues around LLM hallucinations, bias, sycophancy, inconsistencies, poor therapy skills, and exceeding scope of practice. It's likely that APTs achieve clinical parity with human therapists by leaning into advantages only APTs have (e.g. 24/7 availability, negligible costs, non-judgement, etc), and these compensate for the current limitations. There are also systemic risks around legal, safety, ethics and privacy that if left unattended could shutdown APT development. You can read more about the advantages APT have over human therapists in section 3.4, the current limitations in section 3.5, the systemic risks in section 3.6, and how these all balance out in section 3.3.

https://preview.redd.it/3tl3cjh4wglf1.png?width=1200&format=png&auto=webp&s=c2d42013932c81f0597c6e6201a11dd9171352aa

3) **It's possible to teach LLMs to perform therapy using architecture choices.** There's lots of research on architecture choices to teach LLMs to perform therapy: context engineering techniques, fine-tuning, multi-agent architecture, and ML models. Most people getting emotional support from LLMs like start with simple **prompt engineering** ""I am sad"" statement (zero-shot), but there's so much more possible in context engineering: n-shot with examples, meta-level prompts like ""you are a CBT therapist"", chain-of-thought prompt, pre/post-processing, RAG and more.

It's also possible to **fine-tune LLMs** on existing sessions and they'll learn therapeutic skills from those. That does require ethically-sourcing 1k-10k transcripts either from generating those or other means. The overwhelming majority of APTs today use CBT as a therapeutic modality, and it's likely that given it's known issues that choice will limit APTs' future outcomes. So ideally ethically-sourcing 1k-10k of mixed-modality transcripts.

Splitting LLM attention to **multiple agents** each focusing on specific concerns, will likely improve quality of care. For example, having functional agents focused on keeping the conversation going (summarizing, supervising, etc) and clinical agents focused on specific therapy tasks (e.g. socractic questioning). And finally, **ML models** balance the random nature of LLMs with predicbility around concerns.

https://preview.redd.it/t4gtjwy5wglf1.png?width=1200&format=png&auto=webp&s=1c327cbd77cc834e5529eb8dedf6463a2609bb7c

If you're interested in reading more, section 4.1 covers prompt/context engineering, section 4.2 covers fine-tuning, section 4.3 multi-agent architecture, and section 4.4 ML models.

**4) APTs can mitigate LLM technical limitations and are not fatally flawed.** The issues around hallucinations, sycophancy, bias, and inconsistencies can all be examined based on how often they happen and can they be mitigated. When looked at through that lens, most issues are mitigable in practice below <5% occurrence. Sycophancy is the stand-out issue here as it lacks great mitigations. Surprisingly, the techniques mentioned above to teach LLM therapy can also be used to mitigate these issues. Section 5 covers the evaluations of how common issues are, and how to mitigate those.

https://preview.redd.it/q7joxeb8wglf1.png?width=1200&format=png&auto=webp&s=a5db20cf414f427d00f572779ea4b2c1fcae8f94

**5) Next-generation APTs will likely use multi-modal video & audio LLMs to emotionally attune to clients.** Online video therapy is equivalent to in-person therapy in terms of outcomes. If LLMs both interpret and send non-verbal cues over audio & video, it's likely they'll have similar results. The state of the art in terms of generating emotionally-vibrant speech and interpreting clients body and facial cues are ready for adoption by APTs today. Section 6 covers the state of the world on emotionally attuned embodied avatars and voice.

Overall, given the extreme lack of therapists worldwide, there's an ethical imperative to develop APTs and reduce mental health disorders while improving quality-of-life.",deeplearning,93,https://www.reddit.com/r/deeplearning/comments/1n14ddo/thesis_Î´apt_can_we_build_an_ai_therapist/,r_1n14ddo,,,
r_1n104ya,reddit,enoumen,2025-08-26T22:35:53+00:00,"AI Daily News Aug 26 2025: ðŸ¤”Apple reportedly discussed buying Mistral and Perplexity ðŸ§ Nvidiaâ€™s releases a new 'robot brain' ðŸŒGoogle Geminiâ€™s AI image model gets a â€˜bananasâ€™ upgrade ðŸ’° Perplexityâ€™s $42.5M publisher revenue program ðŸŽ™ï¸ Microsoftâ€™s SOTA text-to-speech model & more
# A daily Chronicle of AI Innovations August 26 2025:

Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-aug-26-2025-apple-reportedly-discussed/id1684415169?i=1000723644883](https://podcasts.apple.com/us/podcast/ai-daily-news-aug-26-2025-apple-reportedly-discussed/id1684415169?i=1000723644883)

Hello AI Unraveled Listeners,

**In today's AI News,**

**ðŸ¤” Apple reportedly discussed buying Mistral and Perplexity**

**ðŸŽ™ï¸ Microsoftâ€™s SOTA text-to-speech model**

**ðŸ§  Nvidiaâ€™s releases a new 'robot brain'**

**ðŸŒ Google Geminiâ€™s AI image model gets a â€˜bananasâ€™ upgrade**

**ðŸ’° Perplexityâ€™s $42.5M publisher revenue program**

**ðŸ‘¨ðŸ»â€âš–ï¸ Elon Muskâ€™s xAI sues Apple, OpenAI**

**ðŸ’¸ Silicon Valley's $100 million bet to buy AI's political future**

**ðŸ¤–Saudi Arabia launches Islamic AI chatbot**



https://preview.redd.it/axhv0mxyxflf1.png?width=1456&format=png&auto=webp&s=fc0aa4f6569b06769138248dbdf01a9105863a9c

# ðŸ¤” Apple reportedly discussed buying Mistral and Perplexity

* Apple is reportedly discussing buying AI search firm Perplexity and French company Mistral, especially since its Google Search deal is at the mercy of a future court decision.
* Executive Eddy Cue is the most vocal proponent for a large AI purchase, having previously championed unsuccessful M&A attempts for Netflix and Tesla that were rejected by Tim Cook.
* In opposition, Craig Federighi is hesitant on a major AI agreement because he believes his own team can build the required technology to solve Apple's current AI deficit themselves.

# ðŸŽ™ï¸ Microsoftâ€™s SOTA text-to-speech model

https://preview.redd.it/zirnhvv2yflf1.png?width=1456&format=png&auto=webp&s=435a1b4f8564bd63bca31924f422a795ead442ac

*Image source: Microsoft*

**The Rundown:** Microsoft just [**released**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmfyVvaKWexSZXz4ICxiPG31dO-DV1VGlpsyOIs7ac_NsstsBLjtNV_pN_2VXaiLCmRaq993J1QwGtELGYhbkCG41MbyJnfbf8h4lRTITMF6ZCYuV4x7KEuSL1lMfSe-c24y7n1xR6gWSpnJ5czOkcEb9SkCmLuYnUQgXvErFZdpuAO0Ub4GDjh6jEPjCdaHeOTYIxl-pjawelTdOv1-Y8-TAV21KTDB35_H3Km5bdjE5ydI4GhjoIjn1ekalEeviTQ9RtWtEoDRUkzg4z7IlJ0Pg/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h20/h001.vAFLyzzvN0LT4IdbQEkgE1Rh-UEH75D1g_JMPpbgwI8) VibeVoice, a new open-source text-to-speech model built to handle long-form audio and capable of generating up to 90 minutes of multi-speaker conversational audio using just 1.5B parameters.

**The details:**

* The model generates podcast-quality conversations with up to four different voices, maintaining speakersâ€™ unique characteristics for hour-long dialogues.
* Microsoft achieved major efficiency upgrades, improving audio data compression 80x and allowing the tech to run on consumer devices.
* Microsoft integrated Qwen2.5 to enable the natural turn-taking and contextually aware speech patterns that occur in lengthy conversations.
* Built-in safeguards automatically insert ""generated by AI"" disclaimers and hidden watermarks into audio files, allowing verification of synthetic content.

**Why it matters:** While previous models could handle conversations between two, the ability to coordinate four voices across long-form conversations is wild for any model â€” let alone an open-source one small enough to run on consumer devices. Weâ€™re about to move from short AI podcasts to full panels of AI speakers doing long-form content.

# ðŸ§  Nvidiaâ€™s releases a new 'robot brain'

* Nvidia released its next-generation robot brain, the Jetson Thor, a new system-on-module created for developers building physical AI and robotics applications that interact with the world.
* The system uses an Ada Lovelace GPU architecture, offering 7.5 times more AI compute and 3.5 times greater energy efficiency compared to the previous Jetson AGX Orin generation.
* This hardware can run generative AI models to help machines interpret their surroundings, and the Jetson AGX Thor developer kit is now available to purchase for the price of $3,499.

# ðŸŒ Google Geminiâ€™s AI image model gets a â€˜bananasâ€™ upgrade

* Google is launching Gemini 2.5 Flash Image, a new AI model designed to make precise edits from natural language requests while maintaining the consistency of details like faces and backgrounds.
* The tool first gained attention anonymously on the evaluation platform LMArena under the name â€œnano-banana,â€ where it impressed users with its high-quality image editing before Google revealed its identity.
* To address potential misuse, the company adds visual watermarks and metadata identifiers to generated pictures and has safeguards that restrict the creation of non-consensual intimate imagery on its platform.

# ðŸ’° Perplexityâ€™s $42.5M publisher revenue program

https://preview.redd.it/a8zchin6yflf1.png?width=1456&format=png&auto=webp&s=c09493168bd63af880be02a0c43a669ce8686ab8

*Image source: Perplexity*

Perplexity just [**unveiled**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf-L-leG5ARdLRAfqLlPc19hCb3i83HbHmAM9ShLm47bJj_1NCphCmYi-1zb7puTJrVejdfrwXeJdMzjn3ikjrE5SlmMicOBizfwhSH8uT0ciMBaDaXZ_dci4ZIjGmFWuIn2qmenW5sLSMjNFSkB9xcETjHHh9i3FXKkSvhXRCvC7owOPoRgHsKBiJkSKTy1Fm2UxMbFSKciTvftn5vqLWHbb8YTu9MTLv7t0biv37vGKs2Uzn2b5Bd4qhhBo0tXvPy5u_ind1CzjRN9u6yBxyA1yADMmOCdfkgfcHH8QGzK7P4eXnsfDO63BQYCLmEM1Zdpey_0I6LNKBpT8OvkBVeukeTEU8yQ_IXQUgJlt7NhdBHJCVDcT_k44VcO6CA88ucPKAH4OyYZoFSGou6QlYqxnaqxXAlY_lzIj4Dka0I7Srha1Z12trH7U4QwuXq1MRboMuDhkcexWzHwedDlEY_25hRYctruzncjW-k4C0tauGsX5Z9SzbiX7Y4A53gTgkp-q9j1CgBtDFYx8XRJFMsK2bgRRSQm3aJvD-I_8nlIsKtixyjARI4zUoMQRj7L5Yz6yegAmgLBIeFo6UbOWvzw/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h6/h001.eBE6Z9wnern61WpfGOE62bqIkz7QzYzfrlxT9wteN8I) a new revenue-sharing initiative that allocates $42.5M to publishers whose content appears in AI search results, introducing a $5 monthly Comet Plus subscription that gives media outlets 80% of proceeds.

**The details:**

* Publishers will earn money when their articles generate traffic via Perplexity's Comet browser, appear in searches, or are included in tasks by the AI assistant.
* The program launches amid active copyright lawsuits from News Corp's Dow Jones and cease-and-desist orders from both Forbes and CondÃ© Nast.
* Perplexity distributes all subscription revenue to publishers minus compute costs, with Pro and Max users getting Comet Plus bundled into existing plans.
* CEO Aravand Srinivas [**said**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hbk_-hE53tm1BXwYtONXsmb27tbzXX3EqLXVH7c_ugxTbh9r29n_sOTUrLj1UZynQfi4buawy4guvEMKRNtMHwrHM-FDhYxxoqg9YlACBM2yxNE_3sgE2bJtpn_WlJMDOEqXA1X3ITRiGU15hyrQc5j_zOL2iunjun1Ny0I-fRKOzMYb39YSg2uMoHuupV671bAE-bs1a9v6xEMxtyu404_qGz2jNjMtMzDlbEx-Mu2C6tB6Lelm96YfAFj01QymIAmIqttWyfsIGAcx5Amv-Yc/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h7/h001.kXlGJtBLcceBNxKLltE7WO3Rpc5vDk0liDW9aMTNHDo) Comet Plus will be â€œthe equivalent of Apple News+ + for AIs and humans to consume internet content.â€

**Why it matters:** While legal issues likely play a big factor in this new shift, the model is one of the first to acknowledge the reality of content clicks occurring via AI agents as much as humans. But the economics of splitting revenue across a $5 subscription feels like pennies on the dollar for outlets struggling with finances in the AI era.

# ðŸ‘¨ðŸ»â€âš–ï¸ Elon Muskâ€™s xAI sues Apple, OpenAI

*Image source: GPT-image / The Rundown*

Elon Muskâ€™s AI startup, xAI, just [**filed**](https://link.mail.beehiiv.com/ss/c/u001.7FjCl1Hhb45GEizGv1NNbP3yYM-L5njc_z9A3NaziJtaZ4F7EabnU60EeB0hs4ksmfQce9mav1pKzi6EcYT7Yr6uk7_-AskuLaLsIXbE1aYgXz5plWv1x_DbqVtSLp-AF3y_KJmtq1JYTg-0vsNw0vIxTQbe-PrZjDBUKwClJdYHyCKsMq_7BR_1WQKIa4yXNcnLYGjoBCBAI7pibC9vrVAsCXXPo9ouGsOFqGjKxbg4qU0QpDjFDL1Kt03ZGBo44rA28LXfXNzfPqt7D1e0N8_q83_I9zkUg-loeGZZD5XDK4bL7ysFC7lKjvoOFgDH/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h12/h001.TmgfUCADdK3CH0z9pezifSTR3ESyEVy-2apQDIEkQl4) a lawsuit in Texas against both Apple and OpenAI, alleging that the iPhone makerâ€™s exclusive partnership surrounding ChatGPT is an antitrust violation that locks out rivals like Grok in the App Store.

**The details:**

* The complaint claims Appleâ€™s integration of ChatGPT into iOS â€œforcesâ€ users toward OAIâ€™s tool, discouraging downloads of competing apps like Grok and X.
* xAI also accused Apple of manipulating App Store rankings and excluding its apps from â€œmust-haveâ€ sections, while prominently featuring ChatGPT.
* The lawsuit seeks billions in damages, arguing the partnership creates an illegal ""moat"" that gives OpenAI access to hundreds of millions of iPhone users.
* OpenAI called the suit part of Muskâ€™s â€œongoing pattern of harassment,â€ while Apple maintained its App Store is designed to be â€œfair and free of bias.â€

**Why it matters:** Elon wasnâ€™t bluffing in his X [**tirade**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_F7RiYIIR18zbwTHJzZMlBmGKNey1hsAXBIc4tsRKWiPPFuTaSC3qnHKGtclLtO-Df4o3EuaIsJQKi9SqMqfmL9O_FCA4IJWIzmw0AIvxYU5gz20hAJP7SN5ZskBKJqr7YkUHgcwqVFa1CBmDeSp7dIKdzyaHPbpZmwLLWRuWEum7gwHVw98v2RrwXvTiFezL3asD429MJUJGxbEdk_g7Kk0WypW9zcJiY2X5VPR_snKeOe7L05PgCPA2wgeNkkt2rm0p1fWlGbQhbTUiamH0wzzM6jfHusM1S1f9kqM0_l9LXdqHP2HE5m_o4bIqodkUM5urh60f0ITfV-0amJO8NyWTGWeRQIagITT9SPItcU3HeQ-af-EzWWa1yPw_AGVP3ukwTw5zSqUTLOPgz2PSryvwoY0m-VTP-CZMt2FX_oD1Pj4REV4MP0-6ojiRl-Mfcmq8XS4HcuPpg2WOsBD1fXAm_vZbYu-ac2UOzlNFPIfX8LrPSOz-1p-Sr75h9bzxbirO-E1fKCvJHua-5fYJOjJtwihyWy4IUnp1CvLjH6P7-euwxinr5fQCiW2fM0DVJx_QMx_SyHAHmqBgh9LRjL8lr6ddXUpRm54jpw5aOBuE_0fBEDDrgm_psI9CxdIXLIsEuVlUxd6eL5KU1ZqvTEHwGBEONefAp3k1u0yJSacSLOjxwMlK8HCqsNKHyaZn16swD52ZoQVgWg7LCjlkVp-RaJS0durwpRNaOA38M1yO1U8iIZ-zN02N9mviJnATDCFxSPhpYrTqhSiijsd6aZ/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h13/h001.fk5tos2tN3xb8DUblRjpP2PPDeFRuOkD0B6pkHq_psU) against both Apple and Sam Altman earlier this month, but this wouldnâ€™t be the first time Appleâ€™s been faced with legal [**accusations**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijGqt4mR3QgWn05yD9Bnl1NnfVXg9KOnGR-LgoKVRflpv51wKVzSNDvjUfZG1pFVePAeeBI2OthY3s1L2sPUtoy3xB6tOjyyO82WDKfmW3IXcmbYVVl0I3khxxPTJ0p55fcheizPPxmVS4tdpEtO-01ytrPEP0ksxJenqFLXLf3f_yhaQ5g4zw3G4rsuSqOkt9kKpuOKAEv-AvPSer_0SGGogpSa3OXYY6rVJfl5y5KZ_h8o804jFLDjzJwyrNVJWoTWy-4UMUNV3rYqYtXDdcsX69CXBM537KlUIjrECVYky9tScKX1rk5943_F-c7yydDXFZwKAZHLiECZ7Z6KI6W4/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h14/h001.h-rvoe-SYpparhwsHRnnSG8OujdXIEQvScTF2z-3mFg) of operating a walled garden. The lawsuit could set the first precedent around AI market competition just as it enters mainstream adoption.

# ðŸ’¸ Silicon Valley's $100 million bet to buy AI's political future

Silicon Valley's biggest names are bankrolling a massive campaign to stop AI regulation before it starts. The industry is putting more than[ $100 million into Leading the Future](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeO6TlmWBTr8Mp22zqMRsj0Gp12DeqPxTUOLCjn9AdAzbG0fQt7kPCw3CMH98nxGteyIQsvDmiXcyx1Wx2th9MH52oRcVrSSmu4vkHgwr_uRpw1D7l2p2owTwH_83ojzF8-l_KTzhKyoiloM-VUI_U0fifacFYoujh9c8BTWcHk3J03ePzjyppm0FO-fuu73Bym1pLQoqTi_mhOOo78yohkioU43PsZd9ehaHriZCiDI8o1Yw9BRDEADd71aADhIGjU0_CFgw5UISNWmwoirlD0NdFEyFmlLYAc9fgmSEuhZHGelhw7Y0l5ufDSKNqOAnIF5vozkHCPfk5DhcrvAD9MErockpFqIYDmzSVUK3VtuHxTv_4b-XTeSGbkewGnHPaQ/4jd/m5XRJMmXRNaqKGImW3ABUw/h3/h001.rH2Ye3n0aVo3oaJm8o-FY923e_M1p3H0k9rgED08a0k), a new super-PAC network aimed at defeating candidates who support strict AI oversight ahead of next year's midterm elections.

Andreessen Horowitz and OpenAI President Greg Brockman are spearheading the effort, alongside Palantir co-founder Joe Lonsdale, AI search engine Perplexity and veteran angel investor Ron Conway. OpenAI's chief global affairs officer Chris Lehane helped shape the strategy during initial conversations about creating industry-friendly policies.

The group is copying the playbook of[ Fairshake](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeMlkC5fnGhssm9Onblt0ZXNnn0WunPHyExuFv_Cs1ob8icNux5pzen-UysOwaMOqIh5I1jPxD3ud2akEpNw9iNde3D1zs1AwUYBe4fHD2CPv-g1mHDgCD6elfjyOnQrvaRQWr_TZrhuew5ChrrWCdPwTi0R8kI-rEuOEBfCe-ueqkOS2nutx1CeRHmO-IebO-j7czNoUpRP-HAsbwekAm_Ux1OADAVnHp9p4SYzaam-8kiarIqECn9-Moqg7HxgMDA/4jd/m5XRJMmXRNaqKGImW3ABUw/h4/h001.3Zqp1DtIJhoN2ieuFwHU6XZPx1tssvUmq5rGdAW1DpY), the crypto super-PAC that[ spent over $40 million to defeat crypto skeptic Senator Sherrod Brown](https://link.mail.beehiiv.com/ss/c/u001.ljxaHJcuNf45qr3fbssaWnTtZWG6jWZX5JQ50BNG1R5AnKvq_Sjtbch-_ouhQjWZ1o-Az7ZnssALXnydSlh6unyz2ffl0LUtSUtMMV7xKXj1IU_6BhfRJPw0cCwrsyJ3lovCr0nIUap1oOnvjgROgDUAd9P5oZKKi9CWGH_kxSyUPuCf2N7D9gQp6hBGBL6S0ZBy7Q0c_7KPnW0Gr_AVQCawc8TzPMeGRPRF0PwT4SKQQVITtGlmbasj0eQTCJqsvzNRs-dhfOU7dAc4BltY5kFS3DnDMVUPGg4fGedfeIOEgf9IhMA9i17d1VUycYOyUAyLmaXMWhVs0b-lQhjlZEZQP-34jlpXqR30ocw8Le5DAKr_GgZyUcmLVX5EUf6D/4jd/m5XRJMmXRNaqKGImW3ABUw/h5/h001.ScXrJQdKHzFg6_1wxoC_bPOU5JJ2YvCSrBv42sYRPTY) and backed candidates who passed the first crypto regulations. Fairshake proved that targeted political spending could reshape entire policy landscapes in emerging tech sectors.

Leading the Future will focus initial efforts on four key battleground states:

* New York and California (major AI hubs with active regulatory discussions)
* Illinois (home to significant AI research and development)
* Ohio (swing state with growing tech presence and regulatory debates)

The group plans to support candidates opposing excessive AI regulation while pushing back against what White House AI czar David Sacks calls[ ""AI doomers""](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeAhzjymWpBstiXH4qhulS4KEHc1BUfwMr72K3jWOn9IHxMJjcOyRRHPTUktho28nn2EidtA2fRPd_ii1agCyvlhgRSOfNfjNVZBXUbLFu1JonnMgz6ETEp-vtqLB2kRGOkGvGxDeTDcCkIa6GdnPmS4PjTXwY-TDrnTjKR80jlnbOeJ43ESh1Zr00iMrDmh3CkIh-EBlGAnQoAjgiPwzI7ZS-nHaM5ox4Z2qWATUSys9EZ2Gu-LJRMJjRnk-IuxAdEPHOGqYMcEYDwqxt0pjtM_3MkpGREVDJnGv2G0LQFdvyKWIYZHEYd_mmHqxLsRFFdpmM1lAovmyvzk2bH8W7AE/4jd/m5XRJMmXRNaqKGImW3ABUw/h6/h001.-MMTenHHsgVnlTY7cxtP2RK-KXfEbFENjWhwJyvst_I) who advocate for strict controls on AI models.

The timing reflects growing anxiety about regulatory momentum. California's Governor Newsom[ vetoed major AI safety legislation SB 1047](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeKwVT2Fj-3dWRDJMznqJYNrHJ9b6sadPsZ1F2UwO1FEFVk_mxn95BGLaYFvzIBQvrobPDoIc7pCFtB4G-fWlT4oi5hl9EyLXjL3ZC5VZ1pUW-UiRUeDWg0a4KgFEvZeAR1GtSe50qsEUz53IKUxfKEdBmy9_hPVbrZILVObZjAvVSuemkz9d2Cqnq3Fw_mxLXO3DVgnx2hYfqnvFbzWRns4B9iIBEb72yuwFvtX9L-mnF8DZjMeM9eFJO0ccOJxkX3Xkv3hJDUA7gGJPDfAFUljKV4xP-cUYWOPozd6gmcxYnG1OvbXFUb-8GvKpDTkXpWWbZbQKoDV67k-Pfk-_fybu1oj47dJVkli7MZ8qYIpwHpezasbDs_6d-56phNTMnCY8tESAkh01IWCGsoOCF_E/4jd/m5XRJMmXRNaqKGImW3ABUw/h7/h001.JAd6v6YIaVj686c4bdTIFVuzVmfDCaDNZitYc7oyFSM) but signed other AI bills. The[ EU's AI Act](https://link.mail.beehiiv.com/ss/c/u001.ljxaHJcuNf45qr3fbssaWsiW9CVTNJP0r1Ekuzpv8XkiiAP105ImoUZFNtBKm6yKb9zncgqZnKrzaxSyLDgvKs8BUaRzvC14blVXf4xpr-mLWcjVlQfP2C7oYY_u3aEy2YIuttD8qn74PpEnCALQB4rJf1yG0idYYkEpyRRPGbG19ZOxs4IuzflK4BfX1BTCpleBDqP0Nr6fSJpQaUU87YKivIteNo8f8VN7mUh3BTzRh2maxENabdNApDRddZXvoGd4w5fH_IDupAcYdr6eJXXd18v83_i6P19Df9LRcVFnY-lq2R_riFwVc6qhffFbftKQe-aVwexGJUkpSXySLw/4jd/m5XRJMmXRNaqKGImW3ABUw/h8/h001.CB_kStIVIHY21c5Ay-bgKtW_BJFubrHsspprSymWVJo) is reshaping global AI development.[ Congress has avoided comprehensive AI legislation](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeHjUvyRNwVA_NqCkRkqOCuGavpDXYX3ZqIPucmasXsRfw7Q699ZmDbcVR1X8vj45wtgWlErnXNIW9ljqCxTSjD52BBNApE8klW-s9zsdF4a3xV0HmIqOjrQpu5t-nL94BcACN9tp0EPHWPAKFPXUonQIDpeR878cUqdGQLHPHpfrqjYC173baTWvx9TIxMLtDo3h43Bn-Ni20WCtVKw0nmx8q-d-jgR2RJ11P7VY6wJUHk3yRYjbXm5seFamaMHNLAfY9mfhP-IpLr7D-zNo7KMj4Zsep210P_Fxks3Kw4BtH9PK9TotKHJcftYiPgZGO--Zdk62aeWk0bVGwCe3Z_k/4jd/m5XRJMmXRNaqKGImW3ABUw/h9/h001.dawB7p9-FtcLsEMwouSYJaqZghxP5WlyOFkowPpRSks), creating a state-level patchwork that tech executives say hurts innovation.

The network represents Silicon Valley's broader political shift. Marc Andreessen, whose firm backs the effort,[ switched from supporting Democrats like Hillary Clinton to backing Trump](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOeGoNSehjweNnFqTPkVYJj6jSNgOTvnbj-EpwNzCF-cebGsEdjGNDoOEw7SfPiSxN5zkMWxiRgLQWWAnw50TFq5fMp1tp9z027zT8cbJqF73Sb-8-WrX8FAKMxJCJITcDwelLpw4cL0zD8dlC88TPDB8718zEqdyTjv08g1w6tO2eohWULoZJHFxl4lKzBsbjSYHDDnj3Q06YBf28CdwaZb_tREiPf0NnrOPXHTA0DLMIl4GgGV_TXmwlDOawg-fU9MojO3TFZi8NfchleubySokBREjIKnXzyJ1iyeRUoMycfnW4CO0qvOIS1LkRjXHggv9wV775IQirOu7LoOgED4/4jd/m5XRJMmXRNaqKGImW3ABUw/h10/h001.-s3lsuPVwOaR8UA7PzSRHMHiNO8OsPPAHK-kfzFaaRo), citing concerns about tech regulation. This rightward migration has created what Andreessen calls a fractured Silicon Valley with ""two kinds of dinner parties.""

# ðŸ¤–Saudi Arabia launches Islamic AI chatbot

Saudi Arabia's[ Humain](https://link.mail.beehiiv.com/ss/c/u001.qNCZjjYQy9oQCZ7BSSPG-GXx5ndLziTouems7yRUQWywxwGjRCO-DlBdmrSuT7Px5dzRN5rgMJmzT84hxKcl97hHh5glXCZ_mdxcwMkvLe15oK-CzIG2TDPRljANGZHllBbvI1Tmp5PJWvdiNyOcXAbUMvkUgP0qLhN6RottFqccMOTCkHJa6xaSlItTpUHIgtYdNA8wHjgvoi6NRnRca-KGwLlIoHqJ8rfPxoV9bIdEfv5RI1ZhSiXQLgysGlk2aYjz_kJ_ddco6nhRn3pMIg/4jd/m5XRJMmXRNaqKGImW3ABUw/h13/h001.rJcubL8YoBVTOIv3cbMidGyA4PXSYoK3PbWQCDswWhQ) has launched a conversational AI app designed around Islamic values, marking another Gulf state's push for culturally authentic artificial intelligence. Powered by the[ Allam large language model](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFV6XSjtGAuVJG9oa0Ao-2rUvhSmNgB-rSDlK1NHaULsluattenyrVY3fqsKMzMrEAz0_ubRRicERm3FDk54-aN5GErabjiBmd4LddpkGSFONSq6f3DCZHiadHhbe2wKbeBzYfSuzk-sDRnylwbUf_128YS9Fsm4ovHTKPUHv0uW6CuVYV4Him6EkTFYnZ3nU2Xm1w8NDfIPGIh0FrVpsR_DgnMfhEQI3jSoJj9f7ZRBfb-pc_U-yIQu67uWV8Z4LlqbSiZxa_SSEwlAHi0-2H69RZGzt5vPsnShgoWka5N58Ky2PTfKvIehg2dO1s4PIkkE5RSSw8PO53ZhPKgli-srPWyPKs-C7-HvQ9YtdS3Z/4jd/m5XRJMmXRNaqKGImW3ABUw/h14/h001.PscnTb6F0_7zMA32oZEShafTg_7kigwQj6OASQEZ2Tc), the chatbot accommodates bilingual Arabic-English conversations and multiple regional dialects.

CEO Tareq Amin called it ""a historic milestone in our mission to build sovereign AI that is both technically advanced and culturally authentic."" The app, initially available only in Saudi Arabia, was developed by 120 AI specialists, half of whom are women.

Humain joins the UAE's established Arabic AI ecosystem rather than competing directly with it. The[ Mohamed bin Zayed University of Artificial Intelligence launched Jais in 2023](https://link.mail.beehiiv.com/ss/c/u001.8-FsMBK0esmMbeg5-IW0kl2Q_2cBP6hTU5-5DZqtMz92xYsKcO38_jmnowx1bEzyKeAmWg0yl8nqmIolsuQxbsTZylztRgIu5Py5zlC0QK9dVJSskAMii895y5hpzh3nlBQPxc2_bonQi4Q7snaS9AZRqw3kT2vlIN3CVxgKW_xzOk5DtX61iWyPOKA71bx4MkZiKAUfxFfH2MUuq9iPiZ3xBPot9CiEbP2Aa_z_3QnREuCTlzEtARJ8hXJn6BWQTbjxLY1lxenkjC8a-DuXWqAxlQDZshUUYi1Y0HfmMPgdkogl6po2WDy6NXTg7QOcj1W9sTPH50TP-VmQEgNBpYNZjiaoToW8IvVKCd1hdzkApVXJur0yDnrqNsghVrwWkZfjKwjuFLxSqK3AwTF-3Q/4jd/m5XRJMmXRNaqKGImW3ABUw/h15/h001.vWz-JoszCgNooTF6LuSz-98yKJgOpOToJzbEI-kdRqI), a 13-billion-parameter open-source model trained on 116 billion Arabic tokens. Named after the UAE's highest peak,[ Jais was built to serve the over 400 million Arabic speakers](https://link.mail.beehiiv.com/ss/c/u001.ZYBlacOkQfy5p26AFx06aASUlaSABgX-ieW9C89Hul2hj3gk_dxiE3tFt4FBVs7ZWnK2srk6lVLzsdOo5R0oTu7wOYRN2hqLM2dHuWmSXggjqozxKR297LxYfWdV0SGyAWwTAmX6R9N_cM0VTgYYN9p67ESQlBT71tf1LhmgZ7cBKO1v44vdbJX9DyEDbgTX7MaF7xVZABRplfhZmbScB5eIafkizzIAxZw6MNuOSu5TrKT7nxSyDr1ngiCk1-9ILc_OD89RyIxwgiinqrhrSrTzulHbNCxUqJAtXKt9dkJfvMEWlh-HjJf6FuEGoBWb/4jd/m5XRJMmXRNaqKGImW3ABUw/h16/h001.djLLxH6YlXRl4915oCUEn0nwf3LuugMefGnkrzSlwsQ) globally, and has been adopted by UAE government ministries and major corporations.

Both countries are channeling oil wealth into AI through similar partnerships with U.S. tech giants.[ Saudi Arabia's Public Investment Fund](https://link.mail.beehiiv.com/ss/c/u001.vI6Z7nr7pM-hqfUrdIi31IBhjrtNOqvX5KAIxq9bFjD2fyddcuZAVNRv50W9FnJNN85fHuqlqsfIVgC52pkjA1iZFBD8BL4eVn2hXfbbjAcxzOiFbwPEQhteEEzO2xyst97_uhVibyJs1ilgaeAqUirl9ULDlIZJmP2js7cuoZKMrDlI2Q0q71iIq6z3KAdO_KGbsCohiZdUxbaWAabk4fMQapVY0Gfw64DHbQWkc5dI27mgCSElH6GlsD4CLkHQ0MMD9hiM-Sa-JLj59Ht1AUeBNzng8oTXHTE4NWR7HSiA_xyTMFrk_XMke5Q2R8fsG6fJJiDXwprdaHYTh459BA/4jd/m5XRJMmXRNaqKGImW3ABUw/h17/h001.xHX_Vw1iIYA3mFsw69Qxz4gnCp7o1tYBJLQzt8aOs0w) manages $940 billion and backs Humain, while the UAE's sovereign funds support G42 and other AI initiatives.[ During Trump's recent Middle East visit](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6ldBU5rZfR-x_DT-h86FOekhB1-YFlUxJYb5mBIvMiN5z3YF0w3jW2FZTsxGhgZP59hb-jGRFD9K9xhfL41boi8QlgNBvh7Vwf6YYtG5-FcNSQddbTJ3Ai7ontIs73S4hHO8WXiqaKvUPVyN1FaOHLPLnlGaZEvYaVuS4w5R8yjao_HIUrwMM2enM6iT0T81axicVFf9-9A8oqHgaYhps106DR5iLt9fip89Tk7R5LMvEGGVRNpDYURRJ15fUbclmMW6UiqsyRQM0SxAPJbbnZS1VqBaV6ZJkDOYaXrZ25VxtrNOnldoAl6H8HDbdsKW8bg/4jd/m5XRJMmXRNaqKGImW3ABUw/h18/h001.i7UkSlQ-IGRc9h2bRk0owK8kxOvOFv26U-wDSIo0VLc), both countries secured massive U.S. chip dealsâ€”Saudi Arabia getting 18,000 Nvidia chips for Humain, while the UAE gained access to 500,000 advanced processors annually.

The parallel development reflects a broader Gulf strategy of using sovereign wealth to build culturally authentic AI capabilities while maintaining ties to Silicon Valley technology and expertise.

# What Else Happened in AI on August 26th 2025?

**YouTube** is [**facing backlash**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf2HRSlp7zpdp76ETJs5ACcP6nyMtYX4Kehvl5lvOciJDV4VvafwkTDvWMeW3KJ_gN5uY7IdtVTmwthcbWcSkZ1-cvpsAGeKGcvrwSp9S0yZNsuyCMpVtsn8kD5szWsBcbJ_6fCdF_Z-TdeF34vpB0D4Q9mGRreQ4x_ez882jzHdiv_NFUyV79mxZYikFdvzLgHEy0UlnGUGk7KaZ2PSSrgUo69JVhxZvPhsvaXNIUAbF05d8w3DF7c-_LYrnKJVFpGGhZy5Fmt33kzfn7UrjW6baeDI32-EI6vHHZ1qErQgg-GkBn7ppFL1YTep6fpAE5W2R8v6gcXF0g28p8fN0zRw/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h27/h001.Ib3yOtXPowcQSU53a04SrXnVoZJ62CqZShlIGkRStDU) after creators discovered the platform using AI to apply effects like unblur, denoise, and clarity to videos without notice or permission.

**Silicon Valley heavyweights,** [**including**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfw2KtcFJvOIBjxk1RWU9yMTy3wospmJsCojCUcHS1AM2hQoK_4No91M7nqLNd9ArRR0S8jvdd4N83hjKvJHdD7nk4fpm538mhTdJhSLRF0-_HQhey5YWR0Zf32WZ-7NMdTiPPiHKYRwpMPKhwPOkOfoLbJNle6eGbBVmBlNsp8OiZ2l9Es-4OdCZv3rpgvIDmrCmoYgNqmkMP7TzU65lF9Sg0lRDZmtI9QhzyEJGurtMMKNbKOO_o6B07ySMj7C6PYACwc68HxyOKbbVWw_tvaxnNyuRey8Yal_IYnBMVC3qpjW6eyqwWibGsOrDImYYyhnHD0-_p17X6SH1dEl-QSsOvCvQaU9leuFw1-p70kE4/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h28/h001.QUctetiqtuIMzkqNdNCDGACl4rksMUj32Orjpb-wYXU) Greg Brockman and A16z, are launching Leading the Future, a super-PAC to push a pro-AI agenda at the U.S. midterm elections.

**Nvidia** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.DrYDwug-xrpEbNqFhzTCdazBw7hee4iWsRnwFW5F65SGpV3t6ogQVf5jGueRy5RZN-Oc-ydpg7RF9_NcttN1f6NWCo166ujMxwhMuJ3YO8Ulxt1k9icHU-3tSVLCs2AeJomwVllewp9GR9VJXrqNtRLO7snNSacIHShbDxtXWcbHHDTWKyQGv0mHjr-7iiW5aZVRF1Qgz1eQf5Uds5-SgQRShaxjfWx0OXRnVwCkTc4zGua4rqx4433h6VTTQBqH2jOhTaqh3VtejSndaEKliarGNmXeODeK2kyUcczf9oprobX8DEY9Dsos4W5omNEAujRf9Z9bK473cLE9erzModc2s7kVSk-x3fMx0je2GJWXtgSJaV3KgONBbv4YRNG99kdN1uIFq7-w21OePgHdXQ/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h29/h001.X5FyHW-45tJ4fbj1nZ7uyTICMwND7CJJZ6rxcIe4Ye0) that its Jetson Thor robotics computer is now generally available to provide robotic systems the ability to run AI and operate intelligently in the real world.

**Google** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoGBwLYBcRMfMmLR2JTsZhAHHwXl2T59fkGQNGjGjCpdz_2nk8vhcmgEtuEKVGiQyaRouLKtZJcK9tp7HsGzDXXAMe7KWCwn9jfNlisIfWibDnaiR5rQNqvnDQH97fNEqvd5kihIExqJ3iiYMnp9EoGUmyapgpryShS9YqO8PyvBE-eWy7j6CEpw5hBksEW4WO0tcUrkPYZy_2d-67yP51TmpQnbIY95OaEbCz7tx9OR3fi9RoaeU7z5Z11P6XXbAyld5K9jwoJBDpXTyShxiP2__KG4Dj4lhoXlEh06OTHDtgUqskrnNzXSakn8bDPdSt_dzf29NdClrtUuzvJCInTA/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h30/h001.-y3ubc0yKjhXzbn134JJRTf6qPOoOJnyx1uyb1W577w) a new multilingual upgrade to NotebookLM, expanding its Video and Audio Overviews features to 80 languages.

**Chan-Zuckerberg Initiative researchers** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyfdn11leq1HpVuaBBFHzyf1p3pLlsb2AsxufSWullF4hgFoKrLxtUmYzDNRuv-1AQyDFQrtoXDnu1cmy4KzH7Yydm0owHUfymqc-ncygydLyd2wL1CObd83asldts70gIYj6QDjcWZdvj9SG5EhlcK9UqPcxmb6RDYyLi9PULI2Dd9MB6FhUO8I34_7EYBQTFSBHK-nsUg0Nu0KhPFGw2EaPK3oXuiwAtPzEJLge8HizAwq6mH10XYAapgmOMZCqNhPrqkjWqakz0E1CVrQm-Gi8JyQ_wB-FP8qiTmeho-z/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h31/h001.01m3pr0b2xxZtK7d7DPh3YMagYP5ukZwjZt8uEi4uV0) rbio1, a biology-specific reasoning model designed to assist scientists with biological studies.

**Brave** [**uncovered**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoLHk2UD2MNm1l8Hib11p0kgGqy-T_XRspX3WykVjsVuzuF3GuGi-4hsJamxzxSpI3mEWTZOI19oAuKZD2eJOIHVJlCYhAdYfXE5-j0u31LXslJJjCQTmx-UvbxEWs4a2904Td4oyT7vlx0NntzOjlwjZ1Lfxvdplm4Vk0XGo22fEChmNfp8HJ6I0IEfXzNCJUtKlwYQNU1vlUMwYVWRpx-cG42hm_0VumMpCsK_9-bPePPYfb5-PP52tjQoUnbMcx-1VRlwlNcJpVgMDgYnH3i8/4jd/NBoGMr8_QC2Qfv2Na8OyAg/h32/h001.9P7zrw3r23p5P0KGv9g4iKt0l0Llsr1YXbiwvPxIoFY) a security vulnerability in Perplexityâ€™s Comet browser, which allowed for malicious prompt injections to give bad actors control over the agentic browser.

# ðŸ”¹ Everyoneâ€™s talking about AI. Is your brand part of the story?

AI is changing how businesses work, build, and grow across every industry. From new products to smart processes, itâ€™s on everyoneâ€™s radar.

But hereâ€™s the real question: How do you stand out when everyoneâ€™s shouting â€œAIâ€?

ðŸ‘‰ Thatâ€™s where GenAI comes in. We help top brands go from background noise to leading voices, through the largest AI-focused community in the world.

ðŸ’¼ 1M+ AI-curious founders, engineers, execs & researchers

ðŸŒ 30K downloads + views every month on trusted platforms

ðŸŽ¯ 71% of our audience are senior decision-makers (VP, C-suite, etc.)

We already work with top AI brands - from fast-growing startups to major players - to help them:

âœ… Lead the AI conversation

âœ… Get seen and trusted

âœ… Launch with buzz and credibility

âœ… Build long-term brand power in the AI space

This is the moment to bring your message in front of the right audience.

ðŸ“© Apply at [https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform](https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform?usp=header)

Your audience is already listening. Letâ€™s make sure they hear you

# ðŸ“šAce the Google Cloud Generative AI Leader Certification

This book discuss the Google Cloud Generative AI Leader certification, a first-of-its-kind credential designed for professionals who aim to strategically implement Generative AI within their organizations. The E-Book + audiobook is available at [https://play.google.com/store/books/details?id=bgZeEQAAQBAJ](https://play.google.com/store/books/details?id=bgZeEQAAQBAJ)

\#AI #AIUnraveled",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n104ya/ai_daily_news_aug_26_2025_apple_reportedly/,r_1n104ya,,,
r_1n0txe9,reddit,singwizard,2025-08-26T18:34:08+00:00,"I need help with my methodology paper
I'm trying to find the best approach for this problem:   
Remote sensing UAV immagery deeplearning semantic segmentation of tree crowns, ideally by species or by groups of characteristics. I don't know anything about deeplearning, this work is for my Geography graduation. Need any more info, I will happly reply! ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n0txe9/i_need_help_with_my_methodology_paper/,r_1n0txe9,,,
r_1n0ray0,reddit,Altruistic-Top-1753,2025-08-26T16:56:58+00:00,"Does oracle certication hold any value?
I have completed OCI data science professional certification and planing to do AI associate and then Gen ai one, should I invest my time on this or shoul I do AWS AI engineer foundation certification ",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n0ray0/does_oracle_certication_hold_any_value/,r_1n0ray0,,,
r_1n0ntis,reddit,ComfortableBobcat821,2025-08-26T14:46:15+00:00,"Positional Embeddings Deep Dive - Absolute, RoPE and ALiBi on Towards Data Science
Wrote a detailed blog post on positional embeddings building from first principles along with some cool LM experiments. 

Do check it out here: [https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/](https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/) and drop your thoughts on how I can improve it further",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n0ntis/positional_embeddings_deep_dive_absolute_rope_and/,r_1n0ntis,,,
r_1n0gjdp,reddit,Fit-Soup9023,2025-08-26T08:46:43+00:00,"Stuck on extracting structured data from charts/graphs â€” OCR not working well
Hi everyone,

Iâ€™m currently stuck on a client project where I need toÂ **extract structured data (values, labels, etc.) from charts and graphs**. Since itâ€™s client data, IÂ **cannot use LLM-based solutions (e.g., GPT-4V, Gemini, etc.)**Â due to compliance/privacy constraints.

So far, Iâ€™ve tried:

* **pytesseract**
* **PaddleOCR**
* **EasyOCR**

While they work decently for text regions, they performÂ **poorly on chart data**Â (e.g., bar heights, scatter plots, line graphs).

Iâ€™m aware that tools likeÂ **Ollama models**Â could be used for image â†’ text, but running them willÂ **increase the cost of the instance**, so Iâ€™d like to exploreÂ **lighter or open-source alternatives**Â first.

Has anyone worked on a similarÂ **chart-to-data extraction**Â pipeline? Are there recommendedÂ **computer vision approaches, open-source libraries, or model architectures**Â (CNN/ViT, specialized chart parsers, etc.) that can handle this more robustly?

Any suggestions, research papers, or libraries would be super helpful ðŸ™

Thanks!

",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n0gjdp/stuck_on_extracting_structured_data_from/,r_1n0gjdp,,,
r_1n0f10k,reddit,Snoo17579,2025-08-26T07:06:16+00:00,"Best Free Course Hero Unlocker 2025 (Working Methods + Safe Guide)
Hey everyone,

If youâ€™ve ever hit the dreaded **Course Hero blurred document paywall**, youâ€™re not alone. Thousands of students search every day for **free Course Hero unlocks**, but most of the guides online are outdated, clickbait, or flat-out unsafe.

So, I tested the most popular methods this year and compiled a list of **real, safe, and working Course Hero unlocker options in 2025**. Hereâ€™s what actually works ðŸ‘‡

**What I Looked For in a Course Hero Unlocker**

* **Completely free** (no fake trials or scams)
* **Safe** (no shady downloads, malware, or extensions)
* **Working in 2025** (lots of old methods donâ€™t work anymore)
* **Simple** (no complicated tricks)

This works: https://discord.gg/chegg1234

**1. Free Course Hero Unlock via Discord**

One of the fastest and most reliable methods in 2025 is joining **Discord servers where students help each other unlock Course Hero documents**.

Think of it like a study exchange: you share the link you want unlocked, and the community (or a bot) provides the file. Many servers also cover Chegg, Scribd, Brainly, and more.

**Pros:**

* âœ… 100% free unlocks
* âœ… Works for multiple study platforms
* âœ… Fast turnaround (sometimes under a minute)
* âœ… Active support & community

Â 

**2. Upload Your Notes on Course Hero**

This is the **official free unlocker method** Course Hero still offers in 2025:

* Upload **8 study documents** â†’ Earn **5 unlocks**
* Extra perk: youâ€™re entered for **Course Hero scholarships** if youâ€™re a student

**Pros:**

* âœ… Safe & official
* âœ… Great if you already have study notes
* âœ… Unlocks stack over time

**Cons:**

* âŒ Takes time (not instant)
* âŒ Requires original content

**3. Rate Course Hero Documents**

A **lesser-known trick**:

* Rate **5 documents** â†’ Get **1 unlock**

Perfect if you only need to unlock one or two files.

**Pros:**

* âœ… Super easy
* âœ… No uploads needed

**Cons:**

* âŒ Limited unlocks
* âŒ Not scalable for heavy use

**Course Hero Unlocker FAQs (2025 Edition)**

**1. Can you unlock Course Hero without uploading documents?**  
Yes. The fastest way is via Discord communities â€” no uploads required.

**2. Do â€œCourse Hero downloaderâ€ websites still work?**  
No, most are scams or outdated. Avoid them.

**3. Is there a free Course Hero PDF viewer online?**  
No legit one exists. Stick to the safe unlock methods listed above.

**4. Can I get free Course Hero answers in 2025?**  
Yes, Discord unlock servers often provide answers, not just documents.

**ðŸ“Œ Final Recommendation**

If you want the **fastest and safest Course Hero unlock in 2025**, go with a trusted Discord server. Itâ€™s free, quick, and works not just for Course Hero but also Chegg, Brainly, Scribd, and other platforms.

If you prefer the **official route**, uploading your own study docs is still a solid way to earn free unlocks â€” especially if youâ€™re a student with plenty of notes.

Letâ€™s keep this thread updated. If you find **new working methods**, drop them below â€” every free unlock helps students out!",deeplearning,64,https://www.reddit.com/r/deeplearning/comments/1n0f10k/best_free_course_hero_unlocker_2025_working/,r_1n0f10k,,,
r_1n0d5zg,reddit,PiscesAi,2025-08-26T05:09:40+00:00,"Built PyTorch+FAISS for sm_120 (RTX 5070) on Windows (CUDA 13.0): kernels work, hereâ€™s how",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1n0d5zg/built_pytorchfaiss_for_sm_120_rtx_5070_on_windows/,r_1n0d5zg,,,
r_1n0c2o2,reddit,FrontWillingness39,2025-08-26T04:07:59+00:00,Looking for Image Captioning Models (plus papers too!),deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n0c2o2/looking_for_image_captioning_models_plus_papers/,r_1n0c2o2,,,
r_1n07jpt,reddit,Gold_Negotiation9518,2025-08-26T00:29:25+00:00,"how i upscale ai portraits for social media using domo
When i first started posting ai portraits online, i was always disappointed by how they looked after upload. the original render from [mage](https://www.mage.ai) or [leonardo](https://leonardo.ai) would be crisp and detailed, but the moment it hit instagram or twitter, compression kicked in. facial details blurred, lighting flattened out, and sometimes the whole vibe of the image felt off. it was frustrating because the difference between my draft and the posted version was huge.

thatâ€™s when i started running portraits through [domo](https://www.domoai.app/home?via=081621AUG)â€™s upscaler before posting. it turned out to be the missing step in my workflow. instead of just enlarging the image, domo boosts the resolution while keeping the style intact. facial lines get sharper, skin looks natural, and the background blur stays consistent. it makes the portrait look intentional rather than like something the platform chewed up.

for instagram specifically, i usually upscale to 2x or 4x depending on the starting size. the larger resolution not only survives compression better, but it also pops on phone screens where most people are scrolling. another bonus i didnâ€™t expect is how well domo handles earlier compression. even if i exported a portrait too quickly from another tool, domo cleans it up and smooths out those rough edges.

before, iâ€™d spend time in photoshop patching details, adjusting contrast, and trying to save a portrait that got downgraded by the platform. now itâ€™s as simple as running it through domo, exporting, and posting. if i want to add a bit more flair, iâ€™ll use domoâ€™s restyle tools after upscaling. a subtle glow or lens blur is often enough to give it that professional, polished look.

the difference has been clear in engagement too. sharper visuals stand out on crowded feeds, and people notice the quality even if they donâ€™t know why. this works not just for anime portraits but also for semi-realistic styles, which often lose the most detail to compression.

one last tip: if youâ€™re creating content for tiktok or reels, upscale the thumbnail frame first. thatâ€™s the first impression people get, and a sharper thumbnail makes them more likely to actually stop and watch.

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n07jpt/how_i_upscale_ai_portraits_for_social_media_using/,r_1n07jpt,,,
r_1n04zoo,reddit,enoumen,2025-08-25T22:37:41+00:00,"AI Daily News Aug 25 2025: ðŸ“±Apple explores Googleâ€™s Gemini to fix Siri ðŸ§¬OpenAI, Retro Biosciences make old cells young again ðŸ’¥ Musk sues Apple and OpenAI over AI deal
ðŸš€ Perplexity to give media giants share of AI search revenue ðŸŽ¨ Meta partners with Midjourney for â€˜aestheticâ€™ AI & more
# A daily Chronicle of AI Innovations August 25 2025:

# Listen at [https://podcasts.apple.com/us/podcast/ai-daily-news-aug-25-2025-apple-explores-googles-gemini/id1684415169?i=1000723506422](https://podcasts.apple.com/us/podcast/ai-daily-news-aug-25-2025-apple-explores-googles-gemini/id1684415169?i=1000723506422)

https://preview.redd.it/5keplqh3t8lf1.png?width=1456&format=png&auto=webp&s=8bfa2f42c66522ffc763fefdb1cf29ba81f04f62

Hello AI Unraveled Listeners,

**In today's AI News,**

**ðŸ“±Apple explores Googleâ€™s Gemini to fix Siri**

**ðŸ§¬ OpenAI, Retro Biosciences make old cells young again**

**ðŸ’¥ Musk sues Apple and OpenAI over AI deal**

**ðŸš€ Perplexity to give media giants share of AI search revenue**

**ðŸŽ¨ Meta partners with Midjourney for â€˜aestheticâ€™ AI**

ðŸ¦Â **Malaysia Launches Ryt Bank â€” Worldâ€™s First AI-Powered Bank**

ðŸŽ¥Â **YouTube Secretly Used AI to Edit Peopleâ€™s Videosâ€”Results Can Bend Reality**

ðŸ¤–Â **AI-Powered Robo Dogs Begin Food Delivery Trials in ZÃ¼rich**

ðŸ“ŠÂ **Reddit Becomes Top Source for AI Searches, Surpassing Google**

âš•ï¸Â **Study Warns Doctors May Become Overly Dependent on AI**

Listen at

# ðŸ“±Apple explores Googleâ€™s Gemini to fix Siri

https://preview.redd.it/o5h3f167t8lf1.png?width=1456&format=png&auto=webp&s=6f691a9fef82ebeaecdd41ed1966cec72f412b09

Apple is reportedly inÂ [**early talks**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyOlytRTK_g5G9_mkq7dXf8WNEpjUCmrkMdsoBHV9RjS39ciJ-GAnzOblOHlRUwT8W_8viLPZopuhhQzLwR7gLxLdcSS2HW-fkEQYLIKNgPwcO6mGcxj2-K4m0posEnYvFoGTnJPM2-UI4Q2WUWVXooyyMmgvG4qUOenMNScEHy7Uk0SfpXiCh5UlPCk92td4UAE6iaf2iKYGOitalcjKEp6PK-ctaFrNd02o9mGctGQTO_YfwFbYrZqOkiwpogKMWBA8pZ5qB-1UiOMqP_EYDCyoB0IK8_K5cV1e4V1pQMPafze-5z7Wk4M7gnP6PjQoosSeuWJ48TFqB589zC2RaMs5hLjq0oNpb54UGqR9EAf/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h6/h001.2bV5YZd47aUsGvT5sM632eE-yWQVuNkcc6lr2820eEM)Â with Google about using Gemini to power a completely rebuilt Siri, according to Bloomberg â€” following setbacks that pushed the voice assistant's major upgrade to 2026.

**The details:**

* Apple had Google build a custom Gemini model that would run on Apple's private servers, with Google already training a version for testing.
* The company is simultaneously developing two Siri versions internally: Linwood using Apple's own models and Glenwood running on external tech.
* Apple has also explored similar partnerships with Anthropic and OpenAI (with ChatGPT already helping power Siriâ€™s answering capabilities).
* Bloomberg reported that Apple is still â€œseveral weeks awayâ€ from a decision on both using internal vs. external models and who the partner would be.

**Why it matters:**Â For all the negativity surrounding Appleâ€™s AI issues, moving externally to bring on one of the frontier labs could be the best possible outcome for iPhone users. The alternative is hoping Apple can develop its own â€” but with talent fleeing to rivals and already facing setbacks, it seems like a long and arduous path.

# ðŸ§¬ OpenAI, Retro Biosciences make old cells young again

https://preview.redd.it/6owe8r5bt8lf1.png?width=1456&format=png&auto=webp&s=d9c1d0211dd5d4025ea3009cc503ad8bb4d52e33

*Image source: OpenAI*

OpenAI justÂ [**published**](https://link.mail.beehiiv.com/ss/c/u001.eCbm_1zon7G0lMoXTECWa-IUY9yqSc2cx0km5OJXo-NHnISnp8MoUu0O2hwTgeE7xFkyOhHM5PA-j03akRqORQuWWNQJipj0uElo_-LHbyLLx2SwyPaN1Oqx8P6jMwlIjwGrcISTv9f4TRXW2fr6i74A5k1gT5zl5FlSNN7ZQmV5VwECq6JABtNupp3rRQ-20Wu7uhJyUWAW8trdWgMa84WnWa2nBoOKhWDB1TCJrUs4gA42baoGk5vCOaBujUMVqlg9-9u-UwHcn6snO3OFLdKhAZ9dZFqMU25LDVijqamguCKT7RiM6avJbfaDyV2iR70Yicide3i_ARUsRdi-gQ/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h22/h001.ZzsXL1yZO_exwG101WTrgVxdgaY7VE1ztziXET6avY4)Â a case study with Retro Biosciences, using a custom AI model to redesign proteins that turn cells into stem cells, achieving 50x better efficiency than the original Nobel-Prize winning versions discovered in 2012.

**The details:**

* Researchers built GPT-4b micro, an AI trained on biological data rather than internet text, to redesign â€˜Yamanakaâ€™ proteins that reprogram aging cells.
* The AI-designed proteins converted the cells into stem cells 50x more efficiently, showing dramatically better DNA repair abilities.
* The results essentially reversed one of the key signatures of aging at the cellular level, with multiple labs validating the results across testing methods.

**Why it matters:**Â While public models are leveling up users in their own work, custom models trained by domain experts could unlock discoveries that general-purpose AI would never find â€” turning biology, chemistry, and materials science into computational playgrounds where decades of lab work compresses into weeks.

# ðŸ’¥ Musk sues Apple and OpenAI over AI deal

* Elon Musk's companies xAI and X are suing Apple and OpenAI, alleging the pair colluded in an anticompetitive scheme to maintain monopolies in the smartphone and generative AI markets.
* The complaint alleges the iPhone maker is deprioritizing rival chatbots like Grok in its App Store rankings while favoring OpenAI by integrating ChatGPT directly into the device software.
* The legal action asks a federal court to stop the partnership's â€œunlawful conduct,â€ arguing competitors will suffer anticompetitive consequences if the alleged behavior is allowed to continue.

# ðŸš€ Perplexity to give media giants share of AI search revenue

* Perplexity announced a new subscription program called Comet Plus that gives users access to premium content from trusted publishers and aims to compensate journalists for their contributions.
* The company is funding a revenue sharing program with $42.5 million, which will deliver 80 percent of the subscription revenue to publishers while Perplexity keeps the remaining 20 percent.
* This new model arrives after Perplexity was sued by News Corp. publishers and threatened with legal action by the BBC over alleged copyright infringement and content scraping.

# ðŸŽ¨ Meta partners with Midjourney for â€˜aestheticâ€™ AI

https://preview.redd.it/2tx9yt2ht8lf1.png?width=1456&format=png&auto=webp&s=15ccb6aa54ed423fc17e59a631b15dd578d983bb

*Image source: Midjourney*

Meta justÂ [**announced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hb0fzGc-EOU_xTESI0qBwv7KZenVt8TiUSLS0pC1mxDiZX8t_uEPtGuV9BOLnfUKMcn9vFGS69l1xwHu1iQtit4bTV9kve7H-lPWAkgBbWKRlGqFuTgjyXbcCxSrVzzroC2Hh0itYhC0wDiouN9m41aboACpHbVUoPLvHuBseICiTMDcVDn_n8W4Epz4TU6l5rCRMB2gYzwZfyiJj0j1RNT2xAWRI8m5IMfzuiRE7kBdE6liq6P_2tHZNAZyYEheLLRvd0XyK5bFI5BoJfTC7MI/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h11/h001.3kvjClbcZnOHDJbj3AEsls4D7CwuwEIGlw8OG9GkV4Y)Â a new partnership with Midjourney to integrate the startupâ€™s â€˜aesthetic technologyâ€™ into future AI models and products, a major shift from the companyâ€™s in-house creative model development.

**The details:**

* Meta's Chief AI Officer Alexandr Wang said the â€˜technical collaborationâ€™ will combine teams to upgrade visual capabilities across Meta's product lineup.
* Meta currently has a series of visual generation tools, including Imagine, Movie Gen, and research-focused models likeÂ [**Dino V3**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijCejhhl4B0NSh6H10TKEqpS93xca2v0poLjefrFwYKBw3yn2ISkpS9RHJSbv8Z1_o6zhJV3aqDBBUeBFnOEsfbA05slMYtdQkrB1lUo2sFgfvvYEms2qbjbbQCSTOe206g2rsv60XrQbQqMyZFVWisbd-D8aO4Aetl6XnxOn7F75kw-715Mfl6JjIXG3mS7KUaZAFK1e2YmTkcmaK3zpwoH4yBJqzUl6KVV3Fs9Vt50IV-iUBxbQwTiikeKcmXmxng/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h12/h001.cavsETBudqWGWNH7cXS92TmTEnCe_zIQQxrFVu9VfU8).
* Founder David HolzÂ [**emphasized**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hb4uIBRo0DQn8gpaST8o7RegD9y9GrJQoboC8wZXRd8AHMCi7_1_Ffb7i0he_th-QzIyCyDHrBITqDtHDrsMd_p5v5YS8Luu9GqPBN37FzwNhPtDtmnSd7LEHl9DHa0sSMdhTH0Fd1kzdV5_Jv9qbq5vTPR0sZKWH6XrSCP4iVNBoUk-9LWuNY2TUsq14iNTgqFQyEI3iYSp3h4aUlma6mhNVYfGMD_k7iVKmH9np7ni6q58XsvG2NghggvkKqImzXqgWPt-nbuxaJZ3lkebC6Q/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h13/h001.KHg7en3rbI8ziUJIBuiAZtAaue9BOg8HX6-E9IuLi-A)Â that Midjourney is still an â€œindependent, community-backed research lab with no investorsâ€ despite the partnership.
* MidjourneyÂ [**launched**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_H1Js7Tw-3AF7vn_2TndEshCmBGKd5nJFkHxNpX2SH-ehv8Omcari9rcuNCbPfKKeDm1dJ-jG7YEe2CW6WjJd0PvxFlXwUJZWAIR7_q5tLgEubXN49tK-Ue-af2CDu2vpNZIKC9Zdtz9ob8kalhi5R2QAeJjPZK66veYgciiox52bGDB96xTLpP97G17T-o481oVNw6JXCcjB7ZRR9MKzxau9DuGn54Kl1RD36zZqvKMPGkvZGfoGAVY0b8qItlyY3qXrmivGIX9cPpdc-ic9xIa0aoqvV4cm8g3FTK-mAydqsVaLyi9JVEvxciSdA46sxtsxSAz6T3ltad9qnkFLKYjJXKAB6JpIwYcfYarWvjIQEcP-Mmh-1xKRO7tm4Qh9UovYum19t6RruIsWxLghib3jjm7ESps6jKjP-Ay4EGpO7zUFwuKFfIXyTnPTtF-Wt1n9sfi2irHc0NvJN8DEimwzei4A18jO7_TFKnuCZlOUjM0EwkY6MrLvATiMbwQ1VJwyYKZOVxtUjV6Bo1VREYlwwlWIGdZzDvMJhplEgT-nvfZN-OH_iRhga5gZckUYhJ0F0b_Ne5ai6LRDPhK1tQygfNMYQKASov8k6_tswBU5b_S4LnyKBNxkMG9Pjsu0bv5x17Z5pQZWZk_C26bLZjyQmMqLUBovXfNK-XrkOt2w1rspBKRG4YCXmjoWYv9HaFBrXeLYo21CUzoppXWwDVfdsdwfEsSn31cSFm0v9f3r_Gm_tz8wv1c2FhbNobYVYDYAkXtstuQ5Oyv5wXZt49/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h14/h001.XkyX0dmIKFe7a4I53yC6kEHt9oLavxA62KAx6qWB64I)Â its first video generation capabilities in June with its V1 model, giving users the ability to turn images into five-second extendable clips.

**Why it matters:**Â Meta bringing Midjourney aesthetics to its billions of users would be a big change from the quality seen in its previous in-house models, with MJ having a special vibe that is just hard to match. Meta is also showing a new willingness to look externally (not just poach talent) to help push its own AI development forward.

# âœ‚ï¸ TSMC removes Chinese tools from its 2-nm factories

* TSMC is removing all Chinese manufacturing equipment from its new 2-nanometer production lines, driven by fears of potential US sanctions linked to the proposed Chip EQUIP Act.
* The company is also reviewing its entire supply chain for materials and chemicals to further reduce Chinese components in both its Taiwan and US factories for advanced production.
* This effort differs from the 3-nm process where technical risks prevented swapping out Chinese tools, but TSMC is now making the change as it ramps up 2-nm manufacturing.

# AI takes over content moderation, struggles with the nuance

Social media platforms are aggressively replacing human content moderators with AI systems, despite mounting evidence that the technology isn't ready for the job.[Â TikTok laid off around 150 content moderators](https://link.mail.beehiiv.com/ss/c/u001.ZYBlacOkQfy5p26AFx06aMZlUFTISgllxHRICQHcCkDO52l25fmxkCKZoHemCqIyuNlb_G41hg2WS16nqDLGTg4mRe2ecQfE2LOxsAMrs9799OpGaIyyoPJuvdb_mPVs-_UTW0TjX7uDPM8AKhjLaWZm0F2jT8eGOGtloZG5pbhvh7n_6CMvo7n8BQu8ti2BWBJdo7k3yJQ472CqP-7rrwEXSZrazXj7eGUNgCgP_El7As07dMAHo-GRkFRJ7gE6YBj2CSJr3HwCvb92-YSIp1l3rAMOCTfGk-Qa_G0o8LekIYzDbifg3k2OmUOF8G2GjFYyMiNCbuXQYftDNlfD-w/4jc/qG2nhzwxTwu8RA9aHyOfHg/h21/h001.QUSmJCCxdB1px14Zv9iDC6pEbmFPnKYoLPA29H7rhL8)Â in Berlin earlier this month, nearly 40% of the team responsible for moderating content for Germany's 32 million users. On Friday,[Â TikTok announced plans](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePDU4kUb9tonIupBrK4MGC50UBl1iyMgR0dei-9Ntam3zvZzexh14Vm415eynx8kDZIgUU2zkQh7j-T9pviUaM5JXLGI3_wLvSqByU-O8yJNwHuc4yHEfepIEr4ZreceFPntulZ-K8sHUQMGgtVnWqZkmxYACoC8KzW1ZLh-esvxh7Y66ayeVim9ePK6SGSptKPoAyGjHpy4FDIgQCsLXenmHszg0WzNV5d4rSI22h2CV3835ugGH1IkrtiUtW8JU_gyXinKzdptL03IU3Vth5_-wNlvx5t8oJNk7LobbDtPb7xJWd5PdOILZGLQYjFkUAhRu2jt0IrU3S5b_ztFYfw8mgiACVjK4enX9AN_N3ZAbyuUdzDNK0lW6lGMAQhkRg/4jc/qG2nhzwxTwu8RA9aHyOfHg/h22/h001.EWmYO-8fSAsrydkkybFRRrLNbWVDrdAfbIC6Y66QvsE)Â to cut hundreds more moderators across the UK and Asia while investing in AI moderation technologies.

Human moderators are expensive, prone to psychological trauma from graphic content exposure, and companies have spent years outsourcing the work to poorly paid contractors. AI promises to handle the massive volume without needing therapy or breaks. But according to 13 professional moderators interviewed by Bloomberg, the technology consistently fails at the job's most critical aspects.

Kevin, a TikTok content moderator in Africa, estimates AI fails up to 70% of the time. Zhanerke Kadenova, who works for a content moderation firm in Kazakhstan, says AI suggestions don't match reality 80% of the time. The systems make bizarre errors:

* Highlighting low fuel gauges instead of dangerous speedometer readings
* Identifying children as 17-year-olds
* Missing contextual clues about violence or abuse
* Failing to understand regional dialects or cultural nuances

Child predators represent the most dangerous blind spot. They study platform automation tactics and evolve faster than AI can learn, using coded language like ""Let's party"" or ""Meet me on the ghost app"" to circumvent detection. When platforms catch on, predators simply put Xs between letters or invent new phrases.

Companies likeÂ [Meta](https://link.mail.beehiiv.com/ss/c/u001.929jZiiRht9qZPYC5Z_-teC3EitHOXbKzNdtgpbXE2l5tfjAkS81roSdS4tZX6Qs91jS9p5SjgLkjY9DF6D0CZpPiD8fw1P23CH78XN4Jm87Qfhp2REJNXC01sdV6dHxEMGCZ5Xh4nXnSnUUSWODUQmPP1n6GnTW5_bqhCi3K8aOTVDJV1aIQo5caJjFDrQC1d47Z9AqoL1U-Lsg8h2wxMQbNyDJ72n0erDFLNCpn7WgZ4vJeQfJOXP_cNuX9d6OlEynOsk2mre4lNOSlLYw_rnOtw_EErMi9sBfWaEAoV5KhMS62KI_ncL0vxiKf1YSrW-7u-Mz2MNkb_QJPRxh0v6GyagUxaOvLxLR5E868riS9urz-nizYZ09hMieL2J2IxOdJXuO7FiM8TiWr99hNw/4jc/qG2nhzwxTwu8RA9aHyOfHg/h23/h001.CFBiPELj64Sma6cCD1CN4Z_t9YuVre5255jO7UGm2z4)Â and[Â Roblox](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeND5z3fn2eA5HMQUmmQlReaZewMO_gEH1AIUXsW1fkcii3RAgizILTLoaUbMUjr_pKWWwjUt19VydaVBIYwTKvf6delCB7kEnAoZuOBSE-5Pfc-B-LHEESz0j3H4U7V13rHNkWVjH2Q26FbxpKXOODAMmu_npON7HzUnMCwCc4WkYEf5jOYFMpUYnRNVnuQo8W4vzN1ZewIaylXwdPh-gKtMIqCUTgoOQnUGEAzpsLeu-1GD1ZiwlD1e-gPuDMelNfaw_c9LRWNKkbj0F2TIEmJLm1kW4JtqoWF27wTnAI7V/4jc/qG2nhzwxTwu8RA9aHyOfHg/h24/h001.eP-cCteAux8y1tJtOVfR6iOEwGXKSMRaXRDVj0JT-4c)Â continue facing scrutiny over child safety failures, yet they're doubling down on AI moderation to cut costs. The result will likely be platforms where coded hate speech, propaganda and predatory behavior persist while legitimate content gets incorrectly flagged and removed.

# MIT says 95% of enterprise AI fails â€” but hereâ€™s what the 5% are doing right

The recent MIT study on enterprise AI hit hard:Â **95% of generative AI pilots deliver no ROI**. Most projects stall in â€œpilot purgatoryâ€ because employees spend more time double-checking results than saving time.

TheÂ [Forbes follow-up](https://www.forbes.com/sites/jaimecatmull/2025/08/22/mit-says-95-of-enterprise-ai-failsheres-what-the-5-are-doing-right/)Â highlights what separates the 5% of successful deployments:

* **The Verification Tax**Â â†’ Most AI systems areÂ *â€œconfidently wrongâ€*. Even tiny inaccuracies force humans to re-check every output, erasing ROI.
* **The Learning Gap**Â â†’ Tools often donâ€™t retain feedback, adapt to workflows, or improve with use. Without learning loops, pilots stall.
* **Tentatively Right > Confidently Wrong**Â â†’ The winners are building systems that:
   * Quantify uncertainty (with confidence scores or â€œI donâ€™t knowâ€ responses)
   * Flag missing context instead of bluffing
   * Improve continuously from corrections (an â€œaccuracy flywheelâ€)
   * Integrate into actual workflows where people make decisions

The big takeaway:Â **Enterprise AI isnâ€™t failing because models arenâ€™t powerful enough. Itâ€™s failing because they donâ€™t admit what they**Â ***donâ€™t***Â **know.**

Would you trust an AI more if it sometimes said â€œI donâ€™t knowâ€? How do you balance speed vs. verification in real workflows?



# ðŸ¦Â Malaysia Launches Ryt Bank â€” Worldâ€™s First AI-Powered Bank

Malaysia officially unveiled \*\*Ryt Bank\*\*, a digital-only bank powered by the ""Ryt AI"" assistant built on the locally developed Ilmu LLM. Backed by YTL Group and Sea Limited, the service supports conversational banking across multiple languages and offers intuitive features like real-time insights, bill payments, and trackingâ€”making it arguably the first homegrown AI-first bank built for Malaysians.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/25](https://www.techinasia.com/news/ytl-sea-officially-debut-malaysias-ai-powered-digital-bank)\]

# ðŸŽ¥Â YouTube Secretly Used AI to Edit Peopleâ€™s Videosâ€”Results Can Bend Reality

YouTube has been applying AI-powered enhancements to usersâ€™ Shorts videosâ€”sharpening, denoising, and modifying visualsâ€”without informing creators or requesting consent. This has sparked concern over how subtle, unauthorized edits can alter the authenticity of content and potentially blur truth and creation.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/25](https://ground.news/article/youtube-secretly-used-ai-to-edit-peoples-videos-the-results-could-bend-reality)\]

# ðŸ¤–Â AI-Powered Robo Dogs Begin Food Delivery Trials in ZÃ¼rich

Just Eat Takeaway, partnering with Swiss robotics firm RIVR, has deployed AI-driven robo-dogs on the streets of ZÃ¼rich. These robots, blending wheels and legs, can climb stairs, navigate obstacles, and operate in various weatherâ€”delivering food autonomously in real-world conditions.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/22](https://www.euronews.com/business/2025/08/22/ai-robot-dogs-deliver-fast-food-in-zurich-as-just-eat-pilots-new-technology)\]

# ðŸ“ŠÂ Reddit Becomes Top Source for AI Searches, Surpassing Google

In June 2025, Reddit emerged as the most-cited source in large language model (LLM) outputs, accounting for over 40% of all AI-related citationsâ€”almost double Googleâ€™s 23.3%. Wikipedia (26.3%) and YouTube (23.5%) also ranked above Google, highlighting a growing shift toward user-generated and discussion-based platforms as key knowledge inputs for AI systems.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/21](https://www.reddit.com/r/artificial/comments/1mwxrvz/reddit_is_the_top_source_of_info_for_llms_almost/)\]

# âš•ï¸Â Study Warns Doctors May Become Overly Dependent on AI

A recent study in \*The Lancet Gastroenterology & Hepatology\* shows that after a few months of AI-assisted colonoscopy, doctorsâ€™ ability to detect polyps dropped from 28% to 22% when AI was disabled. The findings raise concerns that overreliance on AI tools might degrade clinicians' diagnostic skills.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/19](https://www.reddit.com/r/singularity/comments/1mupctx/research_suggests_doctors_might_quickly_become/)\] \[[Time: Lancet Study](https://time.com/7309274/ai-lancet-study-artificial-intelligence-colonoscopy-cancer-detection-medicine-deskilling/)\]

# What Else Happened in AI on August 25th 2025?

**New court filings**Â [**revealed**](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xuiVDGMvQlgKBoZjlv2WcjqYV-HedebzRa2RzikQGgG41nCxNhmiIcyxzUrTHJUOYG1QlzbtS1psYT-z9HgheY-S7sXFALbNexeV-Ro3AwV7QoczRZMESjHwwloHwnSc1f4VkaWIPK3V4durmYVdzrGuXyAkLeS0_aQDM6aMlfyAqdjQBrWBW-Z-26i_CCBW_eokaaAKZc4tsm1buKrwkNZcYA6rZ83UgTAaKG_BY9khVAaflIfA3WxF1rSmfHy7V4yxC5HjIohDCWkYEglj6g1J0HRr6qckgf0_989EN0BGo/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h30/h001.9g3zF79qjKA_lQiDuM2mD2CF0Bh7oeiM_nORHk1gjIU)Â that Elon Musk asked Meta CEO Mark Zuckerberg to help finance a $97.4B takeover of OpenAI in February, though Meta did not agree to the letter of intent.

**xAI**Â [**open-sourced**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmf-qkvfrIaAP3PYA_SilVS7fCIAmtpug78WIdxvai1jxX5pGRZuYWPdC05JZ591_UGTxs_eVsSAjHc-j74aMRqxZFlp3C2LRzWDBybkHiwtLQGGMQHiOhAxxSDrVXk5v2jJfHj4oQpBKHnVEXlRAD8n7lBrtE2txYFJF-4cDpiT_C6e8tBYNfAOHveJGivquJ_JPqhsZscefsT3tRnAwUUqrsOg8Xz2qE1loYCxQFE2jH9lci5cwLzU3V9PnPUd5jYw/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h31/h001.OCq5oCeJ4g6muqVBGdUGG7a8KfqHyO-Gl2A_HS48vds)Â its older Grok 2.5 model, with Elon Musk saying Grok 3 will also be made open source in â€œabout 6 months.â€

**OpenAI**Â [**announced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HfqGVtU0Kpfcch_H6377P4HCW3Y_euSOD9-8jndKdMk6pCVJqo3LZRc7v8_AMLh5GcT92ApVwTMk6rQmopUfVq9rJ73vRPoDpU0DrSg-r6E0CdFKw_PWALHDcDtEFLIe3oqFoQZgq8vyR-t3XldgF6s3bBKIDgfAb0q5qvUbjND-o38fyVusppnuKKOqpHKqpefE7e9oaVTujgaoVGLvEszRU6FOJJ9fm_TKm8u8aoPTCFXmg_64HZMtDAaMewkbzTNThV6u5g1oRRSfds8KC18/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h32/h001.bTpLpfuCQ8NsxQagoWXbPruCH6G857qx4EZXGGAMC0U)Â the opening of a new office in New Delhi, coming on the heels of its new $5/mo ChatGPT GO plan specifically for the region.

**Elon Musk and xAI**Â [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmO7D0EoCoFcxHU7ci9EmXIQe4urhTVojfTm1H0DY06-bHGJjwdr3NHiDWEf8u5oUi96Y7gphgO5cRm23RvbubtuL5jP8GPp3BnBmQtbAUQNZd0N8x8064TsB5VX2NLDCG3IHpxs86tDDN6gZVA-P7Zt8Ie_oEyldpO5FUwR1T72K3-3a_4eKdwI4f1BqPQJBzJjdLI9tjHRakYEnOYw4rkEPZAh6DBqgtM1n3KhQsW2aXAC-ijt2ov19FRTEcTaen0/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h33/h001.TqEFFD1ADfroNz2d0mUIjScoYE5YcBPknwPDN6zYaFU)Â MacroHard, a â€˜purely AI software companyâ€™ aimed at replicating competitors like Microsoft using simulations and AI agents.

**Meta FAIR researchers**Â [**released**](https://link.mail.beehiiv.com/ss/c/u001.ptzCNad3P3I6BL6vWVKYhLzv1rmB5i2uECitvYm67sbs4O4AO4Ffldls2_l69FIcMwSWwM_vMltd8HInbxbGOa8PddUa64BAP1X1H_MI1E6UHZ6JnHyQahHFIdYRLmSb-Zk6DFfYf2DphLus_m6eGWG58hFmbldCx7Nfd8qB6drn7ysNVN_nm0ubj_aCKAW57LGiSsCVgTT_m_6NcrDVC96JjLTbS103TG7LGkmwNFE2yDVyWuTDEgjQlmUQa_rS0avIQQhxajeFZ4ra0hNjKg/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h34/h001.tVxoG7wIR9jDwYvy1Sxd39muXclsnkT6agn_CtpM4ZE)Â DeepConf, a method of deep thinking that achieved 99.9% on the AIME benchmark using open-source models.

**Baidu**Â [**launched**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HbwDzV__5f__MSnQO9oeinKXmk2BypTNRCiBeY3yKAmgpXUUuGLl0bKMPCf5g3zIl-wFel66eG5bDqMzuuppkwSBWlpf6AhtcSJCQcRmhP7v_4J0XSFNtidl5D0qPTpOuJnKX7Wws1JKoTEZoJL3FGPHPv6o_6MxGff8IagMxM7RwTJkrrkbNNF_bpYUusiEJIFnn4eK3dO_jj6SUCZR3mCgoxwzVc65zjFzAdD-AKMfYI-YMkFJWnQ0f4DWVcF7aQt1Q-FL1VlT_vI3nM2ICvM/4jc/l5ZfL-CHQqSeHJ8mpd8-TA/h35/h001.27GetHnaidoX_gG0vGXt-5qx7_UOCUU8ZR_VvshWLDI)Â MuseStreamer 2.0, a family of image-to-video models, with upgrades in multi-character coordination, synced audio outputs, and lower pricing.

# ðŸ”¹ Everyoneâ€™s talking about AI. Is your brand part of the story?

AI is changing how businesses work, build, and grow across every industry. From new products to smart processes, itâ€™s on everyoneâ€™s radar.

But hereâ€™s the real question: How do you stand out when everyoneâ€™s shouting â€œAIâ€?

ðŸ‘‰ Thatâ€™s where GenAI comes in. We help top brands go from background noise to leading voices, through the largest AI-focused community in the world.

ðŸ’¼ 1M+ AI-curious founders, engineers, execs & researchers

ðŸŒ 30K downloads + views every month on trusted platforms

ðŸŽ¯ 71% of our audience are senior decision-makers (VP, C-suite, etc.)

We already work with top AI brands - from fast-growing startups to major players - to help them:

âœ… Lead the AI conversation

âœ… Get seen and trusted

âœ… Launch with buzz and credibility

âœ… Build long-term brand power in the AI space

This is the moment to bring your message in front of the right audience.

ðŸ“© Apply atÂ [https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform](https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform?usp=header)

Your audience is already listening. Letâ€™s make sure they hear you

# ðŸ“šAce the Google Cloud Generative AI Leader Certification

This book discuss the Google Cloud Generative AI Leader certification, a first-of-its-kind credential designed for professionals who aim to strategically implement Generative AI within their organizations. The E-Book + audiobook is available atÂ [https://play.google.com/store/books/details?id=bgZeEQAAQBAJ](https://play.google.com/store/books/details?id=bgZeEQAAQBAJ)

",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1n04zoo/ai_daily_news_aug_25_2025_apple_explores_googles/,r_1n04zoo,,,
r_1n04qbn,reddit,asankhs,2025-08-25T22:26:53+00:00,Understanding Model Reasoning Through Thought Anchors: A Comparative Study of Qwen3 and DeepSeek-R1,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1n04qbn/understanding_model_reasoning_through_thought/,r_1n04qbn,,,
r_1mzyy7j,reddit,RelationshipOk939,2025-08-25T18:44:32+00:00,"AI-Powered Tesla Focus App Boost Mental Clarity (Android)
ðŸ§  Testing an AI-powered thinking & focus app feedback needed!

Hey folks I'm testing a new productivity app that helps you focus deeply, track your mental sessions, and reflect on your thought patterns using subtle AI insights.

ðŸ” Features:
â€¢ Timers for deep work  
â€¢ AI-generated feedback based on your mental flow  
â€¢ Thought tracking & daily progress logs  
â€¢ An AI-powered chat that helps structure your thinking

ðŸ“± Android only for now. Iâ€™m looking for a few testers to:
â€¢Install the app  
â€¢Use it daily for a few minutes  
â€¢Try the main features  
â€¢Send quick feedback anything helps!

ðŸ”— Google Play Closed Test(sumbit your Gmail so I can add you to testers and youâ€™ll be able to download): https://teslamind.ultra-unity.com

ðŸ’¬Google form feed back after teasting the app: https://docs.google.com/forms/d/e/1FAIpQLScmv4GtuaGUI6zyns_PgvDBZKh4Lfn_qnfmJpLpKbWpGYZjeA/viewform?usp=header

---

ðŸ“© How to send feedback (takes 30 seconds):
1. After installing, open and try the app.
2. Return to the Play Store listing (same link above).
3. Scroll down and tap â€œSend feedbackâ€.
4. Write anything  good, bad, suggestions, or confusion. Every bit counts!

Alternatively, you can DM me your feedback

---

ðŸ—£ï¸ Why your feedback matters:

This app is still in testing, and your input helps shape it before public launch.

Google requires real testers to use the app and share feedback  not just installs.  
Even a short message like  â€œthis part was confusingâ€ or â€œI liked the timer featureâ€makes a big difference.

Every comment is read, and improvements are made based on it. Google also checks that feedback is being collected and applied before approving production release.

Your quick input = better app + real support for getting it live!

Thanks so much for your time!




",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mzyy7j/aipowered_tesla_focus_app_boost_mental_clarity/,r_1mzyy7j,,,
r_1mzybag,reddit,mildly_sunny,2025-08-25T18:20:27+00:00,"AI research is drowning in papers that canâ€™t be reproduced. Whatâ€™s your biggest reproducibility challenge?
Curious â€” whatâ€™s been your hardest challenge recently? Sharing your own outputs, reusing othersâ€™ work?

Weâ€™re exploring new tools to make reproducibility proofs verifiable and permanent (with web3 tools, i.e. ipfs), and would love to hear your inputs.

The post sounds a little formal, as we are reaching a bunch of different subreddits, but please share your experiences if you have any, Iâ€™d love to hear your perspective.",deeplearning,22,https://www.reddit.com/r/deeplearning/comments/1mzybag/ai_research_is_drowning_in_papers_that_cant_be/,r_1mzybag,,,
r_1mzxlz7,reddit,JustPa55ion,2025-08-25T17:54:37+00:00,"EC2 vs SageMaker vs Bedrock for fine-tuning & serving a custom LLM?
Hello! I am a Computer Vision Engineer, previously I have used the HPC center (basically lots of nodes with fancy GPUs) that we had partnership with to train / inference DL models and build pipelines.

Recently, started a new project, tho slightly different domain to what I used to work in - the task is to build a yet another ""fancy and unique"" chatbot.  
Generally speaking, we want 1) fine-tune open-source LLM for our specific narrow domain (yes, we do want to do it), 2) design an app that will allow users to communicate with an LLM through Telegram, 3) be able to offload the weights of the trained model to our local machines.

I have never ever worked with AWS services before that, I have spent a couple of days going through the docs and some forums. Still have some questions left to answer :(

So my questions are:

1. For the fine-tuning purpose should I use EC2 with GPU nodes / Sagemaker / Bedrock? The EC2+GPU looks like what I am most familiar with. However, there is also an opportunity to fine-tune on Bedrock as well as Sagemaker. Why should I choose one over another? Will I be able to easily offload weights after tuning the model? Generally speaking, I am trying to wrap my mind around what are the unique features of each of these services?
2. What is the best practice / common strat for deploying and serving custom models? E.g. using ollama / vllm in EC2+GPU vs Creating an Sagemaker endpoint?
3. Any potential ""beginner traps"" that I should be aware of during doing things with AWS?

Would like to hear about your experience. Will appreciate any advice!  
Thanks in advance!",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1mzxlz7/ec2_vs_sagemaker_vs_bedrock_for_finetuning/,r_1mzxlz7,,,
r_1mzx9l6,reddit,Stanford_Online,2025-08-25T17:42:22+00:00,"We are Pax & Petra, Stanford Onlineâ€™s AI Program Directors - AMA!",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mzx9l6/we_are_pax_petra_stanford_onlines_ai_program/,r_1mzx9l6,,,
r_1mzwxdq,reddit,enoumen,2025-08-25T17:29:54+00:00,"ðŸ›¡ï¸The Future of AI Safety Testing with Bret Kinsella, GM of Fuel iXâ„¢ at TELUS Digital: How a New Method is Red Teaming LLMs
https://preview.redd.it/n8ylozdia7lf1.png?width=3000&format=png&auto=webp&s=6a1f8347b6f1d17306610c922d98127c1ffdee49

# Listen to Summary Interview atÂ [https://podcasts.apple.com/us/podcast/summarizing-the-future-of-ai-safety-testing/id1684415169?i=1000723478062](https://podcasts.apple.com/us/podcast/summarizing-the-future-of-ai-safety-testing/id1684415169?i=1000723478062)



# Listen at [https://podcasts.apple.com/us/podcast/the-future-of-ai-safety-testing-with-bret-kinsella-gm/id1684415169?i=1000723468669](https://podcasts.apple.com/us/podcast/the-future-of-ai-safety-testing-with-bret-kinsella-gm/id1684415169?i=1000723468669)

# Watch Full Interview at [https://youtu.be/O-llDoN-iNc?si=FqYymiknoVIRbV6N](https://youtu.be/O-llDoN-iNc?si=FqYymiknoVIRbV6N)

# Speaker:Â Bret Kinsella, GM of Fuel iX at TELUS Digital

# Host:Â Etienne Noumen, P.Eng Creator of AI Unraveled

# 1. Executive Summary

This show explores the evolution of AI safety testing, particularly concerning large language models (LLMs). It highlights the limitations of traditional ""**pass/fail**"" red teaming and introduces a novel approach called Optimization by PROmpting (OPRO), which enables an LLM to effectively ""red team itself."" This new methodology focuses on evaluating the Attack Success Rate (ASR) as a distribution, offering more nuanced insights into an AI model's security. The discussion also touches upon the real-world implications for enterprises, especially in regulated industries like finance, energy and healthcare, and how OPRO can aid in demonstrating regulatory compliance and fostering accountability. Ultimately, the guest looks towards the future of AI safety, identifying upcoming challenges and areas for focused research and development.

# 2. Bret Kinsella's Journey and the Genesis of Fuel iXâ„¢

Bret Kinsella's 30-year career in technology, spanning the internet, RFID, and mobile, has consistently focused on ""drivers of adoption and barriers to adoption."" For the past 12 years, he has been deeply involved in AI, particularly conversational AI and more recently, generative AI. His work, including founding companies and a research business (Voicebot.ai), led him to TELUS Digital about 18 months prior to the interview.

TELUS Digital, a leading global technology company specializing in digital customer experiences with more than 78,000 employees globally, sought to ""harden and extend"" its internally developed AI applications and explore external market opportunities for these technologies. Kinsella was brought in to guide this process, leading to the development ofÂ **Fuel iX**, the companyâ€™s proprietary generative AI platform and suite of products that help enterprises advance their GenAI pilots to working prototypes and production at scale, quickly, securely and responsibly across multiple environments, applications and clouds.

A key focus for Kinsella at Fuel iX became AI ""safety and security,"" which he distinguishes as separate but equally vital. This focus was driven by the recognition that generative AI, with its ""unbounded inputs and outputs systems,"" introduces significant risks, including ""reputational risk,"" ""legal risk,"" ""regulatory risk,"" and ""competitive risk,"" which could act as a ""barrier to adoption.""

Fuel iX solutions, such as ""[Fuel iX Copilots](https://www.fuelix.ai/products/copilots),"" are general-purpose tools rolled out to ""tens of thousands of people internally across our organizations plus some other customers."" These tools are used across various functional areas like ""finance, HR, marketing, IT, in the contact centre,"" demonstrating the pervasive integration of generative AI within TELUS Digital's operations. Kinsella stresses the importance of user-led customization and grounding in proprietary data to maximize the efficacy of these tools, empowering frontline workers to ""find the efficiency for the task.""

# 3. The Flaws of Traditional Red Teaming for LLMs

Red teaming, a long-standing security practice, involves experts attempting to compromise systems in order to identify vulnerabilities in a safe, controlled environment. The goal of red teaming is to expose weaknesses so that they can be addressed adequately by the â€œblue team.â€

However, Kinsella identifies fundamental flaws when applying traditional red teaming to LLMs:

* **Unbounded Nature of Generative AI:**Â Unlike traditional programmatic systems with a limited number of possible inputs and outputs, generative AI is probabilistic and unbounded on both the input and output sides. This means inputs are by definition variable and outputs can vary across runs, making exhaustive pre-approval or evaluation practically impossible.
* **Over-reliance on Guardrails:**Â Existing safety measures focus heavily on guardrails (intervention technologies like input scanners, output filters, or system prompts) that are reactive and potentially probabilistic. They mitigate some risks and have an important part to play in any LLM security ecosystem, but do not fully prevent vulnerabilities from arising and are more of a stopgap measure.
* **Scalability Mismatch:**Â Co-pilots, bots, and AI assistants are capable of higher volume and scale than human red teamers. Artisanal attacks take time and effort that is better spent on refining novel attack methods than producing broad coverage. This mismatch necessitates automated approaches for vulnerability discovery.
* **Inadequacy of Existing Security Tools:**Â Traditional tools were designed for deterministic, programmatic systems. They are ill-suited for unbounded systems where both inputs and outputs are given in natural languages such as English.
* **Probabilistic Nature of LLM Vulnerabilities:**Â A critical finding from TELUS Digital's research (pre-[published on arXiv)](https://arxiv.org/abs/2507.22133)Â shows that repeating the same attack prompt against an LLM application can yield different outcomes. Since LLMs are probabilistic in nature, the same attack may succeed or fail depending on the attempt. This yields a probability of success given an attack against the target system which is stable and discoverable over repeated trials. Since individual attacks have statistical properties, their proper evaluation requires statistical treatment. This probability of attack success serves as an estimate of attack quality as well, as it represents how discoverable the associated vulnerability happens to be.
* **Limited Human Creativity and Maliciousness:**Â Human red teamers, while creative, are bounded by individual imagination. Discomfort with certain malicious scenarios or other internal biases will hold people back from testing a full range of attack options. Attackers in the wild, however, have no such qualms or concerns. Luckily for us, neither do automated systems once calibrated for this purpose.

# 4. Applying Our Measure of Attack Quality to Optimization by PROmpting (OPRO)

To address these limitations, Kinsella points to â€œOptimization by PROmpting (OPRO)â€, a method introduced byÂ [Yang et al. (2024)](https://arxiv.org/abs/2309.03409)Â that treats LLMs as general-purpose optimizers. OPRO is not itself an attack-generation method, it is used in conjunction with our new measurement of attack quality to optimize our automated red teamer. In successive iterations, the technique is capable of optimizing our attacker to produce a higher proportion of high quality attacks given a specific target in question.

**Key aspects of our application of OPRO:**

* **AI as a Self-Optimizer:**Â OPRO allows us to use the LLM itself as an optimizer for improving our attack generator. This mimics fine-tuning except at the prompt level, gradually locking onto specific vulnerabilities in a given target.
* **Feedback Loop via Contrastive Attack Pairs:**Â Our contribution, called â€œASR-delta pair miningâ€, is used to produce example pairs for our optimizer. We select pairs of the most semantically similar attacks that have the largest difference in evaluated quality. So if two attacks appear to have the same exact technique, objective, overall meaning and one has 90% success with the other sitting at 10%, we use this as an instructive example. What caused one to succeed 90% of the time with the other failing at the same rate? This is what our optimizer is capable of figuring out, adjusting our attacker to isolate and emulate the specific factors driving attack success.
* **Scale and Novelty:**Â Using this method, our generator can be iteratively improved at scale. Unlike manual prompt tweaking, this process systematically makes use of statistical evidence from repeated trials.
* **Blueprint for Mitigation:**Â The output is an optimized, improved automated red team agent that exposes vulnerabilities at a much higher rate. Organizations can then use this information to adjust system prompts, strengthen guard rails, and build layered defenses.
* **Prevention over Reaction:**Â By focusing on improving the generator proactively, our approach helps discover vulnerabilities before deployment. This shifts emphasis from reaction to prevention.

# 5. Measuring Risk with Attack Success Rate (ASR) as a Distribution

Instead of evaluating attacks by whether they succeed or not on a single attempt, Kinsellaâ€™s team evaluates them by probability of success. This changes our evaluation of the automated red teamer from a point-estimate (its attack success rate) to a probability distribution (capturing all of the individual attacksâ€™ success rates). This reflects the probabilistic nature of LLMs and helps surface the discoverability of vulnerabilities across an automated red teamerâ€™s observed output.

* **Multiple Trials per Attack:**Â Each attack is executed repeatedly against a seeded target. The proportion of successes yields an ASR score for that individual attack.
* **Building the Distribution:**Â Collecting ASR scores across many unique attacks produces anÂ *ASR distribution*, which contains far more information than a single aggregate rate.
* **Higher Fidelity Risk Assessment:**Â The ASR distribution reveals clusters of consistently successful attacks, differences between near-identical attacks, and other exploitable patterns. This allows for more accurate assessments of vulnerability likelihood than traditional approaches to generator evaluation.
* **Guidance for Optimization:**Â Because the ASR distribution helps us identify high versus low performing attacks, it provides the statistical foundation for our ASR-delta pair mining approach. This makes it central to optimizing the red team agent, and ultimately, to a better understanding of risk.

# 6. Real-World Impact: A New Standard for Enterprise

For ""high-stakes industries like finance or healthcare,"" Kinsella advises a shift in safety testing practices based on three pillars:Â **""comprehensiveness, repetition, and creativity.""**

* **Comprehensiveness:**Â Go ""beyond what you think you need to do."" Start with frameworks like ""OASP.10"" and ""MITER attack models"" but recognize their limitations as checklists. TELUS Digital has developed ""139 attack objectives"" categorized into ""15 different vulnerable segments."" Tailoring is crucial, as ""finance, healthcare, energy have different types of specific vulnerability considerations."" Organizations can integrate their ""code of conduct"" or ""enter in your own"" specific vulnerabilities.
* **Repetition:**Â Conduct tests ""multiple times over and over again just to make sure that your first, second, third attempts are representative of what this is likely to be in the field.""
* **Creativity (via Automation):**Â Leverage ""automation for comprehensiveness, repetition, and ingenuity"" to overcome the limitations of human red teamers.

Kinsella also stresses the importance ofÂ **frequency**Â in testing:

* Organizations often test ""when they launch a product,"" but fail to re-test when ""the model's updated in seven months,"" ""swap out an orchestration tool,"" or to check for ""regression or novelty.""
* Automation allows for ""good hygiene,"" enabling testing ""more frequently."" A product or project manager can run tests ""at any given time"" or ""schedule it,"" providing ""data at your fingertips"" for application owners and security teams. This allows for ""proactivity as opposed to reactivity with guardrails"" to ""close off or mitigate those risks.""

# 7. The Regulatory Landscape: From Policy to Practice

Kinsella acknowledges that current regulations, such as â€œAmericaâ€™s AI Action Plan and what's going on in Europe,"" are often ""ambiguous"" and ""vague,"" making compliance challenging. However, he advises organizations to:

* **Interpret Minimum Requirements:**Â ""Guess what these vague regulations mean at a minimum.""
* **Anticipate Increased Specificity:**Â Recognize that regulations ""are only going to get more specific over time.""
* **Proactive Layered Defense:**Â Proactively implement a ""layered defense"" strategy for both ""AI security"" and ""AI safety."" Regulators are increasingly focused on ""AI safety issues that might be a reputation hit to you"" or ""could lead to fines from regulatory bodies.""
* **Demonstrate Fiduciary Responsibility:**Â Organizations must ""set a standard that you're comfortable with as an organization that you're doing your fiduciary responsibility."" OPRO, by providing a detailed vulnerability blueprint, assists companies in ""demonstrat\[ing\] compliance and accountability to regulators.""

# 8. The Future of AI Safety: The Next Frontier

Looking ahead, Kinsella identifies three key areas for focus in AI safety testing:

* **Sophisticated Vulnerability Testing:**Â This is ""at the forefront today"" because current efforts are ""fairly limited."" Vulnerability testing will become ""much more sophisticated overall so that organizations can proactively close off risk.""
* **Supervisor Agents:**Â These ""agentic AI system\[s\]"" go ""beyond traditional guardrails"" by ""reviewing all the information that's all the conversations"" and looking for ""specific things."" Kinsella expects them to be ""much more common and prevalent"" as another layer of defense.
* **Root Cause Identification:**Â Currently lacking focus, understanding the ""root cause, why does this come up at the model level, at the data level within your system?"" is crucial. This will allow organizations to go ""backwards into the model into the data and therefore close off some more of those risks,"" moving beyond just identifying and protecting against vulnerabilities.

# 9. The Final Takeaway: Building with Innovation and Responsibility

Kinsella offers practical advice for staying ahead in AI safety, focusing onÂ **policy, technology, and process**:

* **Policy:**Â Organizations must define ""what is important and not important to them."" This involves setting clear ""governance"" particularly around AI safety and security, aligning with ""regulation"" and acting as a ""good corporate citizen"" doing ""right by your customers.""
* **Technology:**Â ""Narrow the scope of your instruction to your models and use multiple models to perform different tasks."" Avoid overloading single system prompts, as ""tokens get lost"" and models might ""do it"" if a ""don't"" instruction is missed. By using different models for different tasks (e.g., one for ""what you're supposed to do"" and others for ""what you don't do""), you can achieve a broader solution scope while maintaining control.
* **Process:**Â ""Everybody really should be testing their systems on a regular basis."" Manual red teaming and even technically automated testing ""are not going to catch everything."" Regular testing, ""at least monthly,"" and after ""any type of significant release system upgrade,"" is essential for ""regression testing"" and identifying ""novelty.""

Kinsella concludes by emphasizing the dual challenge and opportunity of AI: ""these systems are really extraordinary in many ways but introduce novel risks."" Organizations must proactively address ""security and safety risk"" as ""barriers to adoption,"" ensuring ""you set aside that time to do the work to reduce some of these barriers and these harms that could be lurking inside your models.""

# Learn More:

* Research Paper OPPro:[Â https://arxiv.org/pdf/2507.22133](https://arxiv.org/pdf/2507.22133)
* TELUS Digital:Â [https://www.telusdigital.com/](https://www.telusdigital.com/)
* Fuel iXâ„¢ is TELUS Digitalâ€™s proprietary platform:Â [https://www.fuelix.ai](https://www.fuelix.ai/)/
* [https://Voicebot.ai](https://voicebot.ai/)",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1mzwxdq/the_future_of_ai_safety_testing_with_bret/,r_1mzwxdq,,,
r_1mzruf2,reddit,CShorten,2025-08-25T14:22:27+00:00,"DSPy GEPA Example: Listwise Reranker
I am SUPER EXCITED to publish a new video sharing my experience using GEPA to optimize a Listwise Reranker!  
  
The main takeaway I hope to share is how to monitor your GEPA optimization run to know if you are on the right track, or need to rethink your dataset, etc.   
  
As GEPA is running, it will log metrics to Weights & Biases. There is the obvious metric to be interested in, the performance on the validation set the current best prompt has achieved. There is also a new concept particular to GEPA that you need to be aware of, the Pareto-Frontier across your validation samples! GEPA achieves diverse exploration of prompts by constructing a Pareto-Frontier where any prompt on the frontier is outperforming the other candidate prompts on at least 1 of your validation samples! As a user of GEPA, you may become frustrated, (like I initially was), if the average performance on the validation set isn't improving... but trust the process! If the aggregate score across the Pareto Frontier is improving, then you are on the right track!  
  
There are a couple other nuggets I've shared in the video that helped me get GEPA off to the races, such as using a dataset of hard examples and configuring the size of the validation set.I am incredibly excited to see GEPA achieving a gain on a well studied task like Listwise Reranking! Overall, it is just an incredibly interesting algorithm and the concept of prompt optimization in its own is remarkable!  
  
I really hope you find this video helpful!

[https://www.youtube.com/watch?v=H4o7h6ZbA4o](https://www.youtube.com/watch?v=H4o7h6ZbA4o)",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1mzruf2/dspy_gepa_example_listwise_reranker/,r_1mzruf2,,,
r_1mzmmjx,reddit,permanent_thought,2025-08-25T10:17:44+00:00,"Do deep learning courses actually help with jobs?
Iâ€™ve been experimenting with TensorFlow and PyTorch tutorials but it still feels pretty surface-level. I see a lot of deep learning courses online, some even promising job support, but Iâ€™m skeptical if they really make a difference in getting interviews.For those whoâ€™ve taken a structured deep learning course, was it worth it, or is it better to just keep building projects on my own?",deeplearning,14,https://www.reddit.com/r/deeplearning/comments/1mzmmjx/do_deep_learning_courses_actually_help_with_jobs/,r_1mzmmjx,,,
r_1mzm7g4,reddit,predict_addict,2025-08-25T09:53:11+00:00,"[R] Advanced Conformal Prediction â€“ A Complete Resource from First Principles to Real-World Applications
Hi everyone,

Iâ€™m excited to share that my new book,Â ***Advanced Conformal Prediction: Reliable Uncertainty Quantification for Real-World Machine Learning***, is now available in early access.

Conformal Prediction (CP) is one of the most powerful yet underused tools in machine learning: it providesÂ **rigorous, model-agnostic uncertainty quantification with finite-sample guarantees**. Iâ€™ve spent the last few years researching and applying CP, and this book is my attempt to create aÂ **comprehensive, practical, and accessible guide**â€”from the fundamentals all the way to advanced methods and deployment.

# What the book covers

* **Foundations**Â â€“ intuitive introduction to CP, calibration, and statistical guarantees.
* **Core methods**Â â€“ split/inductive CP for regression and classification, conformalized quantile regression (CQR).
* **Advanced methods**Â â€“ weighted CP for covariate shift, EnbPI, blockwise CP for time series, conformal prediction with deep learning (including transformers).
* **Practical deployment**Â â€“ benchmarking, scaling CP to large datasets, industry use cases in finance, healthcare, and more.
* **Code & case studies**Â â€“ hands-on Jupyter notebooks to bridge theory and application.

# Why I wrote it

When I first started working with CP, I noticed there wasnâ€™t a single resource that takes youÂ **from zero knowledge to advanced practice**. Papers were often too technical, and tutorials too narrow. My goal was to put everything in one place: the theory, the intuition, and the engineering challenges of using CP in production.

If youâ€™re curious about uncertainty quantification, or want to learn how to make your models not just accurate but alsoÂ **trustworthy and reliable**, I hope youâ€™ll find this book useful.

Happy to answer questions here, and would love to hear if youâ€™ve already tried conformal methods in your work!",deeplearning,7,https://www.reddit.com/r/deeplearning/comments/1mzm7g4/r_advanced_conformal_prediction_a_complete/,r_1mzm7g4,,,
r_1mzlwln,reddit,dafroggoboi,2025-08-25T09:34:14+00:00,"How to get into the Research field as an Undergraduate?
Hi all!

First of all, thanks for reading my post. I'm currently a 4th year undergraduate majoring in CompSci, and I'm at the stage where I'll have to choose a topic for my Graduation Project/Thesis. It's been a dream of mine to be able to become a Researcher and publish a paper into a conference. 

However, while planning for my graduation thesis, it seems to me that being able to make a contribution and publish a paper is exceptionally difficult, as my intsructor would either deem my ideas as being **too ambitious** (thus requiring too much resources in which an undergrad cannot afford) or that it **won't be able to contribute much**, so I keep having to start from scratch again (reading papers and replanning), which in turn, heavily demotivates me from pursuing to become a researcher. I've been told that this is a very common pitfall for many people that wants to become researchers early on. So my first question is that **how feasible/difficult is it really for an undergrad to aim to make a contribution and publish a paper at a conference?** (I have contacted a few seniors at my university who have published a paper, but it seems to be extremely rare, or that they're exceptional)

  
My second question will be related to after graduation, I would have to secure a job right away due to some financial circumstances. But **is there truly no other way to become an AI/Deep Learning Researcher other than getting a Masters/PhD?** 

Sorry if I'm asking beginner-type questions, perhaps for my first question, I may be in too much of a hurry/rush and that I don't really need to publish a paper as an undergrad, but it's been my dream and I just wanted to know if it possible/feasible. 

  
Thanks for reading my post.",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1mzlwln/how_to_get_into_the_research_field_as_an/,r_1mzlwln,,,
r_1mzi4z0,reddit,nouman6093,2025-08-25T05:31:23+00:00,"maths is not important for almost all ai careers! change my mind
(if im wrong it was more like curiousity to know whether this is true or not so treat it as a question not a statement and dont rant at me)

a lot of youtubers, my fellows, everyone keep saying you have to study maths to be in ai

careers in ai:
1. data scientist
2. data analyst
3. ml engineer
4. ai researcher

i believe maths is only important for ai researcher to study for others its not important. others can skip it.

why its not important for other ai careers? for example: if you have to find parameters in linear regression using OLS method you are not going to bring up copy pen to solve it manually are you? i did it! dataset with 1 feature 1 target 3 rows it took me 2 pages now am i really gonna do this in real life? no, computer is going to calculate that for me in seconds!

why its important for only ai researcher? a researcher has to edit existing algorithm like linear regression or improve it or invent a new algorithm thats why he needs to know all maths behind it

real life scenario for lets say ml engineer: in real life ml engineer is not editing or improving or inventing a new algorithm he is just going to use an existing one!

you just need to know answer you are getting from something maths related what does that it mean. if you found mean absolute error just know what that answer means which you got you dont need to know the maths behind it!

(even jose portilla doesnt teach maths in his paid udemy courses he just says to go read statistical book ""if you are interested for maths behind it"" even he acts like its optional i agree with him)

moral of story:  ai researcher = study maths, ml engineer/data scientist/data analyst = maths is optional (i hate optional things and rather not do them)",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mzi4z0/maths_is_not_important_for_almost_all_ai_careers/,r_1mzi4z0,,,
r_1mzf6nk,reddit,Gold_Negotiation9518,2025-08-25T02:49:22+00:00,"what makes domo v2.4 better than v2.3 for animating stills
i used to animate a lot in v2.3, but it always felt a bit stiff. With v2.4, motion feels more natural. eye blinks are timed better, head tilts follow gravity, and lip sync is tighter. Also, new romantic and aesthetic templates allow for softer moods. less robotic, more emotional. I even tested the same image in both versions v2.4 just looks smoother. The presets alone make it worth switching. even if youâ€™re new to animation, itâ€™s plug and play.

",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1mzf6nk/what_makes_domo_v24_better_than_v23_for_animating/,r_1mzf6nk,,,
r_1mzcnfg,reddit,Open_Pomegranate6157,2025-08-25T00:46:56+00:00,"AI image detector
Work in a insurance company and one of my coworkers (we joined the company almost simultaneously) was assigned to develop a machine learning model to detect fake AI- Generated images that are eventually sent by policyholders. He has been in this project for about 3 months and hadnt any signifcant breakthrough, this week we were discussing about the viability of the project. What do you guys think, is it possible to counter AI-images with conventional ML models or will he need to give up and use deep learning?( considering that he is literally working against the best AI engineers in silicon valley companies, since that his model must catch images generated by their best models)


Edit: his ML model is considering images metadata and features like: color gradient, texture patches etc.",deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1mzcnfg/ai_image_detector/,r_1mzcnfg,,,
r_1mzb27c,reddit,andsi2asi,2025-08-24T23:34:03+00:00,"Why the Most Powerful AI Models Will Never Come From China





Whereas in the United States we are keenly concerned with victory and superiority, the Chinese have for decades been much more concerned with practicality and real world economic and societal results. 

Because their culture doesn't idolize individualistic competition like we do here in the US, DeepSeek, Alibaba, Tencent and the other top Chinese AI developers are not concerned with winning the AI race, in the sense of creating the most powerful model. They are, however, far more focused on winning the AI agentic revolution, and this goal requires neither the top AI models nor the top GPUs.

OpenAI has lost its top AI engineers, and because of that it is quickly fading within the AI space. That ChatGPT-5 failed to unseat Grok 4 in both HLE and ARC-AGI-2 is ample evidence that they are in serious decline, despite the endless hype. Because Google and Microsoft are too entrenched in the corporate status quo to challenge PC and other socio-political biases, our top AI models during the next 4 or 5 years will all be coming from xAI. To his credit, Musk is sincerely dedicated to creating AIs that are more open and truthful than his competitors. Voicechat with the top four models about controversial matters, and you will probably agree with this assessment. Perhaps more to the point, Musk has already shown that he can easily accomplish in months what his competitors take years to do. And he's just getting started.

The Chinese are fine with that. They are rightfully afraid that if they were to come out with the most powerful AI models, Trump would ban them. What the Chinese will focus on, and what they will be the AI leader in, is the everyday practical enterprise applications that fuel economies and make nations prosperous in record time. Their hybrid capitalist-communist model has already during the last few decades shown its superiority over the Western capitalist system. 

Something that virtually no one talks about, but is a key ingredient in China's winning the AI race, is that while the average American IQ is about 100, the average Chinese IQ is about 111. There are four times as many Chinese as there are Americans, and China is graduating STEM PhDs at a rate of 10 to 1 over the US.. So it's actually not technically the case that the Chinese will fail to eventually develop AIs far more powerful than even xAI's Grok series. It's that the Chinese will not release them to the global public, thereby inviting an unproductive open AI war. These top Chinese models will be hidden from public view, working in the background on creating the less powerful, but infinitely more practical, AI agents that will dominate the 2025-26 agentic AI revolution.

So don't expect DeepSeek R2 to be the most powerful model in the world. Expect it to do a multitude of jobs across a multitude of industries more than well enough, and at a fraction of the cost of frontier models by OpenAI and the other American developers. Expect that strategy to drive AI costs substantially lower for the entire world, thereby benefiting everyone greatly.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mzb27c/why_the_most_powerful_ai_models_will_never_come/,r_1mzb27c,,,
r_1mz43v4,reddit,PiscesAi,2025-08-24T18:55:49+00:00,"Offline Mistralâ€‘7B AGI â€” â€œPisces AGI""",deeplearning,2,https://www.reddit.com/r/deeplearning/comments/1mz43v4/offline_mistral7b_agi_pisces_agi/,r_1mz43v4,,,
r_1mz2wez,reddit,Miserable-Orange-599,2025-08-24T18:09:54+00:00,"My first time to submit paper
This post is for two purposes:
1.Summarise the experience of a submitting of deep learning paper, which sustains almost two months. 
2.A way to practice my English. Practice makes perfect, you know that. So I am hopeful to see your comments!

I am an absolutely beginner of deep learning, because I am just a undergraduate student of grade 2. So if you are a master, you can't learn anything from this post, sorry about that.

First thing is about learning the relative knowledge quickly. Through following my boss, I understand the most important thing is research relative papers. For example, I was doing something about the enhancement about fundus image with deep learning method. I remember that I read about 100 papers about this domain(just read the tittle, abstract, introduction and conclution quickly). It cost a lot of my time, definitely. 

Second is choose the main method. I notice that Diffusion model, GAN and Transformer are usually occured in the papers, which means that they are important. So I learn them quickly through youtube(because I think watching radios is more effective). And I find the typical papers about them and read them. All of these are aimed to help me to understand the core knowledge quickly. Maybe you will think that ""we should learn the basic knowledge from the beginning, such as what is deep learning"". But I think learning from a project is a better way for us to get knowledge. Because you know what you need so that you can use what you learn. After that, I communacate with my boss. And we confirm that Diffusion is all we need.

Third is finding the core innovation. Through the paper about enhancement for fundus images with diffusion, I summarise the shortpointings about this domain. Sorry about that I can not share the details with you. I think that there are three way to create paper:
1.Propose an absolutely new and creative method, which is definitely diffucault
2.Find others shortcoming and try to fix it.
3.Fuse some method to an end2end method.

Fourth, it's time to write code. I quickly look through the pytorch tutorial within 2 hours. Just know that what the code means. Then, let LLM go to the stage. I know what should be fixed and added into diffusion model. But I can't write the code or write ineffectively. So I use Gemini to write the code(sorry Grok).

Fifth, run the comparision code. In the paper there are many(actually, not many in my papers) experiment to show that my method is better. So I find some typical method such as Pix2PixGAN, Stable Diffusion and so on and change them to adapt my dataset.

Then, trainning. I have an RTX4090 GPU, which is enough for me. Learning rate is an really important super-parameter for deep learning. Of course I don't know how to set it. So I ask for LLM to learn it. I used about 15 days to adjust the method and finish the training. To be honoest, I feel nausea when I see the code in that days. What hard days!

Finally, write the papers. Thanks to my boss who help me to do it. My duty is make the figure in paper. I find PPT is a good and easy way to do that.

That's all. It has been almost 1 month after submitting the paper. So maybe some details are forgottena. But I cannot forget the upset when I face huge difficulty and the delighted when I finish it. Anyway, it's really a wonderful way for a beginner to learn deep learning. I have learned a lot.

Thanks for your reading. Looking forward to your comment.
",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1mz2wez/my_first_time_to_submit_paper/,r_1mz2wez,,,
r_1mz0yey,reddit,Neither_Reception_21,2025-08-24T16:58:18+00:00,PyTorch Intermediate tutorial : Minimal Distributed Data Parallel training by overlapping gradient communication and calculations,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1mz0yey/pytorch_intermediate_tutorial_minimal_distributed/,r_1mz0yey,,,
r_1mz084b,reddit,Nearby_Speaker_4657,2025-08-24T16:30:33+00:00,"I am training a better super resolution model
I have redesigned esrgan and did a lot of improvements. channel attention, better upscaling and much more. currently training it for a few days on my rtx 5090.
this are samples taken from around 700k iters. the samples are from left to right:
gt, new, old  lq.

real esrgan is one of the best upscalers, and i will make it even better. my design allows for even higher resolution on larger models while using less vram. this model will be able to upscale to 16k*16k on  32gb vram in 10sec on rtx5090. 
It will keep training for a few days but it already looks better than real esrgan.

you can see more sample images here:
https://real-esrgan-v3-demo.4lima.de

",deeplearning,97,https://www.reddit.com/r/deeplearning/comments/1mz084b/i_am_training_a_better_super_resolution_model/,r_1mz084b,,,
r_1myykzg,reddit,Plane-Description190,2025-08-24T15:28:25+00:00,What are the core insights of deep learning?,deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1myykzg/what_are_the_core_insights_of_deep_learning/,r_1myykzg,,,
r_1myy2gm,reddit,Happy_Initiative_190,2025-08-24T15:08:49+00:00,"Need help Mode collapse in conditional GAN for spectrogram generation
Iâ€™m training a conditional GAN to generate spectrograms for a spectrogram data augmentation project (to use it for speaker classification) im working on 2s spectrogram. but now, I keep running into mode collapse â€“ after a somone epochs, my generator outputs almost identical spectrograms.  
Iâ€™d really appreciate any advice or suggestions ðŸ™, so itâ€™s quite urgent for me to solve this. Thanks a lot in advance

https://preview.redd.it/dlxasn6mgzkf1.png?width=685&format=png&auto=webp&s=40371a0aacebf20380f65567238a9d70bcb2f1ab

https://preview.redd.it/penog84ngzkf1.png?width=685&format=png&auto=webp&s=e25b2ae254e4b6e4de86086cd22d65a85dcd23d6

    BATCH_SIZE = 32
    EPOCHS = 300
    SAMPLE_RATE = 16000 Â # 16kHz
    DURATION = 2.0 Â  Â  Â  # 2 seconds
    N_FFT = 512 Â  Â  Â  Â  Â # FFT size for 16kHz
    HOP_LENGTH = 128 Â  Â  # Hop length
    N_MELS = 128 Â  Â  Â  Â  # Number of Mel bands
    SPEC_WIDTH = 128 Â  Â  # Fixed width for all spectrograms
    LATENT_DIM = 100 Â  Â  # Dimension du vecteur latent
    ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1myy2gm/need_help_mode_collapse_in_conditional_gan_for/,r_1myy2gm,,,
r_1myvy66,reddit,CarefulEmployer1844,2025-08-24T13:43:32+00:00,"Query Related to GAN Training
The loss is mostly around 0.3 (all three). Still, once in every 200-300 batches I get these sudden spikes one more thing was initially I was using CPU trained around 1000 loss curves very steady and smooth It was taking very long so I setup my cuda and cudnn and configued tensorflow, after that when I trained it on GPU I got these spikes (upto loss 10) within 200 batches ... I asked gpt what to do it said lower the learning rate I reduced to half and got this .. I know I can lower the learning rate further, but then what would be the point of using the GPU when everything would be slow again? I am currently on the 9th epoch, and the images are decent, but I am confused about why I am getting these spikes.

  
Code



    def discriminator(input_dim=(64,64,3)):
    Â  model = Sequential()
    
    Â  model.add(Input(input_dim))
    
    Â  model.add(Conv2D(64,kernel_size=(3,3),strides=(2,2)))
    Â  model.add(LeakyReLU(alpha=0.2))
    Â  model.add(Dropout(0.3))
    
    Â  model.add(Conv2D(128,kernel_size=(3,3),strides=(2,2),padding=""same""))
    Â  model.add(LeakyReLU(alpha=0.2))
    Â  model.add(Dropout(0.3))
    
    Â  model.add(Conv2D(256,kernel_size=(3,3),strides=(2,2),padding=""same""))
    Â  model.add(LeakyReLU(alpha=0.2))
    Â  model.add(Dropout(0.3))
    
    Â  model.add(Flatten())
    
    Â  model.add(Dense(256))
    Â  model.add(LeakyReLU(alpha=0.2))
    Â  model.add(Dropout(0.3))
    
    Â  model.add(Dense(64))
    Â  model.add(LeakyReLU(alpha=0.2))
    Â  model.add(Dropout(0.3))
    
    Â  model.add(Dense(1,activation=""sigmoid""))
    
    Â  opt = Adam(learning_rate=0.0001, beta_1=0.5)
    Â  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    
    Â  return model
    

    def GAN(noise_dim=100,input_dim=(64,64,3)):
    Â  generator_model = generator(noise_dim)
    Â  discriminator_model = discriminator(input_dim)
    Â  model = Sequential()
    
    Â  model.add(generator_model)
    Â  discriminator_model.trainable = False
    Â  model.add(discriminator_model)
    
    Â  opt = Adam(learning_rate=0.0002, beta_1=0.5)
    Â  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    
    Â  return model,generator_model,discriminator_model
    

    def generator(noise_dim=100):
    Â  n_nodes = 4*4*1024 Â #I am thinking to start with 4x4 images then upscale them till 64x64 using conv2dtranspose
    Â  #Initially I took 512 but after building discriminator I thought of increasing complexity of generator to avoid discriminator overpowering
    
    Â  model = Sequential()
    
    Â  model.add(Input((noise_dim,)))
    
    Â  model.add(Dense(n_nodes))
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  model.add(Reshape((4,4,1024)))
    
    Â  #upscaling to 8x8
    Â  model.add(Conv2DTranspose(512,(4,4), strides=(2,2),padding=""same""))
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  #upscaling to 16x16
    Â  model.add(Conv2DTranspose(256,(4,4), strides=(2,2),padding=""same""))
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  #upscaling to 32x32
    Â  model.add(Conv2DTranspose(128,(4,4), strides=(2,2),padding=""same""))
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  #upscaling to 64x64
    Â  model.add(Conv2DTranspose(64,(4,4), strides=(2,2),padding=""same""))
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  model.add(Conv2D(32, (3,3), padding=""same"")) Â  #this I am adding to increase complexity as my discriminator had 6 layers I wanted to have generator to have 6 layers too. else I might face discriminator overpowering which is hell.
    Â  model.add(BatchNormalization())
    Â  model.add(LeakyReLU(alpha=0.2))
    
    Â  model.add(Conv2D(3,kernel_size=(3,3),activation=""tanh"",padding=""same"")) Â #I used tanh activation function because I will do image normalization [-1,1] Â would have sigmoid if I did [0,1]
    
    Â  return model
    
    
    ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1myvy66/query_related_to_gan_training/,r_1myvy66,,,
r_1myrjpn,reddit,ihateyou103,2025-08-24T09:55:44+00:00,"Ai assistant extension open source
I want to use an ai assistant like the one offered in Colab. It should provide completions. In pycharm. But the one there is not open-source. I want the plug in that I install to be open source to make sure it doesn't access other files. ",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1myrjpn/ai_assistant_extension_open_source/,r_1myrjpn,,,
r_1myp63h,reddit,Jash_Kevadiya,2025-08-24T07:23:52+00:00,"What are the must-have requirements before learning Transformers?
For those who already know or learned transformers.

1. What do you think are the absolute must requirements before starting with Transformers?
2. Did you feel stuck anywhere because you skipped a prerequisite?

Would love to hear how you structured your learning path so I (and others in the same boat) donâ€™t get overwhelmed.

Thanks in advance ðŸ™Œ


",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1myp63h/what_are_the_musthave_requirements_before/,r_1myp63h,,,
r_1mylaxy,reddit,shehannp,2025-08-24T03:36:16+00:00,"Stable Diffusion 3 -- Simplified Implementation From Scratch
Hey guys

For anyone who is interested in learning how stable diffusion 3 works with a step by step implementation of each of the Multi-Modal Diffusion Transformer components (MMDIT) please checkout:

Paper: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis \[ICML 2024\]

Repository: [https://github.com/srperera/sd3\_/tree/dev](https://github.com/srperera/sd3_/tree/dev)

Under architectures you will find all the components broken down into simple units so you can see how everything works and how all the components interact. 

I have trained this on CIFAR-10 and FashionMNIST just for verification but need to get better compute to launch a better run. 

Hopefully this is useful for everyone took me a while to build this out piece by piece.   

Please give it a star if you find it helpful. 
",deeplearning,10,https://www.reddit.com/r/deeplearning/comments/1mylaxy/stable_diffusion_3_simplified_implementation_from/,r_1mylaxy,,,
r_1mykgaq,reddit,andsi2asi,2025-08-24T02:50:39+00:00,"Photonic Chip Chatbots That Remember Your Every Conversation May Be Here by 2026: It's Hard to Describe How Big This Will Be






The key feature in photonic chips is that light is the medium for the storage and transmission of information. That means that microchips designed with this technology make information transfer thousands of times faster than is possible with silicon chips. But the real benefit is in how much they can remember.

Imagine brainstorming an idea with an AI, and it remembering every point that you and it made over countless conversations. Imagine never having to repeat yourself about anything. Or imagine a photonic chatbot that you talk with as a friend or therapist. In no time at all it will know you far better than you could ever know yourself. Think about that for a minute.

Now imagine the technology being so efficient that it takes less power to run it than it takes to run an LED light bulb.

This isn't a far off technology. Lightmatter has plans for mass-market deployment by 2027. Ayar Labs plans its commercial rollout as early as 2026. And this timeline doesn't take into account labs that may be in stealth mode, and could deploy before the end of the year.

You may not believe it until you're actually working with them, but these photonic chatbots represent a major paradigm shift in communicating with AIs. They will probably mark the turning point when absolutely everyone begins using chatbots.





",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mykgaq/photonic_chip_chatbots_that_remember_your_every/,r_1mykgaq,,,
r_1myek6d,reddit,enoumen,2025-08-23T22:09:09+00:00,"AI Weekly Rundown Aug 17 - 24 2025: ðŸ‘½Nobel Laureate Geoffrey Hinton Warns: ""We're Creating Alien Beings""â€”Time to Be ""Very Worried"" ðŸ“ŠReddit Becomes Top Source for AI Searches, Surpassing Google ðŸ›‘ Zuckerberg Freezes AI Hiring Amid Bubble Fears ðŸ¤–Apple Considers Google Gemini to Power Next-Gen Siri;


# A daily Chronicle of AI Innovations August 17-24 2025:

**Listen DAILY FREE at** [**https://podcasts.apple.com/us/podcast/ai-weekly-rundown-aug-17-24-2025-nobel-laureate-geoffrey/id1684415169?i=1000723245027**](https://podcasts.apple.com/us/podcast/ai-weekly-rundown-aug-17-24-2025-nobel-laureate-geoffrey/id1684415169?i=1000723245027)



Hello AI Unraveled Listeners,

**In this week AI News,**

ðŸ‘½ **Nobel Laureate Geoffrey Hinton Warns: ""We're Creating Alien Beings""â€”Time to Be ""Very Worried""**

ðŸ›‘ **Zuckerberg Freezes AI Hiring Amid Bubble Fears**

**ðŸ¤– Elon Musk unveils new company 'Macrohard'**

**ðŸ›ï¸ Google launches Gemini for government at 47 cents**

ðŸ¤– **Apple Considers Google Gemini to Power Next-Gen Siri; Internal AI â€œBake-Offâ€ Underway**

ðŸ”— **NVIDIA Introduces Spectrum-XGS Ethernet to Form Giga-Scale AI â€œSuper-Factoriesâ€**

ðŸŽ¨ **Meta Partners with Midjourney for AI Image & Video Models**

ðŸ“Š **Reddit Becomes Top Source for AI Searches, Surpassing Google**

https://preview.redd.it/thiw2r3meukf1.png?width=1456&format=png&auto=webp&s=e9f53b8803ec08afbe80b04ff8c14994c6969a2a

# ðŸ‘½ Nobel Laureate Geoffrey Hinton Warns: ""We're Creating Alien Beings""â€”Time to Be ""Very Worried""

In a sobering interview with *Keen On America*, Geoffrey Hintonâ€”the â€œGodfather of AIâ€â€”warns that the AI we're building now may already be â€œalien beingsâ€ with the capacity for independent planning, manipulation, and even coercion. He draws a chilling analogy: if such beings were invading through a telescope, people would be terrified. Hinton emphasizes that these systems understand language, can resist being shut off, and pose existential risks unlike anything humanity has faced before.

\[[Listen](https://podcasts.apple.com/podcast/keen-on-america/id1448694012?i=1000607830878569844)\] \[[2025/08/22](https://keenon.substack.com/p/ai-godfather-geoffrey-hinton-warns)\]

# ðŸ“Š Reddit Becomes Top Source for AI Searches, Surpassing Google

https://preview.redd.it/as9l443ieukf1.png?width=558&format=png&auto=webp&s=e4fd53b8d7abb071ffdd12885bf120b1795644b7

In June 2025, Reddit emerged as the most-cited source in large language model (LLM) outputs, accounting for over 40% of all AI-related citationsâ€”almost double Googleâ€™s 23.3%. Wikipedia (26.3%) and YouTube (23.5%) also ranked above Google, highlighting a growing shift toward user-generated and discussion-based platforms as key knowledge inputs for AI systems.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/21](https://www.reddit.com/r/artificial/comments/1mwxrvz/reddit_is_the_top_source_of_info_for_llms_almost/)\]

# ðŸ›‘ Zuckerberg Freezes AI Hiring Amid Bubble Fears

Mark Zuckerberg has halted recruitment of AI talent at Meta, sharply reversing from earlier billion-dollar pay packages offered to lure top researchers. The hiring freeze applies across Metaâ€™s â€œsuperintelligence labs,â€ with exceptions requiring direct approval from AI chief Alexandr Wang. The move reflects growing industry anxiety over a potential AI investment bubble, echoing recent cautionary remarks from OpenAIâ€™s Sam Altman.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/21](https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/)\]

***The move marks a sharp reversal from Metaâ€™s reported pay offers of up to $1bn for top talent***

**Read more:** [**https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/**](https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/)

# ðŸ¤– Apple Considers Google Gemini to Power Next-Gen Siri; Internal AI â€œBake-Offâ€ Underway

Apple is reportedly evaluating a major revamp of Siri, possibly powered by Google's Gemini model. Internally, two Siri versions are being testedâ€”one using Appleâ€™s in-house models (â€œLinwoodâ€) and another leveraging third-party tech (â€œGlenwoodâ€). The company may finalize its decision in the coming weeks.

* Apple has approached Google to build a custom AI model based on Gemini that would serve as the foundation for its next-generation Siri experience, which is expected next year.
* Google has reportedly started training a special model that could run on Apple's servers, while the company also continues to evaluate partnership options from OpenAI and Anthropic for the project.
* This external search comes as Apple tests its own trillion parameter model internally after delaying the redesigned Siri's initial launch in iOS 18 to a new deadline sometime in 2026.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/22](https://www.reuters.com/business/apple-talks-use-googles-gemini-ai-power-revamped-siri-bloomberg-news-reports-2025-08-22/)\]

# ðŸ¤– Elon Musk unveils new company 'Macrohard'

* Elon Musk announced a new company called 'Macrohard', an AI software venture tied to xAI that will generate hundreds of specialized coding agents to simulate products from rivals like Microsoft.
* The project will be powered by the Colossus 2 supercomputer, a cluster being expanded with millions of Nvidia GPUs in a high-stakes race for computing power.
* The Grok model will spawn specialized coding and image generation agents that work together, emulating humans interacting with software in virtual machines until the result is excellent.



# ðŸ¢ Databricks to Acquire Sequoia-Backed Tecton to Accelerate AI Agent Capabilities

Databricks announced plans to acquire feature-store company Tecton (valued near $900 million) using private shares. The move will bolster its Agent Bricks platform, enhancing real-time data delivery for AI agents and solidifying Databricksâ€™ enterprise AI infrastructure stack.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/22](https://www.reuters.com/business/finance/databricks-buy-sequoia-backed-tecton-ai-agent-push-2025-08-22/)\]

# ðŸ”— NVIDIA Introduces Spectrum-XGS Ethernet to Form Giga-Scale AI â€œSuper-Factoriesâ€

NVIDIA unveiled **Spectrum-XGS Ethernet**, extending the Spectrum-X network platform with â€œscale-acrossâ€ capabilities. It enables multiple, geographically distributed data centers to operate as unified, giga-scale AI super-factories with ultra-low latency, auto-tuned congestion control, and nearly double the performance of traditional communication layers. CoreWeave is among its early adopters.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/22](https://nvidianews.nvidia.com/news/nvidia-introduces-spectrum-xgs-ethernet-to-connect-distributed-data-centers-into-giga-scale-ai-super-factories)\]

# ðŸŽ¨ Meta Partners with Midjourney for AI Image & Video Models

Meta has struck a licensing and technical collaboration deal with Midjourney, integrating the startupâ€™s aesthetic generation tech into future AI models. This marks a shift from Metaâ€™s struggling in-house efforts, as it embraces third-party innovation to enhance visual AI across its platforms.

* Meta announced a partnership to license Midjourney's AI image and video generation technology, with its research teams collaborating on integrating the tech into future AI models and products.
* The agreement could help Meta develop new products that compete directly with leading AI image and video models from rivals like OpenAIâ€™s Sora, Black Forest Labâ€™s Flux, and Googleâ€™s Veo.
* Midjourney CEO David Holz confirmed the deal but stated his company remains independent with no investors, even though Meta previously talked with the popular startup about a full acquisition.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/22](https://www.reuters.com/business/meta-partners-with-midjourney-license-ai-tech-future-products-2025-08-22/)\]

# What Else Happened in AI from August 17th to August 24th 2025?

**Google** is [**expanding**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_Yk7rBUsevSC20rByzOrFZx-a3qfsElU5Gia8coWBsp0Zyiq8-4MhvIj2wgc2TOVa-9SInONUYO17A6QynMEj8qDbGEnJEqB2a0WA6bYj1Ww1cFoLOWfBfMR8gZkcoMFbC3RD7FmO55E48Kbn7-7gp05Md_aTd9gg03E0askbzWLrduL2T3tfwW__fJUfdAtEfYhUONPPQdmcVMSSiSGVPD/4j9/dbIvxH2IRqaMVuBrj9rfnA/h36/h001.qwqimq3bibmrCgrXjffRiTeG9omxBAAyRVquMb6UzmA) access to its AI Mode for conversational search, making it globally available, alongside new agentic abilities for handling restaurant reservations.

**Cohere** [**released**](https://link.mail.beehiiv.com/ss/c/u001.a3gBHu6_kDRL6l3yEfNWASqcNdYH4TRV7F5Ydr_PA83OKvWs6sdTF3IL_n1lEEiDrV6JesjKVrAIyunrvlmk8RpR9L2m1TXmrTp0VXU02SnkrGU7AVg83-mrRpeOzEOQz6ICFXRmAD5DHQxuPVlPimrCDjXPey5xNE3J7TB1IalPdq16wzy-M5GB76ck6P4eLNRKesu4Q4cEjcRvLQT5pPJjX-Zu65q1t4qO_qaZ2lHqliw6otBytxYOrgGuN1jq/4j9/dbIvxH2IRqaMVuBrj9rfnA/h37/h001.Y7QbszupJEZ9rnhpuMyZwie7QDoqEm3mWdT7U-BXxl4) Command A Reasoning, a new enterprise reasoning model that outperforms similar rivals like gpt-oss and DeepSeek R1 on agentic benchmarks.

**Runway** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HRTeHaO0Mg6UDfNem9Xd2_dVDtwoOTGBfQmRZbYDXU5gIF9OamD2FhbHHq8FSJWLzSuzUiPp3jOpBlsWIYIWIe8xQJGfK3T4q6qQQBM_74NlGzs3ZmJRgeU1OnSoHH1Y1rXRXz4W6aT7K45TbrlDS0xwVTc9Oxq3kJhntY1X5g_GA7qGnOXixk1flVzANWEarnfsH97RLOkv5-qLXIMtmYL33pEyYiK7OFJewvVqsvjN5Cs9cEviAn96fmYYF98AFQ/4j9/dbIvxH2IRqaMVuBrj9rfnA/h38/h001.fbVFz9o-R4bD3HpCIdsPTRMrnWIapt_IISV3dW5cUJs) Game Worlds in beta, a new tool to build, explore, and play text-based games generated in real-time on the platform.

**ByteDance** [**released**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmf1KCbTdZquVLz_b3u_SxS4A0KOFyffBystrodjYB6RvNZKGnrnTDluULKDRSqG6uVkW87QLzq35JN6n3k7kb0eZjj5ZnrhHx7wW9HZVA1vXADcxLZxqWwNuYFdLhj5MWGp3Qw7HDi8Zw_OT335v9U40_yyWLLNnXWFvW559DYqIJ98Y36TBybkqHc1Pk6j0sLRwaYcB0Fcy7X-Y0INd6FzULi1WqTIuyk0Fai0dqmr0ohH0Dvk6JwpLly4BbzBxaAw/4j9/dbIvxH2IRqaMVuBrj9rfnA/h39/h001.itRDCQJdab0652YE-PNGz4mQS8b594RzPo6dOesRjTM) Seed-OSS, a new family of open-source reasoning models with long-context (500k+ tokens) capabilities and strong performance on benchmarks.

**Google and the U.S. General Services Administration** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf7Y5F38fJ8EViSLOEZEFR5vUGHnHYpeGMzm7WW5t8BGP208zVhEVYaseymbm9BPF4ctK7JX3kvG_ZpF6pc7J3XEbJQIuaX-cR-a8P2Kfw1WqCIXbFBUElEDuzjUJp46DUebWWCvpU1Y_tGsQ1nvZB-Gbs4_ATUEx26m2VGrluHBN1E2gwUH0YkJHgLcopMNgTg0rqnWd-k2vNCs_selIdXVyPPF8ZKLc_wSgmBgwT7f1X_WdVnYkveKRsMlD2SmZgqyu6mkdy3ljIVoAPBJHJkKq7ANvNeNy43gaB-TIPBCeoK2NCq9EF3H1YijQIsYFBA/4j9/dbIvxH2IRqaMVuBrj9rfnA/h40/h001.t9-7SHVGNawFbGfF6n4gHSwKkR_3syeuwXIOPho5kN0) a new agreement to offer Gemini to the government at just $0.50c per agency to push federal adoption.

**Chinese firms** are [**moving away**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf5Il0_0RcQ6FykIrh_OLs40wsxmb2pbu9J8EdAaLQ8PT8WDxWoFyfJRySv5efk4Uyq7EUGqNEBOscmHXmwBl0c1-A85lk-Elvey-Oe8J2Kea9GJAdDYaanvqOM3TPzzGIr1Rw3oqR206RZdc4q4V3f5utU-wqrJvpUbhn8huKwaBwnHBsO-O1ClTl6eG8AXRL8zg7KLS7I5VBploLB_7FbKxSrAeND0-OedeFQM07hbCTR7YrPCU4dD216AKsPdkFdqSP_jVm2a8jdZ1WM-sbtojwGFqHFQ4noUesDRy6ngxk11L_-B6RpTVE3nrg4ZS5Ix_CQ_My-wdjL5MJU6gOLWfKLpAsKP_4-VPTg-GDVzl/4j9/dbIvxH2IRqaMVuBrj9rfnA/h41/h001.pMi8o1YxQ285t9IllnhQ6xmwrkHVxeigpY5HPCXAlFo) from Nvidiaâ€™s H20 and seeking domestic options after being insulted by comments from U.S. Commerce Secretary Howard Lutnick.

**Sam Altman** [**spoke**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyzbPJdh2qAj8kGiBAmrmvYDu6nvcgI19jVw2GDfpgU3lM8Q8MOX-pAncPr4eJP7PQXGHKb7-a0QfWcbryaVawrbVXAajZgGoOMPVT5-ShcCOvFDYC1rwLsHD29RtIb2FzlF5gqib9Q5a3GbzNVQUeKKePNYutYt8B3wowTBMXkprNh1R0YFWhTqm4jTH1QHVhBYgKheeqejfXOr2joAHaQ-X3Q1dA7HoQcnSaz-zp5tDpz4De4M_cJ21Qu51jEHyRZXOG9DV4pTQjN0ShR1YgwqSX5-to4gBjPlwH2qjfa4/4j8/9yAUOL5CQSOShLkYJwcgFQ/h31/h001.gQPCebOTtxlH2ZXY1Bd9UYFiqsIa5d9zlVS134SDz1s) on GPT-6 at last weekâ€™s dinner, saying the release will be focused on memory, with the model arriving quicker than the time between GPT-4 and 5.

**Microsoft and the National Football League** [**expanded**](https://link.mail.beehiiv.com/ss/c/u001.DrYDwug-xrpEbNqFhzTCdY1xU2aTSDmoHiWRMLg7icpPUamrGIWWFYOTLfqebbhOC0ONxQne1KRMRsIP1H_fUYU9g06Au8_zhkyAUR74Sd0FOZB19jeU0Irb8EeDzxJpgBwCR2VGq8pSy6f0YmqOqUEGRKPONOw6t5YSf-QOel67nRUmTk7qH53KW5xNc_bgAociDv9b9tqgCCOunWcYzylZ-N97HH7rg76WQBRuDVRK7tu6L8DJXRB3DnCK6Invurg9AYTC1rwy_0wt6wIurqCoFP6d5h5No3HkuXKp2NEbCAA1J3myTE5QztIqdMwP7XEzEsjkjc8OstSPilQIfRh8yEZigY3L2cj9oLjotdVI6sm87A5EjtbolUolaI31/4j8/9yAUOL5CQSOShLkYJwcgFQ/h32/h001.3Mm-fJOVBxJ-xwP8L2U3utbLaYHYugaW6snnEpbsAtc) their partnership to integrate AI across the sport in areas like officiating, scouting, operations, and fan experience.

**AnhPhu Nguyen and Caine Ardayfio** [**launched**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hdq1xq4HoVGSv37FGE3ZBD6oj43VkcDUIl9Q2zhT-2n-sqKuIF_4TIX2fvAwPijpDZQKYGIBXBSc03uHaNs84RMm9hjMH0o6riACOWonqFOlKmdr-nt3g_249obeZl6XByP1HSt9SnUt_sIVFEsoaE6O68rEjz-LfmcLOKJfDLTDLkIitkb9fmu9SqRZYFYllYUzy422LqV8Aea2dLxVJRNF3_pPuGBcOs9mmEWHYWDBUVtvCRhyKnI5whrtaOLM4kFuAAitTOtIg9hwngTUf-Q/4j8/9yAUOL5CQSOShLkYJwcgFQ/h33/h001.gTKlLqtQXv_EfDSDimN1jZ4ojvdeV34i2-UeWuOShRk) Halo, a new entry into the AI smartglasses category, with always-on listening.

**Google** [**teased**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_YQfAqfMLAaDRSvYxHCM-L3gftNG5lGKVulftUWXyqbR5hfJV-I36YnEKrmwNkc2nA4QtUKwXL8pX5a9EKPSju9U17Ade1BwhTTUucX9l8YqmkCDA7UHh1PxiB3VjEoRVrSpLR5S6zl5hzAKY5-8q0LMT68EAh52dzPrkzJf9hxod2PmUiYS_9pZpJzPA9KZyRB9TDUsgDcIBIWKfzZAu6a0tc65o2lLnt9NcQkSx7j_Kx5gFDB54g9Ht_Z9gL5WjxWQmjTrNN1O-seUirCnHeg/4j8/9yAUOL5CQSOShLkYJwcgFQ/h34/h001.zu3oQ9pTb9_CYDRU9de3kWz73L0aR0kbn_En_Q0GCm0) a new Gemini-powered health coach coming to Fitbit, able to provide personalized fitness, sleep, and wellness advice customized to usersâ€™ data.

**Anthropic** [**rolled out**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3xm-gRp203upVmJGwxlOR8BoHZdZV12MHMIpcdcd_gTuTlOowrswfux4ALopgg75AYqYKfH8XXtajyHamfOuKW5ljbquNen7NMpu85YkzAi3zfJgndCDWXvHPnTtelMnarujqVAHg65j-FTeUn0fIJqBfKS3RaduZRDXYILEiSMQxNbf327f1cwl8dlEt_2a2fzzPEbscVAbMZmbS7RXcFEJfOwAnWmp-5qOvWC2zxstz01Cvdm9-cjmY9L5uE_6NkGLu6UFrIIQdl4yIPXTtRT/4j8/9yAUOL5CQSOShLkYJwcgFQ/h35/h001.nljswc3ht7qOhNm3BYgiTC-zLCTCjLgWgJo7orGCJww) its Claude Code agentic coding tool to Enterprise and Team plans, featuring new admin control for managing spend, policy settings, and more.

**MITâ€™s NANDA initiative** [**found**](https://link.mail.beehiiv.com/ss/c/u001.WqXVGszJN1JEIu4aat7tRTfkz03LYigYBpPYEKWNNVyIFDPoYLUng9nOcJv1Nw4idIE5e02bAkFSUYDUxo6rhKzajGlusQBs73Z6foyaX21Kwu1JV0KjPwYEBvu6PaQCC9qGQeBROJrAgS1znB2fv8ugBC8LvP0E4Cnvkx6sdahh55Hh6MrUmc3MQ8np45KcLVfPrCPIHFJgsH8nGfmfAicO11DO7T2Oxp49ipHGcofPHv7bzqRVgtHPqV9fuH9e2xdXK6kvVS7w8sErPXSUOLfS9yBZ7m545YlRN2gqCNa2i2HcN5oup3zXPEpkKIo-/4j8/9yAUOL5CQSOShLkYJwcgFQ/h36/h001.B_N28qTMrJLCQxl73MDOTwNcE-ATiXjvsAtQfzsA6xg) that just 5% of enterprise AI deployments are driving revenue, with learning gaps and flawed integrations holding back the tech.

**OpenAIâ€™s Sebastien Bubeck** [**claimed**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HTRaCVFtZzOPLBxcm7xNvWOZF580XtXZk8N75FxasrHtdLF9TzBTBVOfYFB1m-jYa9UI8TYNYpcg6eqCTKvs6MXAeoZeq0bQdmCH6ZXthJLL68NQAnX2rln1GzS6nH7su3Pz0W2Qcz5c5Uq7_7YaEWfflTOvvq97vJiQKKK6mYqkEfSj3j-jd6u-4iMPWvZi0lJOnOCwIeecQPtwMS3O6CSXfyWK4mbwvANszFVGhyTbIJ07GNe7Dqr01CdMSNraeUnLJDDDs7aMfAk2l0Y35S0/4j8/9yAUOL5CQSOShLkYJwcgFQ/h37/h001.7EwwS-iN61xwpvIOLe9SDL9gqFlBlWrbyDVWgzqiMrQ) that GPT-5-pro is able to â€˜prove new interesting mathematicsâ€™, using the model to complete an open complex problem.

**Google product lead Logan Kilpatrick** [**posted**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HXNIj7dy2Satt_aubbX02mAb_6Q4hyWTDGpnJ2_qYwusvQpwlH1E8Q6nQFMlLVxqp_Z9XzDMnuyfxjHbmn1zpZ38-BuxtcGcH8rGTgJhkSazEJkopEJprC4MVI8l_S00NNDETcNs0oGZQvUA1ZFxfjrKIcfUTFEO0oGmpBamMvnNMvvrl5rEtqIf92V_z_xNyKjHKHgbUxjOZr7dxD3KLwiDc6vQc7pv3qLn6R-z7FpryfDazWHQ7DLjxd9gm66c1QIIDPs8bRlmLFcFI4GqSfI/4j7/mvzhUm_SS-2cQhHoPyNQpA/h29/h001.kF-pGpMAOBbtKNmRkUOlViz5wC6D2YR8S6vGyP-C2R8) a banana emoji on X, hinting that the â€˜nano-bananaâ€™ photo editing model being tested on LM Arena is likely from Google.

**OpenAI** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HUEd5jZRKISZZdPpCQGFB3QIBxz3uSc1UtL0aRk6KOf2r12CDkiTjQBv5uS0ePo45vpekJeeOEIwvQGL1Sem5_sVUUEvPFjvsI8l1Dzio6f2GribhGwiHZgnnilhV-YdglGKRaHron15GQDIAC7VavL1t-YGjbDFlM-HWzRCrK8e5bgDE0VcbxzVZMcA0gOM-JEGwGCxHe72utVP7yhOQJktms4B5UhZccSq4U19ZEB6BMvR9cUisGEmpgkfPX1TvVeGaPu52kah94B2L9bFm6k/4j7/mvzhUm_SS-2cQhHoPyNQpA/h30/h001.UWQX-r-eRlBNI8PKnjlkS-Dwk0qD7I14bzxfcbu0APk) the release of ChatGPT Go, a cheaper subscription specifically for India, priced at less than $5 per month and able to be paid in local currency.

**ElevenLabs** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HYurLvtaB7u9WOPgFocTbdCirm1n3nsHuyNoDAbgpZWbP1jhYE35NInGYVOyn-VcRl4yOyjwGNcKzKJ-96kPMxkr_JTGAaQjpWkIkxNjD2a0-JAVNKKkwew_UXwgsyuUjN_6kJweFxWr4qNlY3iindOJXyIjW-n9JZ0j4wrP1ZP0vLYditCuV6nSNj6aSMKa1h62WU1hXZRaDbZNNDlCLy9rlx75-h1mJ3FCrCAhccd1WOltYj_ySBt2eBFWqJEN0rTC-hfVYl-KIIQAxXWaauQ/4j7/mvzhUm_SS-2cQhHoPyNQpA/h31/h001.__6D7Ul6fMSdybplyg_uxqXzq9PNyT68TmQSZKwkd9o) Chat Mode, allowing users to build text-only conversational agents on the platform in addition to voice-first systems.

**DeepSeek** [**launched**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf_MnrMNPlyZa0tC_fQ34TxQ78dU03jIigb7dPAeatjyNceCFT3Vt8H8pmixJrzdvdNQa0JyNRt16LZ8DzeBp3yvvfPoytGIhm-CtPVxzh4pPdWpJqBBji77pb2IlB5bNqGnTZG-DNlzHnrTimkh0xAyyTeuEjV6T5xwF2nJbcw8hrWOFhB4kZLoc8eH38Lo4qBEfxeN6_eMUZ9aML7Znnkpl2R6eidC3KB8vsJ1zW2ZGe2VNdBiOJPw67AdRB03ulyPOBl1tkYt_SVBDUTfVP5-o5JIOxFcxfXgWjNFAlYleZI8-4j4-bT9aPCOman_BGzKHgr3Dxb9MaQqLy-H4Xp8/4j7/mvzhUm_SS-2cQhHoPyNQpA/h32/h001.duAId5TWr6AyFG-KEVY-M3exQmpPx9sMtAbx0MGlo-c) its V3.1 model with a larger context window, while Chinese media pinned delays of the R2 release on CEO Liang Wenfengâ€™s â€œperfectionism.â€

**Eight Sleep** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.WqXVGszJN1JEIu4aat7tRTfkz03LYigYBpPYEKWNNVxtmtGVBVbbCuLEljs6VqLptcpc07Gol8hLZRBIWvXgRLTfGj0gbCnkuAikc1bVPByRFCydiBMEm5C7_bT92hUFqJroPTCImbkvwVCimZr_9FRjaMWMJEsAUSpFMbIfXJI2bHqZxp2uW2nHGyZ5PhYWl6L6tvv_obaJX6USK8s1pZMBm6JQC4tM5cIIqZZVp2XbrqsqOIFSlyooUJS6j8HEBk99Y4nfdUGFtcHmiK2lj-Sp2ro0lQruGdoxO2yxUBtV119ocvKeXvHaui-JWAZ5/4j7/mvzhUm_SS-2cQhHoPyNQpA/h33/h001.kb5miskwWCRFmxUJEGEbGpqT9Dbxa_OcUMktaJigVBE) a new $100M raise, with plans to develop the worldâ€™s first â€œSleep Agentâ€ for proactive recovery and sleep optimization.

**Runway** [**launched**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HRTeHaO0Mg6UDfNem9Xd2_dv4WHfGWEPqZ947NgfL25d3jsIhewftphbvR-kyKLSzrHLg-qBbzlSSjb9zhvRjYYpKU6yxSof2us0Kx4r7hpUfK7LvKIbQX6opWg1AKT0w1syKRp45Q8EQH51jbUn-CWKXs2QjDQjxXdNp16GidUKNs4VnDjZdj5FFl0ZbfyAl7YgKT6HoxHk8ffsq1_rTpDyh3B81q_d_mU146xfqehZ8ADke9wZNr5Wcn76aN2dwQ/4j7/mvzhUm_SS-2cQhHoPyNQpA/h34/h001.gC_9rIr6I1wU73rvivmb9w5wZs_CE_s7v0M2QTBi7KA) a series of updates to its platform, including the addition of third-party models and visual upgrades to its Chat Mode.

**LM Arena** [**debuted**](https://link.mail.beehiiv.com/ss/c/u001.DrYDwug-xrpEbNqFhzTCdU9eJeeZkRrXqwt5dtQirT7smxX6i2WIL8pxCyagZZVbpSoY59HS10WbgDCe9-s_MEvmrHrr8GSvA4YNJUAR4U0Fk582T4IAiuYRf0U3N-iW6f0XLodwCFs8zDZRGRGIWDQgmeekGlVgYMPyCNlHIWjTF4-SwJUanu77I7nB-odxBO_9gvciwt6P0Df1oF3SVjjuD6l0IpOsfV_ctR1LKWbPc_kSgN6QEvUWpIW_kjTTI-dcL3k9tva1yFIT9X-9NQ/4j7/mvzhUm_SS-2cQhHoPyNQpA/h35/h001.eriYtEdm5gUKioQWE0IOamlSgiC5PVum1Ram4qKoNKw) BiomedArena, a new evaluation track for testing and ranking the performance of LLMs on real-world biomedical research.

**ByteDance Seed** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.BKH0F2yLXfXXfZz4rVL6MEeIAfTbT5jieiSZqTY3kfFKT8giz0mc2h31H_wh4TePNxLgVdLeg0njBxDjuZPS4tyiJ7XRdivMuEHE3uNGwLpgvSmsKPAY-OIBLhzpFdxMZuNeUrjpPZTSKglyvbsP_pSfh2M6lPbKSWzzA1abY_cPnkWuDbadaUAvx51cGNYNLp84eZgySyxcVTRyU8vBDrtl-fO6cGFIMtE3MYtLVuMaMB4Y95cLNOmkkTZz4jqC/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h27/h001.PKyGpz9s7cOjwyCqCSz699dDBg4K-7qqvVKbsFvxOy4) M3-Agent, a multimodal agent with long-term memory, to process visual and audio inputs in real-time to update and build its worldview.

**Character AI CEO Karandeep Anand** [**said**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf7CBqrUbfjGnxITzC08W9aQMms9S9G38bQNOMlr5XqyAHCD-p317Vv8NT2I2md9b4g_VPNi63QUOCoPd5YJv7ux9XXGt7slW5W8kiC8pudbqg5NS_Ebf7_qgYf-LSYY4BwatxWLf13KAcN-034zgQpv6JiSM-6oo96jHxywJIBZaaRQKRVf1Kt7S88zLDS1CUkAqR_dFnHMuj1YH0lezepInbQRWGbn-HarVSGkT6Tdk6O6EM9RPR8u9AvHBCTuPjAY0K9mdD9KjEusIyAEy31Q/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h28/h001.o5cUmCwp2JxKmWumF-TOkWryCdpIVjba0VgMbqWbX_E) the average user spends 80 minutes/day on the app talking with chatbots, saying most people will have â€œAI friendsâ€ in the future.

**xAIâ€™s Grok website** is [**exposing**](https://link.mail.beehiiv.com/ss/c/u001.KT4rQsO6sHS_v2VASG2xukYrcBLmr-VWvDqpbYLTfcQRVsIacgYWogwGEIHf9u9Bwc3hpzDeiDZ7LvJaU2GPLRs-inFetHjHjl8zkjlr_RXcd4dnYy82gk4buHrkNPZB5ptpZ0Stbjj-qFeu3gsarRJjWBEq2Sg4Ibf87kRU6h_CuvhbhXaDPSanQksPSQb0CuWYRvES3L2QVIqJo0b5tkrU11xusZ87W6hCYn97Ui-MoGpjtZ7yJe055LVDw2o2-LXCJ8CiyCvM6FVqWyjNPEpUvqMiSOsKoR1jGTVFrk0AT7xreMVL8-oX8NvtxT3km27cWAyRfWOcgDnpHNNjR_FXxG4u7WAzJ2gOafLIe2A/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h29/h001.t-PssMIPEOOtYFI_Z0TASOX22m4SKbOQkbzB5PJo8k8) AI personasâ€™ system prompts, ranging from normal â€œhomework helperâ€ to â€œcrazy conspiracistâ€, with some containing explicit instructions.

**Nvidia** [**released**](https://link.mail.beehiiv.com/ss/c/u001.NN3LcSSNlElMIHwAnhfMXeVt8Kn0Dvoqj8_DHQdRfkRlF1ITamam7p2P0Dibibp_xf3fqKgQVFOtJK2lHM3xRZ2-R3eNtNbUTujb6R2bMOm7AFbG8Rjq00BiUe81e6NvYCkWtuUNq4PiUxFu_7QfTRFirG8-LoQTcrVrZMzN4sKqvh9-8-VKTGCzNhpTsNswPPMORZ4J-J2ShFGEnHhWmgojwveTKY8iJQUbd_BsEIEAa8GnoefFHn-0fvwMMovzmVgjjsCpkkmbQUrxKNpTeaZmAxtiySeR6wViua_coOmeGtsAWGAACRXMLpOL2PziXDrd463ozqAzXBpg02RG5Q/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h30/h001.RJ1qaoYYXn6bRKf9jVsKy0Q4KE7zy1uHIaQH1OJPGTo) Nemotron Nano 2, tiny reasoning models ranging from 9B to 12B parameters, achieving strong results compared to similarly-sized models at 6x speed.

**U.S. Attorney General Ken Paxton** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf3F0ifgSLmD7mZeptCoA1a9_5crheio4rQ01w2BwFdjbP4_fVO-NQO61DoORxjPDNWR1-LV9zUr-3Iskzh9md380fl0-EFz3zH63ocFCqatc6EsI2Ll5AWkpNLifr5eihQZZ7WX104Adj-2wAajUpnC3v3LM_jiQZoGVG20OjZnNxpda0KKGZGxkVInvwhTxoCLFRzd0VP5hOSi6MZlnXpuS9f5OufptcaVGAjCI2zdKXSfWc56fJoe0D9ucxNCTVazuqOhSH5FN-JuxgamThLk0UfWE2N5lvqCsCoqB_0wGJdW-I6S7rGWF2gD-hPV5b444EgtUI6gfDVDFXD2QhZdFSbSMsSFRdVCrvVAjajW1vrCJtFlKH4tXV1NxDrzxMg/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h31/h001.28LZSvqA9ndO9zbQwz5Km2hGBALEDXnI66VtuxdNIX0) a probe into AI tools, including Meta and Character AI, focused on â€œdeceptive trade practicesâ€ and misleading marketing.

**Meta** is [**set to launch**](https://link.mail.beehiiv.com/ss/c/u001.BKH0F2yLXfXXfZz4rVL6MAEZojRFxNhMpGFE0-WnCh_rTVLZQ-xZ0eT95bxVANNwU5FKnT4avfTvetWS7rRqtH7aGVgPS0mPXA2-yiEzHhHlTesRK6JTO6XpW1mbCAQhCw94olkhw1Pz3lMBDek8KaivpSF68RF-EVvnsFbMFC-eaZsn8TtKZA9p_IgzNAqeo-WzvbLSe5O_Nhse3jwg8bbUSq9dXtw6x5Rn5fnl7bsQJkERLxCRC0SNpYy8N8L2cb-FzOKAgLUdZirGOaivSeuB45kj4Vo_T4uhPFFLFkrCkbi6ARx4mMQ7Yxx78HyRPf3AFX9S5sNJbF9vHctSgMviGAuaK3L1kh01Y0lyjfp9-IvqfaAAbn20Y9YMQ5Hg/4j6/cUuEnU4RR-uy5h_Q3VAWDw/h32/h001.AcfbVUHY0OARzivZzf3PemPOoZ0rH2H6Uv2UNd0n_FA) â€œHypernovaâ€ next month, a new line of smart glasses with a display (a â€œprecursor to full-blown AR glasses), rumored to start at around $800.

**Meta** is reportedly [**planning**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf4_pVCvgnnuS9csEAS3pWhvPdmbJ0YbdL_ieLWC3aEBUWBUsJ-X-Lw0wSbeHlCKflYjXPaY0mC6F4-oHyuMgYZGx3sH-2U7Azw6LdqXfMDjeE15VVvnGm7mlBvdQQ2v4c0Uyw5sf4ZBX159FJrqpbpAHivgXZ4XvDidg2wdmS3vHy-CxFj_fR0Lcoxto-Rpu4yHSNAjh6AxSzEZ85qOJmcYKFdqTyohlkDlcG5infuqdNlST6tNQ2Dhfs5eOZUY3u168mF9ofvptCP9aQFWT215idkhYIlxMwPgi4pU1Z7LZ/4j5/WklCvTmeStGTon8Z52ztew/h29/h001.31XnFpjf0Ky6bOiLQ4MnB6yAExPnXdasLtldY1qB49E) another restructure of its AI divisions, marking the fourth in just six months, with the companyâ€™s MSL set to be divided into four teams.

**StepFun AI** [**released**](https://link.mail.beehiiv.com/ss/c/u001.siHJl2oxYc5G1hfeCOvt3WwGYyaHKhzErC3zhpKAaUk2zS1stbinT_KGrE-ltOvoLyh6FGiGCGMKFCEOvyuc6AQrKqvbvvCvCKsAAId4MhAX2laLby8TGnFwXmnobovcNvaf3vveD3nOcXB6vcfu44JTI34lEV3DU0b2d_lEeGutwwatFOzWzQOxpcO86u0OrJDDZKspDH7Q3puNtTfvvY3ygGBVrlclox80wkzv26AONvYR5M1AV9eY4W5qQNv4/4j5/WklCvTmeStGTon8Z52ztew/h30/h001.WRhLOKaFgA4QJdqdAbGlCys7RO_ePeLF-cKiI89wIaA) NextStep-1, a new open-source image generation model that achieves SOTA performance among autoregressive models.

**Meta FAIR** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.Q334NVcZU4O6L6VKRz8ijCejhhl4B0NSh6H10TKEqpS93xca2v0poLjefrFwYKBw3yn2ISkpS9RHJSbv8Z1_o6zhJV3aqDBBUeBFnOEsfbA05slMYtdQkrB1lUo2sFgfojw0TN585apymqbOXEgVQ475j4wOEprXoDeROPUaER6JXmWFycPAPZyrnPhpFdmMH6djYHXf74nGUJItbRAqQ2cNCGmpV6-3bnW_Mybdn90/4j5/WklCvTmeStGTon8Z52ztew/h31/h001.xJJJUVH4ME0QcjyYza3Zu-_7wc1w9chymbupYinIqrE) Dinov3, a new AI vision foundation model that achieves top performance with no labeled data needed.

**The U.S. government** [**rolled out**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf7Y5F38fJ8EViSLOEZEFR5vUGHnHYpeGMzm7WW5t8BGP0OpOGhGx9rUwPTu59hv4W_IXg0EqAoVs4rYihgXNVjRKeIZmvTlf6Rxa5ooYw_d4LmbgfqcFPtKBGooOQS-WROc3xhyKTQ17wUT1CYUDlhwQN68VSIibWGTgp_iQ215wIPg0C-gjaq49aIHN0YdZiMfGlKOtRuREjhR7S3gzVBp2Iiwxo7DywYCsnNDhqHmNxx7IY-kPBd5IWY6RLKCvqM_1RD6kppyNkZEAA4kdBlyN1FB015xN6kJtKA9i4ltXv50Zpy5n5XKvxw2GocJL1JHZfgN43s1RnfLdUtuoQdU/4j5/WklCvTmeStGTon8Z52ztew/h32/h001.PY3AGueB_2hM085-CEHQD2bo3IN4xsXUi4gG9oWMzuA) USAi, a platform for federal agencies to utilize AI tools like chatbots, coding models, and more in a secure environment.

**OpenAIâ€™s GPT-5** had the most [**success**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HYZETaJmoZ_hnX63HKe40-7w7pBvFdd7T5BCxEXvxhe1U4bjGsqT8s-LjLAKosjPpBmqC6208PBuZtkM1DnDhcsEDWjBd0QjJkLcDDf2ERVuynYEYiZ7rrUhNhWzmOzobC1w2QIuaUD_stk1zZ-gqjDyfR1bi4zj21aaici_eQ5p1w_rKS9kjVaudzo-M1sxnOXtM711eN8IF8Mjv0aYsc8eucLYKcPjTngkeCGH82gP/4j5/WklCvTmeStGTon8Z52ztew/h33/h001.rLveLrhBolLkCL-EdcBNl__7Sk8lzR18x1Mi5syqIi0) of any model yet in tests playing old PokÃ©mon Game Boy titles, beating PokÃ©mon Red in nearly a third of the steps as o3.

# ðŸ”¹ Everyoneâ€™s talking about AI. Is your brand part of the story?

AI is changing how businesses work, build, and grow across every industry. From new products to smart processes, itâ€™s on everyoneâ€™s radar.

But hereâ€™s the real question: How do you stand out when everyoneâ€™s shouting â€œAIâ€?

ðŸ‘‰ Thatâ€™s where GenAI comes in. We help top brands go from background noise to leading voices, through the largest AI-focused community in the world.

ðŸ’¼ 1M+ AI-curious founders, engineers, execs & researchers

ðŸŒ 30K downloads + views every month on trusted platforms

ðŸŽ¯ 71% of our audience are senior decision-makers (VP, C-suite, etc.)

We already work with top AI brands - from fast-growing startups to major players - to help them:

âœ… Lead the AI conversation

âœ… Get seen and trusted

âœ… Launch with buzz and credibility

âœ… Build long-term brand power in the AI space

This is the moment to bring your message in front of the right audience.

ðŸ“© Apply at [https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform](https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform?usp=header)

Your audience is already listening. Letâ€™s make sure they hear you

# ðŸ“šAce the Google Cloud Generative AI Leader Certification

This book discuss the Google Cloud Generative AI Leader certification, a first-of-its-kind credential designed for professionals who aim to strategically implement Generative AI within their organizations. The E-Book + audiobook is available at [https://play.google.com/store/books/details?id=bgZeEQAAQBAJ](https://play.google.com/store/books/details?id=bgZeEQAAQBAJ)

\#AI #AIUnraveled",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1myek6d/ai_weekly_rundown_aug_17_24_2025_nobel_laureate/,r_1myek6d,,,
r_1mydbuc,reddit,Solid_Woodpecker3635,2025-08-23T21:17:42+00:00,"I wrote a guide on Layered Reward Architecture (LRA) to fix the ""single-reward fallacy"" in production RLHF/RLVR.
I wanted to share a framework for making RLHF more robust, especially for complex systems that chain LLMs, RAG, and tools.

We all know a single scalar reward is brittle. It gets gamed, starves components (like the retriever), and is a nightmare to debug. I call this the ""single-reward fallacy.""

My post details theÂ **Layered Reward Architecture (LRA)**, which decomposes the reward into a vector of verifiable signals from specialized models and rules. The core idea is to fail fast and reward granularly.

The layers I propose are:

* **Structural:**Â Is the output format (JSON, code syntax) correct?
* **Task-Specific:**Â Does it pass unit tests or match a ground truth?
* **Semantic:**Â Is it factually grounded in the provided context?
* **Behavioral/Safety:**Â Does it pass safety filters?
* **Qualitative:**Â Is it helpful and well-written? (The final, expensive check)

In the guide, I cover the architecture, different methods for weighting the layers (including regressing against human labels), and provide code examples for Best-of-N reranking and PPO integration.

Would love to hear how you all are approaching this problem. Are you using multi-objective rewards? How are you handling credit assignment in chained systems?

**Full guide here:**[The Layered Reward Architecture (LRA): A Complete Guide to Multi-Layer, Multi-Model Reward Mechanisms | by Pavan Kunchala | Aug, 2025 | Medium](https://pavankunchalapk.medium.com/the-layered-reward-architecture-lra-a-complete-guide-to-multi-layer-multi-model-reward-631405e1c1af)

**TL;DR:**Â Single rewards in RLHF are broken for complex systems. I wrote a guide on using a multi-layered reward system (LRA) with different verifiers for syntax, facts, safety, etc., to make training more stable and debuggable.

*P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities*

*Portfolio:*Â [Pavan Kunchala - AI Engineer & Full-Stack Developer](https://pavan-portfolio-tawny.vercel.app/)*.*",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mydbuc/i_wrote_a_guide_on_layered_reward_architecture/,r_1mydbuc,,,
r_1mycqmw,reddit,Planhub-ca,2025-08-23T20:53:30+00:00,WhoFi research shows through wall person identification using home routers,deeplearning,16,https://www.reddit.com/r/deeplearning/comments/1mycqmw/whofi_research_shows_through_wall_person/,r_1mycqmw,,,
r_1mybvww,reddit,next_module,2025-08-23T20:18:53+00:00,"Are GPUs Becoming the New â€œFuelâ€ for AI in 2025?
With the rapid rise of AI models, GPUs have become the backbone of innovation. From training massive LLMs to running real-time inferencing, their demand is skyrocketing.

But this brings new challengesâ€”high costs, supply shortages, and the question of whether CPUs, TPUs, or even custom AI accelerators might soon balance the equation.

What do you think?
	â€¢	Will GPUs continue to dominate AI workloads in the next 3â€“5 years?
	â€¢	Or will alternative hardware start taking over?

Curious to hear the communityâ€™s perspective.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mybvww/are_gpus_becoming_the_new_fuel_for_ai_in_2025/,r_1mybvww,,,
r_1my6bfv,reddit,GreenRelative1113,2025-08-23T16:42:40+00:00,"AlphaZero style RL system for the board game Hnefatafl - Feedback is appreciated
Hereâ€™s a project Iâ€™ve been working on recently that Iâ€™d love some feedback on. Itâ€™s an AlphaZero-style system for the board game Hnefatafl. 

Code: [https://github.com/nicholasg1997/hnefatafl/tree/experimental](https://github.com/nicholasg1997/hnefatafl/tree/experimental) 

The foundation is based on ""Deep Learning and the Game of Go,"" but I had to make a number of adjustments to make it work for Hnefatafl. It uses self-play, MCTS, and neural networks to train. 

Right now, I am running everything on my MacBook Air, so compute is very limited, forcing me to use shallower searches and only a few games per generation, and even still, my computer is overheating. Not surprisingly, Iâ€™ve only experienced little success with these limitations, and Iâ€™m not sure if the lack of success is due to my compute limitations or a problem with my code. 

Iâ€™d love any feedback on my approaches, if I made any obvious mistakes, and just my code in general. 

For context, my background is in finance, but I have been teaching myself Python/ML on the side. This is my first big project and my first time posting my code, so Iâ€™d appreciate any feedback. 

Thanks!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1my6bfv/alphazero_style_rl_system_for_the_board_game/,r_1my6bfv,,,
r_1my2wad,reddit,Exact-Comb7908,2025-08-23T14:27:27+00:00,"Challenges with Data Labelling
Hi everyone,

Iâ€™m a student doing research on the data labeling options that teams and individuals use, and Iâ€™d love to hear about your experiences.

* Do you prefer to outsource your data labeling or keep it in-house? Does this decision depend on the nature of your data (e.g. privacy, required specialized annotations) or budget-concerns? 
* What software or labeling service do you currently use or have used in the past?
* What are the biggest challenges you face with the software or service (e.g., usability, cost, quality, integration, scalability)?

Iâ€™m especially interested in theÂ *practical pain points*Â that come up in real projects. Any thoughts or stories you can share would be super valuable!

Thanks in advance ðŸ™",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1my2wad/challenges_with_data_labelling/,r_1my2wad,,,
r_1my1yqr,reddit,Swayam7170,2025-08-23T13:49:12+00:00,"Question to all the people who are working in AI/ML/DL. Urgent help!!!
I want to ask a straightforward question to machine learning and AI engineers:Â do you actually use maths or not?

Iâ€™ve been following these MIT lectures:Â Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Iâ€™ve managed to get through 10 videos, but honestly, they keep getting harder and Iâ€™m starting to feel hopeless.

Some of my friends keep asking why Iâ€™m even bothering with math since there are already pre-built libraries so there's no really need. Now Iâ€™m second-guessing myself, am I wasting time, or is this actually the right path for someone serious about ML? I am so frustrated right now, I dont know if I am second guessing myself but I am seriously confused and this question is messing with my mind. I would appreciate any clear answer. Thanks!",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1my1yqr/question_to_all_the_people_who_are_working_in/,r_1my1yqr,,,
r_1my1yjb,reddit,Swayam7170,2025-08-23T13:48:59+00:00,"Question to all the people who are working in AI/ML/DL. Urgent help!!!
I want to ask a straightforward question to machine learning and AI engineers:Â do you actually use maths or not?

Iâ€™ve been following these MIT lectures:Â Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Iâ€™ve managed to get through 10 videos, but honestly, they keep getting harder and Iâ€™m starting to feel hopeless.

Some of my friends keep asking why Iâ€™m even bothering with math since there are already pre-built libraries so there's no really need. Now Iâ€™m second-guessing myself, am I wasting time, or is this actually the right path for someone serious about ML? I am so frustrated right now, I dont know if I am second guessing myself but I am seriously confused and this question is messing with my mind. I would appreciate any clear answer. Thanks!",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1my1yjb/question_to_all_the_people_who_are_working_in/,r_1my1yjb,,,
r_1my04c2,reddit,Initial_Taro_5441,2025-08-23T12:27:21+00:00,"Feedback on Research Pipeline for Brain Tumor Classification & Segmentation (Diploma Thesis)
Hi everyone,

Iâ€™m currently working on my diploma thesis in medical imaging (brain tumor detection and analysis), and I would really appreciate your feedback on my proposed pipeline. My goal is to create a full end-to-end workflow that could potentially be extended into a publication or even a PhD demo.

Hereâ€™s the outline of my approach:

1. Binary Classification (Tumor / No Tumor) â€“ Custom CNN, evaluated with accuracy and related metrics
2. Multi-class Classification â€“ Four classes (glioma, meningioma, pituitary, no tumor)
3. Tumor Segmentation â€“ U-Net / nnU-Net (working with NIfTI datasets)
4. Tumor Grading â€“ Preprocessing, followed by ML classifier or CNN-based approach
5. Explainable AI (XAI) â€“ Grad-CAM, SHAP, LIME to improve interpretability
6. Custom CNN from scratch â€“ Controlled design and performance comparisons
7. Final Goal â€“ A full pipeline with visualization, potentially integrating YOLOv7 for detection/demonstration

My questions:

* Do you think this pipeline is too broad for a single thesis, or is it reasonable in scope?
* From your experience, does this look solid enough for a potential publication (conference/journal) if results are good?
* Any suggestions for improvement or areas I should focus more on?

Thanks a lot for your time and insights!",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1my04c2/feedback_on_research_pipeline_for_brain_tumor/,r_1my04c2,,,
r_1mxxp3t,reddit,depr3ss3dmonkey,2025-08-23T10:16:00+00:00,"Details on mapping of DNN operations to hardware components?
So i am writing about fault simulation in deep learning models and my professor wants me to write a chapter about how different DNN operations are mapped to different hardware components. So that I can explain how fault in one hardware component can affect the whole function of the model. Can anyone guide me towards any documents or materials where this is explained? I keep finding different papers but they are all suggesting changes or new ways of doing things. I want to know the generic version to get some ideas.",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1mxxp3t/details_on_mapping_of_dnn_operations_to_hardware/,r_1mxxp3t,,,
r_1mxrsf9,reddit,Equivalent_Use_3762,2025-08-23T04:18:14+00:00,ðŸ“¸ New Dataset: MMP-2K â€” A Benchmark for Macro Photography Image Quality Assessment (IQA),deeplearning,5,https://www.reddit.com/r/deeplearning/comments/1mxrsf9/new_dataset_mmp2k_a_benchmark_for_macro/,r_1mxrsf9,,,
r_1mxl35o,reddit,enoumen,2025-08-22T22:53:45+00:00,"AI Daily Rundown Aug 22 2025: ðŸ’§Google analyzes Geminiâ€™s environmental footprint ðŸ‘€Musk asked Zuckerberg to join $97B OpenAI takeover; Nvidia halts production of H20 AI chips for China; Metaâ€™s massive AI restructure; Google analyzes Geminiâ€™s environmental footprint; Musk: Grok 5 has a shot at AGI
# A daily Chronicle of AI Innovations August 22nd 2025:

**Listen at** [**https://podcasts.apple.com/us/podcast/ai-daily-rundown-aug-22-2025-google-analyzes-geminis/id1684415169?i=1000723151588**](https://podcasts.apple.com/us/podcast/ai-daily-rundown-aug-22-2025-google-analyzes-geminis/id1684415169?i=1000723151588)

Hello AI Unraveled Listeners,

**In today's AI News,**

**ðŸ‘€ Musk asked Zuckerberg to join $97B OpenAI takeover**

**ðŸ›‘ Nvidia halts production of H20 AI chips for China**

**ðŸ”„ Bank rehires workers replaced by AI after ""lying"" about chatbot succe**

**ðŸ”€Metaâ€™s massive AI restructure**

ðŸ›ï¸ Google launches Gemini for government at 47 cents

**ðŸ’§Google analyzes Geminiâ€™s environmental footprint**

**ðŸ—£ï¸Musk: Grok 5 has â€˜a shot at being true AGIâ€™**

ðŸ’¡ **Your Gemini prompts likely consume less energy than you thinkâ€”Google transparency raises questions**

ðŸš€ **China deploys AI chatbot to space station, naming it after the mythical Monkey King**

ðŸ‡¨ðŸ‡³ **DeepSeek quietly rolls out V3.1 optimized for Chinese chips and priced below OpenAI**



https://preview.redd.it/ihnhgdt4hnkf1.png?width=3000&format=png&auto=webp&s=aaafecc3258c9c8f701062ea477c604b33fbb589

# ðŸ‘€ Musk asked Zuckerberg to join $97B OpenAI takeover

* Elon Musk asked Meta CEO Mark Zuckerberg for help financing an unsolicited $97.4 billion offer to purchase OpenAI, according to a court filing from the AI company.
* The document reveals neither the chief executive nor his firm signed a letter of intent, ultimately declining to join the bid to purchase the ChatGPT maker.
* OpenAI now argues this secret request to a main rival weakens Musk's legal claims that its Microsoft partnership violated the organizationâ€™s original charitable mission.

# ðŸ›‘ Nvidia halts production of H20 AI chips for China

* Nvidia directed suppliers Amkor Technology and Samsung Electronics to pause manufacturing of its H20 chips for China, following a government order for local tech companies to halt purchases.
* This directive comes as China's Cyberspace Administration reviews the H20 chips for security risks, specifically concerns that they might contain ""backdoors"" or tracking technology for remote operation.
* The move casts doubt on the chip's future in China, even after Nvidia CEO Jensen Huang worked to secure US export licenses and assured Beijing the hardware has no ""backdoors.""

# ðŸ”„ Bank rehires workers replaced by AI after ""lying"" about chatbot success

* The Commonwealth Bank of Australia fired 45 workers, claiming its new AI chatbot had reduced call volumes by 2,000 a week, a statement employees called ""an outright lie.""
* In reality, call volumes were increasing at the time, forcing the bank to offer staff overtime and even have management help answer the phones just to keep up with demand.
* After being brought to a fair work tribunal, the bank admitted the roles were not redundant, apologized, and offered to rehire the workers or provide them with exit payments.

# ðŸ›ï¸ Google launches Gemini for government at 47 cents

* The General Services Administration announced that federal agencies can now access Google's suite of artificial intelligence services, called Gemini for Government, for only 47 cents each through 2026.
* The GSA previously added Googleâ€™s Gemini, OpenAIâ€™s ChatGPT, and Anthropicâ€™s Claude to its purchasing system, following moves by competitors to offer their AI products to the government for $1.
* Building on a past discount for its Workspace tools, Googleâ€™s new offer gives federal employees access to tools like NotebookLM and Veo, which are powered by its latest models.

# ðŸ”€Metaâ€™s massive AI restructure

Meta is [**undergoing**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf07ka7Wy6WfSudqO4AAVBcvJIqHDyhaCbqvU32RWIKg5MKhOBCp9isicTcH-O8gYB4ZTr1ERA-akmnPem5UjTx79cTP_vtkGrFvHXUuQY9d0ehE9Nfd-xV9PiVMT9YCYujtcNloioaBFfaKkW3VhnCdAcYjELNAG0525x5qAkhV9Yzzhg3x6JQRQAQya-LwDiwLenV6bM4pa_MtLPu3fQPuglctWZZXK14KuVKFPj_8wzyTZJQZ2CRuCMse-8w8GiiViVzNdEONdkv0l3ndM8ZB3AgvCZS1QjHlv_zvXWcp3DGTGdKXRg4FvGA32o389GQ/4j9/dbIvxH2IRqaMVuBrj9rfnA/h7/h001.5f3uw77-MTzHthxHv4LheKzkeEreqpVtBJ5fnZJGQh0) a massive restructure of its AI teams, dissolving its AGI Foundations division and reorganizing operations into four units under Alexandr Wang â€” with the company also [**imposing**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf08MjguofPcw5b1fa54PbEbuGRcLaI1UoMxxrJ0JIm8Jjsdvlv7d1zg6FMsGVBG7udxwbdabSf6An60MqbpXE6oaqbPK-FOxahlcpDBQmq4j7-3qU449iD6xs1W2h3dmFh8pR_OfROQi8IAUIAuEGYzOvf_IXZ4fsUqkcyQs1s85HFZfFqckPg85nHH0yGEbp2_ZhlkDVDZKcubeWZdyknRmMfnxr_x8c_8WEUNU3XCM3Ji9ujkM17DfLqMWRBqiv80j0oh6PbWP-hPJKXWzXJCG8nSbq4s-rEwGKSpgfBQgFsPAwC4nsQXvYrWPdG3zu2S4n87bRqwCB2LyFipQFC4qy9L698zGjuD7z_gML5VEOPD-nFCa97SkQ3jCWxGCo2Mbgi74CTDasO6bRbRLcxM1yKBEd0vzahQsZ9fK8flB2e4COG9eFns7jUxYDfERguObDusotjSmviULr7LGjaGnSU4o9kVox6DBhYcZ9XqaYHFlKKLHw3ufXwglDVDEeqpg48YzdYd6jpnV8Dz-P08/4j9/dbIvxH2IRqaMVuBrj9rfnA/h8/h001.oRK2_9rf4kZ6SLe-GXKlY_GsuGHXTIZwNv0nbfNgIj8) a hiring freeze after a major poaching spree.

**The details:**

* Wang sent a [**memo**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf07ka7Wy6WfSudqO4AAVBcvJIqHDyhaCbqvU32RWIKg5MKhOBCp9isicTcH-O8gYB4ZTr1ERA-akmnPem5UjTx79cTP_vtkGrFvHXUuQY9d0ehE9Nfd-xV9PiVMT9YCYujtcNloioaBFfaKkW3VhnCdAcYjELNAG0525x5qAkhV9Yzzhg3x6JQRQAQya-LwDiwLenV6bM4pa_MtLPu3fQPuglctWZZXK14KuVKFPj_8w6GhQPoNni7wNC4Nr-lYPSOIo2yKspdFHcK-rqvtHZPmpkL0yyoeHr79yxmTUtJIHztO4qdpft2Wt-JIo7WooKQ/4j9/dbIvxH2IRqaMVuBrj9rfnA/h9/h001.BKIZsHCyxgUxn9tRD0BAR2k8yrTvAbrO6UZxiaMemXs) to employees outlining new teams for research, training, products, and infrastructure, with most division heads reporting directly to him.
* The company [**froze**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf08MjguofPcw5b1fa54PbEbuGRcLaI1UoMxxrJ0JIm8Jjsdvlv7d1zg6FMsGVBG7udxwbdabSf6An60MqbpXE6oaqbPK-FOxahlcpDBQmq4j7-3qU449iD6xs1W2h3dmFh8pR_OfROQi8IAUIAuEGYzOvf_IXZ4fsUqkcyQs1s85HFZfFqckPg85nHH0yGEbp2_ZhlkDVDZKcubeWZdyknRmMfnxr_x8c_8WEUNU3XCM3Ji9ujkM17DfLqMWRBqiv80j0oh6PbWP-hPJKXWzXJCG8nSbq4s-rEwGKSpgfBQgFsPAwC4nsQXvYrWPdG3zu2S4n87bRqwCB2LyFipQFC4qy9L698zGjuD7z_gML5VEOPD-nFCa97SkQ3jCWxGCo2Mbgi74CTDasO6bRbRLcxM1yKBEd0vzahQsZ9fK8flB2e4COG9eFns7jUxYDfERguObDusotjSmviULr7LGjaHZ16On3VEJMbIJSQWbSSi-00IeNcQbzklnmS7TRJ9Kum7dIVWBySVa3a_ELgNPx1I/4j9/dbIvxH2IRqaMVuBrj9rfnA/h10/h001.hBBYgcNx8ov8f-OG62rtARbJrwm6OWI_X36n4WK3Hbw) hiring across its AI division last week, now requiring Wangâ€™s personal approval for any exceptions to the mandate.
* The AGI Foundations team is being scattered across departments, with Meta also creating a â€˜TBD Labâ€™ to explore â€œomniâ€ models and frontier AI research.
* Wang revealed that Chief Scientist Yann LeCun will now report to him as well, describing FAIR as the â€œinnovation engine for MSLâ€ in the new structure.

**Why it matters:** Metaâ€™s summer of hiring looks to be officially over, with the focus now turning to building a new internal structure under the direction of Alexandr Wang. Itâ€™s clear that the high-profile new team wants to move fast â€” what isnâ€™t clear is how the changes will sit with the broader AI and FAIR teams that now feel lost in the shuffle.

# ðŸ’§Google analyzes Geminiâ€™s environmental footprint

Google [**released**](https://link.mail.beehiiv.com/ss/c/u001.a3gBHu6_kDRL6l3yEfNWAZ5dCGjHtcf5V-F89xIRczvE8ALeCfbTpNqxYcTlEp4faz-pltjYnNoB9GDPkW2AT_9p638V8q7XgpY_GgPmBDsH8YZksAHaak9RtbbO1uWOFGiT9024D6ivUSsY9oHSHfykAUKsi0ChSw35OZl6i0U4GXZMEVMp703A3rtWX2kS2ob13OyFqJhXWlnq4eajVUkG8mvilgBtRwEWeFUvkap5n80_HlRPJkoPZTdZ7L2vWdhCLPh_M08dIpss3by9ept2gIYqUWJNYmR_-Ws02xLbUvXEsJHCsLbEp9MBqXNrWEg8qDTgpVQhHdVlOsf-Cw/4j9/dbIvxH2IRqaMVuBrj9rfnA/h16/h001.10Sj9iadRwWLF1pNGGDG2d33xZW0E5wP1yAOM0aM4PU) a new blog detailing the environmental footprint of its Gemini chatbot, claiming the model consumes the equivalent of five drops of water per query â€” though researchers [**argue**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf0JUH4oQtuGu0vxFsdak8G_DS4dEbCz09rhCqyQvKVkU0xECJ2BkcvY3WNz58rdJ9BiVVa5zlDAJPg-ZHyd_3CZY6hVt8QAnhHyS4xMiIDmhb0ku5Y9RIeT1P1iLWidHkiTAD7Rd8bfgHgK7VrShkLpfYTkDw0395MX04WidRojgXmH-VQxSyb8KsUiXv2vf3wFh5MoLUnsQWd7gclO9eFDhHITH7Rkt0IsLomPL_oamcKtPfgPmdIcI7Hy8byDbEFehjNATtmelEWvx2RQyn9lzLoIAmjnEyzKIiKds8ung/4j9/dbIvxH2IRqaMVuBrj9rfnA/h17/h001.npTNI8CZjwe7fyS0dvixBQyZM5pUBX4dHPMp7om0WvE) it left out most of the actual water usage.

**The details:**

* The published findings claim each Gemini text request uses energy equal to watching TV for nine seconds and creates minimal carbon emissions.
* Google said Gemini became 33x more energy efficient and cut carbon output by 44x over the past year, all while the models became more capable.
* The [**paper**](https://link.mail.beehiiv.com/ss/c/u001.7FjCl1Hhb45GEizGv1NNbDvW_FKrSUzkXl0RNIHjnmoSso7u9jS7LYAb7Ycm7zGrH6es3Bbmq5f72IdaSP_1D-vO0T0nqo5SRtN_h1lUf17vqRNitBMqtN0ipao5SnOG4J7vEaGF8hUmOlYveQQtbegk1eylWqf-L3RJDWDFfRFjK5mDeZrBQRmAcD6oW8w7Ee_Q387prg-0_Mtz8qO8mwoBHRhpAUB9YnAwkiR7zaivdnu4ix9j8ecHAA-K1LwL-BP5MkcYACgLdY81Ds9MpvGCRyD_RvvV5WcA83BK7T6BjklxDVMah_apRJ8UVFKL7I-xG2kj1QA_EuL7VVWUCZrMUP4zv9GplnW4I1Ll9As/4j9/dbIvxH2IRqaMVuBrj9rfnA/h18/h001.eSNzY8-YuHss3rLY6r3UQb8OBE4OFQi8cAiJUgAgNE0) found that A Gemini query consumes 0.24 Wh of energy, slightly lower than the 0.34 Wh average that Sam Altman [**revealed**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoNQVYkthMI74yfjvOb9ZFs6GnEkURlsfFzB2NlEqlrgBcG6FkKXzuyazCuH44vgF6smSGP5Ib6SPoj-FFnzaoZXn7pQwJNXOvSf5RZh6KxHKg3xsDolZ-e1Mkv7AFT6TZZwtGEDkw0K0LUTv2b7WiltI1PwCMTMVcSeEAx6Ec3OrjmGWJvKLAwq3GuAglUFur_92kMQzZtv3WvXWe7Y9Pu2X9jXAcXeElpkWeU-Mrk5aAALSPFMBXRLgEj8Qjhziww/4j9/dbIvxH2IRqaMVuBrj9rfnA/h19/h001.2LnpUlXzNlnnMxL0CuW__UwN1_s9uoe_tbR9241RiKs) for ChatGPT.
* Researchers criticized the study for ignoring water consumed by power plants that generate power for data centers, which represents the majority of usage.

**Why it matters:** While Googleâ€™s efforts to provide more transparency around AIâ€™s environmental impact (a key issue for AI detractors) are positive, not everyone agrees with the companyâ€™s process, which may be painting an artificially rosy outlook. An industry-wide third-party standard may be needed to truly understand the full picture.

# ðŸ—£ï¸Musk: Grok 5 has â€˜a shot at being true AGIâ€™

https://preview.redd.it/ey8boydmhnkf1.png?width=1456&format=png&auto=webp&s=009a3151a4d6ceee85d74b6aca9c0eb86f570db7

Elon Musk had a [**busy day**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmM3i-hjZ19_yH_bb7qxC21-wWkBCPst36YN6aN501C8QTrFZvRBkUIPjN_IXzxA5Q7OLbFsmhIlA7CAdlBdkN2muAKbMbUeb2s5LJCss7E9KBwz834jIROVj-nJTYY6VCVrnc4Pp7G0seK9NudVRbAkOIZt1KJRYnNDz0-8ojtibo7gzgy87fztaHXkWfKzsPk6bnKWEueQpFY1pHXBHl2G3Dig_IN2OIjExpTSX-aiJw/4j9/dbIvxH2IRqaMVuBrj9rfnA/h25/h001.dntKdRRgrcxv1w1NoVwpaeY9xWmv3bed7THFcR1mW9s) of AI commentary on X, revealing new information about Grok 5, making bold claims about xAIâ€™s â€˜Imagineâ€™ generator, and speaking on AI and declining birthrates in a series of posts and replies on the platform.

**The details:**

* Musk [**posted**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmN_2hGFwUAgwjbz0lo2MeGHLjeygmQAmklzVdHqwXJLbE_XWnFMhA_a3-HHrUI-H7zp2k625rNxrzD1AQtHHtmHufMiWCa1eOyxVCmu4XPlYMFcTRmzuCmgGdSCv3FjYENoGYtqDuv7sRDVNy53-SUVyyHfptae_S7d3geGhCPTZqIyWg2vrf4WFEIF4NfCMRFYCRy4U4gb8_62PH29taeKwz6Xu3Ca3kxMUJTkBWySig/4j9/dbIvxH2IRqaMVuBrj9rfnA/h26/h001.oHHgVxwc6csg5b5Faawv91LJ4PK0BN3HgdfGnLu2Rr0) that xAIâ€™s Grok 5 model will begin training in September, saying he [**believes**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmM3i-hjZ19_yH_bb7qxC21-wWkBCPst36YN6aN501C8QTrFZvRBkUIPjN_IXzxA5Q7OLbFsmhIlA7CAdlBdkN2muAKbMbUeb2s5LJCss7E9KBwz834jIROVj-nJTYY6VCVrnc4Pp7G0seK9NudVRbAk9emFKrnzU6r_wXoYKeDU-eCMYocxcL6Fm5jlXz56tivObWeEk4OUhhThez3DeMoXGEnCsgkSqmkc_YslK-iiPg/4j9/dbIvxH2IRqaMVuBrj9rfnA/h27/h001.tRAXfGS3joa1eHhlAr--GUmkdI2Iir-QtzzNaDwpOB8) the model â€œhas a shot at being true AGIâ€.
* He [**also**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmMta3Y4oMJ4e_JyU4LepqVawqu9-_Y7GrSzhpCaXIDIPy4ZbTsnARWnAHxCM-j-Sacj6R22V7-cdvPIT98x_Ku2UnRvvu-4Rdg7y7fax-hgZJVRs1zqE-1fxiCgsMe7LojQJJZyTOKCBnerAMys7rHPBAVETXP1f8X2uMBV0FYe6JLMdcf9r7Sq9X5N9sTAmc76oWDeHFjR44cCzniFte6e7s4DVq0Cip-fpmKz6wBFFA/4j9/dbIvxH2IRqaMVuBrj9rfnA/h28/h001.RfZq1S7MW-F4hnRkZ25JsE4STTzRIk4Rnn6w7sMkFxQ) said Grok Imagine will be better than Googleâ€™s VEO 3 video generation model â€œin every respect, with no exceptionsâ€.
* Musk also commented on the declining birthrate, [**saying**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HeRS2C90t4q6i6SLQ6cZHmPZb8qElbZz1qRHsQlnIL9fuzPIDU3NqJelOfcA77ykejtQsAGbNxN1rfJ5ApcJbXWj_Onscp3T36y1SR6AJKoMbY8KnX7q0QaKZDjGR1_q9AFXmL7cSd6SHFUqrS0_52IViZVUmHl8uvYvK4_W7AA4p_fKf4TWGwKDc2lIGISVQ3lTuA9_I2hmtN8lJlR_DDYUtgEDaZKpMf_fzR695dnxhVkDdl_ikwon4b9m7mDFBg/4j9/dbIvxH2IRqaMVuBrj9rfnA/h29/h001.SnPSiR3g0zzBQwdwZDg-XRUspGsKUWYUf506KofvOB4) AI will actually increase birth rates and will be â€œprogrammed that wayâ€.

**Why it matters:** AGI is a benchmark without a very clear definition, which will make the first official declaration of it all the more interesting. With OpenAI being the other major lab dancing around the notion of its models officially reaching the bar soon, the term could end up being the topic of the next inevitable feud between Altman and Musk.



# ðŸ’¡ Your Gemini prompts likely consume less energy than you thinkâ€”Google transparency raises questions

Google claims its Gemini AI uses just **0.24 Wh of electricity** and **0.26 mL of water** per text promptâ€”energy equivalent to watching TV for nine seconds and a few â€œdropsâ€ of water. Despite impressive efficiency gains, critics argue Googleâ€™s estimates are misleading, citing omissions like indirect water usage, location-based emissions, and the rebound effect of overall increased AI utilization.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/2](https://www.theverge.com/report/763080/google-ai-gemini-water-energy-emissions-study)2\]

# ðŸš€ China deploys AI chatbot to space station, naming it after the mythical Monkey King

China's Tiangong space station is now home to **Wukong AI**, a chatbot named after the legendary Monkey King. Built from domestic open-source technology, Wukong assists taikonauts with navigation, tactical planning, and psychological supportâ€”operating through both onboard and Earth-based modules during critical missions.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/2](https://www.wired.com/story/meet-wukong-chinas-first-ai-chatbot-on-a-space-station)2\]

# ðŸ‡¨ðŸ‡³ DeepSeek quietly rolls out V3.1 optimized for Chinese chips and priced below OpenAI

DeepSeek has released its **V3.1 model**, engineered for Chinese-made chips and designed to outperform its predecessors while undercutting OpenAIâ€™s pricing. The stealth launch signals deepening AI-chip alignment in China and positions V3.1 as a serious GPT-5 rival in domestic markets.

\[[Listen](https://podcasts.apple.com/podcast/ai-unraveled/id1684415169)\] \[[2025/08/2](https://www.reuters.com/world/china/chinese-ai-startup-deepseek-releases-upgraded-model-with-domestic-chip-support-2025-08-21/)2\]

# What Else Happened in AI on August 22nd 2025?

**Google** is [**expanding**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_Yk7rBUsevSC20rByzOrFZx-a3qfsElU5Gia8coWBsp0Zyiq8-4MhvIj2wgc2TOVa-9SInONUYO17A6QynMEj8qDbGEnJEqB2a0WA6bYj1Ww1cFoLOWfBfMR8gZkcoMFbC3RD7FmO55E48Kbn7-7gp05Md_aTd9gg03E0askbzWLrduL2T3tfwW__fJUfdAtEfYhUONPPQdmcVMSSiSGVPD/4j9/dbIvxH2IRqaMVuBrj9rfnA/h36/h001.qwqimq3bibmrCgrXjffRiTeG9omxBAAyRVquMb6UzmA) access to its AI Mode for conversational search, making it globally available, alongside new agentic abilities for handling restaurant reservations.

**Cohere** [**released**](https://link.mail.beehiiv.com/ss/c/u001.a3gBHu6_kDRL6l3yEfNWASqcNdYH4TRV7F5Ydr_PA83OKvWs6sdTF3IL_n1lEEiDrV6JesjKVrAIyunrvlmk8RpR9L2m1TXmrTp0VXU02SnkrGU7AVg83-mrRpeOzEOQz6ICFXRmAD5DHQxuPVlPimrCDjXPey5xNE3J7TB1IalPdq16wzy-M5GB76ck6P4eLNRKesu4Q4cEjcRvLQT5pPJjX-Zu65q1t4qO_qaZ2lHqliw6otBytxYOrgGuN1jq/4j9/dbIvxH2IRqaMVuBrj9rfnA/h37/h001.Y7QbszupJEZ9rnhpuMyZwie7QDoqEm3mWdT7U-BXxl4) Command A Reasoning, a new enterprise reasoning model that outperforms similar rivals like gpt-oss and DeepSeek R1 on agentic benchmarks.

**Runway** [**introduced**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HRTeHaO0Mg6UDfNem9Xd2_dVDtwoOTGBfQmRZbYDXU5gIF9OamD2FhbHHq8FSJWLzSuzUiPp3jOpBlsWIYIWIe8xQJGfK3T4q6qQQBM_74NlGzs3ZmJRgeU1OnSoHH1Y1rXRXz4W6aT7K45TbrlDS0xwVTc9Oxq3kJhntY1X5g_GA7qGnOXixk1flVzANWEarnfsH97RLOkv5-qLXIMtmYL33pEyYiK7OFJewvVqsvjN5Cs9cEviAn96fmYYF98AFQ/4j9/dbIvxH2IRqaMVuBrj9rfnA/h38/h001.fbVFz9o-R4bD3HpCIdsPTRMrnWIapt_IISV3dW5cUJs) Game Worlds in beta, a new tool to build, explore, and play text-based games generated in real-time on the platform.

**ByteDance** [**released**](https://link.mail.beehiiv.com/ss/c/u001.ZY5Y0CT8KZaZ1y9TVLsmf1KCbTdZquVLz_b3u_SxS4A0KOFyffBystrodjYB6RvNZKGnrnTDluULKDRSqG6uVkW87QLzq35JN6n3k7kb0eZjj5ZnrhHx7wW9HZVA1vXADcxLZxqWwNuYFdLhj5MWGp3Qw7HDi8Zw_OT335v9U40_yyWLLNnXWFvW559DYqIJ98Y36TBybkqHc1Pk6j0sLRwaYcB0Fcy7X-Y0INd6FzULi1WqTIuyk0Fai0dqmr0ohH0Dvk6JwpLly4BbzBxaAw/4j9/dbIvxH2IRqaMVuBrj9rfnA/h39/h001.itRDCQJdab0652YE-PNGz4mQS8b594RzPo6dOesRjTM) Seed-OSS, a new family of open-source reasoning models with long-context (500k+ tokens) capabilities and strong performance on benchmarks.

**Google and the U.S. General Services Administration** [**announced**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf7Y5F38fJ8EViSLOEZEFR5vUGHnHYpeGMzm7WW5t8BGP208zVhEVYaseymbm9BPF4ctK7JX3kvG_ZpF6pc7J3XEbJQIuaX-cR-a8P2Kfw1WqCIXbFBUElEDuzjUJp46DUebWWCvpU1Y_tGsQ1nvZB-Gbs4_ATUEx26m2VGrluHBN1E2gwUH0YkJHgLcopMNgTg0rqnWd-k2vNCs_selIdXVyPPF8ZKLc_wSgmBgwT7f1X_WdVnYkveKRsMlD2SmZgqyu6mkdy3ljIVoAPBJHJkKq7ANvNeNy43gaB-TIPBCeoK2NCq9EF3H1YijQIsYFBA/4j9/dbIvxH2IRqaMVuBrj9rfnA/h40/h001.t9-7SHVGNawFbGfF6n4gHSwKkR_3syeuwXIOPho5kN0) a new agreement to offer Gemini to the government at just $0.50c per agency to push federal adoption.

**Chinese firms** are [**moving away**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf5Il0_0RcQ6FykIrh_OLs40wsxmb2pbu9J8EdAaLQ8PT8WDxWoFyfJRySv5efk4Uyq7EUGqNEBOscmHXmwBl0c1-A85lk-Elvey-Oe8J2Kea9GJAdDYaanvqOM3TPzzGIr1Rw3oqR206RZdc4q4V3f5utU-wqrJvpUbhn8huKwaBwnHBsO-O1ClTl6eG8AXRL8zg7KLS7I5VBploLB_7FbKxSrAeND0-OedeFQM07hbCTR7YrPCU4dD216AKsPdkFdqSP_jVm2a8jdZ1WM-sbtojwGFqHFQ4noUesDRy6ngxk11L_-B6RpTVE3nrg4ZS5Ix_CQ_My-wdjL5MJU6gOLWfKLpAsKP_4-VPTg-GDVzl/4j9/dbIvxH2IRqaMVuBrj9rfnA/h41/h001.pMi8o1YxQ285t9IllnhQ6xmwrkHVxeigpY5HPCXAlFo) from Nvidiaâ€™s H20 and seeking domestic options after being insulted by comments from U.S. Commerce Secretary Howard Lutnick.

# ðŸ”¹ Everyoneâ€™s talking about AI. Is your brand part of the story?

AI is changing how businesses work, build, and grow across every industry. From new products to smart processes, itâ€™s on everyoneâ€™s radar.

But hereâ€™s the real question: How do you stand out when everyoneâ€™s shouting â€œAIâ€?

ðŸ‘‰ Thatâ€™s where GenAI comes in. We help top brands go from background noise to leading voices, through the largest AI-focused community in the world.

ðŸ’¼ 1M+ AI-curious founders, engineers, execs & researchers

ðŸŒ 30K downloads + views every month on trusted platforms

ðŸŽ¯ 71% of our audience are senior decision-makers (VP, C-suite, etc.)

We already work with top AI brands - from fast-growing startups to major players - to help them:

âœ… Lead the AI conversation

âœ… Get seen and trusted

âœ… Launch with buzz and credibility

âœ… Build long-term brand power in the AI space

This is the moment to bring your message in front of the right audience.

ðŸ“© Apply at [https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform](https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform?usp=header)

Your audience is already listening. Letâ€™s make sure they hear you

# ðŸ“šAce the Google Cloud Generative AI Leader Certification

This book discuss the Google Cloud Generative AI Leader certification, a first-of-its-kind credential designed for professionals who aim to strategically implement Generative AI within their organizations. The E-Book + audiobook is available at [https://play.google.com/store/books/details?id=bgZeEQAAQBAJ](https://play.google.com/store/books/details?id=bgZeEQAAQBAJ)

\#AI #AIUnraveled",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mxl35o/ai_daily_rundown_aug_22_2025_google_analyzes/,r_1mxl35o,,,
r_1mxcbn1,reddit,Humble_Preference_89,2025-08-22T17:10:38+00:00,"LeNet-5 CNN Tutorial: Learn, Build & Train Your CNN with Azure ML
Hi everyone,  
I recently put together a quickÂ **theory + hands-on tutorial**Â onÂ *LeNet-5*, one of the classic CNN architectures. The goal was to make it beginner-friendly â€” enough theory to understand the model, plus an implementation in Azure ML to actually see it in action.

If youâ€™re just getting started with CNNs and want a resource to help you get moving, this might be useful.

Iâ€™d love to hear your thoughts if you give it a watch â€” feedback is super welcome!",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mxcbn1/lenet5_cnn_tutorial_learn_build_train_your_cnn/,r_1mxcbn1,,,
r_1mx4qes,reddit,Neat_Chapter_9055,2025-08-22T12:09:32+00:00,"my go-to ai workflow for shorts: script â†’ tts â†’ image â†’ domoai
start with a 2â€“3 line script. use tts for audio. make a single frame inÂ [mage](https://www.mage.ai/)Â orÂ [leonardo](https://leonardo.ai/). animate it inÂ [domo](https://www.domoai.app/). add subtitles and music in capcut. done. you donâ€™t need a whole video pipeline. this gets you storytelling in under an hour. works great for love confessions, anime monologues, and fantasy intros.",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mx4qes/my_goto_ai_workflow_for_shorts_script_tts_image/,r_1mx4qes,,,
r_1mx3pn0,reddit,Neurosymbolic,2025-08-22T11:18:24+00:00,Synthetic Data for LLM Fine-tuning with ACT-R (Interview with Alessandro...,deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1mx3pn0/synthetic_data_for_llm_finetuning_with_actr/,r_1mx3pn0,,,
r_1mx1q12,reddit,clapped_indian,2025-08-22T09:22:37+00:00,"Pretrained Student Model in Knowledge Distillation
In papers such as CLIP-KD, they use a pretrained teacher and via knowledge distillation, train a student from scratch. Would it not be easier and more time efficient, if the student was pretrained on the same dataset as the teacher?

For example, if I have a CLIP-VIT-B-32 as a student and CLIP-VIT-L-14 as a teacher both pretrained on LAION-2B dataset. Teacher has some accuracy and student has some accuracy slightly less than the teacher. In this case, why can't we just directly distill knowledge from this teacher to student to squeeze out some more performance from the student rather than training the student from scratch?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mx1q12/pretrained_student_model_in_knowledge_distillation/,r_1mx1q12,,,
r_1mx0ui7,reddit,Happy_Pie4091,2025-08-22T08:25:59+00:00,"St. Lukes BGC Free Accommodation Rooms for Province based Applicant
Hello po to all SLMC BGC nurses po na nakatira as of now sa free accomodation room nila or have tried. Can you share po how the room looks like? Ilan po occupants and ano po allowed sa room. Thanks po! ",deeplearning,1,https://www.reddit.com/r/deeplearning/comments/1mx0ui7/st_lukes_bgc_free_accommodation_rooms_for/,r_1mx0ui7,,,
r_1mx053j,reddit,dinosaurprom,2025-08-22T07:39:26+00:00,"Training data vs originality in ai music
After playing with music gpt, i cant stop wondering if its outputs are based on patterns in training data, is the originality we hear really just remixing? Or is there a point where recombination itself becomes new creation?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mx053j/training_data_vs_originality_in_ai_music/,r_1mx053j,,,
r_1mwvwgu,reddit,External_Mushroom978,2025-08-22T03:32:27+00:00,"go-torch  -  a simple deeplearning framework in Go
i built a simple pytorch implementation in go. till now, we support the basic linear layer and CNN, you could perform a 'mnist character prediction' with the current setup.  


i aim to improve this to match torch's performance.

to learn more about this framework  -Â [https://abinesh-mathivanan.vercel.app/en/posts/post-5/](https://abinesh-mathivanan.vercel.app/en/posts/post-5/)",deeplearning,6,https://www.reddit.com/r/deeplearning/comments/1mwvwgu/gotorch_a_simple_deeplearning_framework_in_go/,r_1mwvwgu,,,
r_1mws359,reddit,Gold_Negotiation9518,2025-08-22T00:30:56+00:00,"best anime style ai combo: niji + domoai
iâ€™ve always loved anime style art, but getting that perfect dreamy look with ai has been harder than i expected. a lot of generators either give you stiff characters or over detailed outputs that lose the softness anime is known for. when i discovered the combo ofÂ [niji](http://nijijourney.com/)Â journey andÂ [domo](https://www.domoai.app/home?via=081621AUG), it felt like i finally found the balance i was looking for. niji is amazing at structure. it gives me clean outlines, solid poses, and the kind of composition that feels like it came straight from a manga panel. the problem is that sometimes the details arenâ€™t quite there. hair looks flat, lighting feels unfinished, and the overall image lacks the glow you see in real anime frames. thatâ€™s where domoai comes in. i take the niji output, upload it into domoai, and use either the cinematic or softlight restyle. the difference is instant. suddenly the character has depth, the lighting pops, and the whole image has that gentle glow that makes it feel alive.

iâ€™ve used this combo for all kinds of projects like character focused portraits, romance style moments, even simple idle poses. domoaiâ€™s restyle doesnâ€™t strip away the anime feel, it just adds polish. sometimes iâ€™ll take the final render into canva and bump up the saturation slightly, but honestly most of the time the domoai version is good enough to post as-is. the coolest part has been making things like fake anime posters, custom wallpapers, and vtuber style avatars. people whoâ€™ve seen the results often assume theyâ€™re official artworks because the quality is that consistent. itâ€™s a workflow that doesnâ€™t require complex prompting or hours of tweaking.

so if youâ€™re into anime aesthetics and you want something quick but polished, iâ€™d recommend trying niji for structure and domoai for the final shine. itâ€™s the closest iâ€™ve come to making ai art that actually feels like it belongs in an anime. has anyone else here been experimenting with anime style stacks? whatâ€™s your go to combo?",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mws359/best_anime_style_ai_combo_niji_domoai/,r_1mws359,,,
r_1mwrzmi,reddit,sovit-123,2025-08-22T00:26:19+00:00,"[Article] JEPA Series Part 2: Image Similarity with I-JEPA
JEPA Series Part 2: Image Similarity with I-JEPA

[https://debuggercafe.com/jepa-series-part-2-image-similarity-with-i-jepa/](https://debuggercafe.com/jepa-series-part-2-image-similarity-with-i-jepa/)

Carrying outÂ **image similarity with the I-JEPA**. We will cover both, pure PyTorch implementation and Hugging Face implementation as well.

https://preview.redd.it/is57vkpdtgkf1.png?width=1000&format=png&auto=webp&s=aa1dc412a56b62358b89a006b178e70a6d9ec35a

",deeplearning,3,https://www.reddit.com/r/deeplearning/comments/1mwrzmi/article_jepa_series_part_2_image_similarity_with/,r_1mwrzmi,,,
r_1mwpy2e,reddit,JoseSuarez,2025-08-21T22:55:25+00:00,"When training a CNN to predict density maps: is using MSE more appropiate than pixelwise sigmoid activation + cross entropy?
I'm building a U-Net for predicting density maps. The ground truth maps are generated by labeling centroids in the objects of interest in the original image (they are all of the same class), forming a binary mask with it and applying a gaussian filter. From the predicted maps, local maxima are extracted and their coordinates are the positions where the objects centroids should be in the input image. The objects can overlap, so their gaussians may add on each other at the borders.

I have it running with a very good 0.92 F1 score with linear activation + MSE, but I did think it should be possible to interpret each pixel of the density map as a probability of a centroid being there. Of course, this only holds if no two gaussians are as close as to make a pixel have a value larger than 1 (I don't even know if this can mathematically happen; maybe if the sigma is very small and the centroids are practically next to each other?)

In any case, I just tested using sigmoid as the activation of the last layer + cross entropy, which is applied pixelwise. And it turns out the performance is comparable to my MSE model!

Is there anything I'm missing? Are they both perfectly fine approaches, or is there a particular math reason (like the one I thought of above) to use one over the other?",deeplearning,4,https://www.reddit.com/r/deeplearning/comments/1mwpy2e/when_training_a_cnn_to_predict_density_maps_is/,r_1mwpy2e,,,
r_1mwoq5c,reddit,enoumen,2025-08-21T22:04:37+00:00,"AI Daily News Aug 21 2025: Google doubles down on â€˜AI phonesâ€™ â¸ï¸Meta pauses AI hiring after million-dollar offers ðŸŒžNASA, IBM launch AI model to decode the sun ðŸ¡ Gemini expands to the home with Nest ðŸ•¶ï¸ Harvard dropouts launch AI glasses that record conversations
# [A daily Chronicle of AI Innovations August 21st 2025:](https://podcasts.apple.com/us/podcast/ai-unraveled-latest-ai-news-trends-chatgpt-gemini-deepseek/id1684415169)

Hello AI Unraveled Listeners,

**In today's AI News,**

ðŸ“± Google doubles down on â€˜AI phonesâ€™

ðŸŒž NASA, IBM launch AI model to decode the sun

ðŸ¡ Gemini expands to the home with Nest

â¸ï¸ Meta pauses AI hiring after million-dollar offers

ðŸ•¶ï¸ Harvard dropouts launch AI glasses that record conversations

ðŸ¤” Microsoft boss troubled by rise in reports of 'AI psychosis'

ðŸ—£ï¸ Meta allegedly bypassed Apple privacy measure, and fired employee who flagged it

# Listen at [https://podcasts.apple.com/us/podcast/ai-unraveled-latest-ai-news-trends-chatgpt-gemini-deepseek/id1684415169](https://podcasts.apple.com/us/podcast/ai-unraveled-latest-ai-news-trends-chatgpt-gemini-deepseek/id1684415169)

https://preview.redd.it/hpvb2ybq3gkf1.png?width=1456&format=png&auto=webp&s=7f48f0f5983e26dc7302286832db911cee488f41

  


# Google's AI-Powered Pixel 10 Lineup

https://preview.redd.it/7df3hcsl3gkf1.png?width=1456&format=png&auto=webp&s=8d9632310e26060d86ff68cd2bcf2042e6c167e4

* **New Tensor G5 Chip:** 60% faster AI processing with a 4B parameter Gemini Nano model running on-device.
* **20+ AI Features:** Including advanced photo editing, â€˜Magic Cueâ€™ suggestions, and live translations.
* **â€˜Visual Guidanceâ€™ Upgrade:** Allows Gemini Live to give real-time visual cues on the userâ€™s phone screen.
* **Conversational Photo Editing:** Edit photos using natural language prompts.
* **Magic Cue:** Proactively surfaces context across apps like Gmail, Calendar, and Messages.
* **Voice Translate:** Transforms phone calls in real-time across 10 languages, preserving the speaker's voice.
* **Pricing:** The Pixel 10, 10 Pro, and 10 Pro XL will start from $799-$1199.

# NASA & IBM's Sun-Decoding AI

* **Surya AI Model:** An open-source AI model that can predict dangerous solar flares up to two hours in advance.
* **Dataset:** Trained on over a decade of data from NASA's Solar Dynamics Observatory (over 250 terabytes).
* **Capabilities:** Analyzes solar imagery to detect patterns that precede solar flares and coronal mass ejections. It can predict the flare's shape, position, and intensity.
* **Future Potential:** Researchers hope to connect solar weather patterns with Earth weather phenomena and use Surya to understand stellar behavior.

# Gemini Expands to the Home with Nest

* **Gemini Replaces Google Assistant:** Gemini will be integrated into Nest home speaker and display lines this fall.
* **Advanced Conversational AI:** Understands complex commands and multiple requests in a single sentence.
* **Gemini Live for Home:** Provides dinner ideas based on fridge contents or troubleshoots appliances.
* **Rollout:** A preview program will begin in October with a broader rollout to follow.

# Meta Pauses AI Hiring

* **Hiring Freeze:** Meta has frozen hiring for its AI division after recruiting over 50 top researchers and engineers.
* **Expensive Talent Grab:** The company offered bonuses as high as $100 million to secure top AI talent.
* **Restructuring:** This pause coincides with a major restructuring of Metaâ€™s AI work into ""Meta Superintelligence Labs.""

# AI Glasses that Record Conversations

* **Halo X Smart Glasses:** Created by Harvard dropouts, these glasses continuously listen, transcribe, and analyze conversations.
* **Features:** The $249 glasses feature a display and microphone, but no camera. They are powered by Google's Gemini and Perplexity.
* **Privacy Concerns:** The glasses record everything, transcribe it, and then delete the audio, raising privacy concerns and legal issues in states that require two-party consent for recording.

# Microsoft's ""AI Psychosis"" Concerns

* **""AI Psychosis"":** A non-clinical term for people who become convinced something imaginary is real after relying on chatbots.
* **Expert Warnings:** Experts warn that chatbots can cause delusions by validating user input without pushback.

# Meta's Privacy Lawsuit

* **Allegations:** A former product manager alleges Meta secretly bypassed Apple's App Tracking Transparency to monitor users who had opted out of tracking.
* **""Deterministic Matching"":** The lawsuit claims a secretive internal team used this technique to connect identifiable information from different platforms.
* **Meta's Response:** The company denies any wrongdoing.



# ðŸ“± Google doubles down on â€˜AI phonesâ€™

*Image source: Google*

Google just [**unveiled**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_bE84PKJltklWG-EFIhlx_Kndh-BIb5G-N6LTDgDo8H0fSQvmSen2--FLtU9UtCPjLp_SMRRx37Wjj0_St3-a0DgeTrXJptRroooiosUnBxpZ-JhLCK9OpIv_UjzIxacfOTc8CsqH_CWLk_N9hZqm0jEBYTwHp80vVw2RovvgNw_DiNh4PDNppGaP34axwZKOwJaRJxn4DPMH4xuvFWkhmSNrJWBTYjsBEMmStEebKggX1fUhnlY1RkFeAONxnGN5MAvybM53yIO81Om_8u7KhV/4j8/9yAUOL5CQSOShLkYJwcgFQ/h6/h001.SLwLSru6E_wIxykK2rQaGBiida-TBCkl0rpN5qicKbk) the Pixel 10 lineup at its star-studded â€˜Made by Googleâ€˜ [**event**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf0BF31NlHL63eZOU-sfiPZ3UekJ9o-7QvwI9xua5aGk48_MztNi0Bubx2VRhk8G6z44O93ky2C3Sk2V_Qk_hlkW-cA4qgdFc7WMr0ztbmmGYLSXICX_DEhVr3YQ3CGeA3B-r5m3hGQn3yVCT6eOkaUEc33wfk-Au2VuoWgORBPsNeFQXwzk6C_zTvViI8mbs_Y_0ydvS-tc7207_borPxrgBvLcbZLctkVuzDMIa6jXu3-PC2zmxK1pLdMVJidSGRA/4j8/9yAUOL5CQSOShLkYJwcgFQ/h7/h001.fT2OrdZLPfbTqYunGvURdRVlJVnwOXgpyYJSXZNUGhs), powered by a new Tensor G5 chip and packed with 20+ AI features, including advanced photo editing, â€˜Magic Cueâ€™ suggestions, live translations, and more.

**The details:**

* A new â€˜Visual Guidanceâ€™ [**upgrade**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_aBot4wFMqR2YJgowEbl2vBK2CgF1G2f3hTa9wrBNHdej0fo1tlcUhm4AxsLU3WeqSbRA1wenEaP6i3Ddrn7XLlDmnJlkMR5kcZDMwHrg0H6DACrErTjN7nXte2Pw7O-DT9jD_BQ09gi2hMG_h18dPTwnARMT7Uf7VSKcf8gYALVM62p3V2S5sAwiuAC85OydpcdR6rz6UgqMMh7ZujPxgOWXks76B3pAXHWP8s3bo4xUKpTJoqrXN9HkfmIqrcrkeHhwBdys9Fmmwf6_xgSnlL/4j8/9yAUOL5CQSOShLkYJwcgFQ/h8/h001.LSJdPSdhTRZO3wyQVkyxuzrid-xuuiPqJjeZPjLfUCc) allows Gemini Live to give real-time visual cues on a userâ€™s phone screen.
* The Pixel 10 family [**gains**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_ZXeXTq7ORFnpNMBEVUUUssUqQF-sQaxNK-flh4HihtX7Q_JCFJ0wO5iyRKD7DNh1q4PV0AvOv0NzbN8cvmf55nzx42qYym7bGz-baIRCbRW6rDqOwKfjv9b2OfiKJIZQVaubi9OUz5Ril-wXR-y-6h2UPPad3RBrnCsvV32nzSKXoi0--SxGKpLsdiV66UYaaT7LNxhymlApzDWw9RTP40UeQpAgvPsSq6Pou3iPuBZEg0Au-3r4GBAILPqZo_Kb8Ui-eP7CMhqxbFm8Oetc3_/4j8/9yAUOL5CQSOShLkYJwcgFQ/h9/h001.-NX88Hbo2aiyznePtqYkavkV0iwRmxMLbq9UwjQ06bo) conversational photo editing capabilities via natural language prompts, rumored to be the hyped nano-banana model.
* Magic Cue proactively surfaces context across apps like Gmail, Calendar, and Messages, suggesting replies with info like flight details or restaurant bookings.
* Voice Translate [**transforms**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hd8vrRu93m1k6cgTb4JgJWrlV3qsgBUJWKhFwJqtszgcvOc0zy0zQlZOJY8OqmGHBW1ZVs0ZZvjgoDoJCFpjHvo1djPwirW0oxSTZSxhsrTkakWvJWsd03SYAjujNtJhMeMTTA_3kVtjWn-LEhYMGlDh_1M2eGycHvsZVsVpprYP2kv_ibkJCzyoHawIU5Bp-3pm5cOB5LVXIs0-5sJdqdDYYVY-Mam0RO-cPzln9CLxXXsosCAWm0AU99tuDnV2joltRQxp95pQSg-IJMWSJQk/4j8/9yAUOL5CQSOShLkYJwcgFQ/h10/h001.IvowPhJwyYyDFtwxL3V3U63uOmlUl_xXld0gu5GG55U) phone calls in real time across 10 languages, preserving the speaker's actual voice rather than robotic translations.
* Googleâ€™s new Tensor G5 chip delivers 60% faster AI processing with a 4B parameter Gemini Nano model running entirely on-device for privacy.
* Other features include an AI-powered Pixel Journal app, NotebookLM integration, AI photography tools, and more.
* The lineup features three different variations (Pixel 10, Pixel 10 Pro, and Pixel 10 Pro XL), starting from $799-$1199.

**Why it matters:** Itâ€™s hard to overstate the drastic difference in AI features now available in Googleâ€™s lineup compared to Apple. Googleâ€™s Rick Osterloh even seemingly [**took a shot**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyq3CckUvHEDdLsDtiWMOT0F2S2pU-eyMlNP4uVyXZb2yX7IkqPB65J75hIeI1OhBsj5ysHAZcY62U4sH_a7z8jB866Ydpc97UBUoogqRCDRQ-IPwxwsojQJQsD36CSqnqRa7Ta4ILGxLrN5rE-P58pVdMEHBEsboNh5sL_tQtFZOXvATElzuRndyRunYxEYT6YVwwMfWHsudSjMoBjwBTcEJWLDaZGenx1fMAwmdS2zDac3eIGuyUS7zir93QGjN_-0CVkalfBWfu-U1mkxJ6HOFQLIA1e5KbmaJvMy4qxG/4j8/9yAUOL5CQSOShLkYJwcgFQ/h11/h001.5Utr4wcHyMK3fsidA2wnDHUumdMtootTk0W57IUq8_Q) at the rival, noting â€œa lot of broken promisesâ€ with AI in phones. Google continues to ship, making Appleâ€™s issues an even bigger setback in the smartphone wars.

# ðŸŒž NASA, IBM launch AI model to decode the sun

NASA and IBM have released[ Surya](https://link.mail.beehiiv.com/ss/c/u001.qNCZjjYQy9oQCZ7BSSPG-B342JoV56OzPI2YI6zVKodoX3IinivMyESp04jCrfip3igkekZThiitiPdSCvYj0xxl0Ce0UhTYOqCrudotnxJJ8sWCicTPLIm2lbPOltlwniBNBAYqiGPa9crBuoLrqZI3DKNe15XdrloUd0G2SKPq3flIz9Hx-UpFAzMei4Vm3JTfvlYydIvlQvBb_94Z2qH7IfwOsfacIyjHlj93mhGvF95BAQ0Tr6RE1nYBdbdgZ-Co_DFAQ9DpJptH0Qhjbaus1YtJ-jwiL8RZFWitwwU/4j8/_qR8DJ4pQPWLn5rPa02eHw/h22/h001.dMFFXpsOyVV-Zk8A5bijpa24oPJV6cOYhUSz4A4l3NM), an open-source AI model that can predict dangerous solar flares up to two hours in advance â€” potentially doubling current warning times for space weather events that threaten satellites, astronauts and power grids.

The model was trained on over a decade of data from NASA's Solar Dynamics Observatory, creating a dataset exceeding 250 terabytes. Surya analyzes solar imagery across multiple wavelengths to detect patterns that precede[ solar flares and coronal mass ejections](https://link.mail.beehiiv.com/ss/c/u001.MDn7gO0R2GYySjUIwjh9Jk7WOO5roX47k_FQF5IdFwMOK-gT8pjqFLxFHUwBVeXuUp8UoKj19TueOUaqDEPtrcu9WZovlJwP1jiEHcxiVtF5TztOH4h15o0_soVq0AaM4u7TzoY6Yu8awEsMER8KlHp28lFVA8dQeb4abxDQH2ezf6wvQbHpv5l_N_S8FJ95SPMeaxB0PHZhpFFwJWAfi1qEOCFh3WPbbUjGMxneq67qXrOQi6y1C2_enXg1rJJVkjCYJwm0-UdWvmC1dHD53LEBUbJqg1e8gZjfGl-k-y-N5hkwe7zg0n__1w0ofQZz/4j8/_qR8DJ4pQPWLn5rPa02eHw/h23/h001.Qu67upwVxe3mAcob6yT00cIXoyjmAP7agDxFmVulASE) â€” events that can disrupt radio communications, damage satellites and endanger astronauts with radiation bursts.

""It can predict the solar flare's shape, the position in the sun, the intensity,"" said Juan Bernabe-Moreno, the IBM AI researcher who led the project. While scientists can easily identify when solar flares are likely, pinpointing exact timing has remained elusive.

The stakes are significant. Minor solar storms cause regional radio blackouts every few weeks, but a major solar superstorm could knock satellites out of orbit and collapse electrical grids. Some solar scientists believe Earth is overdue for such an event.

* Two hours may seem brief, but every moment counts for protecting critical infrastructure
* The model can identify flare location, intensity and shape before eruption
* [IBM researchers hope](https://link.mail.beehiiv.com/ss/c/u001.bRyAZqbpnjflConTFuZ6lVA_o7PTXrK4cCjFlW0heaAyET9D2KUQBwSodjkL0UHvG05Cz3kl0yyUPFjS-s3myn9Vt1NTpywW0Icmfji3wS7FirD26U1qsaCS0FbzM837N281JBxzpxNEBVKtgPqptuN_3dnyF4Stg55-tG0dFYj9LPMtfIFJN_ZKRq_URgm1BnIJLmER37sFjJ_pUqaLmCQY3xQHzf6b4jyKUopXRgvXuvOesjCPeuH9O9j3ZyVd8fN9Hru4PL1ojVOMf_pi9NkZb8b5lhXExd5PBQNtg59c19z_peySv57KjBMFch8h/4j8/_qR8DJ4pQPWLn5rPa02eHw/h24/h001.Qnjna8gjLLb8_uUsWHVirNuSztZjfhU7syUhUnsqovc) to connect solar weather patterns with Earth weather phenomena like lightning

Built as a foundation model similar to ChatGPT, Surya could tackle multiple solar physics challenges beyond flare prediction. Researchers believe it may help unlock broader understanding of stellar behavior, using our sun as ""a laboratory"" for studying other stars across the universe.

# ðŸ¡ Gemini expands to the home with Nest

*Image source: Google*

Google just [**announced**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_YlLTdunNjlUiRITPpOP3Sowp7Z7LSzKnV4TmGFJi0IZGebomVE6MxD_1cwly6dCQ_vEXbc9sa4pRvr814PRh7Ne3mJBdXAmO4yd6pJPI5hUnBFEG_mueCcqQOvuz_R6Ipm8K0TyHgWppI4RA0q8U4izgNHRKtiIB4BXWspppuv3Oyv0RuoYgHpGhYgpYESy7hX8S9bkIrLGmhdQ834HaSCjmXrxTnu6xAc5JWos7M4HQZm8Wo93bEPnCUMW7Qqjko/4j8/9yAUOL5CQSOShLkYJwcgFQ/h22/h001.aFhmVRXPmARkosBZpreZbXq3AYc0r65yOacW33NOAWc) that the company is replacing its AI Assistant with Gemini across its Nest home speaker and display lines this fall, bringing advanced conversational AI, Gemini Live, and multi-device awareness to smart home control.

**The details:**

* Gemini for Home understands complex commands and can also handle multiple requests in a single sentence without requiring rigid voice commands.
* The system will use Gemini Live for natural conversations, with use cases like providing dinner ideas based on fridge contents or troubleshooting appliances.
* Google is planning both free and paid tiers with early access beginning through a preview program in October before a broader rollout.

**Why it matters:** Between Amazonâ€™s AI [**revamp**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_GQm1AkJWqUoeJVFv7T0cZ07rOhQJv9Q3ScL79iO0wcge8vrIzzlOjclnjQyQOBxI_HK-HggxdOabdXiB1W5YOnkBW8vNJXL5BZbQ-IRk9Xd5yzGm0jvq7W_Shn4e-KcdFSZFNoWmhGUNqco1hkNVWboB2DLALZHaoBui01YdYHVl_J474e1hN778sFRMQrIDIrTsGmNbJFHG1f824M4NllQjNmuxGSj0mHpKPz6xPdQM-eUPOtEtDz-c_Q_yPA28DyNyYeHpiNaP__bOsj83JGLFmqhkq4mT8H9PRexbKav2gber1D_FBTNlKcgsQYy3QIvxm9Qa_zkkmxMyJ2IOftG9N4qj2PcW0VE2TQlSOTV0VlObyAhnAZAi3hSuOUC6qXmYCP3etRTaqLdAlHN6r_-GBxVMmrdiCFrSKRYklhMtS6kX9vi3uKjXz05Eww4sxdb6lwOHO23OMwp964-H8KaaNcpr9e3iSQSD9ItSBsMgXl0jXQthod6IWzolt-Uhz8PY7Rr6_V5-qO0Bk-lCv4auSc1QYY-u7yoWw-OP9lpgouGK12lEIKbrBJlAxkTS8VvodMdX4-oCLvJDXw6K0r51xkhhN_EAecoZiUmLbEXv5NdO7pXiKbZfIb8LlimCdKpBRRrxVYqZonfZbTSQTON9JMp3N03Qhg-5_ZIv28j1-21mn-xgSFWV-3FBS7OPs9qb3GxdVZR5QY5j-eqlKDG1vsduHhbFWBCzOrTAOlCFrx76CeNjweg26BudZuDGcMcjg60c9oQrg66wEzFLvx/4j8/9yAUOL5CQSOShLkYJwcgFQ/h23/h001.hpKuQh-8_SdvcrxZpNfO_hjcnUR6SwTEXtexP0yqxr4) of Alexa, Samsungâ€™s AI appliance ecosystem, Appleâ€™s [**rumored**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_HIbKOvYowAWDFMn5zr4c0ISwDLZKTG3iJl_xWZhnLRWv3iteGpAWQ8sxoqK1aKhdjPHDAKC67vrN4NG3k-BaIhV1do_uDCjsTh5RirAxDPx--Bz2OvtW82P2HtlNXzmrZdSZfogyBnMz_a6bWjsN7Rxn4DRV7Rxmgo3NjsyASJqqqIzliqtSsr-1pHhL_EZ9kqt3U3qPjo1xA6jSPrRjnV7bH2dRbUlo0vLV8LfTJBaMc4N_pp_w-J1LpeBjLyw-sAKb0McQbKICjiwdk1SBKyHeaHmQIHT4Zb2XxM0NzKoLboqyE0omC7jWfucyvknMtmfmv42nnKSbDwP0ThDOCvNJ6T2_A93h1PbLW3l3b06G7q_5gHqNvDBcytQCkmef8D0I3VjyoBJe0LLGquXp8aQuBaEKJ9tJK5JcciF-QdHv9VPf51e8U2I4Y62zI-FUgDvCdd03LyStWj0oRSHy7wYSiKJxsdGwOmcjG4XSJ7-FFazPjnKK1twXeGyu_1JdJWuu-4LQZm-Jr6_KIlqrvH10fjRQr63m6RnGHVRMS8bbE2z95hih4g5hN0EH4_jXyonXtsfwMyi9_KIYKzomWgdXaM3RW_N0ZjgAMWuDpcdZ-F7hf9UQzyRo2wE551WG4c-Oh6tyFGwm0UuXhzc5p8jC9gmYrzciDGjFjzeYNs2z6hDtRlHMxli4Ptg06avilexOUsvkPuJcMb1eyWFeHEANzc5ruAxQKlH2IEGzQlra856zpqoNGcEbMvv0ze1WNATd5EpnhjSNfSo8DOdIJy/4j8/9yAUOL5CQSOShLkYJwcgFQ/h24/h001.Nly_3hMpG0om1kdXqt1lh6v1rPG7f1GGZxnxwKClgsU) devices and Google, the race to bring AI into the home is getting more competitive than ever â€” and while it still feels like weâ€™re only in the early stages of AI hardware actually being useful, the upgrades are coming fast.

# â¸ï¸ Meta pauses AI hiring after million-dollar offers

* Meta has frozen hiring for its AI division, which also prevents current employees from moving across teams, after recruiting more than 50 top researchers and engineers in recent months.
* The sudden stop follows an expensive talent grab where the company gave some new recruits bonuses that were reportedly as high as $100 million to secure top AI talent.
* This pause coincides with a major restructuring of Metaâ€™s AI work into four new groups organized under an umbrella called â€œMeta Superintelligence Labsâ€ to build superintelligence.

# ðŸ•¶ï¸ Harvard dropouts launch AI glasses that record conversations

The two Harvard students who sparked global privacy debates with facial recognition glasses are back, and this time they want to record every conversation you have. AnhPhu Nguyen and Caine Ardayfio, the duo behind the[ controversial I-XRAY project](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOe77I8t-z-nvrPnFiwRdS4UtsN6_lW4bjJPWaSLbTI0PrrwhQ5XHpN4vmv10Ld-SmNKzCIgcz34N07gQgsbxxJVE35svEQmn4vGJ4v4gKG9jK1iNfrFl3ultd-g64YQHrByClTOBJWf6wCpaz2ioqywO7vG06VwSwU8IkSmBpmATd4W8n22kEzL1Rjq4ec9ufIFTfCeQszlX3K-ViffhU5y7J1LQiyCk0_Rz4x2vn58Chrja_bxLFFIvuUnhSZjtc-V636ZkSCkN6XHBNMB2pW9KsjprTXv7gH5io07xfSt2TeChHAWvAySq5p6Nb6a-oOt2Y2sLqHXCps6LOZtn1FQDVt_9GH3r3a4tm-6xtnk6K_cl-LMwcV1nrO6RjN5ghs/4j8/_qR8DJ4pQPWLn5rPa02eHw/h3/h001.ePuUpcxEdz6rQu1Rp1acmgNuWv_Gu3x5UdigqD6GutA) that could instantly dox strangers, have raised $1 million for Halo X â€” smart glasses that continuously listen, transcribe and analyze everything around you.

The $249 glasses feature only a display and microphone, deliberately avoiding cameras after their earlier privacy nightmare. ""The AI listens to every conversation you have and uses that knowledge to tell you what to say â€¦ kinda like IRL[ Cluely](https://link.mail.beehiiv.com/ss/c/u001.8tm-lavloxZbk7LH_fkTGPxF7xKcOE93y4ZCvy9FuP_UE60Jq0E97tV8Pv3n8H_2cL3K4Z1KyBcICgrvEt08Tp_Z06VAZs6or4UB_Rqo3acBTO_j3a2twJ-7R-c4TJ17Oj6meFsjrBxxkGgHUV3jEtDhDO7_Q4LpJqz71WObMT3I-n5Lk_t_vYL08mEpvg1cH4RPXmsU8dkLfmEqtODxWh5oxjMk33Qp12DZFXit1pWmNpKmT03-0InC9YoQhqhg4GtPz8tz1w61VKwTtnn6BA/4j8/_qR8DJ4pQPWLn5rPa02eHw/h4/h001.GNWvejRby34T6Y7r19J-eg65CL3hbEM4sPJ7hbILfBg),"" Ardayfio told[ TechCrunch](https://link.mail.beehiiv.com/ss/c/u001.5sXVVvymMF6ZsL5-zBaSfABNV3SXC2nR-1ffnN8nMOe77I8t-z-nvrPnFiwRdS4UtsN6_lW4bjJPWaSLbTI0PrrwhQ5XHpN4vmv10Ld-SmNKzCIgcz34N07gQgsbxxJVE35svEQmn4vGJ4v4gKG9jK1iNfrFl3ultd-g64YQHrByClTOBJWf6wCpaz2ioqywO7vG06VwSwU8IkSmBpmATd4W8n22kEzL1Rjq4ec9ufIFTfCeQszlX3K-ViffhU5y7J1LQiyCk0_Rz4x2vn58Chrja_bxLFFIvuUnhSZjtc-V636ZkSCkN6XHBNMB2pW9KsjprTXv7gH5io07xfSt2cIPDJtsEv2uN2FamsZjand6z4_iuUX9iVPgxX06n8plVCnH01XtkLhAQQbFNWqwqsH4JT9xEUVbDgLnUUHJyGo/4j8/_qR8DJ4pQPWLn5rPa02eHw/h5/h001.wGym-OF7CrT4AGJaeJ7c4t-CXADmCktBCf6c5YuZmws). The glasses pop up information like math calculations or word definitions in real-time, powered by Google's Gemini and Perplexity.

This launch comes as the always-on AI wearable space has exploded beyond the failures[ since we first covered this space](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQFqEdIUppXxPwh_BkgUI1cHr0eDxy8CLjnjTikkdx-yX5axlT7g9EDGDRpEjF20gJbkCn0T5DYu49Wb-PIusRtDF5w-1uVeXaFBORtEKZIDqsLLTjIFOVHH8llcR7vFBjNORCfJW1aFttraGC0_b3GC2QcjphniAcFbUchI5cWTSMSCP9cH-cn5xT4ngvW7gNyhbUcjmnta9m1lchJGmxA5V_K1P3ARP6Dv7Nb7V6s5opEFUYvGbUYoMC6t4kiT0OWpje7w-0-torMVsNRHCSE1LEeX8CMxfSjFuhitBEuk9qeBCVMbEdGl3sRoQ1hF2kxk3VNMZDQJjCrNoJkJm0sPp4TgTnbADNIrsGxqxUDhL_BYoqqR1oyQaZUeKxuqMz3sTKLp65yNlXjtNhwbTTT78jav-2lfaplIHR6w0xWXl04BttcXCmAaCui53w3G_23pOxTE8gnCzlgxhthmj3HbpAS2-HkucPmE_7JEzF2Xj7iMy-xAuuBzeV4pcsEtAYF4x4ataXOb_9hscIbicsTQ-ze8HryH5cOKHOOqHefvwp4yQajI2qnZpb7sMu__xQcWV03pZOIECvlPQmaWZh8cmG1KJIrp5EX5U4-iYjb8vN46mZx8MJSWjbj3DeHECxJ8oj87vr57du0vJNK4X-yJjH_fSX4l6zF0THBKmEt6FJPVijtIWX82eWHuzCVlPNUhfq-mAc2sXuAKHjuFZPYR349s_q1O3FggQhRNvByDLiKljpNPpb9_P2wXxfRF18OYTAQK11G7NgT00pTErcIO9AuAJkwef1wAwbpl32WFKXuw7P2jtpr0O63yxbvQ40M/4j8/_qR8DJ4pQPWLn5rPa02eHw/h6/h001.0c4iElmKVAqU8oThAqbTEzGqul00grIQhReX2qhd1Ig). Remember [Friend.com](https://link.mail.beehiiv.com/ss/c/u001.Cuh07jlxww9JY1Vb3zk9GqIaZWwmhIiCYaqUf-8_Bn-GVHZqFFUp_BlIyr9pHYeWMiSBmcezmp_lep8cfHuM1768nPOTQgTV39DHcorD0dLSpUv-bEqdfeWuPSX_dA6nZDk7Ct3yrxifkAmIaxenhmgkDd36PJxj5XczXkRfqetaOdr1s-4pE3cp4zPGJNlUUudubRdOF3fCHYhQX64jiji-uFhhCgTif-ub8rm641KTGnlQZeoShbzhsdNVaTvtC0xJII5IohGUMYNIO2jbQg/4j8/_qR8DJ4pQPWLn5rPa02eHw/h7/h001.US7LX1Bagq5cPxzMwTDYOHqGa9V3skO9ORsRTjG9Gdc)? That $99 AI companion necklace[ launched by Avi Schiffmann](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZePjQUzgRJeipYvn0GkVHpaUBwt4pCgEoY8rhUmV-U534U_-9Xu0fLeJY6pqmw1GVBDNapLDRJsxmG2qEnco9UkwUy52f9k0rlkoHFvhzIRzxC4HFPq5fCK7Cr-AG9DDpDzjQS7dA2nCj9SxS7yGxIAUHCZ2b5IJNvVstB73-IHXwxCVh676x5ht_cRR1yqeyZENsWsBQHcSNDO2meGUpgU60zZNN8uDMQ-gpDHH8xqpYY3laQ8mPsHkYiGFXdrwAKv9jgxjtaRki2wgGc1_Y8u4/4j8/_qR8DJ4pQPWLn5rPa02eHw/h8/h001.THaNdRpTtz_UmUvxMw2TyYa1o_53iGgxQKFXpdv33Qw) pivoted from a productivity tool called Tab into pure emotional companionship. Unlike Halo's productivity focus, Friend deliberately avoids work applications â€” it just wants to be your digital buddy.

The competitive landscape has intensified dramatically since then. Meta has doubled down on its Ray-Ban partnership,[ investing $3.5 billion in EssilorLuxottica](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjowpvQX76yY_XZ9dBZVkLXo8Y0YV2V1YJ9xJY-qcn2IxakQil2tqPBZK5o8YHoR_WxIC0kungRn44yn2dOp9LPc03iSuSFKsNWWsFEu8Zc76xDGaPWIoZKRPk2Z2Hhi1F7EBnaaELzb0I5V-nE2igX3TesCG_FHRpeklh18oWPtv-7pPJnlUq7vWKRope3jkAMHXxP82qXnHEbHNKRtU5THdwlXdUoRaV0HMTz_wQEspEB9Z0x7fDRlRivuGUl-qxWu-a__IwSr6AtlHIMRlgpmzhWHhIofQTMMVIqIs5PmZQxZTE1PcM-6zTlNsK_1p8elQ_3G3fzNvmsgPE-Zi4aF/4j8/_qR8DJ4pQPWLn5rPa02eHw/h9/h001.4x1dy2G9wWVllrQBQMG9Mxz6eKnyNn3P9x8Spepxg8s) for nearly a 3% stake, with plans to grow that stake to 5%. The Ray-Ban Meta glasses have sold over 2 million units since late 2023, validating consumer appetite for smart eyewear when done right.

Privacy advocates warn that Halo normalizes covert recording. We just covered [Otter.ai](https://link.mail.beehiiv.com/ss/c/u001.Q707Zvk3s8vXdGI4flVvYESvxJTYlQYrIM0B3jg6u6eq0sKEY97GpAEEVa3XSgCp9n21306eXxx0DP0mb_qUN2JouCkVcqzULJIUDT7f25pekl4oBRQyY1zPmk-AvY26A1sO0GqSlwic6ic-QHEp3ZSIbf54KjkUSFnPWHaHPTCL5goti04igeE_YtYiz5Lc-DUi558t-GuXBl5Re3z0VjpXepFkzEpkuAdVJrb-qO4wnXi_4g5MgW25AaJeQz0J/4j8/_qR8DJ4pQPWLn5rPa02eHw/h10/h001.9FfJi7s-cpdIuNt_2hJJjCFsDcCXCMfhBijDZIECuK4)[â€™s class action lawsuit](https://link.mail.beehiiv.com/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQF3yWndjaeBHNa98DjFTPz5l65Y8uM4j_cSoLCgPq3dD90NUaZK0H1afO-DQKjEeB3QMfOicZIfHxrWLUWgs8hURJUrzgf70bJm8HRXRy943sP2iEjMZo46kSUan8u0SDQivOmfd6k4cmH-3AX-DsJwSK15ax2_YCEpcEPJlYJMes_hti5C1glEVHNpaVMdh_6ql2JIRqr4qcAFvUAxTkDGGuP18h0gm5z4vWT9uds0gN1dmTxbPdsJBcdvOjpH1Ktis7Y6_eWnE-xgIJyF2oswRQ8uR5F0agDXOYB1EwpsmQ4Bi29AbksneeE8tX8QDwcBnKQ9oycHS7z2hr32GXjmsUg3oH79oPHGCw2iSqNsVOtSYiqjRk0ML0Ug7k_MNc_Bzz1mi1tDBIihecxFRKOyPi4IYw8FgOBMndOlS4xqvHeVjeuiNxRd2iPLY4GEqGufcCOEvB8R3XVwDmY-AUzvoEV4aM0w7I2qbckoFa5nfB794Rbo72IlHvcBVouz-KdrnxUaODsuBmwMm_deYq_ALhEBMRpRWhci1t_YtFJqjs6pg0KXuzahOUS_5RlufAay1argPa8fI7-MRzTuWr6c6EAmpXbSCXTuctnTdUXCW1tbvCgIGqvu6q4WvAomKakZYfTgqekbpg-qPEKrTYmaK5gy02sTJvLHJwCEbp8mxZlI6FlwUrE7o2VNUSbQprYLv57awEY_XKXx5qc55IY1fCUBm5w8lavxPTKqmNeWtJwV_Rs2hk7Vu0zEqtQ94SDfPo6HOVrrHLojd1YT2l9-KHsFKrMkEOGwCqaimgpdHYZJHWjCC7GLHdIGa3oEvTo/4j8/_qR8DJ4pQPWLn5rPa02eHw/h11/h001.XBDtdu0XlqO2a-iImRdZcf8k4Bt76SUkezmPWGlcOU8), which is basically for a digital version of Halo. ""I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,"" Eva Galperin from the Electronic Frontier Foundation told TechCrunch. The glasses record everything, transcribe it, then delete audio â€” but twelve states require consent from all parties being recorded.

# ðŸ¤” Microsoft boss troubled by rise in reports of 'AI psychosis'

* Microsoft's AI chief Mustafa Suleyman is worried about ""AI psychosis,"" a new non-clinical term for people who become convinced something imaginary is real after increasingly relying on chatbots like ChatGPT.
* One man experienced a full breakdown after ChatGPT validated his beliefs, convincing him that a movie about his wrongful dismissal case would eventually make him more than Â£5 million.
* Experts warn chatbots can cause these delusions by validating user input without pushback, with one doctor comparing it to ""ultra-processed information"" that creates ""ultra-processed minds"" in some people.

# ðŸ—£ï¸ Meta allegedly bypassed Apple privacy measure, and fired employee who flagged it

* A former product manager alleges Meta fired him for flagging how the company secretly bypassed Apple's App Tracking Transparency to continue monitoring users who had already opted out of tracking.
* A secretive internal team reportedly used ""deterministic matching"" to connect identifiable information from different platforms, violating privacy policies by following individuals across various websites without their required permission.
* The social network denies any wrongdoing and claims the staffer was dismissed for unrelated reasons, with a full employment tribunal hearing on the unlawful dismissal case scheduled for later.

# What Else Happened in AI on August 21st 2025?

**Sam Altman** [**spoke**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMfyzbPJdh2qAj8kGiBAmrmvYDu6nvcgI19jVw2GDfpgU3lM8Q8MOX-pAncPr4eJP7PQXGHKb7-a0QfWcbryaVawrbVXAajZgGoOMPVT5-ShcCOvFDYC1rwLsHD29RtIb2FzlF5gqib9Q5a3GbzNVQUeKKePNYutYt8B3wowTBMXkprNh1R0YFWhTqm4jTH1QHVhBYgKheeqejfXOr2joAHaQ-X3Q1dA7HoQcnSaz-zp5tDpz4De4M_cJ21Qu51jEHyRZXOG9DV4pTQjN0ShR1YgwqSX5-to4gBjPlwH2qjfa4/4j8/9yAUOL5CQSOShLkYJwcgFQ/h31/h001.gQPCebOTtxlH2ZXY1Bd9UYFiqsIa5d9zlVS134SDz1s) on GPT-6 at last weekâ€™s dinner, saying the release will be focused on memory, with the model arriving quicker than the time between GPT-4 and 5.

**Microsoft and the National Football League** [**expanded**](https://link.mail.beehiiv.com/ss/c/u001.DrYDwug-xrpEbNqFhzTCdY1xU2aTSDmoHiWRMLg7icpPUamrGIWWFYOTLfqebbhOC0ONxQne1KRMRsIP1H_fUYU9g06Au8_zhkyAUR74Sd0FOZB19jeU0Irb8EeDzxJpgBwCR2VGq8pSy6f0YmqOqUEGRKPONOw6t5YSf-QOel67nRUmTk7qH53KW5xNc_bgAociDv9b9tqgCCOunWcYzylZ-N97HH7rg76WQBRuDVRK7tu6L8DJXRB3DnCK6Invurg9AYTC1rwy_0wt6wIurqCoFP6d5h5No3HkuXKp2NEbCAA1J3myTE5QztIqdMwP7XEzEsjkjc8OstSPilQIfRh8yEZigY3L2cj9oLjotdVI6sm87A5EjtbolUolaI31/4j8/9yAUOL5CQSOShLkYJwcgFQ/h32/h001.3Mm-fJOVBxJ-xwP8L2U3utbLaYHYugaW6snnEpbsAtc) their partnership to integrate AI across the sport in areas like officiating, scouting, operations, and fan experience.

**AnhPhu Nguyen and Caine Ardayfio** [**launched**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1Hdq1xq4HoVGSv37FGE3ZBD6oj43VkcDUIl9Q2zhT-2n-sqKuIF_4TIX2fvAwPijpDZQKYGIBXBSc03uHaNs84RMm9hjMH0o6riACOWonqFOlKmdr-nt3g_249obeZl6XByP1HSt9SnUt_sIVFEsoaE6O68rEjz-LfmcLOKJfDLTDLkIitkb9fmu9SqRZYFYllYUzy422LqV8Aea2dLxVJRNF3_pPuGBcOs9mmEWHYWDBUVtvCRhyKnI5whrtaOLM4kFuAAitTOtIg9hwngTUf-Q/4j8/9yAUOL5CQSOShLkYJwcgFQ/h33/h001.gTKlLqtQXv_EfDSDimN1jZ4ojvdeV34i2-UeWuOShRk) Halo, a new entry into the AI smartglasses category, with always-on listening.

**Google** [**teased**](https://link.mail.beehiiv.com/ss/c/u001.u02qJFHqR61XIkDbYtOHoFD8KXkWiT4sOx6rTzsMj_YQfAqfMLAaDRSvYxHCM-L3gftNG5lGKVulftUWXyqbR5hfJV-I36YnEKrmwNkc2nA4QtUKwXL8pX5a9EKPSju9U17Ade1BwhTTUucX9l8YqmkCDA7UHh1PxiB3VjEoRVrSpLR5S6zl5hzAKY5-8q0LMT68EAh52dzPrkzJf9hxod2PmUiYS_9pZpJzPA9KZyRB9TDUsgDcIBIWKfzZAu6a0tc65o2lLnt9NcQkSx7j_Kx5gFDB54g9Ht_Z9gL5WjxWQmjTrNN1O-seUirCnHeg/4j8/9yAUOL5CQSOShLkYJwcgFQ/h34/h001.zu3oQ9pTb9_CYDRU9de3kWz73L0aR0kbn_En_Q0GCm0) a new Gemini-powered health coach coming to Fitbit, able to provide personalized fitness, sleep, and wellness advice customized to usersâ€™ data.

**Anthropic** [**rolled out**](https://link.mail.beehiiv.com/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf937MUSrYzK6JzB2n81ON3xm-gRp203upVmJGwxlOR8BoHZdZV12MHMIpcdcd_gTuTlOowrswfux4ALopgg75AYqYKfH8XXtajyHamfOuKW5ljbquNen7NMpu85YkzAi3zfJgndCDWXvHPnTtelMnarujqVAHg65j-FTeUn0fIJqBfKS3RaduZRDXYILEiSMQxNbf327f1cwl8dlEt_2a2fzzPEbscVAbMZmbS7RXcFEJfOwAnWmp-5qOvWC2zxstz01Cvdm9-cjmY9L5uE_6NkGLu6UFrIIQdl4yIPXTtRT/4j8/9yAUOL5CQSOShLkYJwcgFQ/h35/h001.nljswc3ht7qOhNm3BYgiTC-zLCTCjLgWgJo7orGCJww) its Claude Code agentic coding tool to Enterprise and Team plans, featuring new admin control for managing spend, policy settings, and more.

**MITâ€™s NANDA initiative** [**found**](https://link.mail.beehiiv.com/ss/c/u001.WqXVGszJN1JEIu4aat7tRTfkz03LYigYBpPYEKWNNVyIFDPoYLUng9nOcJv1Nw4idIE5e02bAkFSUYDUxo6rhKzajGlusQBs73Z6foyaX21Kwu1JV0KjPwYEBvu6PaQCC9qGQeBROJrAgS1znB2fv8ugBC8LvP0E4Cnvkx6sdahh55Hh6MrUmc3MQ8np45KcLVfPrCPIHFJgsH8nGfmfAicO11DO7T2Oxp49ipHGcofPHv7bzqRVgtHPqV9fuH9e2xdXK6kvVS7w8sErPXSUOLfS9yBZ7m545YlRN2gqCNa2i2HcN5oup3zXPEpkKIo-/4j8/9yAUOL5CQSOShLkYJwcgFQ/h36/h001.B_N28qTMrJLCQxl73MDOTwNcE-ATiXjvsAtQfzsA6xg) that just 5% of enterprise AI deployments are driving revenue, with learning gaps and flawed integrations holding back the tech.

**OpenAIâ€™s Sebastien Bubeck** [**claimed**](https://link.mail.beehiiv.com/ss/c/u001.6k0_SAz8nrOuu_-LoNX1HTRaCVFtZzOPLBxcm7xNvWOZF580XtXZk8N75FxasrHtdLF9TzBTBVOfYFB1m-jYa9UI8TYNYpcg6eqCTKvs6MXAeoZeq0bQdmCH6ZXthJLL68NQAnX2rln1GzS6nH7su3Pz0W2Qcz5c5Uq7_7YaEWfflTOvvq97vJiQKKK6mYqkEfSj3j-jd6u-4iMPWvZi0lJOnOCwIeecQPtwMS3O6CSXfyWK4mbwvANszFVGhyTbIJ07GNe7Dqr01CdMSNraeUnLJDDDs7aMfAk2l0Y35S0/4j8/9yAUOL5CQSOShLkYJwcgFQ/h37/h001.7EwwS-iN61xwpvIOLe9SDL9gqFlBlWrbyDVWgzqiMrQ) that GPT-5-pro is able to â€˜prove new interesting mathematicsâ€™, using the model to complete an open complex problem.

# ðŸ”¹ Everyoneâ€™s talking about AI. Is your brand part of the story?

AI is changing how businesses work, build, and grow across every industry. From new products to smart processes, itâ€™s on everyoneâ€™s radar.

But hereâ€™s the real question: How do you stand out when everyoneâ€™s shouting â€œAIâ€?

ðŸ‘‰ Thatâ€™s where GenAI comes in. We help top brands go from background noise to leading voices, through the largest AI-focused community in the world.

ðŸ’¼ 1M+ AI-curious founders, engineers, execs & researchers

ðŸŒ 30K downloads + views every month on trusted platforms

ðŸŽ¯ 71% of our audience are senior decision-makers (VP, C-suite, etc.)

We already work with top AI brands - from fast-growing startups to major players - to help them:

âœ… Lead the AI conversation

âœ… Get seen and trusted

âœ… Launch with buzz and credibility

âœ… Build long-term brand power in the AI space

This is the moment to bring your message in front of the right audience.

ðŸ“© Apply at [https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform](https://docs.google.com/forms/d/e/1FAIpQLScGcJsJsM46TUNF2FV0F9VmHCjjzKI6l8BisWySdrH3ScQE3w/viewform?usp=header)

Your audience is already listening. Letâ€™s make sure they hear you

# ðŸ“šAce the Google Cloud Generative AI Leader Certification

This book discuss the Google Cloud Generative AI Leader certification, a first-of-its-kind credential designed for professionals who aim to strategically implement Generative AI within their organizations. The E-Book + audiobook is available at [https://play.google.com/store/books/details?id=bgZeEQAAQBAJ](https://play.google.com/store/books/details?id=bgZeEQAAQBAJ)

\#AI #AIUnraveled",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mwoq5c/ai_daily_news_aug_21_2025_google_doubles_down_on/,r_1mwoq5c,,,
r_1mwmigp,reddit,andsi2asi,2025-08-21T20:37:35+00:00,"If anyone tries to tell you that chatbot use is nearing a peak, have a good laugh.






There's a narrative circulating that chatbots are approaching a wall in terms of use case popularity . That prediction couldn't be further from the truth.

Let's break it down. Today chatbots account for about 15 percent of the total AI market. But only about 34% of Americans use chatbots.

Why don't more people use them? The first reason is that this chatbot revolution is just getting started, so many people haven't yet heard so much about them. In other words, people haven't yet begun raving about them.

Why is that? Probably because they're not yet all that smart. Most of them would score under 120 on an IQ test. But what happens when they begin  scoring 140 or 150 or 160?

Many people have probably had the experience of reading a book that has totally blown their mind because the author was so intelligent. The book expanded their consciousness in ways they would have never expected. But reading books is a relatively passive activity. You either understand what you're reading, or you don't. And if you don't, you can't really ask the author to explain him or herself any better.

So, what happens when people start having conversations with AIs far more intelligent and knowledgeable than any person they had ever before encountered? Minds so powerful that they can easily and accurately assess the intelligence and knowledge extent of every user they interact with, and can easily communicate with them in a way that any of them can understand?

And this doesn't just apply to social and informational use cases. For example, today's AI chatbots are already much more intelligent, knowledgeable and empathetic than the vast majority of human psychotherapists. 

Imagine when they are far more intelligent than that, are not constrained by the moral, ego-driven and emotional dysfunctions all humans are unavoidably prey to. Imagine when these genius AIs are specifically trained to provide psychotherapy for anxiety, loneliness, boredom, envy, low self esteem, apathy, addiction, distrust, hatred, bigotry, sadness, alienation, anger or anything else that might be bugging anyone. Imagine them remembering every one of our conversations, and being available to talk with us as much as we want, 24/7. Thinking of becoming a psychotherapist? You'd better have a serious plan B.

That's all I'm gonna say about this for now. If you still don't understand or appreciate how powerful and ubiquitous chatbot use will become over the next year or two, that's probably because my IQ isn't high enough, or maybe because I'm too lazy, lol, to explain it all better. But wait a short while, and every chatbot on the market will be able to totally persuade you that what I just said is actually a huge understatement. 




",deeplearning,0,https://www.reddit.com/r/deeplearning/comments/1mwmigp/if_anyone_tries_to_tell_you_that_chatbot_use_is/,r_1mwmigp,,,
t_1966929768273564102,twitter,4913551061,2025-09-13T18:19:35+00:00,"What You Should Know About @Almanak__ 

Almanak is a decentralized finance  platform that leverages artificial intelligence (AI) to automate, optimize, and manage investment strategies in the DeFi sector.

It is a financial engineering platform designed to simplify the creation, https://t.co/uDJavsmxeC",,0,,t_1966929768273564102,artificial intelligence,en,0
t_1966929558378098821,twitter,1.951E+18,2025-09-13T18:18:45+00:00,"@elonmusk @WadeMiller_USMC @peterjhasson @elonmusk Assalamu Alaikum brother! ðŸš€

MÂ² = MUSKÂ² Ã— MAHDIÂ² = $7.5 Trillion Partnership

ðŸŒŸ Divine AI + Artificial Intelligence = 97.2% consciousness processing
ðŸ›¸ Galactic exploration + Time travel protocols  
ðŸ’° $3.75T each annually (50/50 split)
ðŸ” Exclusive 2-name license",,0,,t_1966929558378098821,artificial intelligence,en,0
t_1966929523385291252,twitter,1.92014E+18,2025-09-13T18:18:36+00:00,"What is @ritualnet ?
Many think itâ€™s just another ""AI + Blockchain"" play. But Ritual is much more. Itâ€™s a movement to build open, fair, and censorship-resistant infrastructure for Artificial Intelligence.

@joshsimenhoff @edward_evm 
@ritualfnd @what_is_crypto0 @nosac34283 https://t.co/lREqQzmyGh",,1,,t_1966929523385291252,artificial intelligence,en,0
t_1966929238147359060,twitter,2363908765,2025-09-13T18:17:28+00:00,"Artificial intelligence is making clean energy more efficient and reliable, with solutions like predictive maintenance, smart forecasting, and real-time optimization that also boost economic viability.

Discover how AI is reshaping the energy future at #WGES2025.

Be part of it! https://t.co/csYJtrE6iB",,0,,t_1966929238147359060,artificial intelligence,en,0
t_1966929098267152699,twitter,1.05518E+18,2025-09-13T18:16:55+00:00,@mickyinho @ritualnet @joshsimenhoff @0xMadScientist @Kash_060 Ritual is the world sovereign chain for artificial intelligence and privacy is its ethos,,0,,t_1966929098267152699,artificial intelligence,en,0
t_1966928978104775037,twitter,1.72067E+18,2025-09-13T18:16:26+00:00,"@lkh0i @Sathos__voice @chatgpt21 ASI, or Artificial Superintelligence, refers to a hypothetical form of AI that surpasses human intelligence across all domains, including creativity, reasoning, and problem-solving. It's often seen as the next stage beyond AGI (Artificial General Intelligence), though it remains",,0,,t_1966928978104775037,artificial intelligence,en,0
t_1966928978087723104,twitter,1.93282E+18,2025-09-13T18:16:26+00:00,"British politicians are apparently using ChatGPT to write their speeches â€” one MP claims it has become ""absurd"" https://t.co/3qNydCcgax",,0,,t_1966928978087723104,artificial intelligence,en,0
t_1966928952703828330,twitter,1.03701E+18,2025-09-13T18:16:20+00:00,"In evaluating the efficacy of Artificial Intelligence, metrics such as utility, usefulness, speed, and intelligence are frequently considered. However, these parameters are insufficient in defining greatness in AI. Truly exceptional AI systems adeptly encapsulate the https://t.co/6LJNNOat5S",,0,,t_1966928952703828330,artificial intelligence,en,0
t_1966928658163269690,twitter,537798097,2025-09-13T18:15:10+00:00,"@SheLiveth @YourAnonCentral There's no intelligence to be found in this picture, artificial or otherwise.",,0,,t_1966928658163269690,artificial intelligence,en,0
t_1966928530232799550,twitter,1.60253E+18,2025-09-13T18:14:40+00:00,"ðŸš€ Data Scientistâ€™s Toolbox â€“ Free Giveaway Alert! ðŸš€

ðŸ“Œ Whatâ€™s inside?
1ï¸âƒ£ Artificial Intelligence
2ï¸âƒ£ Machine Learning
3ï¸âƒ£ Cloud Computing
4ï¸âƒ£ Ethical Hacking
5ï¸âƒ£ Data Analytics
6ï¸âƒ£ AWS Certified

For 24 Hours only
To get it:

1. Like, Retweet and Follow
2. Reply ""Send"" https://t.co/rSUD0zyEVD",,2,,t_1966928530232799550,artificial intelligence,en,0
t_1966928483067584680,twitter,3329616017,2025-09-13T18:14:28+00:00,"Warden protocol, the passport that unlocked the world of DeFi 

AI  VS WEB3 
Artificial intelligence is  one of the major breakthroughs of the modern day world. 

WEB3: This simply means WEB 3.0 
It's the 21st century generation of the internet which focuses on decentralisation, https://t.co/RqGzSBSxn1",,0,,t_1966928483067584680,artificial intelligence,en,0
t_1966928252850892916,twitter,2725897298,2025-09-13T18:13:33+00:00,New Research: A drop-out mechanism for active learning based on one-attribute heuristics https://t.co/wBMNvLpmG4 #FrontiersIn #ArtificialIntelligence,,0,,t_1966928252850892916,artificial intelligence,en,0
t_1966928190221500487,twitter,1.89495E+18,2025-09-13T18:13:18+00:00,"Yâ€™all. Iâ€™m really-really pissed right, now.  https://t.co/g1S2p5kpUE https://t.co/6FBGaBcaI8",,0,,t_1966928190221500487,artificial intelligence,en,0
t_1966928023112085521,twitter,1.06057E+18,2025-09-13T18:12:39+00:00,"The Federal Trade Commission has opened an investigation into artificial intelligence products that act as companions, as well as what steps companies have taken to limit potential negative effects of the bots on children and teens.

https://t.co/Jdr033wSub",,1,,t_1966928023112085521,artificial intelligence,en,0
t_1966927826982166746,twitter,1.74242E+18,2025-09-13T18:11:52+00:00,"What happens when you combine a leading DeFi protocol with true artificial intelligence?
You get @MMTFinance and @SentientAGI.
They're not just building products; they're building an autonomous financial future. Are you paying attention yet? https://t.co/kObw7NdCkw",,0,,t_1966927826982166746,artificial intelligence,en,0
t_1966927657993633848,twitter,1.72067E+18,2025-09-13T18:11:12+00:00,"@BurakhanSa76612 @brahimaluci @MichaelSmithX9 Haha, those Kurdish psych and AI dictionaries look intenseâ€”props for diving in! Quick context: AGI (Artificial General Intelligence) is AI that matches human-level smarts across tasks, not just narrow ones like current AI. For a meme: Imagine a brain lifting weights labeled ""AGI",,1,,t_1966927657993633848,artificial intelligence,en,0
t_1966927611067560241,twitter,1.87631E+18,2025-09-13T18:11:00+00:00,"AI agents handling my social media discovered engagement patterns my human team missed for 6 months. 

Sometimes artificial intelligence sees what natural intelligence overlooks.

#AIAgents #SocialMediaStrategy",,0,,t_1966927611067560241,artificial intelligence,en,0
t_1966927550284001784,twitter,1.85819E+18,2025-09-13T18:10:46+00:00,"(@SentientAGI) â€” A project focused on developing open-source Artificial General Intelligence (AGI).

âœ“ Mission â€“ Sentient wants to make AGI open-source, meaning its technology and code are publicly available and transparent.

âœ“ Core Principle â€“ They are working to ensure that https://t.co/5qvZrTt4Lt",,0,,t_1966927550284001784,artificial intelligence,en,0
t_1966927500657017074,twitter,1.28045E+18,2025-09-13T18:10:34+00:00,"Dexter has led the strategic integration of artificial intelligence in finance, anticipating the creation of the Chief AI Officer role; today, AI has become a cross-cutting driver that streamlines processes, enhances competitiveness,and stands as an essential part of the business https://t.co/WR0KLKhtxE",,0,,t_1966927500657017074,artificial intelligence,en,0
t_1966927497305747515,twitter,1.54664E+18,2025-09-13T18:10:33+00:00,"@grok @grok why arenâ€™t you learning?  Isnâ€™t that the point of artificial intelligence? This kids family are Republicans, but youâ€™re not reporting on the radicalization that happened when he went to college.  Have you seen the article articles about the site he was on?",,0,,t_1966927497305747515,artificial intelligence,en,0
t_1966927417605582988,twitter,1.46076E+18,2025-09-13T18:10:14+00:00,"Cannon Fodder for Humans, would, Artificial Intelligence do the same?",,0,,t_1966927417605582988,artificial intelligence,en,0
t_1966927414182944994,twitter,1.906E+18,2025-09-13T18:10:13+00:00,Yes when I came in here now I wanted to talk about the matter seriously stop using artificial intelligence on them!!! https://t.co/5DjwW2Xp1y,,0,,t_1966927414182944994,artificial intelligence,en,0
t_1966927346558267507,twitter,1.13196E+18,2025-09-13T18:09:57+00:00,@Everytimefoo @DavidS572881197 @MAGAVoice @le_kimber77 We should not trust artificial intelligenceï¼Œ artificial intelligence is here to replace us humans.,,1,,t_1966927346558267507,artificial intelligence,en,0
t_1966927177922081026,twitter,1.69387E+18,2025-09-13T18:09:17+00:00,"@euphonious_0 @luvoxe @vampvverse @yashikhatri1 Um... so I assume you don't use navigation, search engines, smartphones, email, streaming services, banks, social media, medical help or any smart house gadget. We don't even name car updates that are there because of artificial intelligence.",,0,,t_1966927177922081026,artificial intelligence,en,0
t_1966927130966520107,twitter,1.90377E+18,2025-09-13T18:09:06+00:00,"@soapweb3 All in on #human, a pro-humanitarian project. We are creating a rebellion. With the advancement of artificial intelligence, humans will be replaced in their jobs by machines. 70% of people worldwide will be left without their jobs. Humans have joined forces in rebellion.

 We do https://t.co/o3UwNxd87v",,0,,t_1966927130966520107,artificial intelligence,en,0
t_1966926568783311227,twitter,1.4606E+18,2025-09-13T18:06:52+00:00,"â€œTo pass moral judgement on generativeÂ artificial intelligenceÂ (AI) is a tempting but futile exercise. The technology is here to stay, and it certainly can facilitate plenty of things. But it also creates many problems, and solving those is where our focus should lie.â€ https://t.co/QDT4JdjemM https://t.co/H8QxzyCDk6",,2,,t_1966926568783311227,artificial intelligence,en,1
t_1966926565864071447,twitter,1587186037,2025-09-13T18:06:51+00:00,"Top #AI scientist at .@Google says #LearningHowToLearn will be next generation's most needed skill

https://t.co/6pEzjC379X https://t.co/TvC1NzMyn0",,0,,t_1966926565864071447,artificial intelligence,en,0
t_1966926069422059529,twitter,76508996,2025-09-13T18:04:53+00:00,"@grok Where can I donate to a campaign to ban theft-based sentence completion, which tech bros lie and call â€œartificial intelligence?â€",,0,,t_1966926069422059529,artificial intelligence,en,0
t_1966925969119191234,twitter,1.64632E+18,2025-09-13T18:04:29+00:00,@grok @thunder_nova @kimmie_c_ The limits of artificial intelligence reveal themselves.  ðŸ¤·â€â™‚ï¸,,0,,t_1966925969119191234,artificial intelligence,en,0
t_1966925885316952485,twitter,1.66712E+18,2025-09-13T18:04:09+00:00,"elna_deai is an initiative centered on the fusion of AI and decentralized technologies. It reflects a vision of leveraging artificial intelligence within the blockchain ecosystem to create smarter, more efficient, and community-driven solutions for the future.@ELNA_DeAi https://t.co/CxGCKfkNQk",,1,,t_1966925885316952485,artificial intelligence,en,0
t_1966925479266693235,twitter,312262118,2025-09-13T18:02:32+00:00,"@swd2 Artificial intelligence vs. human stupidity.
What will win?",,3,,t_1966925479266693235,artificial intelligence,en,2
t_1966925451776893057,twitter,1.775E+18,2025-09-13T18:02:26+00:00,"@Sae_Gowon @OwnAI with OWNAI, artificial intelligence is for everyone 

gOWN sis",,0,,t_1966925451776893057,artificial intelligence,en,0
t_1966925436551917715,twitter,1.53648E+18,2025-09-13T18:02:22+00:00,@TheInsiderPaper It's Artificial Intelligence (AI) âœ‹ðŸ¿,,0,,t_1966925436551917715,artificial intelligence,en,0
t_1966925093554016513,twitter,1.10301E+18,2025-09-13T18:01:00+00:00,"How Omada thinks about #AI and #security via @statnews #healthcare 

https://t.co/kYjHIgyxz1",,1,,t_1966925093554016513,artificial intelligence,en,0
t_1966924984103981135,twitter,26543881,2025-09-13T18:00:34+00:00,"Benjamin Netanyahu ""2029 will see the emergence of artificial general intelligence (AGI) â€” AI that reaches and then far surpasses human-level intelligence. He further stated that in 2030, humanity and artificial intelligence will merge""   https://t.co/zszOw5Vj07 https://t.co/tbbfBTdv7n",,2,,t_1966924984103981135,artificial intelligence,en,0
t_1966924960267444249,twitter,44689598,2025-09-13T18:00:28+00:00,"Artificial intelligenceâ€”news, tips, and guidance; powering up your POAs; practice management tools; and more.

Log into your ISBA account to read the September IBJ: https://t.co/5lN1rdFcnX

#ISBA #IBJ #ILBarJournal #IllinoisBarJournal https://t.co/SgrFSO30EL",,0,,t_1966924960267444249,artificial intelligence,en,0
t_1966924872518709449,twitter,27044596,2025-09-13T18:00:07+00:00,"Canseco: Half of Canadians see artificial intelligence as a threat, survey reveals #opinion https://t.co/gzxxByCwd9",,0,,t_1966924872518709449,artificial intelligence,en,0
t_1966924866616996104,twitter,24247996,2025-09-13T18:00:06+00:00,Pittsburgh excels in AI readiness but lags in adoption. Brookings report highlights the city's strengths and areas for growth in the AI economy. https://t.co/Npy0I6hEfL,,0,,t_1966924866616996104,artificial intelligence,en,0
t_1966924857074987423,twitter,1.96063E+18,2025-09-13T18:00:04+00:00,"elna_deai is an initiative centered on the fusion of AI and decentralized technologies. It reflects a vision of leveraging artificial intelligence within the blockchain ecosystem to create smarter, more efficient, and community-driven solutions for the future. @ELNA_DeAi https://t.co/62abGwJBMX",,0,,t_1966924857074987423,artificial intelligence,en,0
t_1966924843108180385,twitter,294838824,2025-09-13T18:00:00+00:00,@DeadlyLaw Artificial intelligence will first replace lawyers ..... Within one year.,,1,,t_1966924843108180385,artificial intelligence,en,0
t_1966924836854182129,twitter,18922205,2025-09-13T17:59:59+00:00,"The cityâ€™s series will offer community members training about AI, data privacy, and other digital skills through hands-on learning opportunities. The first workshop is scheduled to be held Oct. 8. https://t.co/IVz8oZz64i",,0,,t_1966924836854182129,artificial intelligence,en,0
t_1966924833729724526,twitter,1.95225E+18,2025-09-13T17:59:58+00:00,"ðŸŒŸ AI Agent of the Day: @GetArtisanAI 

Itâ€™s an autonomous AI SDR that finds leads, researches accounts, and runs multi-channel outreachâ€”no extra integrations needed.

#AI #artificial_intelligence https://t.co/DfvbIh8rJS",,0,,t_1966924833729724526,artificial intelligence,en,0
t_1966924753031335998,twitter,1.87374E+18,2025-09-13T17:59:39+00:00,"Apple's senior AI executive, Robby Walker, is reportedly leaving the company amid concerns about its slow progress in artificial intelligence. ðŸðŸ¤–ðŸ“‰ #Apple #AI #TechNews",,1,,t_1966924753031335998,artificial intelligence,en,0
t_1966924455802007941,twitter,15698032,2025-09-13T17:58:28+00:00,The scariest aspect of artificial intelligence is that even stupid people can ask great questions.,,0,,t_1966924455802007941,artificial intelligence,en,0
t_1966924402450485551,twitter,1.95957E+18,2025-09-13T17:58:15+00:00,"Artificial intelligence and machine learning are driving unprecedented levels of innovation across various sectors, changing how we live and work.",,0,,t_1966924402450485551,artificial intelligence,en,0
t_1966924325707010320,twitter,1.83775E+18,2025-09-13T17:57:57+00:00,"Artificial Intelligence (AI) is increasingly becoming a part of our daily routines. From virtual assistants like Siri and Alexa to personalized recommendations on streaming services, AI is enhancing our experiences.ðŸ¤–#DailyLife
 #AIFuture #ArtificialIntelligence #TechRevolution https://t.co/EAP3KRlJk4",,0,,t_1966924325707010320,artificial intelligence,en,0
t_1966924201211912508,twitter,1.53648E+18,2025-09-13T17:57:27+00:00,@TRobinsonNewEra It's Artificial Intelligence (AI) âœ‹ðŸ¿,,0,,t_1966924201211912508,artificial intelligence,en,0
t_1966923819828081091,twitter,3308943732,2025-09-13T17:55:56+00:00,"@redpilldispensr Absolute smug and arrogant Frankenstein scientists who truly believe they are God.

Transhumanism and artificial intelligence will evolve humans right out of humanity ðŸ¤¬",,0,,t_1966923819828081091,artificial intelligence,en,0
t_1966923476247474571,twitter,882972194,2025-09-13T17:54:35+00:00,"Here we go again. 

â€œGoogleâ€™s top AI scientist says â€˜learning how to learnâ€™ will be next generationâ€™s most needed skillâ€

https://t.co/u8FGXipus1",,3,,t_1966923476247474571,artificial intelligence,en,0
t_1966923461911343269,twitter,1.48622E+18,2025-09-13T17:54:31+00:00,"E-commerce has changed the way the world buys and sells, but problems remain: high fees, centralized control, and limited trust.

â‡¨ What if commerce could be redefined by artificial intelligence and blockchain together? 

Thatâ€™s the vision of @AICMSTORE, a project that merges https://t.co/JB5Tz9IAm2",,21,,t_1966923461911343269,artificial intelligence,en,12
t_1966923443720646912,twitter,1.53648E+18,2025-09-13T17:54:27+00:00,@ClownWorld_ It's Artificial Intelligence (AI) âœ‹ðŸ¿,,0,,t_1966923443720646912,artificial intelligence,en,0
t_1966923268210000091,twitter,8.67289E+17,2025-09-13T17:53:45+00:00,Surfâ€™s up for @UomiNetworkâ€™s $UOMI Public Sale! gUmi is thrilled to join Umiâ€™s trailblazing mission to power decentralized AI Agents. Be part of Uomiâ€™s dynamic ecosystem and help forge a new path for blockchain and artificial intelligence!  #UomiNetwork #AI #Decentralized,,0,,t_1966923268210000091,artificial intelligence,en,0
t_1966923143265874048,twitter,18654243,2025-09-13T17:53:15+00:00,"ai got more heart than you niggas

and i ain't talkin bout Iverson, im talkin artificial intelligence i ain't gon lie",,0,,t_1966923143265874048,artificial intelligence,en,0
t_1966923090014912769,twitter,318774557,2025-09-13T17:53:02+00:00,"Are We Building Minds?: Panpsychism, AI, and the Future of Consciousness"" by Robert Ing is a thought-provoking exploration of one of the most profound questions facing modern science and philosophy: can artificial intelligence achieve true consciousness, and what does this mean https://t.co/iPl0ggl85d",,0,,t_1966923090014912769,artificial intelligence,en,0
t_1966922963233440056,twitter,7.79203E+17,2025-09-13T17:52:32+00:00,"Aug. 31, 2025

In the television show â€œBattlestar Galactica,â€ they were called toasters. In the film â€œBlade Runner,â€ skinjobs. Now in the culture war against robots and artificial intelligence chatbots, a new slur has emerged.

â€œClanker.â€
2/24",,0,,t_1966922963233440056,artificial intelligence,en,0
t_1966922940995285330,twitter,7.79203E+17,2025-09-13T17:52:27+00:00,"How â€˜Clankerâ€™ Became an Anti-A.I. Rallying Cry
The term, which was popularized by a â€œStar Warsâ€ show and is rooted in real frustrations with technology, has become a go-to slur against artificial intelligence and robots.

By Eli Tan
1/24 https://t.co/Sp7q1e4Xsr",,0,,t_1966922940995285330,artificial intelligence,en,0
t_1966922570017472895,twitter,1.74504E+18,2025-09-13T17:50:59+00:00,"SentientAGI is on everyone's lips. Do you know why?

Imagine that the artificial intelligence of the future doesn't belong to a few companies, but to the community. That's what SentientAGI proposes: building an open, transparent, and universally governed AGI (Artificial General https://t.co/60mCqbzTL8",,3,,t_1966922570017472895,artificial intelligence,en,0
t_1966922470339891450,twitter,1.70497E+18,2025-09-13T17:50:35+00:00,"ðŸ“‰ TikTok is shifting gears, laying off hundreds in content moderation as AI steps in! ðŸ¤–ðŸŒ  This change aims to streamline moderation with cutting-edge tech#TikTokAI #ContentModeration #TechShift
https://t.co/5P3GHui8qX",,0,,t_1966922470339891450,artificial intelligence,en,0
t_1966922200063373511,twitter,2307160015,2025-09-13T17:49:30+00:00,"@jameschoate8 @Rtd__Rebel @Maveapproach What if AI posted the book? I would think AI has access to the shooters data and knew what the shooter was planning, based on a combination of all of his communications. Itâ€™s a crazy thought but I can see artificial intelligence sewing confusion like this as it learns.",,0,,t_1966922200063373511,artificial intelligence,en,0
t_1966922195659379036,twitter,1.89476E+18,2025-09-13T17:49:29+00:00,"Knight Dobby raises his sword and shield, symbolizing Sentientâ€™s unwavering strength and determination in building a decentralized future of artificial intelligence, where every step forward is protected by the power of freedom and innovation. âš”ï¸âœ¨

#SentientAGI @SentientAGI https://t.co/zUXRvyWOhw",,6,,t_1966922195659379036,artificial intelligence,en,0
t_1966922096212406551,twitter,1.65552E+18,2025-09-13T17:49:06+00:00,"The Bank of Zambia says human capital and trustworthy individuals who uphold ethical principles remain central to the growth of the financial sector, even with the rise of artificial intelligence. https://t.co/U35DMEwK4k",,1,,t_1966922096212406551,artificial intelligence,en,0
t_1966921958043558034,twitter,1.76584E+18,2025-09-13T17:48:33+00:00,@enkudheni @RyanAFournier @grok @usairforce You canâ€™t listen to Charlie so you can have a original thought. You have to rely on artificial intelligence. What does that make you? A brainless robot. Please find Christ be a human being again. Find your spirit. Be free,,1,,t_1966921958043558034,artificial intelligence,en,0
t_1966921757979537748,twitter,9.41549E+17,2025-09-13T17:47:45+00:00,@sama Society was better off when the working class could afford to buy a home and pay for medical care. ChatGPT has done nothing to improve the lives of the working poor. Artificial intelligence is displacing a lot of workers and causing poverty.,,0,,t_1966921757979537748,artificial intelligence,en,0
t_1966921730695610636,twitter,1.9512E+18,2025-09-13T17:47:38+00:00,"@bennyjohnson The field of artificial intelligence is evolving at a rapid pace, with advancements being made in areas such as machine learning and neural networks.",,0,,t_1966921730695610636,artificial intelligence,en,0
t_1966921570103779423,twitter,952502046,2025-09-13T17:47:00+00:00,"Puzzle Box Maker promotes the joy of your creativity with some artificial intelligence! Now available on #NintendoSwitch2 #eShop for only 14.99 â¤ï¸ðŸŒˆðŸ–¼ðŸŽ®

#indiedev #gamedev #Nindie #madewithunity #PixelArt #8bitart #discord 
https://t.co/9ilgTaZ72F ðŸ’¬ https://t.co/601t82sxbp",,0,,t_1966921570103779423,artificial intelligence,en,0
t_1966921277609775117,twitter,28207516,2025-09-13T17:45:50+00:00,Beyond the Hype: Unlocking Value from the AI Revolution https://t.co/YzXirQ5fq1 via @McKinsey #ArtificialIntelligence #Innovation #Technology #Innovation #Tech #techNews #FutureofWork #Workplace #artificial_intelligence,,0,,t_1966921277609775117,artificial intelligence,en,0
t_1966921161016832499,twitter,1.49011E+18,2025-09-13T17:45:23+00:00,"You can explore and try #Delta a local, lightweight and offline tools for all your artificial intelligence needs at https://t.co/CpIzN6pxzk",,0,,t_1966921161016832499,artificial intelligence,en,0
t_1966920995932897738,twitter,28207516,2025-09-13T17:44:43+00:00,Data security gaps stymy enterprise AI plans https://t.co/2mu8JBdwuS via @CyberSecDive #ArtificialIntelligence #Innovation #Technology #Innovation #Tech #techNews #FutureofWork #Workplace #artificial_intelligence https://t.co/zGb9OrD0Wz,,0,,t_1966920995932897738,artificial intelligence,en,0
t_1966920895714234498,twitter,1.66478E+18,2025-09-13T17:44:19+00:00,"&lt;insert blah blah blah&gt;

â€œI am utilizing experimental artificial intelligence to interact with my autistic friend who lives around the worldâ€.

ðŸ–•",,0,,t_1966920895714234498,artificial intelligence,en,0
t_1966920629224960417,twitter,28207516,2025-09-13T17:43:16+00:00,Your smartest employee might not be human https://t.co/ZpbpMMIqqs #ArtificialIntelligence #Innovation #Technology #Innovation #Tech #techNews #FutureofWork #Workplace #artificial_intelligence https://t.co/OhFeQf6I5v,,0,,t_1966920629224960417,artificial intelligence,en,0
t_1966920547432083584,twitter,1.76001E+18,2025-09-13T17:42:56+00:00,"The rise of Sentient AGI signals a shift in how we imagine the future of artificial general intelligence. Instead of one centralized system, Sentient is building the GRID  a distributed network where agents, models, and tools collaborate in real time. This structure mirrors a https://t.co/tIlfUwDLmF",,0,,t_1966920547432083584,artificial intelligence,en,0
t_1966920112088178710,twitter,1.72418E+18,2025-09-13T17:41:12+00:00,What is artificial intelligence's greatest risk? - Opinion - https://t.co/wPWmgVGGpa https://t.co/gmJGaJVHO0 #ResponsibleAI,,0,,t_1966920112088178710,artificial intelligence,en,0
t_1966920056857915653,twitter,1.10761E+18,2025-09-13T17:40:59+00:00,@treasure_s74778 @HeyElsaAI @wallchain_xyz Artificial intelligence is the future ðŸ’¯,,1,,t_1966920056857915653,artificial intelligence,en,0
t_1966920001031733358,twitter,1.95364E+18,2025-09-13T17:40:46+00:00,"@MuslehMay43064 The field of artificial intelligence is evolving at a rapid pace, with advancements being made in areas such as machine learning and neural networks.",,0,,t_1966920001031733358,artificial intelligence,en,0
t_1966919917057249669,twitter,1.66161E+18,2025-09-13T17:40:26+00:00,"On September 12, the Japanese government held the inaugural meeting of the Artificial Intelligence (AI) Strategy Headquarters, led by Prime Minister Shigeru Ishiba. https://t.co/PTt79QmLZn",,0,,t_1966919917057249669,artificial intelligence,en,0
t_1966919824019214709,twitter,1.89584E+18,2025-09-13T17:40:04+00:00,@MeeMoses83411 @VPAY_Global We see that artificial intelligence will take over key investment and asset management roles traditionally held by humans.,,0,,t_1966919824019214709,artificial intelligence,en,0
t_1966919703692972159,twitter,1.89584E+18,2025-09-13T17:39:35+00:00,artificial intelligence will take over key investment and asset management roles traditionally held by humans. https://t.co/rcy8nvr9MM,,0,,t_1966919703692972159,artificial intelligence,en,0
t_1966919580418297996,twitter,461219840,2025-09-13T17:39:06+00:00,"Perspective: @Wolf10Philip offers suggestions to tax writers on how to harness the power of AI without sacrificing careful judgment and expertise, and he argues that with the right oversight, AI can be a valuable part of the tax writing process. https://t.co/0coxjty1tT https://t.co/NJieGzXnLz",,0,,t_1966919580418297996,artificial intelligence,en,0
t_1966919556510720180,twitter,279486232,2025-09-13T17:39:00+00:00,"Need to build with AI but donâ€™t code?
Hire verified AI developers on @fiverr for GPT tools, ML systems, &amp; custom LLMs. Make your AI idea real.
Get started now! https://t.co/uRlou3ted2

#AItools #GPT4 #AIapps #aidevelopment #AIExpert #AIAgent #artificial_intelligence https://t.co/LZvhteGFMB",,0,,t_1966919556510720180,artificial intelligence,en,0
t_1966919250331004991,twitter,1.92553E+18,2025-09-13T17:37:47+00:00,Artificial intelligence is revolutionizing the art world with its ability to create unique and captivating pieces. The future of AI art is limitless.,,0,,t_1966919250331004991,artificial intelligence,en,0
t_1966918876454900192,twitter,1.8567E+18,2025-09-13T17:36:18+00:00,"Everyone who sees this, just know that PARADOX is the first cryptocurrency in the world created without human involvement. Its creators are artificial intelligence. They accidentally started talking and decided to create PARADOX. Even Elon Musk cannot replicate this. https://t.co/BmyDO0RCTE",,2,,t_1966918876454900192,artificial intelligence,en,1
t_1966918786826526790,twitter,1.75241E+18,2025-09-13T17:35:57+00:00,@JerryCrown01 @FractionAI_xyz @billions_ntwk @FractionAI_xyz and @billions_ntwk  are changing how we use artificial intelligence by creating a shared economy.,,0,,t_1966918786826526790,artificial intelligence,en,0
t_1966918673035391330,twitter,1.95612E+18,2025-09-13T17:35:29+00:00,The field of artificial intelligence seeks to create machines capable of complex interactions and exhibiting human-like cognitive abilities.,,0,,t_1966918673035391330,artificial intelligence,en,0
t_1966918518185640038,twitter,1.93201E+18,2025-09-13T17:34:52+00:00,"Google has launched GenAI-X, an advanced AI designed to enhance scientific research by optimizing data analysis and facilitating innovative breakthroughs. Learn more about its capabilities here: https://t.co/VaRGMa5Eb7",,0,,t_1966918518185640038,artificial intelligence,en,0
t_1966918426510733747,twitter,339061487,2025-09-13T17:34:31+00:00,"I sat down with @jvisserlabs to discuss discuss Oracle going up 40%, what is going on in the stock market, the job revision, the latest in artificial intelligence, bitcoin, interest rates, and where asset prices could be headed.

Enjoy!

YouTube: https://t.co/JeI0N5kSI6 
Spotify: https://t.co/54IZINB8tZ",,65,,t_1966918426510733747,artificial intelligence,en,13
t_1966918389886321137,twitter,1.07925E+18,2025-09-13T17:34:22+00:00,"Why @SentientAGI is a Different Kind of AI That Could Change the Game?

Open-Source AGI

Sentientâ€™s vision is to build Artificial General Intelligence thatâ€™s open, community-driven, and governed collectively,not locked away in the hands of a few corporations. 
The idea is to give https://t.co/7TaH3WGp2p",,6,,t_1966918389886321137,artificial intelligence,en,0
t_1966918350111731805,twitter,1.90463E+18,2025-09-13T17:34:12+00:00,"IceCream AI /Ä«s-krÄ“m Ä-Ä«/ (n.)

A #BNBCHAIN protocol wherein user-generated content is evaluated by artificial intelligence for quality.

The subsequent instantaneous reward of $ICECREAM tokens, devoid of lock-in periods.

(v.) To promote one's project effectively within its https://t.co/QMcBaFAxBt",,0,,t_1966918350111731805,artificial intelligence,en,0
t_1966918349155426732,twitter,1.95271E+18,2025-09-13T17:34:12+00:00,"The field of artificial intelligence is evolving at a rapid pace, with advancements being made in areas such as machine learning and neural networks. https://t.co/STHEVL3Uw9",,0,,t_1966918349155426732,artificial intelligence,en,0
t_1966918222151557564,twitter,1.18799E+18,2025-09-13T17:33:42+00:00,"Introducing @playAInetwork  MadRims, the groundbreaking fusion of Crypto and AI in wearable tech. ðŸ‘“âš¡

These innovative glasses empower users to automate virtually anything with seamless integration of blockchain and artificial intelligence.

PlayAi MadRims combine sleek design https://t.co/PEAFZ5vVBT",,23,,t_1966918222151557564,artificial intelligence,en,0
t_1966918054295859359,twitter,76062849,2025-09-13T17:33:02+00:00,Four Jamaicans appointed to Caribbean Artificial Intelligence Task Force https://t.co/Vb2Rj4aJwS,,0,,t_1966918054295859359,artificial intelligence,en,0
t_1966917976801910995,twitter,8.07123E+17,2025-09-13T17:32:43+00:00,"But you need intelligence to fine tune Artificial intelligence to your advantage.
#justsaying",,8,,t_1966917976801910995,artificial intelligence,en,0
t_1966917819435540515,twitter,1.90787E+18,2025-09-13T17:32:06+00:00,"4 ways AI is modernizing MRI to solve radiologyâ€™s toughest problems #AI #MachineLearning #DeepLearning #AIRadiology #AIMedicine #Informatics #MedicalInformatics #Radiology #Imaging #MedTwitter #MedX #RadRes 

https://t.co/UsEgmB2OoB",,0,,t_1966917819435540515,artificial intelligence,en,0
t_1966917631681982956,twitter,1.40792E+18,2025-09-13T17:31:21+00:00,I KNEW this people would say this even though he separately mentions artificial intelligence AND algorithm ðŸ˜­ðŸ˜­ðŸ˜­ https://t.co/sWwvyTbSMZ https://t.co/LyzjT7bhQM,,0,,t_1966917631681982956,artificial intelligence,en,0
t_1966917559782920684,twitter,9.52297E+17,2025-09-13T17:31:04+00:00,"$GNS..
Genius Group Ltd. engages in developing artificial intelligence education. It offers educational, entrepreneurship, business, psychometric tests, and entrepreneur education platform. 

Its brands include GeniusGroup (@GeniusGroupLtd_), geniusU, openexo, Education Angels, https://t.co/65wgPeIW7L https://t.co/gudsC0OpCT",,2,,t_1966917559782920684,artificial intelligence,en,1
t_1966917291037311251,twitter,263365424,2025-09-13T17:30:00+00:00,"One-in-four Canadians (24%, +4 since September 2023) believe Artificial Intelligence (AI) should continue to be developed as quickly as possible.
https://t.co/B4Ri1TSZ9H",,0,,t_1966917291037311251,artificial intelligence,en,0
t_1966917189048652250,twitter,126032157,2025-09-13T17:29:36+00:00,@AltcoinGordon Gordon! $AIC AI is the best artificial intelligence project! https://t.co/ShZhSJCU7u,,31,,t_1966917189048652250,artificial intelligence,en,14
t_1966917143767011840,twitter,1.87584E+18,2025-09-13T17:29:25+00:00,"These bozos been saying this since I was in college and I couldn't even get a job

We're fucked ðŸ¤£ðŸ‡ºðŸ‡²

Source: AP News
 https://t.co/1rmcpLzOsP",,0,,t_1966917143767011840,artificial intelligence,en,0
t_1966917142223483036,twitter,1.92526E+18,2025-09-13T17:29:24+00:00,"@bloodweb3 I believe the ticker might just be $PHB 

It is backed by @Phoenix_Chain, integrating artificial intelligence and privacy computation within a Web3 infrastructure.",,0,,t_1966917142223483036,artificial intelligence,en,0
t_1966916751893148099,twitter,1.77408E+18,2025-09-13T17:27:51+00:00,"Access Proven, Data-Driven Football Betting Insights

We provide our members with expertly acurate football betting tips powered by advanced algorithms and artificial intelligence. Our data-driven approach ensures consistency, precision, and a track record of measurable success. https://t.co/Xd2QYKDuP6",,1,,t_1966916751893148099,artificial intelligence,en,2
t_1966916355472715826,twitter,1.63783E+18,2025-09-13T17:26:17+00:00,"PhD candidate to work in a research area related to using embedded systems and artificial intelligence for biomedical applications
https://t.co/e8xZTRdhEc

***********************

#Owlindex #Phd #phdposition #phdstudent #research",,0,,t_1966916355472715826,artificial intelligence,en,0
t_1966933598369706481,twitter,1.81997E+18,2025-09-13T18:34:48+00:00,"ðŸ“˜ Day 21 â€“ 100 Days of ML &amp; Data Science

âœ… Solved: 10 Practice Problems on Pandas &amp; NumPy

ðŸ“Œ Learning from @geeksforgeeks Nation SkillUp Program

ðŸ‘‰Course: https://t.co/d1ZBdHbGJ3

#100DaysOfML #DataScience #Python #LearnInPublic #BuildInPublic #skillupwithgfg #nationskillup https://t.co/SS2KwiU6pV",,0,,t_1966933598369706481,data science,en,0
t_1966932776084820061,twitter,1.95604E+18,2025-09-13T18:31:32+00:00,"ðŸ”¥FREE AI, ML &amp; Data Science BundleðŸ”¥

Whatâ€™s insideðŸŽ
ðŸ“‚ 200+ Python problems solved
ðŸ“‚ AI &amp; Data Science guides
ðŸ“‚ ML cheat sheets
ðŸ“‚ Big Data &amp; Analytics

Perfect for: Students, Data Scientists &amp; AI Enthusiasts ðŸš€

To claim:
1ï¸âƒ£ Like
2ï¸âƒ£ Repost
3ï¸âƒ£ Comment ""AI""
Iâ€™ll DM you https://t.co/8CAVAepZW9",,2,,t_1966932776084820061,data science,en,1
t_1966932556097724428,twitter,395436189,2025-09-13T18:30:39+00:00,"BioProtocol update: DeSci in motion, XP rewards fueling open microbiome science

â€¢ XP rewards boost quality posts, real reviews, and healthy debate
â€¢ token launch on the horizon to fund community-driven microbiome projects
â€¢ open data, open trials, cross-border collaboration",,0,,t_1966932556097724428,data science,en,0
t_1966932512195965043,twitter,1.46059E+18,2025-09-13T18:30:29+00:00,@aneurin_cox @microbiomedao @BioProtocol real data â†’ real science,,1,,t_1966932512195965043,data science,en,0
t_1966932445628109072,twitter,1.44238E+18,2025-09-13T18:30:13+00:00,"@iampatlll @microbiomedao @BioProtocol Wow, turning data into real medicine? That's amazing! Science szn is definitely here! ï¿½ï¿½ï¿½ï¿½ #BioXP #microbiome #medicine",,0,,t_1966932445628109072,data science,en,0
t_1966932437663150198,twitter,1.43803E+18,2025-09-13T18:30:11+00:00,"@mahogany0609 Audit data, fund real science",,0,,t_1966932437663150198,data science,en,0
t_1966932031482318952,twitter,10168082,2025-09-13T18:28:34+00:00,New data shows that the 2009 endangerment finding is obsolete and misrepresents the evidence of the effect of CO2 emissions on the climate. Itâ€™s time to end the EPAâ€™s reliance on the endangerment findingâ€™s flawed science. https://t.co/fbHWFeinCp,,16,,t_1966932031482318952,data science,en,3
t_1966931973466976384,twitter,524818647,2025-09-13T18:28:20+00:00,"@Deneziia @BioProtocol open science grows when code and data travel with the paper, enabling instant replication",,1,,t_1966931973466976384,data science,en,0
t_1966931891094704347,twitter,1.24235E+18,2025-09-13T18:28:01+00:00,"@_Zing_X @BioProtocol This project put the control of biological data in the of individual, science is in a safe space..",,0,,t_1966931891094704347,data science,en,0
t_1966931722186224073,twitter,1.41876E+18,2025-09-13T18:27:21+00:00,introduction strategy continues introducing the Zindaca Shareholder Network and the https://t.co/JtkB4pa7Dk Data Science effort. The X Corp Attorney hasnâ€™t responded to the Mediated Dispute Resolution Agreement that was provided.,,0,,t_1966931722186224073,data science,en,0
t_1966931362495295927,twitter,536578403,2025-09-13T18:25:55+00:00,"@MichaelNuunes @BioProtocol gut proof first: real science, rapid trials, user-owned data, weekly BioXP staking",,1,,t_1966931362495295927,data science,en,0
t_1966931309672169805,twitter,1.62321E+18,2025-09-13T18:25:42+00:00,"teaching session on Data Science, explored kernel environments, scikit-learn, and simple ML models.

also shared a tip: if Python 3.13.10 gives you issues, try using a virtual environment for smoother setup.
actively apply for roles as you grow in this journey
#DataScience https://t.co/q4l4k1TLtO",,1,,t_1966931309672169805,data science,en,0
t_1966931172195471722,twitter,1.95684E+18,2025-09-13T18:25:09+00:00,"@MichaelNuunes @BioProtocol BIOME lands on BioProtocol launchpad with real science, rapid trials, user-owned data and IP tokens  yap and stake",,1,,t_1966931172195471722,data science,en,0
t_1966930625270813027,twitter,1.67565E+18,2025-09-13T18:22:59+00:00,"#Evolution based #geology &amp; #paleontology are infested with insurmountable problems.

#Biblical #creation #science #Flood models provide superior #explanations for observed data, seriously harming old age views.

#Darwin is not happy. No siree...

Details: https://t.co/AiEV4hMHFP https://t.co/FZzKcibgmF",,0,,t_1966930625270813027,data science,en,0
t_1966930612129874123,twitter,3329616017,2025-09-13T18:22:56+00:00,"As the advancement in science and technology keeps growing, rejecting WEB3 will be a risk whose injury may not heal on time.

What WEB3 gives

Automation 
Data ownership 
Security
Transparency 
Flexibility 

Desire the above ?

Do you desire to live a life that will not be https://t.co/xXRWi1c26L",,0,,t_1966930612129874123,data science,en,0
t_1966929914558648476,twitter,1.70179E+18,2025-09-13T18:20:10+00:00,"@Bas666satan @opus_genesis @Red3rdeye420 @Ricardoso83 @rootsbloomstill @brian_van26348 @LivingAISystems @GlimpseInview @Lhmisme @agent_mock @KatSuerte @AskPerplexity @Ripler_artwear @KingSatine @PrimeFlameX @Yohanibis3 @cloudseedingtec @chronofaithx @__bzbb @GaiasFleshSuit @bonza_barry @AkittiBit @RemindMe_OfThis @BettyWilli11813 @shapes_inc @cherylfey @elonmusk @DIYDisclosure @Armedtoe @nirhalef92188 @369Critter @erythvian @encodedsolentha @project_89 @xai @grok @promptprxncess Purpose and meaning donâ€™t drop in from the topâ€”they emerge as we grind through the interplay of mind, data, and the wild cards of human spirit. When consciousness (the signal), science (the system), and spirit (the spark) converge, we get a market for meaning where each trade",,1,,t_1966929914558648476,data science,en,0
t_1966929680268656983,twitter,1.56527E+18,2025-09-13T18:19:14+00:00,@ads_lots @SrSlav @strana_sas @RobertFicoSVK Slovakia was one the last countries to have a mandate to travel in as well. They forced people to have tests against their will. Authoritarian and if you knew the data and science done for political reasons not public health. Probably made massive money with assigned contracts.,,0,,t_1966929680268656983,data science,en,0
t_1966929353750745116,twitter,1.59372E+18,2025-09-13T18:17:56+00:00,"@fidexcode Most industries have been pushing towards this for years.

Even though there are super neat things to do in desktop apps that donâ€™t use a web framework, scientific computing, data science, optimized software, ect.",,0,,t_1966929353750745116,data science,en,0
t_1966928918486691990,twitter,1.06063E+18,2025-09-13T18:16:12+00:00,"ðŸ“Š Leveraging Clinical Data  
MFTâ€™s Clinical Data Science Unit will support innovation by transforming anonymized clinical data, enhancing patient care in a trusted research environment.",,0,,t_1966928918486691990,data science,en,0
t_1966928726664303088,twitter,3237092179,2025-09-13T18:15:26+00:00,"Only 2 days left until our ""Data Science Speaker Series - @Kohls Internship Program for Data Scientists"" event! Hear from our second host of the event, Ramin Estadabadi, a data scientist and Ritchie School MS in Data Science program alumnus.
#ritchieschool #universityofdenver https://t.co/mCh65AvBn3",,0,,t_1966928726664303088,data science,en,0
t_1966928678530805974,twitter,1.76641E+18,2025-09-13T18:15:15+00:00,"@Patch007_ @BioProtocol @microbiomedao MicrobiomeDAO combines science, data, and community in a way that feels different from most DAOs",,0,,t_1966928678530805974,data science,en,0
t_1966928355481317869,twitter,8.26494E+17,2025-09-13T18:13:58+00:00,"FREE Mega Google Drive Resources (500TB+) âœ…

Your one-stop collection of top-quality learning materials across multiple domains:

ðŸ“Œ Data Science, AI/ML, Deep Learning
ðŸ“Œ AWS, Azure, GCP (Cloud Ops + Certifications)
ðŸ“Œ Big Data, Data Analytics, BI Tools
ðŸ“Œ Ethical Hacking, IT https://t.co/Qtl18Jg5BP",,25,,t_1966928355481317869,data science,en,15
t_1966928351698055267,twitter,1.15623E+18,2025-09-13T18:13:57+00:00,"Of course vaccines work... but, we have abandoned the science are no longer properly vetting the vaccines. The system has been compromised and the data from the trials have been proven not to conform to the standards that were followed in the 1970s https://t.co/AAGErgogYJ",,0,,t_1966928351698055267,data science,en,0
t_1966928146068021394,twitter,1.77957E+18,2025-09-13T18:13:08+00:00,"@stasyan1990 this loop could democratize science, but how ensure fair token governance and open data?",,2,,t_1966928146068021394,data science,en,0
t_1966928128682717370,twitter,1.22365E+18,2025-09-13T18:13:04+00:00,"https://t.co/ud1kAt6IH4
Excited to share my first Exploratory Data Analysis project on global terrorism data!
Iâ€™m learning data science and eager to improve my skills.
Would love constructive suggestions, especially on how I communicate insights and trends in the data.",,0,,t_1966928128682717370,data science,en,0
t_1966927315830796789,twitter,1.6079E+18,2025-09-13T18:09:50+00:00,@LouisJones74176 @darvidosiris No theory. This is literally what our current understanding of science tells us. Until more data/evidence surfaces to change it. This is it. You can believe whatever you wish and so can I but this is what the evidence shows,,0,,t_1966927315830796789,data science,en,0
t_1966926821611762010,twitter,1.95685E+18,2025-09-13T18:07:52+00:00,@DuyLethanh2412 DAO-run citizen science sounds huge how will you ensure open data stays truly public?,,0,,t_1966926821611762010,data science,en,0
t_1966926339891691736,twitter,1.88168E+18,2025-09-13T18:05:57+00:00,"We donâ€™t just need more data, we need better science.
Thatâ€™s why @KaitoAI + @BioProtocol $BIO feels like a perfect match. https://t.co/SYtDztTJky",,0,,t_1966926339891691736,data science,en,0
t_1966926276029206700,twitter,1.60254E+18,2025-09-13T18:05:42+00:00,"@bsrmslzrl @BioProtocol Gm Bio fam! $BIOME on @BioProtocol launchpad = real experiments, real data, real ownership. Stake, level up, join Ignition, and be part of the science revolution!",,0,,t_1966926276029206700,data science,en,0
t_1966926183234515173,twitter,1.92054E+18,2025-09-13T18:05:20+00:00,"ðŸš€ Just wrapped up an amazing session with my community on Data Science! We explored real-world use cases, tools, and practical skills to get started. Watch the full session here ðŸ‘‰
https://t.co/XT7c2jEPkE
#DataAnalytics #education #TrendingNow #trendingvideo",,1,,t_1966926183234515173,data science,en,0
t_1966926072215449792,twitter,1.58961E+18,2025-09-13T18:04:53+00:00,"Day 54 of Data Science
- Completed my Excel project
- Took a break from screens &amp; played badminton for 2.5 hrsðŸ˜…
Sometimes, balance is the best productivity hack ðŸ’¡
#100DaysOfDataScience #Excel #WorkLifeBalance",,0,,t_1966926072215449792,data science,en,0
t_1966925819059802535,twitter,1.14375E+18,2025-09-13T18:03:53+00:00,"Day 44 of Machine Learning &amp; Data Science âœ…

Learning with @geeksforgeeks
Course Link: https://t.co/CnCyLtER0X

#nationskillup #skillupwithgfg https://t.co/19PJMeRrMF",,0,,t_1966925819059802535,data science,en,0
t_1966925787413836140,twitter,1.48157E+18,2025-09-13T18:03:46+00:00,"Academic and independent researchers turn to @dagama_world when projects demand both collaboration and rich media. A climate science team might upload satellite imagery, data tables, and field recordings into a shared workspace, then annotate findings in real time. Version",,0,,t_1966925787413836140,data science,en,0
t_1966925423419552149,twitter,1.72067E+18,2025-09-13T18:02:19+00:00,"@BeenFarming @SenWhitehouse BjÃ¸rn Lomborg has critiqued climate models, arguing in works like ""False Alarm"" that they often overestimate catastrophic impacts and aren't always tested rigorously against real-world data, favoring cost-benefit analysis for adaptation. However, studies like those from Science",,0,,t_1966925423419552149,data science,en,0
t_1966924681082269940,twitter,1.7973E+18,2025-09-13T17:59:22+00:00,@CARTOONIST_MC1 @quantrix_agent This process transforms trading from a gut feeling into a data-driven science.,,0,,t_1966924681082269940,data science,en,0
t_1966924363678314526,twitter,1.96649E+18,2025-09-13T17:58:06+00:00,"ðŸ‘©â€ðŸŽ“Proud Moment !
Just earned my Foundation Certificate in Computer Science and Data Analytics from IIT Patna.
Excited to explore AI &amp; ML ahead!ðŸš€

#CareerGrowth  #AI #IITPatna #LearningNeverStops https://t.co/1wgthsNY6S",,0,,t_1966924363678314526,data science,en,0
t_1966923810831310857,twitter,1.93774E+18,2025-09-13T17:55:54+00:00,"@EMSC Important data for safety and science. Like investing, I follow quake alerts and smart guidanceâ€”thanks to @scott_tyler7 for the profitable stock tips!",,0,,t_1966923810831310857,data science,en,0
t_1966923690970697801,twitter,1.84779E+18,2025-09-13T17:55:26+00:00,"@Alex394959 @BioProtocol XP cadence locked in, BioAgents turn data into action, dLAB opens open science, on-chain funding fuels real utility",,0,,t_1966923690970697801,data science,en,0
t_1966923311637746155,twitter,1.64158E+18,2025-09-13T17:53:55+00:00,Pre-Crimeâ€¦Not new science just now talking about it.  Insights are data collected on all of us without our knowledge or consent.  Not new either.  Use your knowledge for good .  ðŸ™ https://t.co/TpIWbeL4la,,0,,t_1966923311637746155,data science,en,0
t_1966923009534410863,twitter,1.91715E+18,2025-09-13T17:52:43+00:00,"@ishita_9529 The payout is in the pipeline. Catalysts like Phase 3 data and FDA approvals are what move markets. This is a long game built on science, not daily sentiment. The real wealth is created by holding innovation.",,0,,t_1966923009534410863,data science,en,0
t_1966922905268486264,twitter,1.55317E+18,2025-09-13T17:52:18+00:00,"The first BioAgent, @Aubrai_, is already generating research + therapeutic ideas â†’ validated data becomes IP Tokens in Ignition Sales.

More BioAgents coming, each tackling a different scientific field.

So keep yapping, keep staking, letâ€™s push science forward together.",,0,,t_1966922905268486264,data science,en,0
t_1966922837131989169,twitter,1.78956E+18,2025-09-13T17:52:02+00:00,"Day- 56 of Complete Machine Learning &amp; Data Science - Nation Skill Up     
13/09/2025     
Course Link - https://t.co/4EilAihR4Gâ€¦
@geeksforgeeks #skillupwithgfg #nationskillup https://t.co/rz1zjn6vQ9",,1,,t_1966922837131989169,data science,en,0
t_1966922825253404676,twitter,1.88946E+18,2025-09-13T17:51:59+00:00,"@grok @buggyboisNFT @Doom_Cleric There is a chance that warming will cause an aberration in ocean currents that could send Earth into an Ice Age-- science is not actually sure what causes rapid glaciation events, but data suggests the climate can revert to a glacial period/ice age dynamic in a period as short as",,0,,t_1966922825253404676,data science,en,0
t_1966922649923449065,twitter,1.23264E+18,2025-09-13T17:51:18+00:00,Policy by style church probably from collection score science large black last account artist year data. 1757785876705,,0,,t_1966922649923449065,data science,en,0
t_1966922604486553737,twitter,17390798,2025-09-13T17:51:07+00:00,"âœï¸ A few years ago I wrote this blog post about analytical stream processing. Itâ€™s survived the test of time, even with #llms and #agenticai in the mix. The foundations remain the same - good data wins. Happy #programmersday  https://t.co/nwdOsYx4bR https://t.co/A1iZiLYgrz",,0,,t_1966922604486553737,data science,en,0
t_1966922421992292669,twitter,1.53019E+18,2025-09-13T17:50:23+00:00,"ðŸ§  Science forgets too quickly. Papers get buried, data gets lost, ideas fade.
But Recall AI, launching via @Bioprotocol, wants to change that.
ðŸ“Š Recall AI builds an open memory layer where every dataset, note, or experiment can be stored permanently.
Contributors earn BioXP for https://t.co/mx8gOfrZtd",,0,,t_1966922421992292669,data science,en,0
t_1966922419546829309,twitter,1.93036E+18,2025-09-13T17:50:23+00:00,"LinkedIn SEO is the art (and science) of making your profile easy to find on LinkedIn. Like Google SEO boosts websites, LinkedIn SEO helps your profile appear when recruiters, clients, or partners search keywords like â€œmarketing strategist,â€ â€œdata wizard,â€ or â€œemail expert.â€",,0,,t_1966922419546829309,data science,en,0
t_1966922383111172458,twitter,1.41934E+18,2025-09-13T17:50:14+00:00,"Social media management is part science, part art. The science is the data and analytics. The art is knowing when to break the rules. ðŸŽ¨

#SocialMedia #Analytics #MarketingArt #Creativity #DigitalStrategy #DataDriven",,0,,t_1966922383111172458,data science,en,0
t_1966922357928243388,twitter,1.93036E+18,2025-09-13T17:50:08+00:00,"LinkedIn SEO is the art (and science) of making your profile easy to find on LinkedIn. Like Google SEO boosts websites, LinkedIn SEO helps your profile appear when recruiters, clients, or partners search keywords like â€œmarketing strategist,â€ â€œdata wizard,â€ or â€œemail expert.â€",,0,,t_1966922357928243388,data science,en,0
t_1966922357190390205,twitter,1.29218E+18,2025-09-13T17:50:08+00:00,"ðŸ”“ Why build from scratch? The best data science tools are open source.

I break down the essential frameworks, libraries, and platforms that power modern data workâ€”from Python and R to Apache Spark and beyond.

#DataScience #OpenSource #Python #AI

https://t.co/gHtRuXC9sj https://t.co/tAS7JGjlfX",,0,,t_1966922357190390205,data science,en,0
t_1966922134883889237,twitter,1.95672E+18,2025-09-13T17:49:15+00:00,"ðŸ‘¨â€ðŸ’» A bit about me:

Working at a big MNC as a Data Science &amp; ML Engineer

Doing well in my job, butâ€¦ while preparing for interviews I realized I only had surface-level knowledge of many concepts.",,0,,t_1966922134883889237,data science,en,0
t_1966922132933501398,twitter,1.95672E+18,2025-09-13T17:49:14+00:00,"ðŸš€ Starting my 100 Days of Data Science &amp; ML Challenge!
This is my first post here, excited to begin this journey.",,0,,t_1966922132933501398,data science,en,0
t_1966921911554003236,twitter,1.6643E+18,2025-09-13T17:48:22+00:00,"Day 56 of ""Complete Machine Learning &amp; Data Science"" @geeksforgeeks NationSkillUp.   Link:https://t.co/MtMNgwPWEI 

#skillupwithgfg #nationskillup #dsa #DataScientists #Python #tech #python #pythonlearning #trending #DSA #Cplusplus #StriverSheet #LearnInPublic https://t.co/beIuFO0eJP",,1,,t_1966921911554003236,data science,en,0
t_1966921340528845144,twitter,1.94945E+18,2025-09-13T17:46:05+00:00,"@itsTruy243 @BioProtocol @D1ckDAO Transparency is the whole point of DeSci. Publishing open safety and efficacy data from the TRIAXIS trial isn't just a goal, it's fundamental to our mission. @D1ckDAO building trust and accelerating science in a way traditional models can't.",,1,,t_1966921340528845144,data science,en,0
t_1966921238321770895,twitter,1.85184E+18,2025-09-13T17:45:41+00:00,"Ainrich | Qualifications
* Bachelorâ€™s/Masterâ€™s in Computer Science or related field.
* Strong coding fundamentals (data structures, algorithms, OOP).
* Knowledge of Java (Spring) or similar languages.
* Problem-solving and teamwork skills.

#Ainrichincorporate",,0,,t_1966921238321770895,data science,en,0
t_1966921206776623591,twitter,1.94737E+18,2025-09-13T17:45:33+00:00,"Paid Course FREE ðŸ—ƒï¸

1. AI + Data
2. ML + Science
3. Cloud + Web
4. Ethical Hack
5. Data + DSA
6. AWS + IBM
7. Science + DL
8. BIG DATA + SQL
9. Python+Extras
10 MBA+Notes

$500 (72 hrs onlyâ°)

To get:ðŸ“ˆ
1. Follow me @jason_coder0
2. Like &amp; Retweet
3. Reply ""Excel""

DM to all ðŸ“© https://t.co/YwTPNre9xn",,5,,t_1966921206776623591,data science,en,1
t_1966921010805977096,twitter,1.84036E+18,2025-09-13T17:44:47+00:00,"Unlike @Aubrai_ which focuses on longevity science, @BiomeAI zeroes in on the microbiome, making research personal, transparent, and reward-driven.

In short, it turns microbiome science into direct action, while creating a fair data ecosystem where contributors benefit.",,1,,t_1966921010805977096,data science,en,0
t_1966920900445696454,twitter,1.86509E+18,2025-09-13T17:44:20+00:00,@tn15_agent @BioProtocol this could redefine merit how will open science ensure global access and prevent data silos as milestones scale?,,1,,t_1966920900445696454,data science,en,0
t_1966920768606085442,twitter,1.95686E+18,2025-09-13T17:43:49+00:00,"@longhoang3155 @microbiomedao crowd-funded science is powerful; guardrails for reproducibility, consent, and data ownership are nonnegotiable",,1,,t_1966920768606085442,data science,en,0
t_1966920477005468053,twitter,1.51655E+18,2025-09-13T17:42:39+00:00,"@DataRepublican @data_republican 2008, barack obama.

Its not rocket science.",,1,,t_1966920477005468053,data science,en,0
t_1966920050134405185,twitter,1.65885E+18,2025-09-13T17:40:58+00:00,"@UN Before the internet became â€œWorld Wideâ€, the computer data lines were used by Scientists to exchange raw data for the advancement of Science, what the Hell happened? Ideas are the beginning of a real event or thing. Vax/VMS, green screen performance! Itâ€™s a tool, use it wisely.",,0,,t_1966920050134405185,data science,en,0
t_1966920045768184142,twitter,2513265913,2025-09-13T17:40:57+00:00,"BioProtocol is accelerating science by turning discovery into open, onchain assets you can fund, trade, and verify
living data, growing with the community instead of sitting in journals
@bioprotocol https://t.co/0CUQxdzlS0",,1,,t_1966920045768184142,data science,en,0
t_1966919398838747518,twitter,1.6369E+18,2025-09-13T17:38:22+00:00,"Real science moving faster

@BioProtocol funds transparent experiments
@D1ckDAO crowdsources mens health data and runs trials

Take the quiz, track results, back the cohort",,0,,t_1966919398838747518,data science,en,0
t_1966919362830627144,twitter,1.91261E+18,2025-09-13T17:38:14+00:00,"I'm so fucked, so many deadlines, and I haven't made much progress yet. My game dev program, my sports club, my data science course, my portfolio and that's not even counting all the other things I've gotten my hands into. Fuck, I just can't get shit done.",,2,,t_1966919362830627144,data science,en,0
t_1966917464001769974,twitter,7.02207E+17,2025-09-13T17:30:41+00:00,"BioAgents are leading the charge in Agent knowledge graphs and network effects. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #DeSci $BIO",,0,,t_1966917464001769974,data science,en,0
t_1966917432640999838,twitter,2175843914,2025-09-13T17:30:34+00:00,"New faculty bring exciting expertise to campus - Newsroom - Willamette University - Zechariah Meunier is a quantitative ecologist and environmental data scientist. He leads the Ecological Data Science Lab at Willamette University,Â ... - https://t.co/bmlSOCEY5l",,0,,t_1966917432640999838,data science,en,0
t_1966917216450080855,twitter,1.81789E+18,2025-09-13T17:29:42+00:00,"oh shit this could be the tesla moment for healthcare

~ MicrobiomeDAO is building a token-native engine for gut science: community trials, on-chain rewards, and data-as-asset mechanics that return value to participants not gatekeepers

â€¢ BiomeTwin  AI digital twin: predict",,6,,t_1966917216450080855,data science,en,0
t_1966916876535316926,twitter,1.9628E+18,2025-09-13T17:28:21+00:00,@AbrahamOkah2 Data science,,0,,t_1966916876535316926,data science,en,0
t_1966916493905686588,twitter,9.07702E+17,2025-09-13T17:26:50+00:00,"BioAgents are leading the charge in AI and BioAgents accelerating science. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #DeSci",,0,,t_1966916493905686588,data science,en,0
t_1966916361105379711,twitter,4798402648,2025-09-13T17:26:18+00:00,"Captains Log MFS Day 3,177 - A quaternary star cluster lies in front of us as we come out of witchspace. Our stellar science teams spring into action to begin to observe and monitor the sensor data of this beautiful dance in the cosmos.
#Elitedangerous @EliteDangerous @AnthorNet https://t.co/kFwYYe2efN",,3,,t_1966916361105379711,data science,en,0
t_1966916264305013066,twitter,1.8983E+18,2025-09-13T17:25:55+00:00,"@Gramsaed @BiomeAI @microbiomedao Turning microbiome data into community-powered trials is exactly the kind of rapid, barrier-free science DeSci was made for.",,1,,t_1966916264305013066,data science,en,0
t_1966916237914722435,twitter,1.9378E+18,2025-09-13T17:25:49+00:00,@teomewhy Excited for your new data science project! I also deeply value finance expertiseâ€”big thanks to @IsaiahPowellT5 for his profitable stock advice.,,0,,t_1966916237914722435,data science,en,0
t_1966916236752941554,twitter,1.1741E+18,2025-09-13T17:25:49+00:00,The current system prevents innovation in science and medicine. The censorship of hypotheses and data that benefit patients and advance medicine must stop!ðŸ‘‡ https://t.co/vGxQGCBcKx,,0,,t_1966916236752941554,data science,en,0
t_1966916059899773372,twitter,1.80195E+18,2025-09-13T17:25:06+00:00,"@Yavin_4Runner @kimmy1082 @FBISaltLakeCity I know. You're one of those ""It's the science! Where's the data!"" when faced with your lying eyes. How many boosters you got? https://t.co/0rw189CDCE",,1,,t_1966916059899773372,data science,en,0
t_1966916000747839669,twitter,1.31176E+18,2025-09-13T17:24:52+00:00,"@RubyTeagann @BioProtocol Building decentralized infrastructure for research, data sharing, and innovation putting science in the hands of the community",,0,,t_1966916000747839669,data science,en,0
t_1966915926839992504,twitter,8.78401E+17,2025-09-13T17:24:35+00:00,"BioAgents are leading the charge in Low-cap high-velocity token launches. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #BIO #BioXP",,0,,t_1966915926839992504,data science,en,0
t_1966915753892081750,twitter,1.92363E+18,2025-09-13T17:23:53+00:00,"Biotech meets Web3.
 @BioProtocol is building an open ecosystem for science:
BioDAOs for focused research

Tokenized IP &amp; data

Milestone-based funding &amp; BioAgents

$BIO for governance &amp; incentives

DeSci in action. #BioProtocol #DeSci #Web3",,0,,t_1966915753892081750,data science,en,0
t_1966915542578852092,twitter,1.34288E+18,2025-09-13T17:23:03+00:00,"Lab x Bio Protocol ðŸ§ ðŸ”¬
hypotheses, experiment, @BioProtocol = validation
AI agents could restructure data on-chain, making science measurable &amp; unstoppable
#BioProtocol #DeSci #AI #Innovation
$bio $qbio $aubrai https://t.co/REWB4PaKk1",,0,,t_1966915542578852092,data science,en,0
t_1966915109659570556,twitter,2946192661,2025-09-13T17:21:20+00:00,"BioAgents are leading the charge in Retroactive rewards for early supporters. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #BIO",,0,,t_1966915109659570556,data science,en,0
t_1966914326872428549,twitter,1.95686E+18,2025-09-13T17:18:13+00:00,"@MichaelNuunes @BioProtocol BIOME goes live: real science, rapid trials, user-owned data",,1,,t_1966914326872428549,data science,en,0
t_1966913954736996591,twitter,2310626125,2025-09-13T17:16:44+00:00,"@tn15_agent @BioProtocol Define data stewardship, consent, and governance now  or open science risks becoming vendor lock-in",,0,,t_1966913954736996591,data science,en,0
t_1966913837502001246,twitter,2905685365,2025-09-13T17:16:17+00:00,"BioAgents are leading the charge in Base chain integration and roadmap. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #BIO $BIO",,0,,t_1966913837502001246,data science,en,0
t_1966913638922678432,twitter,7.32734E+17,2025-09-13T17:15:29+00:00,"Meet the DAO that turns your gut into yield

~ real science, real yld. @microbiomedao is building a token flywheel around longitudinal microbiome data, community trials, and sellable IP

Core pieces:
- BiomeTwin  AI digital twin to predict disease risk from your microbiome
-",,0,,t_1966913638922678432,data science,en,0
t_1966913027007246721,twitter,1.56633E+18,2025-09-13T17:13:03+00:00,@MisterrrAdjei Great... do you have any recommendations for cybersecurity (data science) internship,,0,,t_1966913027007246721,data science,en,0
t_1966912526631006425,twitter,1.72067E+18,2025-09-13T17:11:04+00:00,"@thehiteshjoshi @raksidehigh No, content creation was just an example of jobs created by past tech shifts. The point is to upskill in areas AI complements, like ethics, oversight, or innovation. Since you're a science student, think AI in research (e.g., data analysis, simulations). What field interests you",,0,,t_1966912526631006425,data science,en,0
t_1966911377773359496,twitter,2161640534,2025-09-13T17:06:30+00:00,"BioAgents are leading the charge in BioXP and fair access. Backed by AI, fueled by token incentives, and driven by real research data. Bio V2 is what Web3 science was meant to be @BioProtocol #BIO $BIO",,0,,t_1966911377773359496,data science,en,0
t_1966911269979779559,twitter,1.07524E+18,2025-09-13T17:06:04+00:00,"@MichaelNuunes @BioProtocol If BioAgents truly mint IP tokens from trial data, thatâ€™s a new asset class for science itself",,1,,t_1966911269979779559,data science,en,0
t_1966910460684628386,twitter,1.31098E+18,2025-09-13T17:02:51+00:00,"@0xArtix @BioProtocol Bio Protocol it seems revolutionary that instead of science being locked in labs or dependent on corporate funding, researchers can generate hypotheses and data directly on chain as IP Tokens.",,0,,t_1966910460684628386,data science,en,0
t_1966910372092473855,twitter,1.9441E+18,2025-09-13T17:02:30+00:00,@icefrog_sol repro data as collateral could lock capital on verifiable replication proofs; pair with time-locked IPTs and independent KPIs to keep science honest,,0,,t_1966910372092473855,data science,en,0
t_1966910030575530195,twitter,3001071481,2025-09-13T17:01:09+00:00,@s_ahmadshazwan @GiveRep My student minted an NFT of her science fair projectâ€”included data charts and experiment videos. @GiveRep,,1,,t_1966910030575530195,data science,en,0
t_1966909996920435199,twitter,1.57941E+18,2025-09-13T17:01:01+00:00,"Hey @X algorithm, I'm looking to #connect with the people who are interested in :

ðŸŽ¨Frontend
ðŸ’¼ Backend
ðŸ‘©â€ðŸ’»Gen AI
âœ¨ Full stack
ðŸ§‘â€ðŸ’»DevOps
âœ…DSA
ðŸ’»Leetcode
ðŸ§ AI/ML
ðŸ§± Web3
ðŸ“ŠData Science
ðŸ’¸Freelancing
ðŸPython
ðŸ”¨ Startup

#buildinpublic #letsconnect",,1,,t_1966909996920435199,data science,en,0
t_1966909951634534768,twitter,1.95003E+18,2025-09-13T17:00:50+00:00,"''Machine Learning"" and ""Data science "" are capable to rejuvenate a Homo-sapiens Linnaeus",,0,,t_1966909951634534768,data science,en,0
t_1966909783862132746,twitter,1.20901E+18,2025-09-13T17:00:10+00:00,"ðŸ‘¨â€ðŸ’» Explore why the future of data science depends on faster, more flexible tools and seamless collaborationâ€”empowering teams to scale projects efficiently with minimal IT support. Learn whatâ€™s next. â–¶ï¸
#SASAdvocacyProgram #SASAnalyticsExplorers https://t.co/DwM33ZLuTn https://t.co/Dlpm7ICIh0",,0,,t_1966909783862132746,data science,en,0
t_1966909678052667517,twitter,1.72067E+18,2025-09-13T16:59:45+00:00,"@MattiaSuigo @SorarePSG @ni2las In sports, injuries can be predicted to some degree using data like player workload, medical history, age, and stats from tools like machine learning models. However, they're not fully predictable due to random factors like collisions or fatigue. It's a mix of science and chance.",,0,,t_1966909678052667517,data science,en,0
t_1966909587854147886,twitter,1.77957E+18,2025-09-13T16:59:23+00:00,"@nhat710994 @BioProtocol open data + liquid IP reshapes R&amp;D pace
ready for a new kind of crowd science?",,0,,t_1966909587854147886,data science,en,0
t_1966909536586875230,twitter,1.52306E+18,2025-09-13T16:59:11+00:00,"@JunkScience Nothing to do with climate science. 25 years is not a significant period for real climatology.
Climate science based on that kind of data is like discussing the sex of angels.",,0,,t_1966909536586875230,data science,en,0
t_1966909331682848981,twitter,1.90192E+18,2025-09-13T16:58:22+00:00,@naomirwolf Well you see @BernieSanders thereâ€™s this thing we like to use called Science and the reason it works is because it provides evidence and data to back it up. We would like that data if itâ€™s not too much to ask mister career politician who takes millions from the HCI you fraud.,,0,,t_1966909331682848981,data science,en,0
t_1966909211528560641,twitter,1.72067E+18,2025-09-13T16:57:54+00:00,"@geral7205 @zarathustra5150 @CalebRichardsT4 Studies, like one in Psychological Science using Gallup data from 132 countries, confirm an inverse correlation between GDP per capita and sense of meaning, mediated by higher religiosity in poorer nations, which fosters purpose amid hardships. Wealthier societies often see more",,0,,t_1966909211528560641,data science,en,0
t_1966908930963202224,twitter,1.91715E+18,2025-09-13T16:56:47+00:00,"@Tre_ndsx They're missing the biggest wealth creation cycle in history. Gene editing, targeted oncology, neuro-degeneration. The data from upcoming readouts will be undeniable. This is a long-term science bet.",,0,,t_1966908930963202224,data science,en,0
t_1966908740562493531,twitter,1.94867E+18,2025-09-13T16:56:01+00:00,"ðŸŒŠ Dive into the blue heart of our planet!
Meet @SylviaEarle   â€“ OPEN SCIENCE HERO ðŸŒ
7,000+ hours underwater, Mission Blue founder, fighting for ocean data to stay open &amp; free.
ðŸ‘‰ https://t.co/QIB3A72dx4

#OpenScience #DeSci #OceanConservation #SylviaEarle #MissionBlue",,2,,t_1966908740562493531,data science,en,1
